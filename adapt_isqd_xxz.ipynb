{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates, project_operator_to_subspace\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [199]\n",
      "Gradients: [np.float64(-4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [199]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [199]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [199, 244]...\n",
      "Starting point: [np.float64(0.39269908170011053), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634968\n",
      "(change of -0.7978574148887709)\n",
      "Current ansatz: [199, 244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273119251\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.72528801244958)]\n",
      "Initial energy: -4.626284539634968\n",
      "Optimizing energy with indices [199, 244, 79]...\n",
      "Starting point: [np.float64(0.2651612350948415), np.float64(0.44546905162448813), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999417225\n",
      "(change of -1.3737154597822574)\n",
      "Current ansatz: [199, 244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958970181076\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999998057456)]\n",
      "Initial energy: -5.999999999417225\n",
      "Optimizing energy with indices [199, 244, 79, 228]...\n",
      "Starting point: [np.float64(9.855378907157579e-06), np.float64(0.7853981714335022), np.float64(0.7853981407341485), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.1231056250415055\n",
      "(change of -0.12310562562428018)\n",
      "Current ansatz: [199, 244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200501865\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.48507104824933)]\n",
      "Initial energy: -6.1231056250415055\n",
      "Optimizing energy with indices [199, 244, 79, 228, 210]...\n",
      "Starting point: [np.float64(9.693301544538512e-06), np.float64(0.7853983074095231), np.float64(0.7853977639810573), np.float64(0.12248927961503872), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154271411\n",
      "(change of -0.20417052922990564)\n",
      "Current ansatz: [199, 244, 79, 228, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 6.240962748730078\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 8.917526200501865 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429297)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020577)\n",
      "Current ansatz: [241, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531791\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267348776)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 79, 225, 210, 147]...\n",
      "Starting point: [np.float64(-0.7853981607743267), np.float64(0.7853981678304174), np.float64(-0.163570197408367), np.float64(0.16356963668287403), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615008459\n",
      "(change of -0.13682546018875374)\n",
      "Current ansatz: [241, 79, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00015114037946009526\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531791 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-1.9999999999522045)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 201]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625548017\n",
      "(change of -0.12310562561849903)\n",
      "Current ansatz: [228, 79, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917506431665617\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.4850600293890643)]\n",
      "Initial energy: -6.123105625548017\n",
      "Optimizing energy with indices [228, 79, 201, 228]...\n",
      "Starting point: [np.float64(-0.7853977521525508), np.float64(0.7853977933716614), np.float64(0.1224864400623345), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816648\n",
      "(change of -0.2041705292686311)\n",
      "Current ansatz: [228, 79, 201, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 6.2409625787176335\n",
      "Operator(s) added to ansatz: [141]\n",
      "Gradients: [np.float64(2.089492926747546)]\n",
      "Initial energy: -6.327276154816648\n",
      "Optimizing energy with indices [228, 79, 201, 228, 141]...\n",
      "Starting point: [np.float64(-0.7853977521525458), np.float64(0.7853988798879336), np.float64(0.16357019699115416), np.float64(0.16356963656316684), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615068996\n",
      "(change of -0.1368254602523482)\n",
      "Current ansatz: [228, 79, 201, 228, 141]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001373491823016229\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409625787176335 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428078)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481998\n",
      "(change of -0.20417052920233303)\n",
      "Current ansatz: [244, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964040371926\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916447538887)]\n",
      "Initial energy: -6.32727615481998\n",
      "Optimizing energy with indices [244, 26, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.785398156811469), np.float64(0.785398154431449), np.float64(0.16357028642350993), np.float64(0.16356997171157828), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.46410161505858\n",
      "(change of -0.1368254602386001)\n",
      "Current ansatz: [244, 26, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001435936303541698\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964040371926 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0000000000000044)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 210]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 74, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200183383\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071047842222)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [244, 74, 210, 225]...\n",
      "Starting point: [np.float64(0.785398163396527), np.float64(-0.7853981633984565), np.float64(0.12248927944983938), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819706\n",
      "(change of -0.2041705292020639)\n",
      "Current ansatz: [244, 74, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531856\n",
      "Operator(s) added to ansatz: [156]\n",
      "Gradients: [np.float64(-2.0894929267348785)]\n",
      "Initial energy: -6.327276154819706\n",
      "Optimizing energy with indices [244, 74, 210, 225, 156]...\n",
      "Starting point: [np.float64(0.7853981633982173), np.float64(-0.7853981633972483), np.float64(0.1635701974083824), np.float64(-0.16356963668287802), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.46410161513538\n",
      "(change of -0.13682546031567444)\n",
      "Current ansatz: [244, 74, 210, 225, 156]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887658826523747e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531856 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617646\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797047)]\n",
      "Initial energy: -6.123105625617646\n",
      "Optimizing energy with indices [241, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.785398163997711), np.float64(-0.7853981625399766), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206213)\n",
      "Current ansatz: [241, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531922\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267349482)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [241, 74, 225, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853981657335386), np.float64(-0.7853981641901568), np.float64(-0.16357019740840956), np.float64(0.16356963668286917), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615122749\n",
      "(change of -0.1368254603030401)\n",
      "Current ansatz: [241, 74, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 5.333555637387606e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531922 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 147]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819713\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [244, 74, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531879\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734878)]\n",
      "Initial energy: -6.327276154819713\n",
      "Optimizing energy with indices [244, 74, 228, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981767049507), np.float64(-0.7853981525341226), np.float64(0.16357019740838014), np.float64(-0.1635696366828784), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614750672\n",
      "(change of -0.13682545993095907)\n",
      "Current ansatz: [244, 74, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00031096863104971493\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531879 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.99999999999442)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610642\n",
      "(change of -0.12310562561245764)\n",
      "Current ansatz: [228, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955728\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485068789868941)]\n",
      "Initial energy: -6.123105625610642\n",
      "Optimizing energy with indices [228, 74, 225, 147]...\n",
      "Starting point: [np.float64(-0.7853983869831797), np.float64(-0.7853991695302367), np.float64(-0.12248869758310571), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816644\n",
      "(change of -0.2041705292060021)\n",
      "Current ansatz: [228, 74, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640564160105\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.0894916407260165)]\n",
      "Initial energy: -6.327276154816644\n",
      "Optimizing energy with indices [228, 74, 225, 147, 45]...\n",
      "Starting point: [np.float64(-0.785398437918453), np.float64(-0.7853991591701148), np.float64(-0.16357028929940723), np.float64(-0.16356997348722374), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614714424\n",
      "(change of -0.1368254598977794)\n",
      "Current ansatz: [228, 74, 225, 147, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00028938990802516095\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640564160105 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.4850710474292033)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 45]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819981\n",
      "(change of -0.2041705292023419)\n",
      "Current ansatz: [244, 79, 228, 45]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 6.240964044456069\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.089491641167056)]\n",
      "Initial energy: -6.327276154819981\n",
      "Optimizing energy with indices [244, 79, 228, 45, 108]...\n",
      "Starting point: [np.float64(0.7853981613321709), np.float64(0.7853981598792701), np.float64(0.16357028667248125), np.float64(0.1635699726489259), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615062932\n",
      "(change of -0.1368254602429504)\n",
      "Current ansatz: [244, 79, 228, 45, 108]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00014174702751517564\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044456069 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.000000000000006)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 210]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619964306\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071047541058)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 210, 225]...\n",
      "Starting point: [np.float64(0.7853981908695737), np.float64(0.7853981903042655), np.float64(0.12248927937223411), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819498\n",
      "(change of -0.2041705292018472)\n",
      "Current ansatz: [244, 79, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580347369\n",
      "Operator(s) added to ansatz: [54]\n",
      "Gradients: [np.float64(-2.0894929270476665)]\n",
      "Initial energy: -6.327276154819498\n",
      "Optimizing energy with indices [244, 79, 210, 225, 54]...\n",
      "Starting point: [np.float64(0.7853982318859356), np.float64(0.7853984111666573), np.float64(0.16357019742529944), np.float64(-0.1635696366118327), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614606746\n",
      "(change of -0.13682545978724825)\n",
      "Current ansatz: [244, 79, 210, 225, 54]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0003598322032505515\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580347369 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 228]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [244, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981639978813), np.float64(-0.7853981625399111), np.float64(0.12248927961411445), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233392)\n",
      "Current ansatz: [244, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964044667907\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916409814677)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981417276958), np.float64(-0.7853981819240415), np.float64(0.1635702866854681), np.float64(0.16356997269745452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071115\n",
      "(change of -0.1368254602511385)\n",
      "Current ansatz: [244, 31, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0001361285843518749\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044667907 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 225]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562494\n",
      "(change of -0.12310562563297633)\n",
      "Current ansatz: [228, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620177583\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071048468284)]\n",
      "Initial energy: -6.123105625562494\n",
      "Optimizing energy with indices [228, 79, 225, 198]...\n",
      "Starting point: [np.float64(-0.7853947065772614), np.float64(0.7853985308794381), np.float64(-0.12248927961669928), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154764998\n",
      "(change of -0.20417052920250356)\n",
      "Current ansatz: [228, 79, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069151\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089491643863277)]\n",
      "Initial energy: -6.327276154764998\n",
      "Optimizing energy with indices [228, 79, 225, 198, 210]...\n",
      "Starting point: [np.float64(-0.785394706577125), np.float64(0.7853982468601275), np.float64(-0.16357028648717148), np.float64(-0.16356997194354164), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150180395\n",
      "(change of -0.13682546025304188)\n",
      "Current ansatz: [228, 79, 225, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001641866869561707\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069151 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [228, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897114057\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.999999999994421)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [228, 31, 228]...\n",
      "Starting point: [np.float64(-0.7853985607314264), np.float64(-0.7853989420959467), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610651\n",
      "(change of -0.12310562561246297)\n",
      "Current ansatz: [228, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955701\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868923)]\n",
      "Initial energy: -6.123105625610651\n",
      "Optimizing energy with indices [228, 31, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853983869831787), np.float64(-0.7853991695302327), np.float64(0.12248869758309996), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481666\n",
      "(change of -0.2041705292060092)\n",
      "Current ansatz: [228, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964056413554\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916407332094)]\n",
      "Initial energy: -6.32727615481666\n",
      "Optimizing energy with indices [228, 31, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853984379181604), np.float64(-0.7853991591701102), np.float64(0.16357028929925468), np.float64(0.16356997348665894), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615069504\n",
      "(change of -0.1368254602528438)\n",
      "Current ansatz: [228, 31, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013629210398664508\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964056413554 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0000000000000053)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 201]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.12310562561763128)\n",
      "Current ansatz: [241, 31, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200768378\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.485071048168266)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 31, 201, 216]...\n",
      "Starting point: [np.float64(-0.7853981633974474), np.float64(-0.7853981633974486), np.float64(0.12248927953385899), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819703\n",
      "(change of -0.20417052920206125)\n",
      "Current ansatz: [241, 31, 201, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531911\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.089492926734877)]\n",
      "Initial energy: -6.327276154819703\n",
      "Optimizing energy with indices [241, 31, 201, 216, 228]...\n",
      "Starting point: [np.float64(-0.7853981633974365), np.float64(-0.7853981633975148), np.float64(0.16357019740839474), np.float64(-0.16356963668288166), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135381\n",
      "(change of -0.136825460315678)\n",
      "Current ansatz: [241, 31, 201, 216, 228]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.58876588131121e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531911 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [225, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999999944214)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [225, 79, 228]...\n",
      "Starting point: [np.float64(0.7853985607314237), np.float64(0.7853989420959421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610647\n",
      "(change of -0.12310562561245852)\n",
      "Current ansatz: [225, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955765\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850687898689596)]\n",
      "Initial energy: -6.123105625610647\n",
      "Optimizing energy with indices [225, 79, 228, 147]...\n",
      "Starting point: [np.float64(0.7853983869831845), np.float64(0.7853991695302365), np.float64(0.12248869758311001), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481636\n",
      "(change of -0.20417052920571344)\n",
      "Current ansatz: [225, 79, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962567144736\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.089492948558589)]\n",
      "Initial energy: -6.32727615481636\n",
      "Optimizing energy with indices [225, 79, 228, 147, 45]...\n",
      "Starting point: [np.float64(0.785398438850203), np.float64(0.7853991591701985), np.float64(0.16357019849079582), np.float64(-0.1635696317029856), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614774235\n",
      "(change of -0.13682545995787532)\n",
      "Current ansatz: [225, 79, 228, 147, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00025823475568075965\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962567144736 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [199]\n",
      "Gradients: [np.float64(-4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [199]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [199]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [199, 241]...\n",
      "Starting point: [np.float64(0.39269908170011053), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634968\n",
      "(change of -0.7978574148887709)\n",
      "Current ansatz: [199, 241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.267802731032193\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.725288012410362)]\n",
      "Initial energy: -4.626284539634968\n",
      "Optimizing energy with indices [199, 241, 79]...\n",
      "Starting point: [np.float64(0.26516123511422485), np.float64(-0.44546905146305404), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625614734\n",
      "(change of -1.4968210859797662)\n",
      "Current ansatz: [199, 241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91753072080658\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.4850735673672744)]\n",
      "Initial energy: -6.123105625614734\n",
      "Optimizing energy with indices [199, 241, 79, 228]...\n",
      "Starting point: [np.float64(0.12248992871667236), np.float64(-0.7853981486095067), np.float64(0.7853981593384867), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154820054\n",
      "(change of -0.20417052920531997)\n",
      "Current ansatz: [199, 241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963782409839\n",
      "Operator(s) added to ansatz: [39]\n",
      "Gradients: [np.float64(2.0894918716764543)]\n",
      "Initial energy: -6.327276154820054\n",
      "Optimizing energy with indices [199, 241, 79, 228, 39]...\n",
      "Starting point: [np.float64(0.1635702707681713), np.float64(-0.7853981601789695), np.float64(0.7853981254968405), np.float64(0.16356991243662913), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615066063\n",
      "(change of -0.13682546024600928)\n",
      "Current ansatz: [199, 241, 79, 228, 39]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013888448809123538\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963782409839 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.485071047428078)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 108]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206125)\n",
      "Current ansatz: [244, 26, 228, 108]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409625805316296\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.0894929267351734)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [244, 26, 228, 108, 45]...\n",
      "Starting point: [np.float64(0.7853981504601909), np.float64(0.7853981591821104), np.float64(0.1635701974083824), np.float64(-0.1635696366828072), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.464101614823195\n",
      "(change of -0.13682546000348683)\n",
      "Current ansatz: [244, 26, 228, 108, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00027118918560717044\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409625805316296 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0000000000000044)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 201]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [244, 79, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200768387\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.48507104816827)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 79, 201, 216]...\n",
      "Starting point: [np.float64(0.7853981633974513), np.float64(0.7853981633974504), np.float64(0.1224892795338593), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819716\n",
      "(change of -0.20417052920206835)\n",
      "Current ansatz: [244, 79, 201, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531923\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.089492926734882)]\n",
      "Initial energy: -6.327276154819716\n",
      "Optimizing energy with indices [244, 79, 201, 216, 228]...\n",
      "Starting point: [np.float64(0.7853981633974395), np.float64(0.7853981633974665), np.float64(0.16357019740839487), np.float64(-0.16356963668288169), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135393\n",
      "(change of -0.1368254603156771)\n",
      "Current ansatz: [244, 79, 201, 216, 228]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887658817337373e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531923 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113274\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 26, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257758), np.float64(0.7853981815917098), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617638\n",
      "(change of -0.12310562561762772)\n",
      "Current ansatz: [241, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441738\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474288698)]\n",
      "Initial energy: -6.123105625617638\n",
      "Optimizing energy with indices [241, 26, 228, 147]...\n",
      "Starting point: [np.float64(-0.7853981634447681), np.float64(0.7853981634524064), np.float64(0.12248927934332132), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819711\n",
      "(change of -0.2041705292020728)\n",
      "Current ansatz: [241, 26, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531794\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.08949292673488)]\n",
      "Initial energy: -6.327276154819711\n",
      "Optimizing energy with indices [241, 26, 228, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981638732243), np.float64(0.7853981637530733), np.float64(0.16357019740836667), np.float64(-0.16356963668287391), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135374\n",
      "(change of -0.1368254603156629)\n",
      "Current ansatz: [241, 26, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5931506821335947e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531794 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 228]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634969\n",
      "(change of -0.7978574148887718)\n",
      "Current ansatz: [211, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.267802731032194\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(3.7252880124103616)]\n",
      "Initial energy: -4.626284539634969\n",
      "Optimizing energy with indices [211, 228, 74]...\n",
      "Starting point: [np.float64(-0.2651612351142249), np.float64(-0.445469051463054), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999475316\n",
      "(change of -1.373715459840347)\n",
      "Current ansatz: [211, 228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958972842295\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999973729325)]\n",
      "Initial energy: -5.999999999475316\n",
      "Optimizing energy with indices [211, 228, 74, 228]...\n",
      "Starting point: [np.float64(-3.9073961684063536e-07), np.float64(-0.7854096986526888), np.float64(-0.7854095118369421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105622932155\n",
      "(change of -0.12310562345683973)\n",
      "Current ansatz: [211, 228, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917497199235557\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850548682741262)]\n",
      "Initial energy: -6.123105622932155\n",
      "Optimizing energy with indices [211, 228, 74, 228, 147]...\n",
      "Starting point: [np.float64(1.810145735478375e-05), np.float64(-0.7854142712241492), np.float64(-0.7854289466834095), np.float64(0.12248511291590515), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154790829\n",
      "(change of -0.20417053185867395)\n",
      "Current ansatz: [211, 228, 74, 228, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 6.240962204242426\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 8.917497199235557 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 120]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.2041705292023428)\n",
      "Current ansatz: [244, 79, 228, 120]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964043040389\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916424102608)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [244, 79, 228, 120, 147]...\n",
      "Starting point: [np.float64(0.7853981556136755), np.float64(0.7853981624051032), np.float64(0.16357028658616155), np.float64(0.16356997232403198), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615070806\n",
      "(change of -0.1368254602508241)\n",
      "Current ansatz: [244, 79, 228, 120, 147]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013640859207318388\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964043040389 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0000000000000036)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 216]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617644\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200767302\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.4850710481676668)]\n",
      "Initial energy: -6.123105625617644\n",
      "Optimizing energy with indices [241, 79, 216, 201]...\n",
      "Starting point: [np.float64(-0.7853981624814353), np.float64(0.7853981626766198), np.float64(-0.12248927953370452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819713\n",
      "(change of -0.20417052920206924)\n",
      "Current ansatz: [241, 79, 216, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531864\n",
      "Operator(s) added to ansatz: [30]\n",
      "Gradients: [np.float64(2.0894929267349687)]\n",
      "Initial energy: -6.327276154819713\n",
      "Optimizing energy with indices [241, 79, 216, 201, 30]...\n",
      "Starting point: [np.float64(-0.7853981639998062), np.float64(0.7853981647930687), np.float64(-0.16357019740839962), np.float64(0.16356963668286162), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134642\n",
      "(change of -0.13682546031492926)\n",
      "Current ansatz: [241, 79, 216, 201, 30]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.7885910013232333e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531864 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 108]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072751\n",
      "(change of -0.13682546025276388)\n",
      "Current ansatz: [244, 79, 228, 210, 108]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0001350560817700983\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297208)\n",
      "Current ansatz: [228, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000193667)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [228, 26, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773501), np.float64(0.785399377726245), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056255624825\n",
      "(change of -0.12310562563296656)\n",
      "Current ansatz: [228, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201775818\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.485071048441265)]\n",
      "Initial energy: -6.1231056255624825\n",
      "Optimizing energy with indices [228, 26, 228, 120]...\n",
      "Starting point: [np.float64(-0.7853947065772612), np.float64(0.7853985308794263), np.float64(0.12248927961669973), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154760147\n",
      "(change of -0.20417052919766476)\n",
      "Current ansatz: [228, 26, 228, 120]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042214428\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438376)]\n",
      "Initial energy: -6.327276154760147\n",
      "Optimizing energy with indices [228, 26, 228, 120, 198]...\n",
      "Starting point: [np.float64(-0.7853947065771324), np.float64(0.78539824686011), np.float64(0.16357028648724414), np.float64(0.16356997194704978), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615013028\n",
      "(change of -0.13682546025288111)\n",
      "Current ansatz: [228, 26, 228, 120, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00017023659875891768\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042214428 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428078)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.32727615481998\n",
      "(change of -0.20417052920233303)\n",
      "Current ansatz: [244, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964040371926\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916447538887)]\n",
      "Initial energy: -6.32727615481998\n",
      "Optimizing energy with indices [244, 26, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.785398156811469), np.float64(0.785398154431449), np.float64(0.16357028642350993), np.float64(0.16356997171157828), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615058578\n",
      "(change of -0.13682546023859743)\n",
      "Current ansatz: [244, 26, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001435936123970028\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964040371926 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.485071047429297)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 135]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819701\n",
      "(change of -0.20417052920205414)\n",
      "Current ansatz: [241, 79, 225, 135]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531787\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267348767)]\n",
      "Initial energy: -6.327276154819701\n",
      "Optimizing energy with indices [241, 79, 225, 135, 198]...\n",
      "Starting point: [np.float64(-0.7853981607742464), np.float64(0.7853981678304613), np.float64(-0.16357019740836676), np.float64(0.16356963668287383), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150084445\n",
      "(change of -0.1368254601887431)\n",
      "Current ansatz: [241, 79, 225, 135, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00015114255454222225\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531787 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998192\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [225, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140577\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999944222)]\n",
      "Initial energy: -5.999999999998192\n",
      "Optimizing energy with indices [225, 26, 225]...\n",
      "Starting point: [np.float64(0.7853985607314246), np.float64(0.7853989420959444), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610651\n",
      "(change of -0.12310562561245941)\n",
      "Current ansatz: [225, 26, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955753\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850687898689525)]\n",
      "Initial energy: -6.123105625610651\n",
      "Optimizing energy with indices [225, 26, 225, 210]...\n",
      "Starting point: [np.float64(0.7853983869831791), np.float64(0.7853991695302315), np.float64(-0.12248869758310756), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816342\n",
      "(change of -0.20417052920569123)\n",
      "Current ansatz: [225, 26, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962567145994\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089492948562477)]\n",
      "Initial energy: -6.327276154816342\n",
      "Optimizing energy with indices [225, 26, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.785398438857372), np.float64(0.785399159170187), np.float64(-0.16357019849086973), np.float64(0.16356963170328181), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132133\n",
      "(change of -0.1368254603157908)\n",
      "Current ansatz: [225, 26, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.185322846818539e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962567145994 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0000000000000036)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 216]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617644\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200767302\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071048167666)]\n",
      "Initial energy: -6.123105625617644\n",
      "Optimizing energy with indices [241, 79, 216, 225]...\n",
      "Starting point: [np.float64(-0.7853981624814353), np.float64(0.7853981626766198), np.float64(-0.12248927953370452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819948\n",
      "(change of -0.2041705292023046)\n",
      "Current ansatz: [241, 79, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964115398415\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894915788711836)]\n",
      "Initial energy: -6.327276154819948\n",
      "Optimizing energy with indices [241, 79, 216, 225, 201]...\n",
      "Starting point: [np.float64(-0.7853982205487021), np.float64(0.7853982043140048), np.float64(-0.16357029099862372), np.float64(-0.1635699889292778), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614697422\n",
      "(change of -0.13682545987747385)\n",
      "Current ansatz: [241, 79, 216, 225, 201]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0002805605397347146\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964115398415 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998192\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [225, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140577\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999944222)]\n",
      "Initial energy: -5.999999999998192\n",
      "Optimizing energy with indices [225, 26, 225]...\n",
      "Starting point: [np.float64(0.7853985607314246), np.float64(0.7853989420959444), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610651\n",
      "(change of -0.12310562561245941)\n",
      "Current ansatz: [225, 26, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955753\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850687898689525)]\n",
      "Initial energy: -6.123105625610651\n",
      "Optimizing energy with indices [225, 26, 225, 198]...\n",
      "Starting point: [np.float64(0.7853983869831791), np.float64(0.7853991695302315), np.float64(-0.12248869758310756), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816639\n",
      "(change of -0.20417052920598788)\n",
      "Current ansatz: [225, 26, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640564135405\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894916407332023)]\n",
      "Initial energy: -6.327276154816639\n",
      "Optimizing energy with indices [225, 26, 225, 198, 210]...\n",
      "Starting point: [np.float64(0.7853984379181594), np.float64(0.7853991591701045), np.float64(-0.16357028929925463), np.float64(-0.16356997348665922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615069485\n",
      "(change of -0.13682546025284648)\n",
      "Current ansatz: [225, 26, 225, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.000136292103894767\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640564135405 > 1e-05)\n",
      "Pool will be tiled from 24 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 16\n",
      "Got DMRG energy -2.76469e+01\n",
      "Tiled pool has 228 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -14.999999999999936\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -4.000000000000002\n",
      "Operator 1: 3.9999999999999853\n",
      "Operator 2: -3.9999999999999782\n",
      "Operator 3: 3.9999999999999756\n",
      "Operator 4: -3.9999999999999787\n",
      "Operator 5: 3.9999999999999822\n",
      "Operator 6: -4.000000000000005\n",
      "Operator 7: 3.999999999999995\n",
      "Operator 8: -3.9999999999999885\n",
      "Operator 9: 3.999999999999975\n",
      "Operator 10: -3.999999999999987\n",
      "Operator 11: 3.999999999999984\n",
      "Operator 12: -3.999999999999991\n",
      "Operator 13: -3.9999999999999853\n",
      "Operator 14: 3.9999999999999782\n",
      "Operator 15: -3.9999999999999756\n",
      "Operator 16: 3.9999999999999787\n",
      "Operator 17: -3.9999999999999822\n",
      "Operator 18: 4.000000000000005\n",
      "Operator 19: -3.999999999999995\n",
      "Operator 20: 3.9999999999999885\n",
      "Operator 21: -3.999999999999975\n",
      "Operator 22: 3.999999999999987\n",
      "Operator 23: -3.999999999999984\n",
      "Operator 24: 3.999999999999991\n",
      "Operator 25: -3.9999999999999916\n",
      "Operator 26: -3.9999999999999702\n",
      "Operator 27: 4.000000000000002\n",
      "Operator 28: 3.9999999999999853\n",
      "Operator 29: 3.9999999999999782\n",
      "Operator 30: 3.9999999999999756\n",
      "Operator 31: 3.9999999999999787\n",
      "Operator 32: 3.9999999999999822\n",
      "Operator 33: 4.000000000000005\n",
      "Operator 34: 3.999999999999995\n",
      "Operator 35: 3.9999999999999885\n",
      "Operator 36: 3.999999999999975\n",
      "Operator 37: 3.999999999999987\n",
      "Operator 38: 3.999999999999984\n",
      "Operator 39: 3.999999999999991\n",
      "Operator 40: 3.9999999999999916\n",
      "Operator 41: -4.000000000000002\n",
      "Operator 42: -3.9999999999999853\n",
      "Operator 43: -3.9999999999999782\n",
      "Operator 44: -3.9999999999999756\n",
      "Operator 45: -3.9999999999999787\n",
      "Operator 46: -3.9999999999999822\n",
      "Operator 47: -4.000000000000005\n",
      "Operator 48: -3.999999999999995\n",
      "Operator 49: -3.9999999999999885\n",
      "Operator 50: -3.999999999999975\n",
      "Operator 51: -3.999999999999987\n",
      "Operator 52: -3.999999999999984\n",
      "Operator 53: -3.999999999999991\n",
      "Operator 54: 3.9999999999999916\n",
      "Operator 55: -3.9999999999999853\n",
      "Operator 56: -3.9999999999999782\n",
      "Operator 57: -3.9999999999999756\n",
      "Operator 58: -3.9999999999999787\n",
      "Operator 59: -3.9999999999999822\n",
      "Operator 60: -4.000000000000005\n",
      "Operator 61: -3.999999999999995\n",
      "Operator 62: -3.9999999999999885\n",
      "Operator 63: -3.999999999999975\n",
      "Operator 64: -3.999999999999987\n",
      "Operator 65: -3.999999999999984\n",
      "Operator 66: -3.999999999999991\n",
      "Operator 67: -3.9999999999999916\n",
      "Operator 68: 4.000000000000002\n",
      "Operator 95: -3.9999999999999702\n",
      "Operator 96: 4.000000000000002\n",
      "Operator 97: -3.9999999999999853\n",
      "Operator 98: 3.9999999999999782\n",
      "Operator 99: -3.9999999999999756\n",
      "Operator 100: 3.9999999999999787\n",
      "Operator 101: -3.9999999999999822\n",
      "Operator 102: 4.000000000000005\n",
      "Operator 103: -3.999999999999995\n",
      "Operator 104: 3.9999999999999885\n",
      "Operator 105: -3.999999999999975\n",
      "Operator 106: 3.999999999999987\n",
      "Operator 107: -3.999999999999984\n",
      "Operator 108: 4.000000000000002\n",
      "Operator 109: 3.9999999999999853\n",
      "Operator 110: 3.9999999999999782\n",
      "Operator 111: 3.9999999999999756\n",
      "Operator 112: 3.9999999999999787\n",
      "Operator 113: 3.9999999999999822\n",
      "Operator 114: 4.000000000000005\n",
      "Operator 115: 3.999999999999995\n",
      "Operator 116: 3.9999999999999885\n",
      "Operator 117: 3.999999999999975\n",
      "Operator 118: 3.999999999999987\n",
      "Operator 119: 3.999999999999984\n",
      "Operator 120: 3.999999999999991\n",
      "Operator 121: 3.9999999999999702\n",
      "Operator 122: -4.000000000000002\n",
      "Operator 123: 3.9999999999999853\n",
      "Operator 124: -3.9999999999999782\n",
      "Operator 125: 3.9999999999999756\n",
      "Operator 126: -3.9999999999999787\n",
      "Operator 127: 3.9999999999999822\n",
      "Operator 128: -4.000000000000005\n",
      "Operator 129: 3.999999999999995\n",
      "Operator 130: -3.9999999999999885\n",
      "Operator 131: 3.999999999999975\n",
      "Operator 132: -3.999999999999987\n",
      "Operator 133: 3.999999999999984\n",
      "Operator 186: 3.9999999999999702\n",
      "Operator 214: -3.9999999999999702\n",
      "Operator 215: -4.000000000000002\n",
      "Operator 216: -3.9999999999999853\n",
      "Operator 217: -3.9999999999999782\n",
      "Operator 218: -3.9999999999999756\n",
      "Operator 219: -3.9999999999999787\n",
      "Operator 220: -3.9999999999999822\n",
      "Operator 221: -4.000000000000005\n",
      "Operator 222: -3.999999999999995\n",
      "Operator 223: -3.9999999999999885\n",
      "Operator 224: -3.999999999999975\n",
      "Operator 225: -3.999999999999987\n",
      "Operator 226: -3.999999999999984\n",
      "Operator 227: -4.000000000000002\n",
      "Total gradient norm: 44.3621460256375\n",
      "Operators under consideration (1):\n",
      "[226]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.999999999999984)]\n",
      "Operator(s) added to ansatz: [226]\n",
      "Gradients: [np.float64(-3.999999999999984)]\n",
      "Initial energy: -14.999999999999936\n",
      "Optimizing energy with indices [226]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -15.828427\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -15.82842712474612\n",
      "(change of -0.8284271247461845)\n",
      "Current ansatz: [226]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -4.000000000000003\n",
      "Operator 1: 3.999999999999988\n",
      "Operator 2: -3.9999999999999805\n",
      "Operator 3: 3.999999999999977\n",
      "Operator 4: -3.99999999999998\n",
      "Operator 5: 3.999999999999984\n",
      "Operator 6: -4.000000000000008\n",
      "Operator 7: 3.9999999999999956\n",
      "Operator 8: -3.9999999999999902\n",
      "Operator 9: 3.9999999999999765\n",
      "Operator 10: -3.4142135623691576\n",
      "Operator 12: -3.4142135623691594\n",
      "Operator 13: -3.999999999999988\n",
      "Operator 14: 3.9999999999999805\n",
      "Operator 15: -3.999999999999978\n",
      "Operator 16: 3.99999999999998\n",
      "Operator 17: -3.999999999999984\n",
      "Operator 18: 4.000000000000008\n",
      "Operator 19: -3.9999999999999956\n",
      "Operator 20: 3.9999999999999902\n",
      "Operator 21: -3.9999999999999765\n",
      "Operator 22: 3.4142135623691567\n",
      "Operator 24: 3.414213562369161\n",
      "Operator 25: -3.999999999999991\n",
      "Operator 26: -3.999999999999973\n",
      "Operator 27: 4.000000000000003\n",
      "Operator 28: 3.999999999999988\n",
      "Operator 29: 3.9999999999999805\n",
      "Operator 30: 3.999999999999978\n",
      "Operator 31: 3.99999999999998\n",
      "Operator 32: 3.999999999999984\n",
      "Operator 33: 4.000000000000008\n",
      "Operator 34: 3.9999999999999956\n",
      "Operator 35: 3.9999999999999902\n",
      "Operator 36: 3.9999999999999765\n",
      "Operator 37: 3.4142135623691567\n",
      "Operator 39: 3.4142135623691594\n",
      "Operator 40: 2.8284271247383312\n",
      "Operator 41: -4.000000000000003\n",
      "Operator 42: -3.999999999999988\n",
      "Operator 43: -3.9999999999999805\n",
      "Operator 44: -3.999999999999978\n",
      "Operator 45: -3.99999999999998\n",
      "Operator 46: -3.999999999999984\n",
      "Operator 47: -4.000000000000008\n",
      "Operator 48: -3.9999999999999956\n",
      "Operator 49: -3.9999999999999902\n",
      "Operator 50: -2.828427124738317\n",
      "Operator 51: -3.4142135623691576\n",
      "Operator 53: -3.414213562369161\n",
      "Operator 54: 3.999999999999991\n",
      "Operator 55: -3.999999999999988\n",
      "Operator 56: -3.9999999999999805\n",
      "Operator 57: -3.999999999999977\n",
      "Operator 58: -3.99999999999998\n",
      "Operator 59: -3.999999999999984\n",
      "Operator 60: -4.000000000000008\n",
      "Operator 61: -3.9999999999999956\n",
      "Operator 62: -3.9999999999999902\n",
      "Operator 63: -3.9999999999999765\n",
      "Operator 64: -3.4142135623691576\n",
      "Operator 66: -3.414213562369161\n",
      "Operator 67: -2.8284271247383312\n",
      "Operator 68: 4.000000000000003\n",
      "Operator 79: -1.4142135623770153\n",
      "Operator 80: 1.414213562377019\n",
      "Operator 92: 1.4142135623770171\n",
      "Operator 93: -1.4142135623770153\n",
      "Operator 95: -3.999999999999973\n",
      "Operator 96: 4.000000000000003\n",
      "Operator 97: -3.999999999999988\n",
      "Operator 98: 3.9999999999999805\n",
      "Operator 99: -3.999999999999977\n",
      "Operator 100: 3.99999999999998\n",
      "Operator 101: -3.999999999999984\n",
      "Operator 102: 4.000000000000008\n",
      "Operator 103: -3.9999999999999956\n",
      "Operator 104: 2.828427124738327\n",
      "Operator 105: -3.9999999999999765\n",
      "Operator 106: 3.4142135623691567\n",
      "Operator 108: 4.000000000000003\n",
      "Operator 109: 3.999999999999988\n",
      "Operator 110: 3.9999999999999805\n",
      "Operator 111: 3.999999999999977\n",
      "Operator 112: 3.99999999999998\n",
      "Operator 113: 3.999999999999984\n",
      "Operator 114: 4.000000000000008\n",
      "Operator 115: 3.9999999999999956\n",
      "Operator 116: 3.9999999999999902\n",
      "Operator 117: 2.828427124738317\n",
      "Operator 118: 3.4142135623691567\n",
      "Operator 120: 3.4142135623691594\n",
      "Operator 121: 3.999999999999973\n",
      "Operator 122: -4.000000000000003\n",
      "Operator 123: 3.999999999999988\n",
      "Operator 124: -3.9999999999999805\n",
      "Operator 125: 3.999999999999978\n",
      "Operator 126: -3.99999999999998\n",
      "Operator 127: 3.999999999999984\n",
      "Operator 128: -4.000000000000008\n",
      "Operator 129: 3.9999999999999956\n",
      "Operator 130: -2.828427124738327\n",
      "Operator 131: 3.9999999999999765\n",
      "Operator 132: -3.4142135623691576\n",
      "Operator 144: -1.4142135623770153\n",
      "Operator 145: 1.414213562377019\n",
      "Operator 184: 1.4142135623770171\n",
      "Operator 185: -1.4142135623770153\n",
      "Operator 186: 3.999999999999973\n",
      "Operator 197: 1.4142135623770171\n",
      "Operator 198: -1.4142135623770153\n",
      "Operator 214: -3.999999999999973\n",
      "Operator 215: -4.000000000000003\n",
      "Operator 216: -3.999999999999988\n",
      "Operator 217: -3.9999999999999805\n",
      "Operator 218: -3.999999999999977\n",
      "Operator 219: -3.99999999999998\n",
      "Operator 220: -3.999999999999984\n",
      "Operator 221: -4.000000000000008\n",
      "Operator 222: -3.9999999999999956\n",
      "Operator 223: -2.828427124738327\n",
      "Operator 224: -2.828427124738317\n",
      "Operator 225: -3.4142135623691576\n",
      "Operator 227: -4.000000000000003\n",
      "Total gradient norm: 41.4107813708185\n",
      "Operators under consideration (1):\n",
      "[222]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999956)]\n",
      "Operator(s) added to ansatz: [222]\n",
      "Gradients: [np.float64(-3.9999999999999956)]\n",
      "Initial energy: -15.82842712474612\n",
      "Optimizing energy with indices [226, 222]...\n",
      "Starting point: [np.float64(0.39269908170011203), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -16.656854\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -16.656854249492312\n",
      "(change of -0.8284271247461916)\n",
      "Current ansatz: [226, 222]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -4.0000000000000036\n",
      "Operator 1: 3.9999999999999893\n",
      "Operator 2: -3.9999999999999805\n",
      "Operator 3: 3.9999999999999782\n",
      "Operator 4: -3.9999999999999796\n",
      "Operator 5: 3.9999999999999845\n",
      "Operator 6: -3.414213562369179\n",
      "Operator 8: -3.4142135623691616\n",
      "Operator 9: 3.999999999999975\n",
      "Operator 10: -3.414213561913853\n",
      "Operator 12: -3.414213561913852\n",
      "Operator 13: -3.9999999999999893\n",
      "Operator 14: 3.9999999999999805\n",
      "Operator 15: -3.9999999999999787\n",
      "Operator 16: 3.9999999999999796\n",
      "Operator 17: -3.9999999999999845\n",
      "Operator 18: 3.4142135623691767\n",
      "Operator 20: 3.414213562369162\n",
      "Operator 21: -3.999999999999975\n",
      "Operator 22: 3.4142135619138516\n",
      "Operator 24: 3.414213561913854\n",
      "Operator 25: -3.9999999999999845\n",
      "Operator 26: -3.999999999999972\n",
      "Operator 27: 4.0000000000000036\n",
      "Operator 28: 3.9999999999999893\n",
      "Operator 29: 3.9999999999999805\n",
      "Operator 30: 3.9999999999999787\n",
      "Operator 31: 3.9999999999999796\n",
      "Operator 32: 3.9999999999999845\n",
      "Operator 33: 3.4142135623691767\n",
      "Operator 35: 3.4142135623691616\n",
      "Operator 36: 2.8284271247383264\n",
      "Operator 37: 3.4142135619138516\n",
      "Operator 39: 3.414213561913852\n",
      "Operator 40: 2.8284271238277245\n",
      "Operator 41: -4.0000000000000036\n",
      "Operator 42: -3.9999999999999893\n",
      "Operator 43: -3.9999999999999805\n",
      "Operator 44: -3.9999999999999787\n",
      "Operator 45: -3.9999999999999796\n",
      "Operator 46: -2.8284271247383317\n",
      "Operator 47: -3.414213562369179\n",
      "Operator 49: -3.414213562369162\n",
      "Operator 50: -2.828427123827714\n",
      "Operator 51: -3.4142135619138534\n",
      "Operator 53: -3.414213561913854\n",
      "Operator 54: 3.9999999999999845\n",
      "Operator 55: -3.9999999999999893\n",
      "Operator 56: -3.9999999999999805\n",
      "Operator 57: -3.9999999999999782\n",
      "Operator 58: -3.9999999999999796\n",
      "Operator 59: -3.9999999999999845\n",
      "Operator 60: -3.414213562369179\n",
      "Operator 62: -3.414213562369162\n",
      "Operator 63: -2.8284271247383264\n",
      "Operator 64: -3.414213561913853\n",
      "Operator 66: -3.414213561913854\n",
      "Operator 67: -2.8284271238277245\n",
      "Operator 68: 4.0000000000000036\n",
      "Operator 75: -1.4142135623770158\n",
      "Operator 76: 1.4142135623770122\n",
      "Operator 79: -1.4142135628323158\n",
      "Operator 80: 1.414213562832319\n",
      "Operator 88: 1.414213562377021\n",
      "Operator 89: -1.4142135623770116\n",
      "Operator 92: 1.4142135628323178\n",
      "Operator 93: -1.414213562832315\n",
      "Operator 95: -3.999999999999972\n",
      "Operator 96: 4.0000000000000036\n",
      "Operator 97: -3.9999999999999893\n",
      "Operator 98: 3.9999999999999805\n",
      "Operator 99: -3.9999999999999782\n",
      "Operator 100: 2.8284271247383286\n",
      "Operator 101: -3.9999999999999845\n",
      "Operator 102: 3.4142135623691767\n",
      "Operator 104: 2.41421356158636\n",
      "Operator 105: -3.999999999999975\n",
      "Operator 106: 3.4142135619138516\n",
      "Operator 108: 4.0000000000000036\n",
      "Operator 109: 3.9999999999999893\n",
      "Operator 110: 3.9999999999999805\n",
      "Operator 111: 3.9999999999999782\n",
      "Operator 112: 3.9999999999999796\n",
      "Operator 113: 2.8284271247383317\n",
      "Operator 114: 3.4142135623691767\n",
      "Operator 116: 3.4142135623691616\n",
      "Operator 117: 2.828427123827714\n",
      "Operator 118: 3.4142135619138516\n",
      "Operator 120: 3.414213561913852\n",
      "Operator 121: 3.999999999999972\n",
      "Operator 122: -4.0000000000000036\n",
      "Operator 123: 3.9999999999999893\n",
      "Operator 124: -3.9999999999999805\n",
      "Operator 125: 3.9999999999999787\n",
      "Operator 126: -2.8284271247383286\n",
      "Operator 127: 3.9999999999999845\n",
      "Operator 128: -3.414213562369179\n",
      "Operator 130: -2.4142135615863607\n",
      "Operator 131: 3.999999999999975\n",
      "Operator 132: -3.4142135619138534\n",
      "Operator 140: -1.4142135623770158\n",
      "Operator 141: 1.4142135623770122\n",
      "Operator 144: -1.414213562832316\n",
      "Operator 145: 1.414213562832319\n",
      "Operator 180: 1.414213562377021\n",
      "Operator 181: -1.4142135623770116\n",
      "Operator 184: 1.4142135628323178\n",
      "Operator 185: -1.414213562832315\n",
      "Operator 186: 3.999999999999972\n",
      "Operator 193: 1.414213562377021\n",
      "Operator 194: -1.4142135623770116\n",
      "Operator 197: 1.4142135628323178\n",
      "Operator 198: -1.414213562832315\n",
      "Operator 214: -3.999999999999972\n",
      "Operator 215: -4.0000000000000036\n",
      "Operator 216: -3.9999999999999893\n",
      "Operator 217: -3.9999999999999805\n",
      "Operator 218: -3.9999999999999782\n",
      "Operator 219: -2.8284271247383286\n",
      "Operator 220: -2.8284271247383317\n",
      "Operator 221: -3.414213562369179\n",
      "Operator 223: -2.41421356158636\n",
      "Operator 224: -2.828427123827714\n",
      "Operator 225: -3.414213561913853\n",
      "Operator 227: -4.0000000000000036\n",
      "Total gradient norm: 38.14696460777622\n",
      "Operators under consideration (1):\n",
      "[216]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999893)]\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(-3.9999999999999893)]\n",
      "Initial energy: -16.656854249492312\n",
      "Optimizing energy with indices [226, 222, 216]...\n",
      "Starting point: [np.float64(0.3926990818610856), np.float64(0.3926990817001109), np.float64(0.0)]\n",
      "         Current function value: -17.485281\n",
      "         Iterations: 5\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 73\n",
      "\n",
      "Current energy: -17.485281374238554\n",
      "(change of -0.8284271247462414)\n",
      "Current ansatz: [226, 222, 216]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135605665427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator 2: -3.4142135605665254\n",
      "Operator 3: 3.9999999999999916\n",
      "Operator 4: -3.999999999999991\n",
      "Operator 5: 3.999999999999994\n",
      "Operator 6: -3.414213562294366\n",
      "Operator 8: -3.41421356229435\n",
      "Operator 9: 3.999999999999986\n",
      "Operator 10: -3.4142135529705055\n",
      "Operator 11: -3.761030420434963e-08\n",
      "Operator 12: -3.4142135529705073\n",
      "Operator 14: 3.4142135605665214\n",
      "Operator 15: -3.9999999999999916\n",
      "Operator 16: 3.999999999999991\n",
      "Operator 17: -3.999999999999994\n",
      "Operator 18: 3.414213562294364\n",
      "Operator 20: 3.4142135622943512\n",
      "Operator 21: -3.999999999999986\n",
      "Operator 22: 3.4142135529705047\n",
      "Operator 23: 3.761030242799279e-08\n",
      "Operator 24: 3.4142135529705104\n",
      "Operator 25: -3.999999999999996\n",
      "Operator 26: -3.9999999999999867\n",
      "Operator 27: 3.414213560566546\n",
      "Operator 29: 3.4142135605665254\n",
      "Operator 30: 2.8284271211330525\n",
      "Operator 31: 3.999999999999991\n",
      "Operator 32: 3.999999999999994\n",
      "Operator 33: 3.414213562294364\n",
      "Operator 35: 3.41421356229435\n",
      "Operator 36: 2.828427124588692\n",
      "Operator 37: 3.4142135529705047\n",
      "Operator 38: -3.761030242799279e-08\n",
      "Operator 39: 3.4142135529705073\n",
      "Operator 40: 2.8284271059410213\n",
      "Operator 41: -3.4142135605665427\n",
      "Operator 43: -3.4142135605665214\n",
      "Operator 44: -3.9999999999999916\n",
      "Operator 45: -3.999999999999991\n",
      "Operator 46: -2.828427124588698\n",
      "Operator 47: -3.414213562294366\n",
      "Operator 49: -3.4142135622943512\n",
      "Operator 50: -2.8284271059410107\n",
      "Operator 51: -3.4142135529705055\n",
      "Operator 52: 3.761030242799279e-08\n",
      "Operator 53: -3.4142135529705104\n",
      "Operator 54: 3.999999999999996\n",
      "Operator 56: -3.4142135605665214\n",
      "Operator 57: -2.8284271211330525\n",
      "Operator 58: -3.999999999999991\n",
      "Operator 59: -3.999999999999994\n",
      "Operator 60: -3.414213562294366\n",
      "Operator 62: -3.4142135622943512\n",
      "Operator 63: -2.828427124588692\n",
      "Operator 64: -3.4142135529705055\n",
      "Operator 65: 3.761030420434963e-08\n",
      "Operator 66: -3.4142135529705104\n",
      "Operator 67: -2.8284271059410213\n",
      "Operator 68: 3.414213560566546\n",
      "Operator 69: -1.4142135641796676\n",
      "Operator 70: 1.4142135641796498\n",
      "Operator 75: -1.4142135624518393\n",
      "Operator 76: 1.414213562451836\n",
      "Operator 79: -1.4142135717756736\n",
      "Operator 80: 1.4142135717756776\n",
      "Operator 82: 1.4142135641796585\n",
      "Operator 83: -1.4142135641796583\n",
      "Operator 88: 1.4142135624518446\n",
      "Operator 89: -1.414213562451835\n",
      "Operator 92: 1.4142135717756752\n",
      "Operator 93: -1.414213571775674\n",
      "Operator 95: -3.9999999999999867\n",
      "Operator 96: 3.4142135605665453\n",
      "Operator 98: 3.4142135605665254\n",
      "Operator 99: -3.9999999999999916\n",
      "Operator 100: 2.8284271245886967\n",
      "Operator 101: -3.999999999999994\n",
      "Operator 102: 3.414213562294364\n",
      "Operator 104: 2.4142135462661978\n",
      "Operator 105: -3.999999999999986\n",
      "Operator 106: 3.414213552970504\n",
      "Operator 107: 3.761030420434963e-08\n",
      "Operator 108: 3.4142135605665453\n",
      "Operator 110: 3.4142135605665254\n",
      "Operator 111: 3.9999999999999916\n",
      "Operator 112: 3.999999999999991\n",
      "Operator 113: 2.828427124588698\n",
      "Operator 114: 3.414213562294364\n",
      "Operator 116: 3.41421356229435\n",
      "Operator 117: 2.8284271059410107\n",
      "Operator 118: 3.414213552970504\n",
      "Operator 119: -3.761030420434963e-08\n",
      "Operator 120: 3.4142135529705073\n",
      "Operator 121: 3.9999999999999867\n",
      "Operator 122: -3.4142135605665427\n",
      "Operator 124: -3.4142135605665214\n",
      "Operator 125: 3.9999999999999916\n",
      "Operator 126: -2.8284271245886967\n",
      "Operator 127: 3.999999999999994\n",
      "Operator 128: -3.414213562294366\n",
      "Operator 130: -2.414213546266199\n",
      "Operator 131: 3.999999999999986\n",
      "Operator 132: -3.4142135529705055\n",
      "Operator 133: -3.761030242799279e-08\n",
      "Operator 134: -1.4142135641796676\n",
      "Operator 135: 1.41421356417965\n",
      "Operator 140: -1.4142135624518393\n",
      "Operator 141: 1.414213562451836\n",
      "Operator 144: -1.414213571775674\n",
      "Operator 145: 1.4142135717756776\n",
      "Operator 174: 1.4142135641796585\n",
      "Operator 175: -1.4142135641796583\n",
      "Operator 180: 1.4142135624518446\n",
      "Operator 181: -1.414213562451835\n",
      "Operator 184: 1.4142135717756752\n",
      "Operator 185: -1.414213571775674\n",
      "Operator 186: 3.9999999999999867\n",
      "Operator 187: 1.4142135641796583\n",
      "Operator 188: -1.414213564179658\n",
      "Operator 193: 1.4142135624518446\n",
      "Operator 194: -1.414213562451835\n",
      "Operator 197: 1.4142135717756752\n",
      "Operator 198: -1.4142135717756739\n",
      "Operator 214: -2.8284271211330503\n",
      "Operator 215: -3.4142135605665427\n",
      "Operator 217: -3.4142135605665254\n",
      "Operator 218: -3.9999999999999916\n",
      "Operator 219: -2.8284271245886967\n",
      "Operator 220: -2.828427124588698\n",
      "Operator 221: -3.414213562294366\n",
      "Operator 223: -2.4142135462661978\n",
      "Operator 224: -2.8284271059410107\n",
      "Operator 225: -3.4142135529705055\n",
      "Operator 226: 3.761030420434963e-08\n",
      "Operator 227: -3.4142135605665427\n",
      "Total gradient norm: 35.05730000502474\n",
      "Operators under consideration (1):\n",
      "[218]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999916)]\n",
      "Operator(s) added to ansatz: [218]\n",
      "Gradients: [np.float64(-3.9999999999999916)]\n",
      "Initial energy: -17.485281374238554\n",
      "Optimizing energy with indices [226, 222, 216, 218]...\n",
      "Starting point: [np.float64(0.39269908502303913), np.float64(0.39269908172656365), np.float64(0.39269908233744083), np.float64(0.0)]\n",
      "         Current function value: -18.420152\n",
      "         Iterations: 5\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 30\n",
      "\n",
      "Current energy: -18.42015207804683\n",
      "(change of -0.9348707038082757)\n",
      "Current ansatz: [226, 222, 216, 218]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.2645853006247054\n",
      "Operator 1: -4.51554918563637e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator 2: -2.529170705436514\n",
      "Operator 3: -2.3666952160993945e-08\n",
      "Operator 4: -3.264585404811789\n",
      "Operator 5: 3.9999999999999742\n",
      "Operator 6: -3.414213562279753\n",
      "Operator 8: -3.4142135622797376\n",
      "Operator 9: 3.9999999999999614\n",
      "Operator 10: -3.414213549483979\n",
      "Operator 11: -5.155633076014965e-08\n",
      "Operator 12: -3.4142135494839803\n",
      "Operator 13: 4.5155491853283803e-07\n",
      "Operator 14: 2.529170705436508\n",
      "Operator 15: 2.366695213310798e-08\n",
      "Operator 16: 3.2645854048117897\n",
      "Operator 17: -3.9999999999999742\n",
      "Operator 18: 3.4142135622797514\n",
      "Operator 20: 3.4142135622797376\n",
      "Operator 21: -3.9999999999999614\n",
      "Operator 22: 3.414213549483977\n",
      "Operator 23: 5.155633076014965e-08\n",
      "Operator 24: 3.414213549483982\n",
      "Operator 25: -3.9999999999999747\n",
      "Operator 26: -3.9999999999999605\n",
      "Operator 27: 3.264585300624709\n",
      "Operator 28: -4.5155491853283803e-07\n",
      "Operator 29: 2.799588057154765\n",
      "Operator 30: -0.929994718981\n",
      "Operator 31: 3.2645854048117884\n",
      "Operator 32: 2.529170809623607\n",
      "Operator 33: 3.4142135622797514\n",
      "Operator 35: 3.4142135622797376\n",
      "Operator 36: 2.828427124559489\n",
      "Operator 37: 3.414213549483977\n",
      "Operator 38: -5.155633076014965e-08\n",
      "Operator 39: 3.4142135494839803\n",
      "Operator 40: 2.8284270989679903\n",
      "Operator 41: -3.2645853006247054\n",
      "Operator 42: 0.9299949384947754\n",
      "Operator 43: -2.799588057154765\n",
      "Operator 44: 2.366695213310798e-08\n",
      "Operator 45: -3.2645854048117897\n",
      "Operator 46: -2.8284271245594974\n",
      "Operator 47: -3.4142135622797536\n",
      "Operator 49: -3.4142135622797376\n",
      "Operator 50: -2.828427098967978\n",
      "Operator 51: -3.4142135494839785\n",
      "Operator 52: 5.155633076014965e-08\n",
      "Operator 53: -3.414213549483982\n",
      "Operator 54: 3.9999999999999747\n",
      "Operator 55: 4.51554918563637e-07\n",
      "Operator 56: -2.799588057154765\n",
      "Operator 57: 0.9299947189810001\n",
      "Operator 58: -3.2645854048117897\n",
      "Operator 59: -2.529170809623607\n",
      "Operator 60: -3.414213562279753\n",
      "Operator 62: -3.4142135622797394\n",
      "Operator 63: -2.828427124559489\n",
      "Operator 64: -3.414213549483979\n",
      "Operator 65: 5.155633076014965e-08\n",
      "Operator 66: -3.414213549483982\n",
      "Operator 67: -2.8284270989679903\n",
      "Operator 68: 3.264585300624709\n",
      "Operator 69: -1.5494592661454183\n",
      "Operator 70: 1.5494592661454012\n",
      "Operator 71: -1.549459181113492\n",
      "Operator 72: 1.5494591811134963\n",
      "Operator 75: -1.4142135624664247\n",
      "Operator 76: 1.4142135624664223\n",
      "Operator 79: -1.4142135752621738\n",
      "Operator 80: 1.414213575262178\n",
      "Operator 82: 1.5494592661454072\n",
      "Operator 83: -0.9797117866589563\n",
      "Operator 84: 1.5494591811134955\n",
      "Operator 85: -0.9797116521770661\n",
      "Operator 88: 1.4142135624664303\n",
      "Operator 89: -1.414213562466421\n",
      "Operator 92: 1.4142135752621752\n",
      "Operator 93: -1.414213575262174\n",
      "Operator 95: -3.9999999999999605\n",
      "Operator 96: 2.064173461966602\n",
      "Operator 97: 4.5155491883776566e-07\n",
      "Operator 98: 2.799588057154766\n",
      "Operator 99: 2.3666952160993945e-08\n",
      "Operator 100: 2.308410477352693\n",
      "Operator 101: -3.9999999999999742\n",
      "Operator 102: 3.4142135622797514\n",
      "Operator 104: 2.414213540304024\n",
      "Operator 105: -3.9999999999999614\n",
      "Operator 106: 3.414213549483977\n",
      "Operator 107: 5.155633076014965e-08\n",
      "Operator 108: 3.264585300624709\n",
      "Operator 109: -0.9299949384947754\n",
      "Operator 110: 2.799588057154766\n",
      "Operator 111: -2.3666952160993945e-08\n",
      "Operator 112: 3.264585404811789\n",
      "Operator 113: 2.8284271245594974\n",
      "Operator 114: 3.4142135622797514\n",
      "Operator 116: 3.4142135622797376\n",
      "Operator 117: 2.828427098967978\n",
      "Operator 118: 3.414213549483977\n",
      "Operator 119: -5.155633076014965e-08\n",
      "Operator 120: 3.4142135494839803\n",
      "Operator 121: 3.9999999999999605\n",
      "Operator 122: -2.0641734619665995\n",
      "Operator 123: -4.5155491890680593e-07\n",
      "Operator 124: -2.799588057154765\n",
      "Operator 125: -2.366695213310798e-08\n",
      "Operator 126: -2.3084104773526937\n",
      "Operator 127: 3.9999999999999742\n",
      "Operator 128: -3.4142135622797536\n",
      "Operator 130: -2.4142135403040244\n",
      "Operator 131: 3.9999999999999614\n",
      "Operator 132: -3.4142135494839785\n",
      "Operator 133: -5.155633076014965e-08\n",
      "Operator 134: -1.5494592661454183\n",
      "Operator 135: 0.9797117866589488\n",
      "Operator 136: -1.5494591811134917\n",
      "Operator 137: 0.9797116521770685\n",
      "Operator 140: -1.4142135624664245\n",
      "Operator 141: 1.4142135624664216\n",
      "Operator 144: -1.4142135752621738\n",
      "Operator 145: 1.414213575262178\n",
      "Operator 149: -1.2004119428451951\n",
      "Operator 162: 1.2004119428451963\n",
      "Operator 174: 0.9797117866589551\n",
      "Operator 175: -1.5494592661454065\n",
      "Operator 176: 0.9797116521770712\n",
      "Operator 177: -1.5494591811134917\n",
      "Operator 180: 1.4142135624664303\n",
      "Operator 181: -1.414213562466421\n",
      "Operator 184: 1.4142135752621752\n",
      "Operator 185: -1.414213575262174\n",
      "Operator 186: 3.9999999999999605\n",
      "Operator 187: 1.5494592661454076\n",
      "Operator 188: -1.5494592661454065\n",
      "Operator 189: 1.5494591811134955\n",
      "Operator 190: -1.5494591811134921\n",
      "Operator 193: 1.4142135624664303\n",
      "Operator 194: -1.414213562466421\n",
      "Operator 197: 1.4142135752621752\n",
      "Operator 198: -1.4142135752621743\n",
      "Operator 203: -1.2004119428451951\n",
      "Operator 214: -2.5291706012494046\n",
      "Operator 215: -2.0641734619665995\n",
      "Operator 216: 0.9299949384947754\n",
      "Operator 217: -2.529170705436514\n",
      "Operator 218: 2.3666952160993945e-08\n",
      "Operator 219: -2.308410477352693\n",
      "Operator 220: -2.8284271245594974\n",
      "Operator 221: -3.414213562279753\n",
      "Operator 223: -2.414213540304024\n",
      "Operator 224: -2.828427098967978\n",
      "Operator 225: -3.414213549483979\n",
      "Operator 226: 5.155633076014965e-08\n",
      "Operator 227: -3.2645853006247054\n",
      "Total gradient norm: 31.642916751154424\n",
      "Operators under consideration (1):\n",
      "[127]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.9999999999999742)]\n",
      "Operator(s) added to ansatz: [127]\n",
      "Gradients: [np.float64(3.9999999999999742)]\n",
      "Initial energy: -18.42015207804683\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127]...\n",
      "Starting point: [np.float64(0.3926990862557055), np.float64(0.39269908173172385), np.float64(0.4431436925717968), np.float64(0.44314365895132635), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.544229\n",
      "         Iterations: 9\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "Current energy: -19.544229345431848\n",
      "(change of -1.1240772673850188)\n",
      "Current ansatz: [226, 222, 216, 218, 127]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.183606811228171\n",
      "Operator 1: 1.154981147310207e-07\n",
      "Operator 2: -2.1202962873058975\n",
      "Operator 3: 1.2978423194439492e-07\n",
      "Operator 4: -1.8733788527664383\n",
      "Operator 5: -1.2856058120950444e-07\n",
      "Operator 6: -2.1202960927317984\n",
      "Operator 7: -9.736750812605067e-08\n",
      "Operator 8: -3.183606716043089\n",
      "Operator 9: 3.999999999999999\n",
      "Operator 10: -3.414213564114379\n",
      "Operator 12: -3.4142135641143794\n",
      "Operator 13: -1.1549811408365552e-07\n",
      "Operator 14: 2.1202962873058917\n",
      "Operator 15: -1.2978423156473956e-07\n",
      "Operator 16: 1.8733788527664406\n",
      "Operator 17: 1.2856058120950444e-07\n",
      "Operator 18: 2.1202960927317958\n",
      "Operator 19: 9.736751007429731e-08\n",
      "Operator 20: 3.1836067160430903\n",
      "Operator 21: -3.999999999999999\n",
      "Operator 22: 3.4142135641143776\n",
      "Operator 24: 3.414213564114381\n",
      "Operator 25: -4.00000000000001\n",
      "Operator 26: -3.9999999999999982\n",
      "Operator 27: 3.183606811228177\n",
      "Operator 28: 1.1549811395990663e-07\n",
      "Operator 29: 2.554336021945673\n",
      "Operator 30: -1.1482008052008361\n",
      "Operator 31: 2.4386935407490604\n",
      "Operator 32: -1.3794858474608975\n",
      "Operator 33: 2.5543359185474936\n",
      "Operator 34: -1.258541692358746\n",
      "Operator 35: 3.183606716043089\n",
      "Operator 36: 2.3672134320861575\n",
      "Operator 37: 3.4142135641143776\n",
      "Operator 39: 3.4142135641143794\n",
      "Operator 40: 2.8284271282287534\n",
      "Operator 41: -3.1836068112281706\n",
      "Operator 42: 1.2585414630668723\n",
      "Operator 43: -2.5543360219456734\n",
      "Operator 44: 1.3794857675940524\n",
      "Operator 45: -2.4386935407490618\n",
      "Operator 46: 1.1482010918640657\n",
      "Operator 47: -2.5543359185474954\n",
      "Operator 48: 9.736751007429731e-08\n",
      "Operator 49: -3.1836067160430903\n",
      "Operator 50: -2.8284271282287436\n",
      "Operator 51: -3.414213564114379\n",
      "Operator 53: -3.414213564114381\n",
      "Operator 54: 4.00000000000001\n",
      "Operator 55: -1.1549811482614891e-07\n",
      "Operator 56: -2.554336021945672\n",
      "Operator 57: 1.148200805200836\n",
      "Operator 58: -2.4386935407490613\n",
      "Operator 59: 1.3794858474608973\n",
      "Operator 60: -2.5543359185474954\n",
      "Operator 61: 1.2585416923587445\n",
      "Operator 62: -3.1836067160430903\n",
      "Operator 63: -2.3672134320861575\n",
      "Operator 64: -3.414213564114379\n",
      "Operator 66: -3.414213564114381\n",
      "Operator 67: -2.8284271282287534\n",
      "Operator 68: 3.183606811228177\n",
      "Operator 69: -1.6121646678966663\n",
      "Operator 70: 1.6121646678966495\n",
      "Operator 71: -1.767091629034899\n",
      "Operator 72: 1.7670916290349006\n",
      "Operator 73: -1.7670916817184472\n",
      "Operator 74: 1.7670916817184594\n",
      "Operator 75: -1.6121647377789097\n",
      "Operator 76: 1.6121647377789052\n",
      "Operator 79: -1.4142135606318171\n",
      "Operator 80: 1.4142135606318207\n",
      "Operator 82: 1.6121646678966544\n",
      "Operator 83: -0.7550488390615809\n",
      "Operator 84: 1.767091629034902\n",
      "Operator 85: -0.48978122005727254\n",
      "Operator 86: 1.7670916817184477\n",
      "Operator 87: -0.48978124724057953\n",
      "Operator 88: 1.612164737778914\n",
      "Operator 89: -0.7550487916748125\n",
      "Operator 92: 1.4142135606318187\n",
      "Operator 93: -1.4142135606318167\n",
      "Operator 95: -3.999999999999998\n",
      "Operator 96: 1.4910254980234203\n",
      "Operator 97: -1.1549811451214362e-07\n",
      "Operator 98: 1.1963097081248928\n",
      "Operator 99: -1.2978423156473956e-07\n",
      "Operator 100: 1.4432270266007317\n",
      "Operator 101: 1.2856058274807295e-07\n",
      "Operator 102: 2.5543359185474936\n",
      "Operator 103: 9.736750812605067e-08\n",
      "Operator 104: 2.2511499003168742\n",
      "Operator 105: -3.999999999999999\n",
      "Operator 106: 3.4142135641143776\n",
      "Operator 108: 3.1836068112281763\n",
      "Operator 109: -1.258541463066872\n",
      "Operator 110: 2.5543360219456726\n",
      "Operator 111: -1.3794857675940522\n",
      "Operator 112: 2.4386935407490604\n",
      "Operator 113: -1.1482010918640655\n",
      "Operator 114: 2.5543359185474936\n",
      "Operator 115: -9.736750812605067e-08\n",
      "Operator 116: 3.183606716043089\n",
      "Operator 117: 2.8284271282287436\n",
      "Operator 118: 3.4142135641143776\n",
      "Operator 120: 3.4142135641143794\n",
      "Operator 121: 3.9999999999999982\n",
      "Operator 122: -1.4910254980234177\n",
      "Operator 123: 1.1549811408365552e-07\n",
      "Operator 124: -1.1963097081248928\n",
      "Operator 125: 1.2978423156473956e-07\n",
      "Operator 126: -1.4432270266007317\n",
      "Operator 127: -1.2856058274807295e-07\n",
      "Operator 128: -2.5543359185474954\n",
      "Operator 129: -9.736751007429731e-08\n",
      "Operator 130: -2.2511499003168747\n",
      "Operator 131: 3.999999999999999\n",
      "Operator 132: -3.414213564114379\n",
      "Operator 134: -1.6121646678966657\n",
      "Operator 135: 0.7550488390615736\n",
      "Operator 136: -1.767091629034899\n",
      "Operator 137: 0.4897812200572743\n",
      "Operator 138: -1.7670916817184472\n",
      "Operator 139: 0.4897812472405786\n",
      "Operator 140: -1.6121647377789097\n",
      "Operator 141: 0.7550487916748132\n",
      "Operator 144: -1.4142135606318176\n",
      "Operator 145: 1.4142135606318207\n",
      "Operator 149: -1.4244213446329999\n",
      "Operator 151: -1.5613064592509356\n",
      "Operator 153: -1.4244214488444598\n",
      "Operator 162: 1.424421344633001\n",
      "Operator 164: 1.5613064592509345\n",
      "Operator 166: 1.4244214488444578\n",
      "Operator 174: 0.755048839061579\n",
      "Operator 175: -1.6121646678966535\n",
      "Operator 176: 0.48978122005727465\n",
      "Operator 177: -1.7670916290348984\n",
      "Operator 178: 0.48978124724057504\n",
      "Operator 179: -1.7670916817184594\n",
      "Operator 180: 0.7550487916748194\n",
      "Operator 181: -1.6121647377789048\n",
      "Operator 184: 1.4142135606318187\n",
      "Operator 185: -1.4142135606318167\n",
      "Operator 186: 3.999999999999998\n",
      "Operator 187: 1.6121646678966555\n",
      "Operator 188: -1.6121646678966544\n",
      "Operator 189: 1.7670916290349026\n",
      "Operator 190: -1.7670916290348981\n",
      "Operator 191: 1.7670916817184479\n",
      "Operator 192: -1.7670916817184594\n",
      "Operator 193: 1.6121647377789134\n",
      "Operator 194: -1.6121647377789046\n",
      "Operator 197: 1.4142135606318187\n",
      "Operator 198: -1.4142135606318167\n",
      "Operator 203: -1.4244213446329999\n",
      "Operator 205: -1.5613064592509358\n",
      "Operator 207: -1.4244214488444595\n",
      "Operator 214: -2.3672136224563047\n",
      "Operator 215: -1.491025498023418\n",
      "Operator 216: 1.258541463066872\n",
      "Operator 217: -0.9930295038759629\n",
      "Operator 218: 1.3794857675940526\n",
      "Operator 219: -1.1086718959137172\n",
      "Operator 220: 1.1482010918640657\n",
      "Operator 221: -2.1202960927317984\n",
      "Operator 222: 9.736750812605067e-08\n",
      "Operator 223: -2.2511499003168742\n",
      "Operator 224: -2.8284271282287436\n",
      "Operator 225: -3.414213564114379\n",
      "Operator 227: -3.183606811228171\n",
      "Total gradient norm: 28.48531934314501\n",
      "Operators under consideration (1):\n",
      "[131]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.999999999999999)]\n",
      "Operator(s) added to ansatz: [131]\n",
      "Gradients: [np.float64(3.999999999999999)]\n",
      "Initial energy: -19.544229345431848\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131]...\n",
      "Starting point: [np.float64(0.3926990810830893), np.float64(0.46875106507247016), np.float64(0.46875103555157993), np.float64(0.5416899559187188), np.float64(-0.5416899840409259), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.699827\n",
      "         Iterations: 11\n",
      "         Function evaluations: 113\n",
      "         Gradient evaluations: 100\n",
      "\n",
      "Current energy: -20.69982735349857\n",
      "(change of -1.1555980080667219)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.163661536716035\n",
      "Operator 1: -1.1056976461554988e-08\n",
      "Operator 2: -2.0252068696879135\n",
      "Operator 3: 2.3781979003400706e-08\n",
      "Operator 4: -1.6072091229302434\n",
      "Operator 6: -1.4913275827210009\n",
      "Operator 8: -1.6072091171768776\n",
      "Operator 10: -2.0252068556677396\n",
      "Operator 11: -2.0613079243503307e-08\n",
      "Operator 12: -3.163661531253521\n",
      "Operator 13: 1.1056976584263362e-08\n",
      "Operator 14: 2.0252068696879073\n",
      "Operator 15: -2.3781977679467543e-08\n",
      "Operator 16: 1.607209122930244\n",
      "Operator 18: 1.4913275827209973\n",
      "Operator 20: 1.6072091171768779\n",
      "Operator 22: 2.0252068556677374\n",
      "Operator 23: 2.0613079243503307e-08\n",
      "Operator 24: 3.1636615312535232\n",
      "Operator 25: -4.000000000000023\n",
      "Operator 26: -4.00000000000002\n",
      "Operator 27: 3.16366153671604\n",
      "Operator 28: -1.1056976013704732e-08\n",
      "Operator 29: 2.5012735830583046\n",
      "Operator 30: -1.1939063847348022\n",
      "Operator 31: 2.321211579102384\n",
      "Operator 32: -1.5114265256712909\n",
      "Operator 33: 2.2780072448730766\n",
      "Operator 34: -1.597835188740736\n",
      "Operator 35: 2.321211577119833\n",
      "Operator 36: -1.5540304012172297\n",
      "Operator 37: 2.5012735757260898\n",
      "Operator 38: -1.3247759316679728\n",
      "Operator 39: 3.163661531253521\n",
      "Operator 40: 2.327323062507021\n",
      "Operator 41: -3.163661536716035\n",
      "Operator 42: 1.3247759183724321\n",
      "Operator 43: -2.501273583058304\n",
      "Operator 44: 1.5540303926466386\n",
      "Operator 45: -2.321211579102385\n",
      "Operator 46: 1.5978351941299391\n",
      "Operator 47: -2.2780072448730793\n",
      "Operator 48: 1.5114265242472016\n",
      "Operator 49: -2.321211577119833\n",
      "Operator 50: 1.1939064040046898\n",
      "Operator 51: -2.5012735757260893\n",
      "Operator 52: 2.0613079243503307e-08\n",
      "Operator 53: -3.1636615312535232\n",
      "Operator 54: 4.000000000000023\n",
      "Operator 55: 1.1056976187152916e-08\n",
      "Operator 56: -2.501273583058304\n",
      "Operator 57: 1.1939063847348024\n",
      "Operator 58: -2.321211579102385\n",
      "Operator 59: 1.5114265256712907\n",
      "Operator 60: -2.278007244873079\n",
      "Operator 61: 1.5978351887407367\n",
      "Operator 62: -2.3212115771198327\n",
      "Operator 63: 1.5540304012172297\n",
      "Operator 64: -2.501273575726089\n",
      "Operator 65: 1.3247759316679728\n",
      "Operator 66: -3.163661531253524\n",
      "Operator 67: -2.327323062507021\n",
      "Operator 68: 3.16366153671604\n",
      "Operator 69: -1.6266197551879908\n",
      "Operator 70: 1.6266197551879746\n",
      "Operator 71: -1.8049209509655497\n",
      "Operator 72: 1.8049209509655508\n",
      "Operator 73: -1.8557978102005097\n",
      "Operator 74: 1.8557978102005217\n",
      "Operator 75: -1.8557978090737433\n",
      "Operator 76: 1.8557978090737368\n",
      "Operator 77: -1.804920955050398\n",
      "Operator 78: 1.8049209550503926\n",
      "Operator 79: -1.6266197590957692\n",
      "Operator 80: 1.6266197590957725\n",
      "Operator 82: 1.6266197551879797\n",
      "Operator 83: -0.7007033293010491\n",
      "Operator 84: 1.804920950965553\n",
      "Operator 85: -0.39153259988583095\n",
      "Operator 86: 1.8557978102005106\n",
      "Operator 87: -0.29805187367553493\n",
      "Operator 88: 1.8557978090737473\n",
      "Operator 89: -0.2980518694131175\n",
      "Operator 90: 1.804920955050397\n",
      "Operator 91: -0.39153260040647486\n",
      "Operator 92: 1.6266197590957692\n",
      "Operator 93: -0.7007033240243752\n",
      "Operator 95: -4.000000000000021\n",
      "Operator 96: 1.3628189160302027\n",
      "Operator 97: 1.1056976391517672e-08\n",
      "Operator 98: 0.9325545698329613\n",
      "Operator 99: -2.3781978386788166e-08\n",
      "Operator 100: 0.8654217149390192\n",
      "Operator 102: 0.9813032454010611\n",
      "Operator 104: 1.3505523090973108\n",
      "Operator 106: 2.5012735757260898\n",
      "Operator 107: 2.0613079243503307e-08\n",
      "Operator 108: 3.16366153671604\n",
      "Operator 109: -1.3247759183724326\n",
      "Operator 110: 2.5012735830583046\n",
      "Operator 111: -1.5540303926466377\n",
      "Operator 112: 2.3212115791023837\n",
      "Operator 113: -1.5978351941299405\n",
      "Operator 114: 2.278007244873077\n",
      "Operator 115: -1.5114265242472005\n",
      "Operator 116: 2.3212115771198336\n",
      "Operator 117: -1.1939064040046898\n",
      "Operator 118: 2.5012735757260898\n",
      "Operator 119: -2.0613079243503307e-08\n",
      "Operator 120: 3.163661531253521\n",
      "Operator 121: 4.00000000000002\n",
      "Operator 122: -1.3628189160301998\n",
      "Operator 123: -1.1056976546753698e-08\n",
      "Operator 124: -0.9325545698329606\n",
      "Operator 125: 2.3781977647107953e-08\n",
      "Operator 126: -0.8654217149390199\n",
      "Operator 128: -0.9813032454010622\n",
      "Operator 130: -1.3505523090973104\n",
      "Operator 132: -2.5012735757260893\n",
      "Operator 133: -2.0613079243503307e-08\n",
      "Operator 134: -1.6266197551879904\n",
      "Operator 135: 0.7007033293010415\n",
      "Operator 136: -1.8049209509655497\n",
      "Operator 137: 0.3915325998858327\n",
      "Operator 138: -1.8557978102005097\n",
      "Operator 139: 0.29805187367553354\n",
      "Operator 140: -1.8557978090737437\n",
      "Operator 141: 0.29805186941311795\n",
      "Operator 142: -1.8049209550503975\n",
      "Operator 143: 0.39153260040647375\n",
      "Operator 144: -1.6266197590957692\n",
      "Operator 145: 0.7007033240243773\n",
      "Operator 149: -1.4679600376966069\n",
      "Operator 151: -1.6747841741934353\n",
      "Operator 153: -1.7219927551269714\n",
      "Operator 155: -1.67478417696689\n",
      "Operator 157: -1.4679600445454808\n",
      "Operator 162: 1.4679600376966069\n",
      "Operator 164: 1.6747841741934342\n",
      "Operator 166: 1.7219927551269694\n",
      "Operator 168: 1.6747841769668905\n",
      "Operator 170: 1.4679600445454812\n",
      "Operator 174: 0.7007033293010471\n",
      "Operator 175: -1.6266197551879786\n",
      "Operator 176: 0.3915325998858322\n",
      "Operator 177: -1.8049209509655486\n",
      "Operator 178: 0.2980518736755308\n",
      "Operator 179: -1.8557978102005213\n",
      "Operator 180: 0.29805186941312023\n",
      "Operator 181: -1.855797809073737\n",
      "Operator 182: 0.391532600406473\n",
      "Operator 183: -1.8049209550503944\n",
      "Operator 184: 0.7007033240243765\n",
      "Operator 185: -1.6266197590957674\n",
      "Operator 186: 4.000000000000021\n",
      "Operator 187: 1.6266197551879795\n",
      "Operator 188: -1.6266197551879786\n",
      "Operator 189: 1.8049209509655537\n",
      "Operator 190: -1.8049209509655482\n",
      "Operator 191: 1.8557978102005108\n",
      "Operator 192: -1.8557978102005213\n",
      "Operator 193: 1.8557978090737473\n",
      "Operator 194: -1.855797809073737\n",
      "Operator 195: 1.8049209550503973\n",
      "Operator 196: -1.8049209550503944\n",
      "Operator 197: 1.6266197590957694\n",
      "Operator 198: -1.6266197590957674\n",
      "Operator 203: -1.4679600376966069\n",
      "Operator 205: -1.6747841741934353\n",
      "Operator 207: -1.7219927551269714\n",
      "Operator 209: -1.6747841769668899\n",
      "Operator 211: -1.4679600445454808\n",
      "Operator 214: -2.3273230734320083\n",
      "Operator 215: -1.3628189160301998\n",
      "Operator 216: 1.3247759183724326\n",
      "Operator 217: -0.7550617149505738\n",
      "Operator 218: 1.554030392646638\n",
      "Operator 219: -0.59921882518344\n",
      "Operator 220: 1.5978351941299391\n",
      "Operator 221: -0.642423153031621\n",
      "Operator 222: 1.5114265242472005\n",
      "Operator 223: -0.9351237111693171\n",
      "Operator 224: 1.1939064040046898\n",
      "Operator 225: -2.0252068556677396\n",
      "Operator 226: 2.0613079243503307e-08\n",
      "Operator 227: -3.163661536716035\n",
      "Total gradient norm: 25.23393326057587\n",
      "Operators under consideration (1):\n",
      "[186]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000021)]\n",
      "Operator(s) added to ansatz: [186]\n",
      "Gradients: [np.float64(4.000000000000021)]\n",
      "Initial energy: -20.69982735349857\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186]...\n",
      "Starting point: [np.float64(0.47490921875082076), np.float64(0.5943686126970895), np.float64(0.4749092170717269), np.float64(0.5627237734940704), np.float64(-0.5943686134526414), np.float64(-0.5627237758647206), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -22.337190\n",
      "         Iterations: 14\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 47\n",
      "\n",
      "Current energy: -22.33719022813699\n",
      "(change of -1.63736287463842)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.3275485480752415\n",
      "Operator 1: 1.0156699241339909e-08\n",
      "Operator 2: -0.548978342699758\n",
      "Operator 4: -0.7822533472761344\n",
      "Operator 5: -1.1757417308757933e-08\n",
      "Operator 6: -1.0487121197272435\n",
      "Operator 8: -1.396164029586179\n",
      "Operator 10: -1.9459060085093056\n",
      "Operator 11: 1.1149094092388623e-08\n",
      "Operator 12: -3.146705008443441\n",
      "Operator 13: -1.0156699243370292e-08\n",
      "Operator 14: 0.5489783426997488\n",
      "Operator 16: 0.7822533472761378\n",
      "Operator 17: 1.1757417308757933e-08\n",
      "Operator 18: 1.0487121197272395\n",
      "Operator 20: 1.3961640295861804\n",
      "Operator 22: 1.9459060085093034\n",
      "Operator 23: -1.1149086986961265e-08\n",
      "Operator 24: 3.146705008443444\n",
      "Operator 25: -4.000000000000016\n",
      "Operator 26: 1.4005420580379812e-08\n",
      "Operator 27: 0.32754854807525424\n",
      "Operator 28: -1.9821184353910128\n",
      "Operator 29: 2.0361033076801087\n",
      "Operator 30: -1.948965363590184\n",
      "Operator 31: 2.074652498720273\n",
      "Operator 32: -1.895107755994175\n",
      "Operator 33: 2.134838752736506\n",
      "Operator 34: -1.8114435968565612\n",
      "Operator 35: 2.238546725097502\n",
      "Operator 36: -1.670040837936819\n",
      "Operator 37: 2.4582238947642745\n",
      "Operator 38: -1.3769622162092716\n",
      "Operator 39: 3.146705008443441\n",
      "Operator 40: 2.2934100168868685\n",
      "Operator 41: -2.0119149717241394\n",
      "Operator 42: 1.9337417634790555\n",
      "Operator 43: -2.036103307680107\n",
      "Operator 44: 1.8718669815098519\n",
      "Operator 45: -2.0746524987202757\n",
      "Operator 46: 1.774735247961742\n",
      "Operator 47: -2.1348387527365076\n",
      "Operator 48: 1.6040276521345405\n",
      "Operator 49: -2.238546725097502\n",
      "Operator 50: 1.2306864986032469\n",
      "Operator 51: -2.458223894764273\n",
      "Operator 52: -1.1149086986961265e-08\n",
      "Operator 53: -3.146705008443444\n",
      "Operator 54: 4.000000000000016\n",
      "Operator 55: 1.982118435391013\n",
      "Operator 56: -2.036103307680107\n",
      "Operator 57: 1.9489653635901847\n",
      "Operator 58: -2.0746524987202757\n",
      "Operator 59: 1.8951077559941742\n",
      "Operator 60: -2.1348387527365076\n",
      "Operator 61: 1.8114435968565612\n",
      "Operator 62: -2.238546725097502\n",
      "Operator 63: 1.6700408379368181\n",
      "Operator 64: -2.4582238947642727\n",
      "Operator 65: 1.3769622162092716\n",
      "Operator 66: -3.146705008443444\n",
      "Operator 67: -2.2934100168868685\n",
      "Operator 68: 2.011914971724139\n",
      "Operator 69: -1.9880314569714326\n",
      "Operator 70: 1.988031456971425\n",
      "Operator 71: -1.9725027087583857\n",
      "Operator 72: 1.972502708758386\n",
      "Operator 73: -1.9483127981659525\n",
      "Operator 74: 1.948312798165966\n",
      "Operator 75: -1.9088308310025826\n",
      "Operator 76: 1.9088308310025754\n",
      "Operator 77: -1.8333787828743302\n",
      "Operator 78: 1.8333787828743238\n",
      "Operator 79: -1.6386175952951239\n",
      "Operator 80: 1.6386175952951265\n",
      "Operator 82: 1.9880314569714317\n",
      "Operator 83: -0.01791692186783321\n",
      "Operator 84: 1.9725027087583846\n",
      "Operator 85: -0.04866927019199454\n",
      "Operator 86: 1.948312798165955\n",
      "Operator 87: -0.09609995526875553\n",
      "Operator 88: 1.9088308310025857\n",
      "Operator 89: -0.1722902713269625\n",
      "Operator 90: 1.8333787828743295\n",
      "Operator 91: -0.3137555052574759\n",
      "Operator 92: 1.638617595295124\n",
      "Operator 93: -0.6547924104426948\n",
      "Operator 95: 1.400542150200442e-08\n",
      "Operator 96: 0.33247323150805247\n",
      "Operator 97: -1.0156698617618225e-08\n",
      "Operator 98: 0.4599039084058978\n",
      "Operator 100: 0.6192454204189578\n",
      "Operator 101: 1.1757417308757933e-08\n",
      "Operator 102: 0.8530826330831895\n",
      "Operator 104: 1.2834763706519685\n",
      "Operator 106: 2.4582238947642736\n",
      "Operator 107: -1.1149094092388623e-08\n",
      "Operator 108: 2.0119149717241394\n",
      "Operator 109: -1.9337417634790555\n",
      "Operator 110: 2.0361033076801087\n",
      "Operator 111: -1.8718669815098514\n",
      "Operator 112: 2.074652498720273\n",
      "Operator 113: -1.7747352479617415\n",
      "Operator 114: 2.134838752736506\n",
      "Operator 115: -1.6040276521345405\n",
      "Operator 116: 2.238546725097502\n",
      "Operator 117: -1.2306864986032475\n",
      "Operator 118: 2.4582238947642736\n",
      "Operator 119: 1.1149094092388623e-08\n",
      "Operator 120: 3.146705008443441\n",
      "Operator 121: -1.4005420648315375e-08\n",
      "Operator 122: -0.3324732315080524\n",
      "Operator 123: 1.0156699245003165e-08\n",
      "Operator 124: -0.45990390840589723\n",
      "Operator 126: -0.6192454204189584\n",
      "Operator 127: -1.1757417308757933e-08\n",
      "Operator 128: -0.8530826330831904\n",
      "Operator 130: -1.2834763706519685\n",
      "Operator 132: -2.458223894764273\n",
      "Operator 133: 1.1149086986961265e-08\n",
      "Operator 134: -1.9880314569714328\n",
      "Operator 135: 0.017916921867832794\n",
      "Operator 136: -1.9725027087583853\n",
      "Operator 137: 0.04866927019199483\n",
      "Operator 138: -1.9483127981659525\n",
      "Operator 139: 0.09609995526875503\n",
      "Operator 140: -1.9088308310025826\n",
      "Operator 141: 0.17229027132696273\n",
      "Operator 142: -1.8333787828743302\n",
      "Operator 143: 0.31375550525747486\n",
      "Operator 144: -1.6386175952951239\n",
      "Operator 145: 0.6547924104426964\n",
      "Operator 147: -1.9850727447883472\n",
      "Operator 149: -1.9606987169864991\n",
      "Operator 151: -1.9215261359454785\n",
      "Operator 153: -1.8594997687880366\n",
      "Operator 155: -1.7498049728282385\n",
      "Operator 157: -1.5021033662293082\n",
      "Operator 160: 1.9850727447883472\n",
      "Operator 162: 1.9606987169865007\n",
      "Operator 164: 1.9215261359454765\n",
      "Operator 166: 1.8594997687880341\n",
      "Operator 168: 1.7498049728282388\n",
      "Operator 170: 1.5021033662293086\n",
      "Operator 173: -1.9970234754860692\n",
      "Operator 174: 0.01791692186783297\n",
      "Operator 175: -1.9880314569714272\n",
      "Operator 176: 0.04866927019199563\n",
      "Operator 177: -1.9725027087583826\n",
      "Operator 178: 0.09609995526875398\n",
      "Operator 179: -1.948312798165965\n",
      "Operator 180: 0.17229027132696445\n",
      "Operator 181: -1.9088308310025752\n",
      "Operator 182: 0.3137555052574749\n",
      "Operator 183: -1.833378782874326\n",
      "Operator 184: 0.6547924104426954\n",
      "Operator 185: -1.6386175952951216\n",
      "Operator 186: -1.4005420921317371e-08\n",
      "Operator 187: 1.9880314569714317\n",
      "Operator 188: -1.9880314569714281\n",
      "Operator 189: 1.9725027087583846\n",
      "Operator 190: -1.9725027087583833\n",
      "Operator 191: 1.9483127981659552\n",
      "Operator 192: -1.9483127981659651\n",
      "Operator 193: 1.9088308310025859\n",
      "Operator 194: -1.9088308310025752\n",
      "Operator 195: 1.8333787828743295\n",
      "Operator 196: -1.8333787828743262\n",
      "Operator 197: 1.6386175952951239\n",
      "Operator 198: -1.6386175952951219\n",
      "Operator 200: 1.997023475486071\n",
      "Operator 201: -1.9850727447883467\n",
      "Operator 203: -1.9606987169864993\n",
      "Operator 205: -1.9215261359454785\n",
      "Operator 207: -1.8594997687880366\n",
      "Operator 209: -1.749804972828238\n",
      "Operator 211: -1.5021033662293082\n",
      "Operator 214: 1.973193532037823\n",
      "Operator 215: -0.05412809476785244\n",
      "Operator 216: 1.9337417634790555\n",
      "Operator 217: -0.12400023342896008\n",
      "Operator 218: 1.8718669815098519\n",
      "Operator 219: -0.23348816402117867\n",
      "Operator 220: 1.7747352479617415\n",
      "Operator 221: -0.4190658874336031\n",
      "Operator 222: 1.6040276521345405\n",
      "Operator 223: -0.8004941426675143\n",
      "Operator 224: 1.2306864986032469\n",
      "Operator 225: -1.9459060085093056\n",
      "Operator 226: -1.1149094092388623e-08\n",
      "Operator 227: -2.0119149717241394\n",
      "Total gradient norm: 23.635797138264007\n",
      "Operators under consideration (1):\n",
      "[54]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000016)]\n",
      "Operator(s) added to ansatz: [54]\n",
      "Gradients: [np.float64(4.000000000000016)]\n",
      "Initial energy: -22.33719022813699\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54]...\n",
      "Starting point: [np.float64(0.48010221654730123), np.float64(0.6338475401367958), np.float64(0.7306704298356981), np.float64(0.7023913388370133), np.float64(-0.671477818908716), np.float64(-0.5798576647437236), np.float64(-0.7581160121109403), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.000000\n",
      "         Iterations: 15\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 50\n",
      "\n",
      "Current energy: -23.99999999249374\n",
      "(change of -1.6628097643567514)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.00012996480866818427\n",
      "Operator 1: -5.832739221547481e-06\n",
      "Operator 2: 0.00020858824689173243\n",
      "Operator 3: -3.5198700652912088e-06\n",
      "Operator 4: 0.0002778590764155995\n",
      "Operator 5: -7.35086878172998e-06\n",
      "Operator 6: 0.0003362591684300097\n",
      "Operator 7: -2.569782740261624e-06\n",
      "Operator 8: 0.0003847386112198853\n",
      "Operator 9: -8.397945371737308e-06\n",
      "Operator 10: 0.000422250329300003\n",
      "Operator 11: -3.9731915231211445e-06\n",
      "Operator 12: 0.0004473909150232158\n",
      "Operator 13: 5.83273922162185e-06\n",
      "Operator 14: -0.00020858824690064015\n",
      "Operator 15: 3.5198700652912088e-06\n",
      "Operator 16: -0.00027785907641296204\n",
      "Operator 17: 7.3508687829075524e-06\n",
      "Operator 18: -0.000336259168435098\n",
      "Operator 19: 2.5697827389848734e-06\n",
      "Operator 20: -0.00038473861121896924\n",
      "Operator 21: 8.397945371737308e-06\n",
      "Operator 22: -0.00042225032930243666\n",
      "Operator 23: 3.973191521677855e-06\n",
      "Operator 24: -0.00044739091501933004\n",
      "Operator 25: 0.00023957100636806672\n",
      "Operator 26: 1.6402713964884655e-06\n",
      "Operator 27: -0.00012996480865432866\n",
      "Operator 28: -1.999999997056709\n",
      "Operator 29: 2.000000005273061\n",
      "Operator 30: -1.999999992391122\n",
      "Operator 31: 2.000000009515614\n",
      "Operator 32: -1.9999999860129287\n",
      "Operator 33: 2.0000000140523553\n",
      "Operator 34: -1.999999979550418\n",
      "Operator 35: 2.0000000184371167\n",
      "Operator 36: -1.9999999724911393\n",
      "Operator 37: 2.000000022260431\n",
      "Operator 38: -1.9999999668932635\n",
      "Operator 39: 2.000000025005825\n",
      "Operator 40: -1.9999999368795494\n",
      "Operator 41: -2.0000000018884667\n",
      "Operator 42: 1.9999999902875052\n",
      "Operator 43: -2.0000000052730598\n",
      "Operator 44: 1.999999983906016\n",
      "Operator 45: -2.000000009515616\n",
      "Operator 46: 1.9999999769394727\n",
      "Operator 47: -2.0000000140523584\n",
      "Operator 48: 1.999999970780876\n",
      "Operator 49: -2.0000000184371167\n",
      "Operator 50: 1.999999964844492\n",
      "Operator 51: -2.00000002226043\n",
      "Operator 52: 1.999999961402466\n",
      "Operator 53: -2.0000000250058267\n",
      "Operator 54: -0.00023957100636806672\n",
      "Operator 55: 1.9999999970567082\n",
      "Operator 56: -2.0000000052730593\n",
      "Operator 57: 1.999999992391122\n",
      "Operator 58: -2.000000009515615\n",
      "Operator 59: 1.9999999860129292\n",
      "Operator 60: -2.000000014052358\n",
      "Operator 61: 1.9999999795504189\n",
      "Operator 62: -2.0000000184371163\n",
      "Operator 63: 1.9999999724911381\n",
      "Operator 64: -2.00000002226043\n",
      "Operator 65: 1.999999966893263\n",
      "Operator 66: -2.0000000250058267\n",
      "Operator 67: 1.9999999368795494\n",
      "Operator 68: 2.000000001888467\n",
      "Operator 69: -1.9999999981468626\n",
      "Operator 70: 1.9999999981468557\n",
      "Operator 71: -1.9999999962489463\n",
      "Operator 72: 1.9999999962489459\n",
      "Operator 73: -1.9999999939652566\n",
      "Operator 74: 1.9999999939652704\n",
      "Operator 75: -1.9999999918195464\n",
      "Operator 76: 1.999999991819538\n",
      "Operator 77: -1.9999999896116356\n",
      "Operator 78: 1.999999989611629\n",
      "Operator 79: -1.9999999880749673\n",
      "Operator 80: 1.9999999880749657\n",
      "Operator 81: -1.9999999868912015\n",
      "Operator 82: 1.9999999981468615\n",
      "Operator 84: 1.999999996248945\n",
      "Operator 86: 1.9999999939652588\n",
      "Operator 87: -1.1078886679962347e-08\n",
      "Operator 88: 1.9999999918195486\n",
      "Operator 89: -1.5835555110466855e-08\n",
      "Operator 90: 1.9999999896116356\n",
      "Operator 91: -1.975372081691576e-08\n",
      "Operator 92: 1.9999999880749662\n",
      "Operator 93: -2.3339155693515956e-08\n",
      "Operator 94: 1.9999999868912033\n",
      "Operator 95: 1.6402713966493748e-06\n",
      "Operator 96: -0.0001224917983428774\n",
      "Operator 97: 5.832739221303384e-06\n",
      "Operator 98: -0.0001553672785971863\n",
      "Operator 99: 3.519870065134192e-06\n",
      "Operator 100: -0.00018089189111319205\n",
      "Operator 101: 7.350868783143087e-06\n",
      "Operator 102: -0.00020384672240584157\n",
      "Operator 103: 2.56978274023388e-06\n",
      "Operator 104: -0.00021840361034756583\n",
      "Operator 105: 8.397945371737308e-06\n",
      "Operator 106: -0.0002289873092410355\n",
      "Operator 107: 3.9731915231211445e-06\n",
      "Operator 108: 2.0000000018884667\n",
      "Operator 109: -1.9999999902875047\n",
      "Operator 110: 2.0000000052730615\n",
      "Operator 111: -1.999999983906016\n",
      "Operator 112: 2.000000009515614\n",
      "Operator 113: -1.9999999769394723\n",
      "Operator 114: 2.0000000140523557\n",
      "Operator 115: -1.9999999707808767\n",
      "Operator 116: 2.0000000184371163\n",
      "Operator 117: -1.999999964844493\n",
      "Operator 118: 2.000000022260431\n",
      "Operator 119: -1.9999999614024722\n",
      "Operator 120: 2.000000025005825\n",
      "Operator 121: -1.6402713956824255e-06\n",
      "Operator 122: 0.0001224917983427906\n",
      "Operator 123: -5.832739220678721e-06\n",
      "Operator 124: 0.00015536727859718634\n",
      "Operator 125: -3.519870065134192e-06\n",
      "Operator 126: 0.00018089189111287807\n",
      "Operator 127: -7.350868783143087e-06\n",
      "Operator 128: 0.00020384672240589705\n",
      "Operator 129: -2.5697827390681494e-06\n",
      "Operator 130: 0.00021840361034756583\n",
      "Operator 131: -8.397945371737308e-06\n",
      "Operator 132: 0.00022898730924070243\n",
      "Operator 133: -3.973191521677855e-06\n",
      "Operator 134: -1.9999999981468626\n",
      "Operator 136: -1.9999999962489463\n",
      "Operator 138: -1.999999993965257\n",
      "Operator 139: 1.1078886698931168e-08\n",
      "Operator 140: -1.9999999918195464\n",
      "Operator 141: 1.58355548601754e-08\n",
      "Operator 142: -1.999999989611636\n",
      "Operator 143: 1.975372059985891e-08\n",
      "Operator 144: -1.999999988074967\n",
      "Operator 145: 2.3339155527849864e-08\n",
      "Operator 146: -1.9999999868912015\n",
      "Operator 147: -1.999999997665753\n",
      "Operator 149: -1.9999999943957958\n",
      "Operator 151: -1.999999990214198\n",
      "Operator 153: -1.9999999857848014\n",
      "Operator 155: -1.9999999814311669\n",
      "Operator 157: -1.9999999776865947\n",
      "Operator 159: -1.999999974966169\n",
      "Operator 160: 1.9999999976657539\n",
      "Operator 162: 1.9999999943957971\n",
      "Operator 164: 1.9999999902141963\n",
      "Operator 166: 1.9999999857847985\n",
      "Operator 168: 1.9999999814311669\n",
      "Operator 170: 1.9999999776865955\n",
      "Operator 172: 1.9999999749661672\n",
      "Operator 173: -1.9999999995189037\n",
      "Operator 175: -1.9999999981468566\n",
      "Operator 177: -1.9999999962489439\n",
      "Operator 178: 1.1078886411587745e-08\n",
      "Operator 179: -1.9999999939652673\n",
      "Operator 180: 1.5835555317688734e-08\n",
      "Operator 181: -1.999999991819538\n",
      "Operator 182: 1.9753720861157105e-08\n",
      "Operator 183: -1.9999999896116298\n",
      "Operator 184: 2.3339155692011625e-08\n",
      "Operator 185: -1.999999988074964\n",
      "Operator 186: -1.640271396394234e-06\n",
      "Operator 187: 1.9999999981468615\n",
      "Operator 188: -1.9999999981468577\n",
      "Operator 189: 1.9999999962489454\n",
      "Operator 190: -1.999999996248944\n",
      "Operator 191: 1.999999993965259\n",
      "Operator 192: -1.9999999939652677\n",
      "Operator 193: 1.9999999918195492\n",
      "Operator 194: -1.999999991819538\n",
      "Operator 195: 1.999999989611636\n",
      "Operator 196: -1.9999999896116298\n",
      "Operator 197: 1.9999999880749655\n",
      "Operator 198: -1.999999988074964\n",
      "Operator 199: 1.9999999868912033\n",
      "Operator 200: 1.9999999995189035\n",
      "Operator 201: -1.999999997665753\n",
      "Operator 203: -1.9999999943957958\n",
      "Operator 205: -1.9999999902141985\n",
      "Operator 207: -1.9999999857848014\n",
      "Operator 209: -1.9999999814311669\n",
      "Operator 211: -1.9999999776865942\n",
      "Operator 213: -1.999999974966169\n",
      "Operator 214: 1.999999995741991\n",
      "Operator 216: 1.9999999902875052\n",
      "Operator 217: -1.6203894207939366e-08\n",
      "Operator 218: 1.9999999839060165\n",
      "Operator 219: -2.51312268479715e-08\n",
      "Operator 220: 1.9999999769394736\n",
      "Operator 221: -3.42726647496852e-08\n",
      "Operator 222: 1.9999999707808767\n",
      "Operator 223: -4.201415059877346e-08\n",
      "Operator 224: 1.999999964844493\n",
      "Operator 225: -4.834498280848441e-08\n",
      "Operator 226: 1.9999999614024722\n",
      "Operator 227: -2.0000000018884667\n",
      "Total gradient norm: 22.80350845557409\n",
      "Operators under consideration (1):\n",
      "[118]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.000000022260431)]\n",
      "Operator(s) added to ansatz: [118]\n",
      "Gradients: [np.float64(2.000000022260431)]\n",
      "Initial energy: -23.99999999249374\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118]...\n",
      "Starting point: [np.float64(0.7854527642996396), np.float64(0.7854433863700717), np.float64(0.7854196875096203), np.float64(0.785428786347026), np.float64(-0.7854370052170342), np.float64(-0.7854491250777792), np.float64(-0.7854091304874508), np.float64(-0.7854554102242448), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.123106\n",
      "         Iterations: 14\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 42\n",
      "\n",
      "Current energy: -24.123105615011873\n",
      "(change of -0.12310562251813195)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.00017962374859394607\n",
      "Operator 1: 1.5041946690073014e-05\n",
      "Operator 2: 0.0002798584507295256\n",
      "Operator 3: 1.3375687823160546e-05\n",
      "Operator 4: 0.00040851078898918264\n",
      "Operator 5: -2.6954097663537874e-05\n",
      "Operator 6: 0.0005235847218789123\n",
      "Operator 7: 2.5081285499922216e-05\n",
      "Operator 8: 0.0006367858517360665\n",
      "Operator 9: -8.671005046095021e-05\n",
      "Operator 10: 0.00042582674290271246\n",
      "Operator 11: 6.359324514759734e-06\n",
      "Operator 12: 0.0005883112819168312\n",
      "Operator 13: -1.5041946690936572e-05\n",
      "Operator 14: -0.00027985845073863746\n",
      "Operator 15: -1.3375687823160546e-05\n",
      "Operator 16: -0.0004085107889867037\n",
      "Operator 17: 2.6954097664150408e-05\n",
      "Operator 18: -0.0005235847218831828\n",
      "Operator 19: -2.5081285499643166e-05\n",
      "Operator 20: -0.0006367858517348972\n",
      "Operator 21: -1.5325166022553506e-05\n",
      "Operator 22: -0.00041900442951334053\n",
      "Operator 23: 9.706314479085432e-05\n",
      "Operator 24: -0.0005883112819125014\n",
      "Operator 25: 0.0001458678541723657\n",
      "Operator 26: 2.5917308076683585e-05\n",
      "Operator 27: -0.00017962374858050684\n",
      "Operator 28: -1.9999999950818623\n",
      "Operator 29: 2.0000000093748205\n",
      "Operator 30: -1.9999999874513574\n",
      "Operator 31: 2.000000020229754\n",
      "Operator 32: -1.9999999691162555\n",
      "Operator 33: 2.000000034024966\n",
      "Operator 34: -1.9999999541249964\n",
      "Operator 35: 2.0000000500894615\n",
      "Operator 36: -1.9999999307514442\n",
      "Operator 37: 0.0004955892944211756\n",
      "Operator 38: -2.4849545443111296\n",
      "Operator 39: 1.9403141855609602\n",
      "Operator 40: -1.9999999009429477\n",
      "Operator 41: -2.0000000038062824\n",
      "Operator 42: 1.9999999839447795\n",
      "Operator 43: -2.0000000093748183\n",
      "Operator 44: 1.9999999657414824\n",
      "Operator 45: -2.0000000202297565\n",
      "Operator 46: 1.9999999415258607\n",
      "Operator 47: -2.0000000340249695\n",
      "Operator 48: 1.9999999219959852\n",
      "Operator 49: -1.9403141933681334\n",
      "Operator 50: 1.9403140423972118\n",
      "Operator 51: -1.9403141741944405\n",
      "Operator 52: 1.9403140752127803\n",
      "Operator 53: -2.0000000420421333\n",
      "Operator 54: -0.0001458678541723657\n",
      "Operator 55: 1.9999999950818623\n",
      "Operator 56: -2.0000000093748183\n",
      "Operator 57: 1.9999999874513574\n",
      "Operator 58: -2.0000000202297565\n",
      "Operator 59: 1.9999999691162555\n",
      "Operator 60: -2.0000000340249686\n",
      "Operator 61: 1.9999999541249966\n",
      "Operator 62: -2.0000000500894606\n",
      "Operator 63: 1.9403140631764784\n",
      "Operator 64: -1.940314174194441\n",
      "Operator 65: 1.9403140385790376\n",
      "Operator 66: -1.940314185560962\n",
      "Operator 67: 1.9999999009429477\n",
      "Operator 68: 2.0000000038062824\n",
      "Operator 69: -1.9999999969136413\n",
      "Operator 70: 1.9999999969136346\n",
      "Operator 71: -1.9999999928809928\n",
      "Operator 72: 1.9999999928809924\n",
      "Operator 73: -1.9999999856284927\n",
      "Operator 74: 1.9999999856285058\n",
      "Operator 75: -1.999999979861228\n",
      "Operator 76: 1.999999979861219\n",
      "Operator 77: -1.9403141145571545\n",
      "Operator 78: 2.484954615675456\n",
      "Operator 79: -0.0004955161479126837\n",
      "Operator 80: 1.9999999704873428\n",
      "Operator 81: -1.999999985027237\n",
      "Operator 82: 1.9999999969136402\n",
      "Operator 84: 1.9999999928809915\n",
      "Operator 85: -1.3320007897930431e-08\n",
      "Operator 86: 1.9999999856284942\n",
      "Operator 87: -2.3947279594646334e-08\n",
      "Operator 88: 1.9999999798612307\n",
      "Operator 89: -4.231369156193092e-08\n",
      "Operator 90: 1.9403141145571539\n",
      "Operator 91: -1.1834918716277871e-08\n",
      "Operator 92: 2.484954616354324\n",
      "Operator 93: -4.190083423714279e-08\n",
      "Operator 94: 1.940314130247559\n",
      "Operator 95: 2.591730807656403e-05\n",
      "Operator 96: -0.0001687483879788948\n",
      "Operator 97: -1.5041946690465532e-05\n",
      "Operator 98: -0.00023976240245369106\n",
      "Operator 99: -1.337568782371007e-05\n",
      "Operator 100: -0.00028382232342920365\n",
      "Operator 101: 2.695409766355735e-05\n",
      "Operator 102: -0.00035296353718852454\n",
      "Operator 103: -2.4332786512418982e-05\n",
      "Operator 104: -0.00034358491264788405\n",
      "Operator 105: -1.532516602129752e-05\n",
      "Operator 106: -6.06418866215693e-08\n",
      "Operator 107: -6.359324514759734e-06\n",
      "Operator 108: 2.0000000038062824\n",
      "Operator 109: -1.9999999839447802\n",
      "Operator 110: 2.0000000093748196\n",
      "Operator 111: -1.9999999657414818\n",
      "Operator 112: 2.000000020229754\n",
      "Operator 113: -1.9999999415258607\n",
      "Operator 114: 2.000000034024966\n",
      "Operator 115: -1.9999999219959848\n",
      "Operator 116: 1.9403141933681338\n",
      "Operator 117: -2.4849545478507498\n",
      "Operator 118: 0.0004955892944214896\n",
      "Operator 119: -1.9999999413199858\n",
      "Operator 120: 2.000000042042132\n",
      "Operator 121: -2.5917308076323667e-05\n",
      "Operator 122: 0.00016874838797928735\n",
      "Operator 123: 1.5041946691329086e-05\n",
      "Operator 124: 0.00023976240245329858\n",
      "Operator 125: 1.337568782371007e-05\n",
      "Operator 126: 0.00028382232342904514\n",
      "Operator 127: -2.6954097662944802e-05\n",
      "Operator 128: 0.0003529635371883854\n",
      "Operator 129: 2.4332786512423552e-05\n",
      "Operator 130: 0.00034358491264780555\n",
      "Operator 131: -8.671005046047899e-05\n",
      "Operator 132: 0.00023742302989704367\n",
      "Operator 133: -9.706314479085432e-05\n",
      "Operator 134: -1.9999999969136413\n",
      "Operator 136: -1.9999999928809935\n",
      "Operator 137: 1.3320007847702118e-08\n",
      "Operator 138: -1.9999999856284927\n",
      "Operator 139: 2.394727964602864e-08\n",
      "Operator 140: -1.999999979861228\n",
      "Operator 141: 4.231369145047929e-08\n",
      "Operator 142: -1.9999999688541834\n",
      "Operator 143: 0.4849546981372248\n",
      "Operator 144: -0.0004955161479125458\n",
      "Operator 145: 4.31897425400929e-08\n",
      "Operator 146: -1.9403141302475566\n",
      "Operator 147: -1.9999999957401084\n",
      "Operator 149: -1.9999999897946292\n",
      "Operator 151: -1.9999999785094902\n",
      "Operator 153: -1.999999965489729\n",
      "Operator 155: -1.9403140950193767\n",
      "Operator 157: -1.9403140859252437\n",
      "Operator 159: -1.9999999555145924\n",
      "Operator 160: 1.9999999957401098\n",
      "Operator 162: 1.9999999897946306\n",
      "Operator 164: 1.9999999785094875\n",
      "Operator 166: 1.9999999654897263\n",
      "Operator 168: 1.9999999487154057\n",
      "Operator 169: -0.48495464970141355\n",
      "Operator 170: 0.0004955010358888977\n",
      "Operator 171: -1.047253336982257e-08\n",
      "Operator 172: 1.940314101615654\n",
      "Operator 173: -1.9999999988264714\n",
      "Operator 175: -1.9999999969136364\n",
      "Operator 176: 1.3320007829472295e-08\n",
      "Operator 177: -1.9999999928809897\n",
      "Operator 178: 2.394727941298525e-08\n",
      "Operator 179: -1.9999999856285036\n",
      "Operator 180: 4.2313691612188095e-08\n",
      "Operator 181: -1.940314125235708\n",
      "Operator 182: 4.8758531774550365e-08\n",
      "Operator 183: -0.0004955153554960689\n",
      "Operator 184: 0.484954693565426\n",
      "Operator 185: -1.9999999704873408\n",
      "Operator 186: -2.5917308076197785e-05\n",
      "Operator 187: 1.9999999969136402\n",
      "Operator 188: -1.9999999969136355\n",
      "Operator 189: 1.9999999928809915\n",
      "Operator 190: -1.9999999928809897\n",
      "Operator 191: 1.999999985628495\n",
      "Operator 192: -1.9999999856285036\n",
      "Operator 193: 1.9999999798612307\n",
      "Operator 194: -1.9999999798612191\n",
      "Operator 195: 1.999999968854183\n",
      "Operator 196: -0.000495515355495977\n",
      "Operator 197: 2.484954616354325\n",
      "Operator 198: -1.9403141161415736\n",
      "Operator 199: 1.9999999850272396\n",
      "Operator 200: 1.9999999988264707\n",
      "Operator 201: -1.9999999957401091\n",
      "Operator 203: -1.99999998979463\n",
      "Operator 205: -1.9999999785094904\n",
      "Operator 207: -1.999999965489729\n",
      "Operator 209: -1.9999999487154052\n",
      "Operator 211: -1.9403140859252441\n",
      "Operator 213: -1.9403141016156558\n",
      "Operator 214: 1.9999999912139077\n",
      "Operator 215: -1.515560863358517e-08\n",
      "Operator 216: 1.9999999839447795\n",
      "Operator 217: -3.3549767201064493e-08\n",
      "Operator 218: 1.999999965741483\n",
      "Operator 219: -5.7972240177719683e-08\n",
      "Operator 220: 1.9999999415258607\n",
      "Operator 221: -9.240315631838274e-08\n",
      "Operator 222: 1.9403140690973406\n",
      "Operator 223: -1.0613033587760746e-07\n",
      "Operator 224: 1.9403140423972118\n",
      "Operator 225: -5.2105518004794e-08\n",
      "Operator 226: 1.9999999413199858\n",
      "Operator 227: -2.0000000038062824\n",
      "Total gradient norm: 22.319752989206535\n",
      "Operators under consideration (1):\n",
      "[92]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.484954616354324)]\n",
      "Operator(s) added to ansatz: [92]\n",
      "Gradients: [np.float64(2.484954616354324)]\n",
      "Initial energy: -24.123105615011873\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92]...\n",
      "Starting point: [np.float64(0.7854840596238806), np.float64(0.7854691189778248), np.float64(0.7854259409132314), np.float64(0.7854403504944136), np.float64(-0.7854581039979238), np.float64(-0.7854864042807014), np.float64(-0.7854152918188326), np.float64(-0.7854593449920689), np.float64(-0.12245928650731981), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.327276\n",
      "         Iterations: 16\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 62\n",
      "\n",
      "Current energy: -24.327276154821412\n",
      "(change of -0.2041705398095388)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.0970535857093982e-07\n",
      "Operator 1: 2.5164285528684092e-08\n",
      "Operator 2: -2.7715082604867697e-07\n",
      "Operator 3: 2.7499377192041763e-08\n",
      "Operator 4: -2.9193265441151983e-07\n",
      "Operator 5: 5.1350400845493275e-08\n",
      "Operator 6: -2.2786473933197653e-07\n",
      "Operator 7: 1.4733956609605937e-08\n",
      "Operator 8: -9.771254470618623e-08\n",
      "Operator 9: -7.084139409595113e-08\n",
      "Operator 11: 2.7162404325764555e-08\n",
      "Operator 12: -1.1223160623075046e-08\n",
      "Operator 13: -2.5164286235225696e-08\n",
      "Operator 14: 2.7715081678367315e-07\n",
      "Operator 15: -2.749937777490885e-08\n",
      "Operator 16: 2.91932656687477e-07\n",
      "Operator 17: -5.1350400845493275e-08\n",
      "Operator 18: 2.278647348102689e-07\n",
      "Operator 19: -1.4733956609605937e-08\n",
      "Operator 20: 9.771254588375557e-08\n",
      "Operator 21: 7.758407395257262e-08\n",
      "Operator 22: 1.950656930216879e-08\n",
      "Operator 23: -2.0419724355846824e-08\n",
      "Operator 24: 1.1223164730900237e-08\n",
      "Operator 25: 2.6581886913845665e-08\n",
      "Operator 26: -3.5395351716145826e-08\n",
      "Operator 27: 2.0970537222467357e-07\n",
      "Operator 28: -1.9999999999999987\n",
      "Operator 29: 2.0000000000000133\n",
      "Operator 30: -1.999999999999987\n",
      "Operator 31: 2.0000000000000124\n",
      "Operator 32: -1.9999999999999827\n",
      "Operator 33: 2.0000000000000187\n",
      "Operator 34: -1.9999999999999987\n",
      "Operator 35: 2.0000000000000044\n",
      "Operator 36: -1.893930658973121\n",
      "Operator 37: -0.7090316352148591\n",
      "Operator 38: -2.0894916332836138\n",
      "Operator 39: 1.7934866673061398\n",
      "Operator 40: -1.9999999999999982\n",
      "Operator 41: -2.0000000000000133\n",
      "Operator 42: 1.9999999999999865\n",
      "Operator 43: -2.0000000000000115\n",
      "Operator 44: 1.9999999999999847\n",
      "Operator 45: -2.0000000000000138\n",
      "Operator 46: 1.9999999999999996\n",
      "Operator 47: -2.000000000000021\n",
      "Operator 48: 2.0000000000000036\n",
      "Operator 49: -1.7934866673061478\n",
      "Operator 50: 2.089491619418532\n",
      "Operator 51: 0.7090315943549438\n",
      "Operator 52: 1.8939306556012614\n",
      "Operator 53: -1.999999999999997\n",
      "Operator 54: -2.6581886913845665e-08\n",
      "Operator 55: 1.9999999999999987\n",
      "Operator 56: -2.000000000000012\n",
      "Operator 57: 1.999999999999987\n",
      "Operator 58: -2.0000000000000138\n",
      "Operator 59: 1.9999999999999816\n",
      "Operator 60: -2.0000000000000213\n",
      "Operator 61: 1.9999999999999987\n",
      "Operator 62: -2.000000000000004\n",
      "Operator 63: 1.8939306556012705\n",
      "Operator 64: 0.7090315943549439\n",
      "Operator 65: 2.0894916194185313\n",
      "Operator 66: -1.7934866673061407\n",
      "Operator 67: 1.9999999999999982\n",
      "Operator 68: 2.0000000000000138\n",
      "Operator 69: -2.000000000000004\n",
      "Operator 70: 1.9999999999999982\n",
      "Operator 71: -1.9999999999999982\n",
      "Operator 72: 1.9999999999999973\n",
      "Operator 73: -1.9999999999999964\n",
      "Operator 74: 2.0000000000000098\n",
      "Operator 75: -2.00000000000001\n",
      "Operator 76: 2.0000000000000018\n",
      "Operator 77: -1.893930655601269\n",
      "Operator 78: 3.7010235407766264e-08\n",
      "Operator 79: 1.77147599743652e-08\n",
      "Operator 80: 1.8939306589731122\n",
      "Operator 81: -1.9999999999999951\n",
      "Operator 82: 2.0000000000000036\n",
      "Operator 84: 1.9999999999999964\n",
      "Operator 86: 1.9999999999999982\n",
      "Operator 88: 2.0000000000000124\n",
      "Operator 90: 1.893930655601269\n",
      "Operator 91: -0.608587629204981\n",
      "Operator 92: 3.7010235953680355e-08\n",
      "Operator 94: 1.7934866673061416\n",
      "Operator 95: -3.539535053371426e-08\n",
      "Operator 96: 1.491457156588261e-07\n",
      "Operator 97: -2.5164286706253436e-08\n",
      "Operator 98: 1.427869397718773e-07\n",
      "Operator 99: -2.749937780266443e-08\n",
      "Operator 100: 8.507778983192464e-08\n",
      "Operator 101: -5.1350400845493275e-08\n",
      "Operator 102: 1.2634749321450886e-08\n",
      "Operator 103: -1.3212582357281185e-08\n",
      "Operator 104: 2.5127620461328176e-08\n",
      "Operator 105: 7.758407384155033e-08\n",
      "Operator 107: -2.7162404325764555e-08\n",
      "Operator 108: 2.0000000000000133\n",
      "Operator 109: -1.9999999999999867\n",
      "Operator 110: 2.0000000000000133\n",
      "Operator 111: -1.999999999999985\n",
      "Operator 112: 2.0000000000000124\n",
      "Operator 113: -1.9999999999999991\n",
      "Operator 114: 2.0000000000000187\n",
      "Operator 115: -2.000000000000004\n",
      "Operator 116: 1.7934866673061478\n",
      "Operator 117: -2.089491633283612\n",
      "Operator 118: -0.7090316352148591\n",
      "Operator 119: -1.8939306589731133\n",
      "Operator 120: 1.9999999999999951\n",
      "Operator 121: 3.539535152380811e-08\n",
      "Operator 122: -1.491457155803215e-07\n",
      "Operator 123: 2.5164287098776557e-08\n",
      "Operator 124: -1.4278693971636616e-07\n",
      "Operator 125: 2.749937777490885e-08\n",
      "Operator 126: -8.507778983981478e-08\n",
      "Operator 127: 5.1350400845493275e-08\n",
      "Operator 128: -1.2634749323874082e-08\n",
      "Operator 129: 1.3212582357281185e-08\n",
      "Operator 130: -2.512762069565152e-08\n",
      "Operator 131: -7.08413945630338e-08\n",
      "Operator 133: 2.0419724355846824e-08\n",
      "Operator 134: -2.000000000000004\n",
      "Operator 136: -1.9999999999999982\n",
      "Operator 138: -1.9999999999999964\n",
      "Operator 140: -2.00000000000001\n",
      "Operator 142: -1.8939306589731189\n",
      "Operator 143: 0.6085876396982134\n",
      "Operator 144: 1.771476045046429e-08\n",
      "Operator 146: -1.7934866673061398\n",
      "Operator 147: -2.0000000000000018\n",
      "Operator 149: -1.9999999999999922\n",
      "Operator 151: -1.9999999999999925\n",
      "Operator 153: -2.0000000000000075\n",
      "Operator 155: -1.8939306556012672\n",
      "Operator 156: 0.6085876292049808\n",
      "Operator 157: 0.7090315943549452\n",
      "Operator 159: -1.8939306589731117\n",
      "Operator 160: 2.0000000000000027\n",
      "Operator 162: 1.9999999999999942\n",
      "Operator 164: 1.9999999999999907\n",
      "Operator 166: 2.0000000000000053\n",
      "Operator 168: 1.8939306589731173\n",
      "Operator 169: -0.608587639698212\n",
      "Operator 170: -0.7090316352148597\n",
      "Operator 172: 1.8939306556012603\n",
      "Operator 173: -2.000000000000007\n",
      "Operator 175: -2.0\n",
      "Operator 177: -1.9999999999999953\n",
      "Operator 179: -2.0000000000000075\n",
      "Operator 181: -1.7934866673061458\n",
      "Operator 183: 1.7714761261119443e-08\n",
      "Operator 184: 0.6085876396982097\n",
      "Operator 185: -1.8939306589731106\n",
      "Operator 186: 3.5395351499606776e-08\n",
      "Operator 187: 2.000000000000003\n",
      "Operator 188: -1.9999999999999998\n",
      "Operator 189: 1.9999999999999964\n",
      "Operator 190: -1.9999999999999956\n",
      "Operator 191: 1.9999999999999982\n",
      "Operator 192: -2.000000000000007\n",
      "Operator 193: 2.000000000000013\n",
      "Operator 194: -2.0000000000000018\n",
      "Operator 195: 1.8939306589731189\n",
      "Operator 196: 1.7714761169276113e-08\n",
      "Operator 197: 3.701023623378526e-08\n",
      "Operator 198: -1.8939306556012603\n",
      "Operator 199: 1.9999999999999967\n",
      "Operator 200: 2.0000000000000058\n",
      "Operator 201: -2.0000000000000018\n",
      "Operator 203: -1.9999999999999925\n",
      "Operator 205: -1.9999999999999925\n",
      "Operator 207: -2.0000000000000075\n",
      "Operator 209: -1.8939306589731175\n",
      "Operator 211: 0.7090315943549454\n",
      "Operator 212: 0.6085876292049781\n",
      "Operator 213: -1.8939306556012616\n",
      "Operator 214: 1.9999999999999944\n",
      "Operator 216: 1.9999999999999867\n",
      "Operator 218: 1.999999999999985\n",
      "Operator 220: 1.9999999999999991\n",
      "Operator 222: 1.793486667306148\n",
      "Operator 224: 2.0894916194185313\n",
      "Operator 226: 1.8939306589731133\n",
      "Operator 227: -2.0000000000000124\n",
      "Total gradient norm: 21.397885548236854\n",
      "Operators under consideration (1):\n",
      "[117]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.089491633283612)]\n",
      "Operator(s) added to ansatz: [117]\n",
      "Gradients: [np.float64(-2.089491633283612)]\n",
      "Initial energy: -24.327276154821412\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117]...\n",
      "Starting point: [np.float64(0.7853981594273585), np.float64(0.7853981421279996), np.float64(0.7853981313961717), np.float64(0.7853981261110203), np.float64(-0.7853981277007133), np.float64(-0.7853981602387609), np.float64(-0.7853981429723865), np.float64(-0.7853981646773509), np.float64(-0.16356985747654923), np.float64(-0.1635698548532412), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.464102\n",
      "         Iterations: 16\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 58\n",
      "\n",
      "Current energy: -24.464101615137807\n",
      "(change of -0.13682546031639475)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.640233554645482e-08\n",
      "Operator 1: -7.418106279057617e-08\n",
      "Operator 2: 2.6866404623985515e-08\n",
      "Operator 3: 2.7860291379466766e-08\n",
      "Operator 4: -1.8990315736600035e-08\n",
      "Operator 5: 1.6039097351206565e-08\n",
      "Operator 6: -2.0947675909835675e-08\n",
      "Operator 8: -1.055277467105354e-08\n",
      "Operator 9: 1.1048082140764637e-08\n",
      "Operator 11: -2.0043680160064525e-08\n",
      "Operator 12: 2.108511709675298e-08\n",
      "Operator 13: 7.418106279057617e-08\n",
      "Operator 14: -2.6866413415363725e-08\n",
      "Operator 15: -2.7860291379466766e-08\n",
      "Operator 16: 1.899031831786857e-08\n",
      "Operator 17: -1.6039096463028148e-08\n",
      "Operator 18: 2.0947672205691907e-08\n",
      "Operator 20: 1.0552776403734396e-08\n",
      "Operator 21: -1.592260618256249e-08\n",
      "Operator 22: -1.346920713810989e-08\n",
      "Operator 23: 1.5169160150207972e-08\n",
      "Operator 24: -2.1085112766883185e-08\n",
      "Operator 25: 1.4260088221362821e-08\n",
      "Operator 26: -4.278266366629908e-08\n",
      "Operator 27: -2.6402322301670424e-08\n",
      "Operator 28: -2.0000000000000075\n",
      "Operator 29: 2.0000000000000053\n",
      "Operator 30: -2.000000000000003\n",
      "Operator 31: 2.000000000000003\n",
      "Operator 32: -2.000000000000003\n",
      "Operator 33: 2.0000000000000133\n",
      "Operator 34: -2.0000000000000138\n",
      "Operator 35: 2.0000000000000044\n",
      "Operator 36: -1.8213672047742233\n",
      "Operator 37: -1.6778310704457486e-08\n",
      "Operator 39: 1.8213672037406288\n",
      "Operator 40: -1.9999999999999962\n",
      "Operator 41: -2.0000000000000093\n",
      "Operator 42: 2.0000000000000053\n",
      "Operator 43: -2.000000000000004\n",
      "Operator 44: 2.0000000000000036\n",
      "Operator 45: -2.0000000000000053\n",
      "Operator 46: 2.00000000000001\n",
      "Operator 47: -2.000000000000016\n",
      "Operator 48: 2.000000000000007\n",
      "Operator 49: -1.8213672037406372\n",
      "Operator 52: 1.8213672036402093\n",
      "Operator 53: -1.999999999999997\n",
      "Operator 54: -1.4260088221362821e-08\n",
      "Operator 55: 2.0000000000000075\n",
      "Operator 56: -2.000000000000004\n",
      "Operator 57: 2.000000000000003\n",
      "Operator 58: -2.0000000000000053\n",
      "Operator 59: 2.000000000000003\n",
      "Operator 60: -2.000000000000016\n",
      "Operator 61: 2.0000000000000133\n",
      "Operator 62: -2.0000000000000044\n",
      "Operator 63: 1.8213672036402195\n",
      "Operator 66: -1.8213672037406305\n",
      "Operator 67: 1.9999999999999964\n",
      "Operator 68: 2.0000000000000098\n",
      "Operator 69: -2.0000000000000107\n",
      "Operator 70: 2.0000000000000036\n",
      "Operator 71: -2.0000000000000058\n",
      "Operator 72: 2.0000000000000053\n",
      "Operator 73: -2.0000000000000027\n",
      "Operator 74: 2.000000000000016\n",
      "Operator 75: -2.0000000000000133\n",
      "Operator 76: 2.0000000000000044\n",
      "Operator 77: -1.821367203640217\n",
      "Operator 79: 1.725350340382845e-08\n",
      "Operator 80: 1.821367204774214\n",
      "Operator 81: -1.9999999999999951\n",
      "Operator 82: 2.0000000000000093\n",
      "Operator 84: 2.000000000000004\n",
      "Operator 86: 2.0000000000000053\n",
      "Operator 88: 2.000000000000016\n",
      "Operator 90: 1.8213672036402175\n",
      "Operator 91: -0.48803387117117314\n",
      "Operator 94: 1.82136720374063\n",
      "Operator 95: -4.278266298891314e-08\n",
      "Operator 97: 7.418106231954843e-08\n",
      "Operator 98: 1.3994263192707022e-08\n",
      "Operator 99: -2.7860291434977917e-08\n",
      "Operator 101: -1.60390955994773e-08\n",
      "Operator 105: -1.592260600650483e-08\n",
      "Operator 107: 2.0043680160064525e-08\n",
      "Operator 108: 2.0000000000000107\n",
      "Operator 109: -2.0000000000000053\n",
      "Operator 110: 2.0000000000000053\n",
      "Operator 111: -2.0000000000000036\n",
      "Operator 112: 2.000000000000003\n",
      "Operator 113: -2.00000000000001\n",
      "Operator 114: 2.0000000000000133\n",
      "Operator 115: -2.000000000000007\n",
      "Operator 116: 1.8213672037406372\n",
      "Operator 118: -1.6778311079610932e-08\n",
      "Operator 119: -1.821367204774213\n",
      "Operator 120: 1.9999999999999947\n",
      "Operator 121: 4.278266378666926e-08\n",
      "Operator 123: -7.418106318309929e-08\n",
      "Operator 124: -1.3994263137195871e-08\n",
      "Operator 125: 2.7860291434977917e-08\n",
      "Operator 127: 1.6039096463028148e-08\n",
      "Operator 131: 1.10480809116292e-08\n",
      "Operator 133: -1.5169160150207972e-08\n",
      "Operator 134: -2.0000000000000107\n",
      "Operator 136: -2.0000000000000053\n",
      "Operator 138: -2.0000000000000027\n",
      "Operator 140: -2.0000000000000133\n",
      "Operator 142: -1.821367204774221\n",
      "Operator 143: 0.4880338754033326\n",
      "Operator 144: 1.7253503335493823e-08\n",
      "Operator 146: -1.8213672037406283\n",
      "Operator 147: -2.0000000000000098\n",
      "Operator 149: -2.0000000000000036\n",
      "Operator 151: -2.0000000000000053\n",
      "Operator 153: -2.000000000000016\n",
      "Operator 155: -1.8213672036402169\n",
      "Operator 156: 0.48803387117117264\n",
      "Operator 159: -1.8213672047742135\n",
      "Operator 160: 2.0000000000000107\n",
      "Operator 162: 2.0000000000000053\n",
      "Operator 164: 2.0000000000000027\n",
      "Operator 166: 2.0000000000000133\n",
      "Operator 168: 1.8213672047742209\n",
      "Operator 169: -0.4880338754033322\n",
      "Operator 170: -1.6778309832244654e-08\n",
      "Operator 172: 1.8213672036402087\n",
      "Operator 173: -2.0000000000000107\n",
      "Operator 175: -2.0000000000000053\n",
      "Operator 177: -2.000000000000003\n",
      "Operator 179: -2.0000000000000133\n",
      "Operator 181: -1.8213672037406372\n",
      "Operator 183: 1.7253502205904756e-08\n",
      "Operator 184: 0.48803387540332926\n",
      "Operator 185: -1.8213672047742127\n",
      "Operator 186: 4.2782662962291696e-08\n",
      "Operator 187: 2.0000000000000093\n",
      "Operator 188: -2.0000000000000053\n",
      "Operator 189: 2.000000000000004\n",
      "Operator 190: -2.000000000000003\n",
      "Operator 191: 2.0000000000000053\n",
      "Operator 192: -2.0000000000000133\n",
      "Operator 193: 2.0000000000000155\n",
      "Operator 194: -2.0000000000000044\n",
      "Operator 195: 1.8213672047742206\n",
      "Operator 196: 1.7253502343351774e-08\n",
      "Operator 198: -1.821367203640209\n",
      "Operator 199: 1.999999999999997\n",
      "Operator 200: 2.0000000000000098\n",
      "Operator 201: -2.0000000000000093\n",
      "Operator 203: -2.000000000000003\n",
      "Operator 205: -2.0000000000000053\n",
      "Operator 207: -2.000000000000016\n",
      "Operator 209: -1.821367204774221\n",
      "Operator 212: 0.48803387117117025\n",
      "Operator 213: -1.8213672036402102\n",
      "Operator 214: 2.000000000000006\n",
      "Operator 216: 2.0000000000000053\n",
      "Operator 218: 2.000000000000004\n",
      "Operator 220: 2.000000000000011\n",
      "Operator 222: 1.8213672037406392\n",
      "Operator 223: -0.4880338715459448\n",
      "Operator 226: 1.821367204774213\n",
      "Operator 227: -2.0000000000000098\n",
      "Total gradient norm: 20.686205070668915\n",
      "Operators under consideration (1):\n",
      "[218]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.000000000000004)]\n",
      "Operator(s) added to ansatz: [218]\n",
      "Gradients: [np.float64(2.000000000000004)]\n",
      "Initial energy: -24.464101615137807\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218]...\n",
      "Starting point: [np.float64(0.7853981659496168), np.float64(0.7853981616590959), np.float64(0.7853981713630617), np.float64(0.7853981621484368), np.float64(-0.7853981598988818), np.float64(-0.7853981617820109), np.float64(-0.7853981620324183), np.float64(-0.7853981663428905), np.float64(-0.16991845577776366), np.float64(-0.1308996938504836), np.float64(0.13089969395336623), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.587207\n",
      "         Iterations: 4\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 32\n",
      "\n",
      "Current energy: -24.58720724075537\n",
      "(change of -0.123105625617562)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.2407590283016018e-07\n",
      "Operator 1: -3.9034132062743003e-07\n",
      "Operator 2: 9.783725106460314e-08\n",
      "Operator 3: 1.171360293761304e-07\n",
      "Operator 4: -2.0619687984835282e-07\n",
      "Operator 5: 2.470368869854882e-07\n",
      "Operator 6: -1.8779057705945906e-07\n",
      "Operator 7: -4.913428139859106e-08\n",
      "Operator 9: -1.8583396820931672e-08\n",
      "Operator 10: 1.3630032428087673e-08\n",
      "Operator 11: -8.603231249626475e-08\n",
      "Operator 12: 1.7233965254881412e-07\n",
      "Operator 13: 3.9034131996129627e-07\n",
      "Operator 14: -9.783725984924281e-08\n",
      "Operator 15: -1.1713602982021961e-07\n",
      "Operator 16: 2.0619688095857585e-07\n",
      "Operator 17: -2.470368861219374e-07\n",
      "Operator 18: 1.877905732958591e-07\n",
      "Operator 19: 4.913428139859106e-08\n",
      "Operator 21: -1.3356850524930083e-08\n",
      "Operator 22: -1.0089240226120437e-07\n",
      "Operator 23: 5.409207681772443e-08\n",
      "Operator 24: -1.7233964844098892e-07\n",
      "Operator 25: 1.1001818478462155e-07\n",
      "Operator 26: -2.1103904107977717e-07\n",
      "Operator 27: -1.240758888838062e-07\n",
      "Operator 28: -2.0\n",
      "Operator 29: 1.9999999999999973\n",
      "Operator 30: -1.940285047774497\n",
      "Operator 31: 2.4850710601372823\n",
      "Operator 32: -8.072253584936196e-07\n",
      "Operator 33: 2.0000000000000124\n",
      "Operator 34: -1.9999999999999982\n",
      "Operator 35: 2.000000000000001\n",
      "Operator 36: -1.8213672675416261\n",
      "Operator 37: -4.580006177239377e-07\n",
      "Operator 38: -5.746097986575464e-07\n",
      "Operator 39: 1.821367209070751\n",
      "Operator 40: -1.9999999999999807\n",
      "Operator 41: -2.0000000000000036\n",
      "Operator 42: 2.000000000000009\n",
      "Operator 43: -1.999999999999996\n",
      "Operator 44: 8.072253540929604e-07\n",
      "Operator 45: -2.485071060137283\n",
      "Operator 46: 1.9402850477744975\n",
      "Operator 47: -2.0000000000000147\n",
      "Operator 48: 2.0000000000000018\n",
      "Operator 49: -1.8213672090707544\n",
      "Operator 50: 2.9500916932043484e-07\n",
      "Operator 51: -3.058825120829368e-07\n",
      "Operator 52: 1.8213672084550436\n",
      "Operator 53: -1.9999999999999973\n",
      "Operator 54: -1.1001818478462155e-07\n",
      "Operator 55: 2.0000000000000004\n",
      "Operator 56: -1.999999999999996\n",
      "Operator 57: 1.9402850477744964\n",
      "Operator 58: -2.485071060137284\n",
      "Operator 59: 8.072253595388072e-07\n",
      "Operator 60: -2.0000000000000147\n",
      "Operator 61: 1.9999999999999982\n",
      "Operator 62: -2.0\n",
      "Operator 63: 1.821367208455062\n",
      "Operator 64: -3.058825120907226e-07\n",
      "Operator 65: 2.9500917378577665e-07\n",
      "Operator 66: -1.8213672090707527\n",
      "Operator 67: 1.9999999999999807\n",
      "Operator 68: 2.0000000000000036\n",
      "Operator 69: -1.9999999999999998\n",
      "Operator 70: 1.9999999999999942\n",
      "Operator 71: -1.9402850477745013\n",
      "Operator 72: 1.9402850477745006\n",
      "Operator 73: -1.9402850477744935\n",
      "Operator 74: 1.9402850477745064\n",
      "Operator 75: -2.0000000000000093\n",
      "Operator 76: 2.000000000000001\n",
      "Operator 77: -1.8213672084550598\n",
      "Operator 78: 5.825696338960086e-07\n",
      "Operator 79: 4.609141201805997e-07\n",
      "Operator 80: 1.8213672675416166\n",
      "Operator 81: -1.99999999999999\n",
      "Operator 82: 1.9999999999999998\n",
      "Operator 84: 1.9402850477745\n",
      "Operator 86: 1.940285047774496\n",
      "Operator 88: 2.000000000000011\n",
      "Operator 90: 1.8213672084550598\n",
      "Operator 91: -0.48803377413206395\n",
      "Operator 92: 5.825696310257686e-07\n",
      "Operator 94: 1.8213672090707465\n",
      "Operator 95: -2.1103904001262008e-07\n",
      "Operator 96: 5.522637536864443e-08\n",
      "Operator 97: 3.903413210715193e-07\n",
      "Operator 98: 1.1384713621831474e-07\n",
      "Operator 99: -1.171360290430635e-07\n",
      "Operator 100: 4.575023925828552e-08\n",
      "Operator 101: -2.470368861219374e-07\n",
      "Operator 102: -2.819602359977638e-08\n",
      "Operator 103: 4.474578412128676e-08\n",
      "Operator 104: -6.594698409035835e-08\n",
      "Operator 105: -1.3356849632806605e-08\n",
      "Operator 107: 8.603231249626475e-08\n",
      "Operator 108: 2.0000000000000036\n",
      "Operator 109: -2.000000000000008\n",
      "Operator 110: 1.9999999999999967\n",
      "Operator 111: -8.07225353204782e-07\n",
      "Operator 112: 2.4850710601372823\n",
      "Operator 113: -1.940285047774498\n",
      "Operator 114: 2.000000000000012\n",
      "Operator 115: -2.0000000000000018\n",
      "Operator 116: 1.8213672090707553\n",
      "Operator 117: -5.746097950011601e-07\n",
      "Operator 118: -4.580006176091795e-07\n",
      "Operator 119: -1.8213672675416084\n",
      "Operator 120: 1.9999999999999953\n",
      "Operator 121: 2.1103904072504938e-07\n",
      "Operator 122: -5.522637542415558e-08\n",
      "Operator 123: -3.9034132065518566e-07\n",
      "Operator 124: -1.138471343309356e-07\n",
      "Operator 125: 1.171360293761304e-07\n",
      "Operator 126: -4.575023909569713e-08\n",
      "Operator 127: 2.470368861219374e-07\n",
      "Operator 128: 2.8196023597353225e-08\n",
      "Operator 129: -4.474578537736069e-08\n",
      "Operator 130: 6.594698385821182e-08\n",
      "Operator 131: -1.8583397356994767e-08\n",
      "Operator 133: -5.409207726181364e-08\n",
      "Operator 134: -1.9999999999999998\n",
      "Operator 136: -1.9402850477745013\n",
      "Operator 138: -1.9402850477744937\n",
      "Operator 140: -2.0000000000000093\n",
      "Operator 142: -1.8213672675416248\n",
      "Operator 143: 0.4880339946461224\n",
      "Operator 144: 4.60914120124277e-07\n",
      "Operator 146: -1.8213672090707453\n",
      "Operator 147: -1.9999999999999991\n",
      "Operator 149: -1.9402850477744937\n",
      "Operator 151: -2.4850710601372743\n",
      "Operator 153: -1.940285047774506\n",
      "Operator 155: -1.8213672084550594\n",
      "Operator 156: 0.48803377413206317\n",
      "Operator 157: -3.058825094092831e-07\n",
      "Operator 159: -1.8213672675416146\n",
      "Operator 160: 2.0000000000000004\n",
      "Operator 162: 1.9402850477744957\n",
      "Operator 164: 2.4850710601372725\n",
      "Operator 166: 1.9402850477745037\n",
      "Operator 168: 1.8213672675416246\n",
      "Operator 169: -0.4880339946461237\n",
      "Operator 170: -4.5800061653699677e-07\n",
      "Operator 172: 1.8213672084550485\n",
      "Operator 173: -2.0000000000000058\n",
      "Operator 175: -1.9999999999999951\n",
      "Operator 177: -1.9402850477744988\n",
      "Operator 179: -1.940285047774504\n",
      "Operator 181: -1.8213672090707553\n",
      "Operator 183: 4.6091412088675004e-07\n",
      "Operator 184: 0.48803399464611863\n",
      "Operator 185: -1.821367267541615\n",
      "Operator 186: 2.1103904016053193e-07\n",
      "Operator 187: 2.0\n",
      "Operator 188: -1.9999999999999951\n",
      "Operator 189: 1.9402850477745002\n",
      "Operator 190: -1.9402850477744984\n",
      "Operator 191: 1.9402850477744957\n",
      "Operator 192: -1.940285047774504\n",
      "Operator 193: 2.0000000000000115\n",
      "Operator 194: -2.0000000000000013\n",
      "Operator 195: 1.8213672675416244\n",
      "Operator 196: 4.609141210112347e-07\n",
      "Operator 197: 5.825696310778693e-07\n",
      "Operator 198: -1.8213672084550505\n",
      "Operator 199: 1.9999999999999911\n",
      "Operator 200: 2.0000000000000053\n",
      "Operator 201: -2.0\n",
      "Operator 203: -1.9402850477744937\n",
      "Operator 205: -2.4850710601372747\n",
      "Operator 207: -1.9402850477745062\n",
      "Operator 209: -1.8213672675416241\n",
      "Operator 211: -3.058825088126213e-07\n",
      "Operator 212: 0.48803377413206117\n",
      "Operator 213: -1.82136720845505\n",
      "Operator 214: 2.000000000000008\n",
      "Operator 216: 2.000000000000007\n",
      "Operator 217: -0.48507106013727136\n",
      "Operator 218: 8.072253532277756e-07\n",
      "Operator 220: 1.940285047774499\n",
      "Operator 222: 1.8213672090707558\n",
      "Operator 223: -0.4880337764298692\n",
      "Operator 224: 2.950091697699779e-07\n",
      "Operator 226: 1.8213672675416082\n",
      "Operator 227: -2.0000000000000036\n",
      "Total gradient norm: 20.432843062001503\n",
      "Operators under consideration (1):\n",
      "[205]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.4850710601372747)]\n",
      "Operator(s) added to ansatz: [205]\n",
      "Gradients: [np.float64(-2.4850710601372747)]\n",
      "Initial energy: -24.58720724075537\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205]...\n",
      "Starting point: [np.float64(0.7853981834709644), np.float64(0.7853981541924298), np.float64(0.7853982016633569), np.float64(0.785398158601591), np.float64(-0.7853981256548216), np.float64(-0.7853981653164339), np.float64(-0.7853981561505164), np.float64(-0.7853981869272766), np.float64(-0.1699184707530614), np.float64(-0.13089966863007496), np.float64(0.1308996692608669), np.float64(-0.12248928261820427), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.791378\n",
      "         Iterations: 10\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 47\n",
      "\n",
      "Current energy: -24.791377769959226\n",
      "(change of -0.20417052920385714)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.3408617745422013e-07\n",
      "Operator 1: -2.1795684407993637e-07\n",
      "Operator 3: 1.1980541883712368e-07\n",
      "Operator 4: -2.896224243850712e-07\n",
      "Operator 5: 2.9401014731255847e-07\n",
      "Operator 6: -2.4705771079452235e-07\n",
      "Operator 7: -1.780169064520872e-08\n",
      "Operator 8: -7.491276823593774e-08\n",
      "Operator 9: 7.061379318054152e-08\n",
      "Operator 10: 1.4204535163764648e-08\n",
      "Operator 11: -2.509674239536963e-07\n",
      "Operator 12: 2.921672470623804e-07\n",
      "Operator 13: 2.1795684411463084e-07\n",
      "Operator 15: -1.1997192079691275e-07\n",
      "Operator 16: 3.275030612987947e-07\n",
      "Operator 17: -2.415221826392582e-07\n",
      "Operator 18: 2.4705770642982394e-07\n",
      "Operator 19: 1.780169064520872e-08\n",
      "Operator 20: 7.491276935405086e-08\n",
      "Operator 21: -1.3652646240498842e-07\n",
      "Operator 22: -1.9428128953464008e-07\n",
      "Operator 23: 1.8505475951613448e-07\n",
      "Operator 24: -2.921672429545552e-07\n",
      "Operator 25: 1.5713881751366898e-07\n",
      "Operator 27: -1.340861639157876e-07\n",
      "Operator 28: -2.000000000000007\n",
      "Operator 29: 2.0000000000000093\n",
      "Operator 30: -1.793486682365697\n",
      "Operator 31: 2.0894914430597655\n",
      "Operator 32: 0.7090309191075324\n",
      "Operator 33: 1.8939306161806804\n",
      "Operator 34: -2.0000000000000053\n",
      "Operator 35: 2.0000000000000124\n",
      "Operator 36: -1.8213671983798683\n",
      "Operator 37: -8.418243513816992e-08\n",
      "Operator 38: 2.8794969705619587e-08\n",
      "Operator 39: 1.821367195243599\n",
      "Operator 40: -1.9999999999999787\n",
      "Operator 41: -2.000000000000019\n",
      "Operator 42: 2.000000000000016\n",
      "Operator 43: -1.89393061618066\n",
      "Operator 44: 8.803091730011588e-07\n",
      "Operator 45: 7.121100151405049e-07\n",
      "Operator 46: 1.8939307142966773\n",
      "Operator 47: -2.00000000000003\n",
      "Operator 48: 2.0000000000000133\n",
      "Operator 49: -1.8213671952435964\n",
      "Operator 50: -4.3299021827226504e-08\n",
      "Operator 51: 4.4556621173687065e-08\n",
      "Operator 52: 1.821367195314771\n",
      "Operator 53: -2.000000000000016\n",
      "Operator 54: -1.5713881751366898e-07\n",
      "Operator 55: 2.000000000000007\n",
      "Operator 56: -2.000000000000008\n",
      "Operator 57: 1.8939307142966892\n",
      "Operator 58: 7.121100158192064e-07\n",
      "Operator 59: 8.803091794305018e-07\n",
      "Operator 60: -1.8939306161806821\n",
      "Operator 61: 2.0000000000000053\n",
      "Operator 62: -2.0000000000000124\n",
      "Operator 63: 1.8213671953148038\n",
      "Operator 64: 4.455662091357262e-08\n",
      "Operator 65: -4.3299020777278284e-08\n",
      "Operator 66: -1.8213671952436004\n",
      "Operator 67: 1.9999999999999787\n",
      "Operator 68: 2.0000000000000195\n",
      "Operator 69: -2.000000000000016\n",
      "Operator 70: 2.00000000000001\n",
      "Operator 71: -1.8939307142966886\n",
      "Operator 72: -0.70903210807267\n",
      "Operator 73: -2.0894918465138144\n",
      "Operator 74: 1.7934866823656996\n",
      "Operator 75: -2.0000000000000195\n",
      "Operator 76: 2.000000000000011\n",
      "Operator 77: -1.8213671953148074\n",
      "Operator 78: -2.9715592102854256e-08\n",
      "Operator 79: 8.384546093212148e-08\n",
      "Operator 80: 1.8213671983798627\n",
      "Operator 81: -1.9999999999999978\n",
      "Operator 82: 2.000000000000015\n",
      "Operator 84: 1.8939307142966875\n",
      "Operator 85: -0.6085877674857109\n",
      "Operator 86: -0.7090321080726827\n",
      "Operator 88: 1.8939306161806742\n",
      "Operator 90: 1.821367195314807\n",
      "Operator 91: -0.4880338767213489\n",
      "Operator 92: -2.9715605335776833e-08\n",
      "Operator 94: 1.8213671952435848\n",
      "Operator 96: 9.10112605461278e-08\n",
      "Operator 97: 2.1632140667984822e-07\n",
      "Operator 98: 1.4305591780416183e-07\n",
      "Operator 99: -1.1997192006746153e-07\n",
      "Operator 100: 7.944473769307315e-08\n",
      "Operator 101: -2.9401014731255847e-07\n",
      "Operator 103: 1.6211705909305236e-08\n",
      "Operator 104: -1.1705113066969718e-07\n",
      "Operator 105: -1.365264610418372e-07\n",
      "Operator 107: 2.509674239536963e-07\n",
      "Operator 108: 2.0000000000000195\n",
      "Operator 109: -2.000000000000016\n",
      "Operator 110: 1.8939306161806615\n",
      "Operator 111: 0.7090309191075391\n",
      "Operator 112: 2.089491443059766\n",
      "Operator 113: -1.7934866823656825\n",
      "Operator 114: 2.0000000000000275\n",
      "Operator 115: -2.0000000000000138\n",
      "Operator 116: 1.8213671952435964\n",
      "Operator 117: 2.879496619238908e-08\n",
      "Operator 118: -8.418243536359897e-08\n",
      "Operator 119: -1.8213671983798343\n",
      "Operator 120: 2.000000000000014\n",
      "Operator 122: -9.101126051022171e-08\n",
      "Operator 123: -2.1632140667984822e-07\n",
      "Operator 124: -1.430559149043005e-07\n",
      "Operator 125: 1.1980541905851777e-07\n",
      "Operator 127: 2.415221826622517e-07\n",
      "Operator 129: -1.6211705909305236e-08\n",
      "Operator 130: 1.170511304997964e-07\n",
      "Operator 131: 7.061379388313806e-08\n",
      "Operator 133: -1.8505475951613448e-07\n",
      "Operator 134: -2.0000000000000155\n",
      "Operator 136: -1.7934866823656959\n",
      "Operator 138: -2.0894918465138144\n",
      "Operator 140: -1.893930616180672\n",
      "Operator 142: -1.8213671983798738\n",
      "Operator 143: 0.48803388816032733\n",
      "Operator 144: 8.384546071341463e-08\n",
      "Operator 146: -1.8213671952435833\n",
      "Operator 147: -2.000000000000015\n",
      "Operator 149: -1.893930714296686\n",
      "Operator 150: 0.60858776748572\n",
      "Operator 151: 7.121100339638011e-07\n",
      "Operator 153: -1.7934866823656983\n",
      "Operator 155: -1.821367195314806\n",
      "Operator 156: 0.48803387672134835\n",
      "Operator 157: 4.45566311390112e-08\n",
      "Operator 159: -1.8213671983798574\n",
      "Operator 160: 2.0000000000000155\n",
      "Operator 162: 1.7934866823656943\n",
      "Operator 164: 2.0894914430597504\n",
      "Operator 166: 1.893930714296689\n",
      "Operator 168: 1.821367198379873\n",
      "Operator 169: -0.4880338881603361\n",
      "Operator 170: -8.418243220384863e-08\n",
      "Operator 172: 1.8213671953147885\n",
      "Operator 173: -2.000000000000017\n",
      "Operator 175: -1.893930616180664\n",
      "Operator 177: -2.0894918465138206\n",
      "Operator 179: -1.7934866823656979\n",
      "Operator 181: -1.8213671952435952\n",
      "Operator 183: 8.384545935773517e-08\n",
      "Operator 184: 0.48803388816032195\n",
      "Operator 185: -1.8213671983798605\n",
      "Operator 187: 2.0000000000000155\n",
      "Operator 188: -2.000000000000011\n",
      "Operator 189: 1.7934866823656945\n",
      "Operator 190: -2.0894918465138206\n",
      "Operator 191: -0.7090321080726827\n",
      "Operator 192: -1.8939307142966908\n",
      "Operator 193: 2.000000000000022\n",
      "Operator 194: -2.000000000000011\n",
      "Operator 195: 1.8213671983798747\n",
      "Operator 196: 8.384545964356901e-08\n",
      "Operator 197: -2.9715605993844618e-08\n",
      "Operator 198: -1.8213671953147939\n",
      "Operator 199: 1.9999999999999991\n",
      "Operator 200: 2.000000000000017\n",
      "Operator 201: -2.0000000000000147\n",
      "Operator 203: -1.7934866823656925\n",
      "Operator 205: 7.121100339081038e-07\n",
      "Operator 206: 0.6085877674857243\n",
      "Operator 207: -1.8939307142966912\n",
      "Operator 209: -1.8213671983798734\n",
      "Operator 211: 4.4556630757361455e-08\n",
      "Operator 212: 0.4880338767213478\n",
      "Operator 213: -1.82136719531479\n",
      "Operator 214: 2.000000000000006\n",
      "Operator 216: 1.8939306161806693\n",
      "Operator 217: -0.608587462147679\n",
      "Operator 218: 8.803091725219035e-07\n",
      "Operator 220: 1.7934866823656825\n",
      "Operator 222: 1.8213671952435977\n",
      "Operator 223: -0.48803387645558005\n",
      "Operator 224: -4.3299020878358624e-08\n",
      "Operator 226: 1.8213671983798343\n",
      "Operator 227: -2.0000000000000187\n",
      "Total gradient norm: 19.28817180460628\n",
      "Operators under consideration (1):\n",
      "[177]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.0894918465138206)]\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.0894918465138206)]\n",
      "Initial energy: -24.791377769959226\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177]...\n",
      "Starting point: [np.float64(0.7853982011298112), np.float64(0.7853981443869037), np.float64(0.7853981851501), np.float64(0.785398153344998), np.float64(-0.7853981182491395), np.float64(-0.7853981535794056), np.float64(-0.7853981751663435), np.float64(-0.7853982008396282), np.float64(-0.16991846085603363), np.float64(-0.1308996958551998), np.float64(0.13089969578224311), np.float64(-0.16356981181136868), np.float64(0.16356988814587814), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.928203\n",
      "         Iterations: 18\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 81\n",
      "\n",
      "Current energy: -24.92820323027548\n",
      "(change of -0.13682546031625265)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -4.995495447857051e-08\n",
      "Operator 1: -3.67580527793621e-07\n",
      "Operator 2: 1.8564301479395716e-07\n",
      "Operator 3: -9.348328985914211e-08\n",
      "Operator 4: 9.709804124631147e-08\n",
      "Operator 5: 1.0139115210219112e-07\n",
      "Operator 6: -1.6273354344597657e-07\n",
      "Operator 7: 3.0158975700551626e-07\n",
      "Operator 8: 1.5321488188353776e-08\n",
      "Operator 9: -2.3623097520562769e-07\n",
      "Operator 10: 4.189292256826492e-08\n",
      "Operator 11: -3.582353848008424e-07\n",
      "Operator 13: 3.67580527793621e-07\n",
      "Operator 14: -1.8564302348145233e-07\n",
      "Operator 15: 1.243522673999299e-07\n",
      "Operator 16: -1.2762432774423454e-08\n",
      "Operator 17: -7.052217412295384e-08\n",
      "Operator 18: 1.6273353935883397e-07\n",
      "Operator 19: -3.0158975700551626e-07\n",
      "Operator 20: -1.5321487629297204e-08\n",
      "Operator 21: 1.852989985173995e-07\n",
      "Operator 22: -1.8104169114425112e-07\n",
      "Operator 23: 3.0730340050411087e-07\n",
      "Operator 25: -4.4230250040300234e-07\n",
      "Operator 26: -4.773613358766143e-07\n",
      "Operator 27: 4.9954968633914074e-08\n",
      "Operator 28: -2.0000000000000178\n",
      "Operator 29: 2.0000000000000084\n",
      "Operator 30: -1.821367207621861\n",
      "Operator 33: 1.8213672077802456\n",
      "Operator 34: -1.9999999999999987\n",
      "Operator 35: 1.9999999999999971\n",
      "Operator 36: -1.8213672587058143\n",
      "Operator 37: 7.338182759720665e-07\n",
      "Operator 38: -2.2199371984056634e-07\n",
      "Operator 39: 1.8213672876875058\n",
      "Operator 40: -2.000000000000013\n",
      "Operator 41: -1.9999999999999964\n",
      "Operator 42: 1.9999999999999876\n",
      "Operator 43: -1.8213672077802392\n",
      "Operator 45: -3.2495404340883636e-08\n",
      "Operator 46: 1.8213672057257448\n",
      "Operator 47: -2.0000000000000178\n",
      "Operator 48: 2.0000000000000147\n",
      "Operator 49: -1.8213672876875173\n",
      "Operator 50: 3.59566679702522e-07\n",
      "Operator 51: -3.5796192660534804e-07\n",
      "Operator 52: 1.8213672877783924\n",
      "Operator 53: -1.9999999999999858\n",
      "Operator 54: 4.4230250040300234e-07\n",
      "Operator 55: 2.0000000000000178\n",
      "Operator 56: -2.000000000000007\n",
      "Operator 57: 1.8213672057257344\n",
      "Operator 58: -3.249540402778121e-08\n",
      "Operator 60: -1.821367207780248\n",
      "Operator 61: 1.999999999999999\n",
      "Operator 62: -1.999999999999997\n",
      "Operator 63: 1.8213672877784\n",
      "Operator 64: -3.579619270772345e-07\n",
      "Operator 65: 3.595666856029145e-07\n",
      "Operator 66: -1.821367287687507\n",
      "Operator 67: 2.0000000000000133\n",
      "Operator 68: 1.9999999999999973\n",
      "Operator 69: -2.0000000000000044\n",
      "Operator 70: 1.9999999999999996\n",
      "Operator 71: -1.8213672057257475\n",
      "Operator 72: 3.1746040071808434e-08\n",
      "Operator 74: 1.8213672076218859\n",
      "Operator 75: -2.000000000000009\n",
      "Operator 76: 2.000000000000001\n",
      "Operator 77: -1.8213672877783904\n",
      "Operator 78: 2.2081895796299829e-07\n",
      "Operator 79: -7.342482676611575e-07\n",
      "Operator 80: 1.8213672587057987\n",
      "Operator 81: -1.9999999999999898\n",
      "Operator 82: 2.0000000000000036\n",
      "Operator 84: 1.8213672057257462\n",
      "Operator 85: -0.48803386480125693\n",
      "Operator 86: 3.1746043950973044e-08\n",
      "Operator 88: 1.8213672077802427\n",
      "Operator 90: 1.82136728777839\n",
      "Operator 91: -0.4880338349810873\n",
      "Operator 92: 2.208189515316512e-07\n",
      "Operator 94: 1.8213672876875124\n",
      "Operator 95: -4.773613349329247e-07\n",
      "Operator 96: -5.982585938328385e-08\n",
      "Operator 97: 3.235386367705932e-07\n",
      "Operator 99: 1.2435226723339645e-07\n",
      "Operator 101: -1.0139115204667997e-07\n",
      "Operator 102: -1.275719249824639e-07\n",
      "Operator 103: -2.7465285794703527e-07\n",
      "Operator 104: -1.148503559451114e-07\n",
      "Operator 105: 1.8529899878543104e-07\n",
      "Operator 107: 3.582353845787978e-07\n",
      "Operator 108: 1.9999999999999978\n",
      "Operator 109: -1.9999999999999876\n",
      "Operator 110: 1.82136720778024\n",
      "Operator 113: -1.8213672076218714\n",
      "Operator 114: 2.0000000000000147\n",
      "Operator 115: -2.0000000000000147\n",
      "Operator 116: 1.821367287687518\n",
      "Operator 117: -2.2199371825436138e-07\n",
      "Operator 118: 7.33818276120654e-07\n",
      "Operator 119: -1.8213672587058123\n",
      "Operator 120: 1.999999999999984\n",
      "Operator 121: 4.773613360431478e-07\n",
      "Operator 122: 5.98258594191899e-08\n",
      "Operator 123: -3.235386367705932e-07\n",
      "Operator 125: -9.348328994240884e-08\n",
      "Operator 127: 7.052217406744269e-08\n",
      "Operator 128: 1.2757192498004085e-07\n",
      "Operator 129: 2.7465285794703527e-07\n",
      "Operator 130: 1.1485035551391371e-07\n",
      "Operator 131: -2.3623097613026884e-07\n",
      "Operator 133: -3.0730340050411087e-07\n",
      "Operator 134: -2.0000000000000044\n",
      "Operator 136: -1.8213672076218757\n",
      "Operator 137: 0.4880338718776972\n",
      "Operator 140: -1.8213672077802405\n",
      "Operator 142: -1.821367258705808\n",
      "Operator 143: 0.4880337264806992\n",
      "Operator 144: -7.342482677810904e-07\n",
      "Operator 146: -1.821367287687511\n",
      "Operator 147: -1.999999999999996\n",
      "Operator 149: -1.8213672057257428\n",
      "Operator 150: 0.4880338648012546\n",
      "Operator 151: -3.249540278749429e-08\n",
      "Operator 153: -1.8213672076218828\n",
      "Operator 155: -1.8213672877783869\n",
      "Operator 156: 0.48803383498108466\n",
      "Operator 157: -3.5796191834308083e-07\n",
      "Operator 159: -1.8213672587057936\n",
      "Operator 160: 1.9999999999999971\n",
      "Operator 162: 1.821367207621872\n",
      "Operator 163: -0.488033871877702\n",
      "Operator 166: 1.8213672057257526\n",
      "Operator 168: 1.8213672587058047\n",
      "Operator 169: -0.48803372648070686\n",
      "Operator 170: 7.338182680634323e-07\n",
      "Operator 172: 1.8213672877783758\n",
      "Operator 173: -2.0000000000000013\n",
      "Operator 175: -1.8213672077802325\n",
      "Operator 178: 0.488033871877695\n",
      "Operator 179: -1.8213672076218836\n",
      "Operator 181: -1.8213672876875213\n",
      "Operator 183: -7.342482651045225e-07\n",
      "Operator 184: 0.4880337264807066\n",
      "Operator 185: -1.8213672587057976\n",
      "Operator 186: 4.773613349329247e-07\n",
      "Operator 187: 2.0000000000000036\n",
      "Operator 188: -2.000000000000001\n",
      "Operator 189: 1.8213672076218745\n",
      "Operator 191: 3.174604362972348e-08\n",
      "Operator 192: -1.8213672057257548\n",
      "Operator 193: 2.0000000000000115\n",
      "Operator 194: -2.000000000000001\n",
      "Operator 195: 1.8213672587058072\n",
      "Operator 196: -7.342482655189626e-07\n",
      "Operator 197: 2.2081895154053433e-07\n",
      "Operator 198: -1.8213672877783802\n",
      "Operator 199: 1.999999999999991\n",
      "Operator 200: 2.0000000000000004\n",
      "Operator 201: -1.9999999999999964\n",
      "Operator 203: -1.8213672076218717\n",
      "Operator 205: -3.249540306102407e-08\n",
      "Operator 206: 0.48803386480125577\n",
      "Operator 207: -1.8213672057257546\n",
      "Operator 209: -1.8213672587058043\n",
      "Operator 211: -3.5796191844411116e-07\n",
      "Operator 212: 0.488033834981084\n",
      "Operator 213: -1.8213672877783766\n",
      "Operator 214: 2.0000000000000213\n",
      "Operator 216: 1.8213672077802185\n",
      "Operator 217: -0.48803387246870056\n",
      "Operator 220: 1.8213672076218714\n",
      "Operator 222: 1.8213672876875333\n",
      "Operator 223: -0.4880338346419545\n",
      "Operator 224: 3.5956667960292166e-07\n",
      "Operator 226: 1.8213672587058123\n",
      "Operator 227: -1.9999999999999964\n",
      "Total gradient norm: 18.325887890918846\n",
      "Operators under consideration (1):\n",
      "[227]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.9999999999999964)]\n",
      "Operator(s) added to ansatz: [227]\n",
      "Gradients: [np.float64(-1.9999999999999964)]\n",
      "Initial energy: -24.92820323027548\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227]...\n",
      "Starting point: [np.float64(0.7853982007520158), np.float64(0.7853981353348397), np.float64(0.785398194851737), np.float64(0.7853981839870027), np.float64(-0.7853981500110926), np.float64(-0.7853981864063652), np.float64(-0.78539811945442), np.float64(-0.7853981271430431), np.float64(-0.16991840198791783), np.float64(-0.1308996788198713), np.float64(0.1308996787267739), np.float64(-0.1308996939164523), np.float64(0.1699184527141513), np.float64(0.13089969375421046), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.051309\n",
      "         Iterations: 8\n",
      "         Function evaluations: 86\n",
      "         Gradient evaluations: 68\n",
      "\n",
      "Current energy: -25.051308855893307\n",
      "(change of -0.12310562561782845)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.101264784531962e-08\n",
      "Operator 1: -2.330490667720036e-08\n",
      "Operator 3: -1.14610762244117e-08\n",
      "Operator 4: -1.0230915819462784e-08\n",
      "Operator 5: 6.278577657035914e-08\n",
      "Operator 6: -5.500151441281171e-08\n",
      "Operator 7: 2.382978877831704e-08\n",
      "Operator 8: -1.2232595605180207e-08\n",
      "Operator 9: -3.015500503152511e-08\n",
      "Operator 11: 1.1693831902803709e-08\n",
      "Operator 12: 5.248765444498815e-08\n",
      "Operator 13: 2.285507176742751e-08\n",
      "Operator 15: 2.5906273346231733e-08\n",
      "Operator 16: 4.96959293449589e-08\n",
      "Operator 17: -4.8340580370688426e-08\n",
      "Operator 18: 5.5001510362131785e-08\n",
      "Operator 19: -2.382978877831704e-08\n",
      "Operator 20: 1.2232596455261797e-08\n",
      "Operator 21: 2.5187522745754846e-08\n",
      "Operator 22: -1.7191454381164453e-08\n",
      "Operator 23: -1.6661314772292712e-08\n",
      "Operator 24: -5.248764978205145e-08\n",
      "Operator 25: 6.233151239598556e-08\n",
      "Operator 28: -1.940285076991177\n",
      "Operator 29: 1.9402850769911772\n",
      "Operator 30: -1.8213672051821193\n",
      "Operator 33: 1.8213672051620553\n",
      "Operator 34: -2.000000000000023\n",
      "Operator 35: 2.0000000000000173\n",
      "Operator 36: -1.8213672078631897\n",
      "Operator 37: 1.537885602739553e-08\n",
      "Operator 38: -1.7207749693495606e-08\n",
      "Operator 39: 1.8213672077596201\n",
      "Operator 40: -2.000000000000008\n",
      "Operator 41: -1.303908860879655e-06\n",
      "Operator 42: 2.0000000000000164\n",
      "Operator 43: -1.8213672051620464\n",
      "Operator 46: 1.8213672056373063\n",
      "Operator 47: -2.0000000000000284\n",
      "Operator 48: 2.00000000000002\n",
      "Operator 49: -1.8213672077596277\n",
      "Operator 50: 1.6663185142581345e-08\n",
      "Operator 51: -1.6866641675915553e-08\n",
      "Operator 52: 1.8213672077481013\n",
      "Operator 53: -2.0000000000000107\n",
      "Operator 54: -6.233151239598556e-08\n",
      "Operator 55: 2.4850709432705975\n",
      "Operator 56: -1.9402850769911755\n",
      "Operator 57: 1.8213672056373007\n",
      "Operator 60: -1.821367205162057\n",
      "Operator 61: 2.000000000000023\n",
      "Operator 62: -2.0000000000000173\n",
      "Operator 63: 1.8213672077481093\n",
      "Operator 64: -1.6866641732018886e-08\n",
      "Operator 65: 1.6663181339652056e-08\n",
      "Operator 66: -1.821367207759622\n",
      "Operator 67: 2.000000000000008\n",
      "Operator 68: 1.9402850769911817\n",
      "Operator 69: -2.4850709432706015\n",
      "Operator 70: 1.9402850769911761\n",
      "Operator 71: -1.8213672056373014\n",
      "Operator 74: 1.821367205182129\n",
      "Operator 75: -2.0000000000000253\n",
      "Operator 76: 2.0000000000000173\n",
      "Operator 77: -1.8213672077481071\n",
      "Operator 78: 1.735669378932383e-08\n",
      "Operator 79: -1.532433826544662e-08\n",
      "Operator 80: 1.8213672078631813\n",
      "Operator 81: -2.000000000000008\n",
      "Operator 82: 1.3039088622786985e-06\n",
      "Operator 84: 1.7669858044095723\n",
      "Operator 85: -0.4880338725888438\n",
      "Operator 88: 1.8213672051620566\n",
      "Operator 90: 1.8213672077481071\n",
      "Operator 91: -0.4880338688107215\n",
      "Operator 92: 1.735669248723245e-08\n",
      "Operator 94: 1.8213672077596215\n",
      "Operator 97: 2.197682940578012e-08\n",
      "Operator 98: 2.8627171373896877e-08\n",
      "Operator 99: 2.590627283351253e-08\n",
      "Operator 101: -6.27857763808322e-08\n",
      "Operator 102: -1.0533360542296716e-08\n",
      "Operator 103: -2.1701392099939054e-08\n",
      "Operator 105: 2.518752241268794e-08\n",
      "Operator 107: -1.1693831902803709e-08\n",
      "Operator 108: 1.9402850769911817\n",
      "Operator 109: -1.9402850769911772\n",
      "Operator 110: 1.821367205162047\n",
      "Operator 113: -1.8213672051821246\n",
      "Operator 114: 2.0000000000000258\n",
      "Operator 115: -2.0000000000000195\n",
      "Operator 116: 1.8213672077596277\n",
      "Operator 117: -1.7207753520356468e-08\n",
      "Operator 118: 1.5378855364567532e-08\n",
      "Operator 119: -1.8213672078631795\n",
      "Operator 120: 2.000000000000009\n",
      "Operator 123: -2.1590365237387347e-08\n",
      "Operator 124: -2.86271677782759e-08\n",
      "Operator 125: -1.1461076166976092e-08\n",
      "Operator 127: 4.834057999163458e-08\n",
      "Operator 128: 1.053336053987354e-08\n",
      "Operator 129: 2.1701392099939054e-08\n",
      "Operator 131: -3.015500495302049e-08\n",
      "Operator 133: 1.6661314772292712e-08\n",
      "Operator 134: -2.4850709432706015\n",
      "Operator 136: -1.7669858039679824\n",
      "Operator 137: 0.48803387089008343\n",
      "Operator 140: -1.8213672051620549\n",
      "Operator 142: -1.8213672078631875\n",
      "Operator 143: 0.48803386924020864\n",
      "Operator 144: -1.5324338103444376e-08\n",
      "Operator 146: -1.8213672077596197\n",
      "Operator 147: -1.3039088626821356e-06\n",
      "Operator 149: -1.7669858044095723\n",
      "Operator 150: 0.4880338725888439\n",
      "Operator 153: -1.821367205182129\n",
      "Operator 155: -1.8213672077481071\n",
      "Operator 156: 0.48803386881072097\n",
      "Operator 157: -1.6866642255440033e-08\n",
      "Operator 159: -1.821367207863181\n",
      "Operator 160: 1.9402850769911824\n",
      "Operator 162: 1.8213672051821201\n",
      "Operator 163: -0.48803387089008365\n",
      "Operator 166: 1.8213672056373083\n",
      "Operator 168: 1.8213672078631875\n",
      "Operator 169: -0.4880338692402081\n",
      "Operator 170: 1.5378855403893822e-08\n",
      "Operator 172: 1.821367207748099\n",
      "Operator 173: -2.4850709432706006\n",
      "Operator 175: -1.7669858039485096\n",
      "Operator 178: 0.48803387089008493\n",
      "Operator 179: -1.8213672051821272\n",
      "Operator 181: -1.8213672077596277\n",
      "Operator 183: -1.532433951469207e-08\n",
      "Operator 184: 0.4880338692402054\n",
      "Operator 185: -1.82136720786318\n",
      "Operator 187: 1.3039088622817072e-06\n",
      "Operator 188: -2.0000000000000173\n",
      "Operator 189: 1.8213672051821188\n",
      "Operator 192: -1.8213672056373085\n",
      "Operator 193: 2.000000000000028\n",
      "Operator 194: -2.0000000000000173\n",
      "Operator 195: 1.8213672078631875\n",
      "Operator 196: -1.532433902061394e-08\n",
      "Operator 197: 1.7356692043621736e-08\n",
      "Operator 198: -1.8213672077480991\n",
      "Operator 199: 2.0000000000000098\n",
      "Operator 200: 1.303908862831875e-06\n",
      "Operator 201: -1.3039088629080677e-06\n",
      "Operator 202: 0.485070943270579\n",
      "Operator 203: -1.8213672051821184\n",
      "Operator 206: 0.48803387258884595\n",
      "Operator 207: -1.8213672056373105\n",
      "Operator 209: -1.821367207863187\n",
      "Operator 211: -1.6866642351928888e-08\n",
      "Operator 212: 0.4880338688107187\n",
      "Operator 213: -1.8213672077481005\n",
      "Operator 214: 2.4850709432705926\n",
      "Operator 216: 1.7669858039485087\n",
      "Operator 217: -0.48803387081517224\n",
      "Operator 220: 1.8213672051821246\n",
      "Operator 222: 1.82136720775963\n",
      "Operator 223: -0.4880338688537168\n",
      "Operator 224: 1.6663184351052548e-08\n",
      "Operator 226: 1.8213672078631795\n",
      "Operator 227: -1.3039088607724181e-06\n",
      "Total gradient norm: 17.779012733551607\n",
      "Operators under consideration (1):\n",
      "[214]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.4850709432705926)]\n",
      "Operator(s) added to ansatz: [214]\n",
      "Gradients: [np.float64(2.4850709432705926)]\n",
      "Initial energy: -25.051308855893307\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214]...\n",
      "Starting point: [np.float64(0.7853981669171844), np.float64(0.7853981577059582), np.float64(0.7853981661037372), np.float64(0.7853981645099629), np.float64(-0.7853981548497049), np.float64(-0.7853981651805723), np.float64(-0.7853981639238696), np.float64(-0.7853981729657208), np.float64(-0.16991845332443511), np.float64(-0.13089969297008006), np.float64(0.1308996929818832), np.float64(-0.13089969366037343), np.float64(0.16991845480260342), np.float64(0.1308996936809379), np.float64(0.12248925250234449), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.255479\n",
      "         Iterations: 5\n",
      "         Function evaluations: 27\n",
      "         Gradient evaluations: 18\n",
      "\n",
      "Current energy: -25.25547938509537\n",
      "(change of -0.20417052920206302)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.0782455857643534e-08\n",
      "Operator 1: -6.711725919483774e-08\n",
      "Operator 2: 2.3150950601014144e-08\n",
      "Operator 4: -3.547490200206396e-08\n",
      "Operator 5: 9.965949186959503e-08\n",
      "Operator 6: -1.5245530458566983e-07\n",
      "Operator 7: 1.012603244674331e-07\n",
      "Operator 8: -7.402918139327258e-08\n",
      "Operator 9: -2.102168962724234e-08\n",
      "Operator 11: -7.409405133884661e-08\n",
      "Operator 12: 1.910777930325125e-07\n",
      "Operator 13: 5.78504102399791e-08\n",
      "Operator 14: -2.3150960037909853e-08\n",
      "Operator 15: 2.073598587282178e-08\n",
      "Operator 16: 1.1920141326782217e-07\n",
      "Operator 17: -6.901346301510778e-08\n",
      "Operator 18: 1.524552996714391e-07\n",
      "Operator 19: -1.012603244674331e-07\n",
      "Operator 20: 7.402918225287838e-08\n",
      "Operator 21: -1.0863938770356035e-08\n",
      "Operator 22: -9.578079959300294e-08\n",
      "Operator 23: 4.220841276492138e-08\n",
      "Operator 24: -1.910777890357096e-07\n",
      "Operator 25: 1.6813651981806288e-07\n",
      "Operator 26: 4.717853150834104e-08\n",
      "Operator 27: -3.913828212733181e-08\n",
      "Operator 28: 0.7090288086934105\n",
      "Operator 29: 1.8939302066656336\n",
      "Operator 30: -1.8213672025720227\n",
      "Operator 31: -2.0203115852546171e-07\n",
      "Operator 32: 2.018170668416565e-07\n",
      "Operator 33: 1.821367202559912\n",
      "Operator 34: -2.000000000000007\n",
      "Operator 35: 2.000000000000007\n",
      "Operator 36: -1.8213671351423528\n",
      "Operator 37: 7.598133791128566e-07\n",
      "Operator 38: 7.07217057311141e-07\n",
      "Operator 39: 1.8213672182119578\n",
      "Operator 40: -1.9999999999999876\n",
      "Operator 41: 6.57024241213654e-06\n",
      "Operator 42: 1.8939308942179478\n",
      "Operator 43: -1.8213672025599\n",
      "Operator 44: -3.888900890310598e-07\n",
      "Operator 45: -3.090618442050115e-07\n",
      "Operator 46: 1.8213671630388413\n",
      "Operator 47: -2.0000000000000213\n",
      "Operator 48: 2.000000000000009\n",
      "Operator 49: -1.8213672182119613\n",
      "Operator 50: -3.141615283437968e-07\n",
      "Operator 51: 3.14034288745354e-07\n",
      "Operator 52: 1.8213672182047387\n",
      "Operator 53: -2.0000000000000044\n",
      "Operator 54: -1.6813652337077656e-07\n",
      "Operator 55: 4.588698747484846e-06\n",
      "Operator 56: -1.8939302066656323\n",
      "Operator 57: 1.821367163038842\n",
      "Operator 58: -3.0906184392745577e-07\n",
      "Operator 59: -3.8889009210272937e-07\n",
      "Operator 60: -1.8213672025599137\n",
      "Operator 61: 2.000000000000007\n",
      "Operator 62: -2.000000000000007\n",
      "Operator 63: 1.8213672182047578\n",
      "Operator 64: 3.14034288419437e-07\n",
      "Operator 65: -3.1416152256595694e-07\n",
      "Operator 66: -1.8213672182119593\n",
      "Operator 67: 1.9999999999999876\n",
      "Operator 68: 2.0894899650185748\n",
      "Operator 69: -2.089492792239349\n",
      "Operator 70: 1.7934864649483042\n",
      "Operator 71: -1.8213671630388433\n",
      "Operator 72: 3.091192103555906e-07\n",
      "Operator 73: 3.8904681551627955e-07\n",
      "Operator 74: 1.8213672025720324\n",
      "Operator 75: -2.000000000000015\n",
      "Operator 76: 2.000000000000006\n",
      "Operator 77: -1.8213672182047576\n",
      "Operator 78: -7.071239141252368e-07\n",
      "Operator 79: -7.597792858862754e-07\n",
      "Operator 80: 1.8213671351423457\n",
      "Operator 81: -1.999999999999995\n",
      "Operator 82: -0.7090371404137954\n",
      "Operator 84: 1.7247711437540691\n",
      "Operator 85: -0.4880337890234393\n",
      "Operator 86: 3.091192065389297e-07\n",
      "Operator 88: 1.8213672025599092\n",
      "Operator 90: 1.821367218204757\n",
      "Operator 91: -0.48803398956089317\n",
      "Operator 92: -7.071239168422406e-07\n",
      "Operator 94: 1.821367218211952\n",
      "Operator 95: 4.717853178610306e-08\n",
      "Operator 96: 1.7491508491618304e-08\n",
      "Operator 97: 6.393602510618162e-08\n",
      "Operator 98: 5.722133961105772e-08\n",
      "Operator 99: 2.0735985428732585e-08\n",
      "Operator 101: -9.965949186959503e-08\n",
      "Operator 103: -9.221611759712634e-08\n",
      "Operator 104: -4.402758416297799e-08\n",
      "Operator 105: -1.0863938936889498e-08\n",
      "Operator 107: 7.409405133884661e-08\n",
      "Operator 108: 2.0894899650185748\n",
      "Operator 109: -1.7934864649483067\n",
      "Operator 110: 1.8213672025599013\n",
      "Operator 111: 2.0181706140293823e-07\n",
      "Operator 112: -2.0203115813688368e-07\n",
      "Operator 113: -1.8213672025720222\n",
      "Operator 114: 2.0000000000000195\n",
      "Operator 115: -2.000000000000009\n",
      "Operator 116: 1.8213672182119616\n",
      "Operator 117: 7.072170596369403e-07\n",
      "Operator 118: 7.598133791044009e-07\n",
      "Operator 119: -1.8213671351423328\n",
      "Operator 120: 2.000000000000003\n",
      "Operator 121: -3.628364613535391e-08\n",
      "Operator 123: -5.5654424395423825e-08\n",
      "Operator 124: -5.722133647467768e-08\n",
      "Operator 127: 6.901346301510778e-08\n",
      "Operator 129: 9.221611885320027e-08\n",
      "Operator 130: 4.402758375616857e-08\n",
      "Operator 131: -2.1021688731173823e-08\n",
      "Operator 133: -4.2208412653899074e-08\n",
      "Operator 134: -2.089492792239348\n",
      "Operator 136: -1.7247711811906137\n",
      "Operator 137: 0.4880339365632802\n",
      "Operator 138: 3.890468156273018e-07\n",
      "Operator 140: -1.8213672025599075\n",
      "Operator 142: -1.821367135142354\n",
      "Operator 143: 0.48803367956776067\n",
      "Operator 144: -7.597792863552352e-07\n",
      "Operator 146: -1.8213672182119502\n",
      "Operator 147: 6.570242414355745e-06\n",
      "Operator 149: -1.633298677305721\n",
      "Operator 150: 0.4880337890234398\n",
      "Operator 151: -3.090618408061335e-07\n",
      "Operator 153: -1.821367202572031\n",
      "Operator 155: -1.8213672182047558\n",
      "Operator 156: 0.4880339895608926\n",
      "Operator 157: 3.1403429103514114e-07\n",
      "Operator 159: -1.8213671351423426\n",
      "Operator 160: 2.089489965018575\n",
      "Operator 162: 1.7247718073332305\n",
      "Operator 163: -0.4880339365632827\n",
      "Operator 164: -2.0203116069503086e-07\n",
      "Operator 166: 1.821367163038848\n",
      "Operator 168: 1.8213671351423522\n",
      "Operator 169: -0.4880336795677648\n",
      "Operator 170: 7.598133793951754e-07\n",
      "Operator 172: 1.8213672182047442\n",
      "Operator 173: -2.089492792239348\n",
      "Operator 175: -1.633298712745964\n",
      "Operator 177: 3.890468171940223e-07\n",
      "Operator 178: 0.4880339365632828\n",
      "Operator 179: -1.821367202572031\n",
      "Operator 181: -1.8213672182119605\n",
      "Operator 183: -7.597792859357402e-07\n",
      "Operator 184: 0.4880336795677581\n",
      "Operator 185: -1.8213671351423444\n",
      "Operator 186: -3.6283646038912365e-08\n",
      "Operator 187: -0.7090371404137954\n",
      "Operator 188: -1.8939308942179474\n",
      "Operator 189: 1.8213672025720236\n",
      "Operator 190: 3.890468166389108e-07\n",
      "Operator 191: 3.0911920742710814e-07\n",
      "Operator 192: -1.8213671630388495\n",
      "Operator 193: 2.000000000000017\n",
      "Operator 194: -2.000000000000006\n",
      "Operator 195: 1.8213671351423542\n",
      "Operator 196: -7.597792859125565e-07\n",
      "Operator 197: -7.071239169701743e-07\n",
      "Operator 198: -1.8213672182047476\n",
      "Operator 199: 1.9999999999999964\n",
      "Operator 200: -0.709037140413795\n",
      "Operator 201: 6.570242414188679e-06\n",
      "Operator 202: 0.6085889681225133\n",
      "Operator 203: -1.7247718073332297\n",
      "Operator 205: -3.090618400289773e-07\n",
      "Operator 206: 0.4880337890234423\n",
      "Operator 207: -1.8213671630388495\n",
      "Operator 209: -1.8213671351423524\n",
      "Operator 211: 3.1403429110921724e-07\n",
      "Operator 212: 0.4880339895608907\n",
      "Operator 213: -1.8213672182047458\n",
      "Operator 214: 4.5886987465173255e-06\n",
      "Operator 216: 1.6332987127459642\n",
      "Operator 217: -0.4880339365180379\n",
      "Operator 218: -3.8889008769879216e-07\n",
      "Operator 220: 1.8213672025720218\n",
      "Operator 222: 1.8213672182119631\n",
      "Operator 223: -0.488033989587778\n",
      "Operator 224: -3.141615282633763e-07\n",
      "Operator 226: 1.8213671351423328\n",
      "Operator 227: 6.570242411981879e-06\n",
      "Total gradient norm: 17.14284330869953\n",
      "Operators under consideration (1):\n",
      "[173]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.089492792239348)]\n",
      "Operator(s) added to ansatz: [173]\n",
      "Gradients: [np.float64(-2.089492792239348)]\n",
      "Initial energy: -25.25547938509537\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173]...\n",
      "Starting point: [np.float64(0.785398182126964), np.float64(0.785398142913402), np.float64(0.7853981733707808), np.float64(0.785398163649654), np.float64(-0.7853981446982433), np.float64(-0.7853981604253724), np.float64(-0.7853981701472277), np.float64(-0.7853981933319838), np.float64(-0.16991842231295506), np.float64(-0.13089972318045098), np.float64(0.13089972318783252), np.float64(-0.1308997106679631), np.float64(0.16991844393960004), np.float64(0.1308997106803835), np.float64(0.16357020674920938), np.float64(-0.1635696718320939), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.392305\n",
      "         Iterations: 13\n",
      "         Function evaluations: 43\n",
      "         Gradient evaluations: 33\n",
      "\n",
      "Current energy: -25.39230484482511\n",
      "(change of -0.1368254597297387)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.2331432531619224e-07\n",
      "Operator 1: 1.1128435724444507e-06\n",
      "Operator 2: -1.056757484221833e-06\n",
      "Operator 3: 7.269325113210071e-07\n",
      "Operator 4: -3.20970558720721e-07\n",
      "Operator 5: -1.4897659543470405e-06\n",
      "Operator 6: 2.046828945603778e-06\n",
      "Operator 7: -1.5098234137396898e-06\n",
      "Operator 8: 1.2337203117695991e-06\n",
      "Operator 9: -7.740537510672793e-08\n",
      "Operator 10: -9.897241358674786e-08\n",
      "Operator 11: 1.6241539418926365e-06\n",
      "Operator 12: -2.6234418482040667e-06\n",
      "Operator 13: -1.0025515398984542e-06\n",
      "Operator 14: 1.0567574755455593e-06\n",
      "Operator 15: -1.1505359176679113e-06\n",
      "Operator 16: -8.363355318425795e-07\n",
      "Operator 17: 1.0661625807788754e-06\n",
      "Operator 18: -2.0468289495109188e-06\n",
      "Operator 19: 1.5098234137396898e-06\n",
      "Operator 20: -1.2337203110995203e-06\n",
      "Operator 21: 5.874553431422274e-07\n",
      "Operator 22: 1.4924549511110349e-06\n",
      "Operator 23: -1.1141036845296881e-06\n",
      "Operator 24: 2.6234418520898473e-06\n",
      "Operator 25: -1.985991392672304e-06\n",
      "Operator 26: -6.56579632939285e-08\n",
      "Operator 27: 5.246065112988514e-07\n",
      "Operator 28: 4.722312605137693e-05\n",
      "Operator 29: 1.8213552760591638\n",
      "Operator 30: -1.8213672152824185\n",
      "Operator 31: -6.220340966451682e-08\n",
      "Operator 32: 6.213274876359875e-08\n",
      "Operator 33: 1.8213672152797091\n",
      "Operator 34: -1.9999999999986429\n",
      "Operator 35: 2.0000000000000195\n",
      "Operator 36: -1.8213674010656757\n",
      "Operator 37: 3.530460360263254e-06\n",
      "Operator 38: -6.01632943830699e-07\n",
      "Operator 39: 1.8213675669101974\n",
      "Operator 40: -1.999999999997728\n",
      "Operator 41: 0.00012633645054706255\n",
      "Operator 42: 1.8213616138756192\n",
      "Operator 43: -1.8213672152793121\n",
      "Operator 44: -1.8572172232036072e-07\n",
      "Operator 45: -2.7544859413710555e-07\n",
      "Operator 46: 1.8213671891642815\n",
      "Operator 47: -2.0000000000005174\n",
      "Operator 48: 1.9999999999996145\n",
      "Operator 49: -1.8213675669093878\n",
      "Operator 50: 1.3864146324573318e-06\n",
      "Operator 51: -1.3863965279870938e-06\n",
      "Operator 52: 1.821367566908172\n",
      "Operator 53: -2.000000000000826\n",
      "Operator 54: 1.985991392672304e-06\n",
      "Operator 55: -1.4412858831000407e-05\n",
      "Operator 56: -1.8213552760591625\n",
      "Operator 57: 1.8213671891651289\n",
      "Operator 58: -2.754485941424172e-07\n",
      "Operator 59: -1.8572178982222322e-07\n",
      "Operator 60: -1.8213672152797113\n",
      "Operator 61: 1.999999999998643\n",
      "Operator 62: -2.0000000000000195\n",
      "Operator 63: 1.8213675669097376\n",
      "Operator 64: -1.3863965277925382e-06\n",
      "Operator 65: 1.3864148018312262e-06\n",
      "Operator 66: -1.8213675669101987\n",
      "Operator 67: 1.999999999997728\n",
      "Operator 68: -3.66979872840784e-05\n",
      "Operator 69: 6.707942228078528e-06\n",
      "Operator 70: 1.8213546800710363\n",
      "Operator 71: -1.8213671891657353\n",
      "Operator 72: 2.7546748351683753e-07\n",
      "Operator 73: 1.8577344791145733e-07\n",
      "Operator 74: 1.8213672152830267\n",
      "Operator 75: -1.9999999999996478\n",
      "Operator 76: 1.9999999999996396\n",
      "Operator 77: -1.821367566910435\n",
      "Operator 78: 6.016197104572011e-07\n",
      "Operator 79: -3.5304653859957495e-06\n",
      "Operator 80: 1.8213674010663474\n",
      "Operator 81: -1.9999999999993991\n",
      "Operator 82: -0.00012915667277578057\n",
      "Operator 84: 1.6586783698139502\n",
      "Operator 85: -0.4880338052063363\n",
      "Operator 86: 2.754671721568135e-07\n",
      "Operator 88: 1.8213672152788902\n",
      "Operator 90: 1.8213675669104348\n",
      "Operator 91: -0.4880337714432782\n",
      "Operator 92: 6.016190503893235e-07\n",
      "Operator 94: 1.8213675669088238\n",
      "Operator 95: -6.5657963137217e-08\n",
      "Operator 97: -9.730041029571092e-07\n",
      "Operator 98: -5.759817997191212e-07\n",
      "Operator 99: -1.1505359180950865e-06\n",
      "Operator 101: 1.4897659543470405e-06\n",
      "Operator 102: -3.612712792225778e-08\n",
      "Operator 103: 1.3749716948260557e-06\n",
      "Operator 104: 6.877666887044376e-07\n",
      "Operator 105: 5.874553431517517e-07\n",
      "Operator 107: -1.6241539418926365e-06\n",
      "Operator 108: -3.6697987284179626e-05\n",
      "Operator 109: -1.8213546800708764\n",
      "Operator 110: 1.821367215279313\n",
      "Operator 111: 6.213257630840842e-08\n",
      "Operator 112: -6.220340931469176e-08\n",
      "Operator 113: -1.8213672152814797\n",
      "Operator 114: 2.000000000000515\n",
      "Operator 115: -1.9999999999996145\n",
      "Operator 116: 1.8213675669093874\n",
      "Operator 117: -6.016329611200634e-07\n",
      "Operator 118: 3.5304603602123123e-06\n",
      "Operator 119: -1.821367401064025\n",
      "Operator 120: 2.0000000000008242\n",
      "Operator 121: 1.7593325391336982e-07\n",
      "Operator 123: 8.725628046508748e-07\n",
      "Operator 124: 5.759818028286131e-07\n",
      "Operator 125: 7.269325132951224e-07\n",
      "Operator 127: -1.0661625798368202e-06\n",
      "Operator 128: 3.6127127919834636e-08\n",
      "Operator 129: -1.3749716948260557e-06\n",
      "Operator 130: -6.877666887193017e-07\n",
      "Operator 131: -7.740537561979797e-08\n",
      "Operator 133: 1.1141036845296881e-06\n",
      "Operator 134: 6.707942228000987e-06\n",
      "Operator 136: -1.6586783935985414\n",
      "Operator 137: 0.48803390267740865\n",
      "Operator 138: 1.8577344782042106e-07\n",
      "Operator 140: -1.821367215278888\n",
      "Operator 142: -1.821367401066678\n",
      "Operator 143: 0.48803315250428736\n",
      "Operator 144: -3.530465385891268e-06\n",
      "Operator 146: -1.8213675669088225\n",
      "Operator 147: 0.00012633645073326197\n",
      "Operator 149: -1.658677827057422\n",
      "Operator 150: 0.48803380520627504\n",
      "Operator 151: -2.754482179433222e-07\n",
      "Operator 153: -1.8213672152827003\n",
      "Operator 155: -1.8213675669101081\n",
      "Operator 156: 0.4880337714432847\n",
      "Operator 157: -1.386395826083888e-06\n",
      "Operator 159: -1.8213674010658099\n",
      "Operator 160: -3.669798743379353e-05\n",
      "Operator 162: 1.6586841653443094\n",
      "Operator 163: -0.48803390267799485\n",
      "Operator 164: -6.220314005776369e-08\n",
      "Operator 166: 1.8213671891652465\n",
      "Operator 168: 1.8213674010663512\n",
      "Operator 169: -0.4880331525054338\n",
      "Operator 170: 3.5304605582171888e-06\n",
      "Operator 172: 1.8213675669095628\n",
      "Operator 173: 6.707942225084131e-06\n",
      "Operator 174: 0.4880345195491911\n",
      "Operator 175: -1.65867785083837\n",
      "Operator 177: 1.8577344908547051e-07\n",
      "Operator 178: 0.48803390267719043\n",
      "Operator 179: -1.8213672152830245\n",
      "Operator 181: -1.8213675669090417\n",
      "Operator 183: -3.530465468233584e-06\n",
      "Operator 184: 0.4880331525044967\n",
      "Operator 185: -1.8213674010663456\n",
      "Operator 186: 1.7593325322845898e-07\n",
      "Operator 187: -0.00012915667277603606\n",
      "Operator 188: -1.8213616138757653\n",
      "Operator 189: 1.8213672152831968\n",
      "Operator 190: 1.857734491214805e-07\n",
      "Operator 191: 2.7546717176122774e-07\n",
      "Operator 192: -1.821367189165573\n",
      "Operator 193: 1.9999999999996503\n",
      "Operator 194: -1.9999999999996398\n",
      "Operator 195: 1.8213674010666783\n",
      "Operator 196: -3.530465468385647e-06\n",
      "Operator 197: 6.016190505723145e-07\n",
      "Operator 198: -1.8213675669101006\n",
      "Operator 199: 1.9999999999994007\n",
      "Operator 200: -0.0001291566726223563\n",
      "Operator 201: 0.00012633645073303976\n",
      "Operator 202: 0.4880603960117116\n",
      "Operator 203: -1.658684165344308\n",
      "Operator 205: -2.754482184988226e-07\n",
      "Operator 206: 0.4880338052062909\n",
      "Operator 207: -1.8213671891652488\n",
      "Operator 209: -1.8213674010663516\n",
      "Operator 211: -1.3863958266421762e-06\n",
      "Operator 212: 0.48803377144337146\n",
      "Operator 213: -1.8213675669095641\n",
      "Operator 214: -1.441285876522186e-05\n",
      "Operator 216: 1.6586778508381952\n",
      "Operator 217: -0.48803390266252167\n",
      "Operator 218: -1.8572172216230578e-07\n",
      "Operator 220: 1.8213672152814797\n",
      "Operator 222: 1.8213675669090188\n",
      "Operator 223: -0.4880337714386309\n",
      "Operator 224: 1.3864146321633428e-06\n",
      "Operator 226: 1.821367401064025\n",
      "Operator 227: 0.00012633645054684883\n",
      "Total gradient norm: 16.255937395686775\n",
      "Operators under consideration (1):\n",
      "[199]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.9999999999994007)]\n",
      "Operator(s) added to ansatz: [199]\n",
      "Gradients: [np.float64(1.9999999999994007)]\n",
      "Initial energy: -25.39230484482511\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199]...\n",
      "Starting point: [np.float64(0.7853978743169592), np.float64(0.7853984627957435), np.float64(0.7853979584569817), np.float64(0.7853980357037265), np.float64(-0.7853983885850833), np.float64(-0.7853982502066885), np.float64(-0.785398098162899), np.float64(-0.7853977792780218), np.float64(-0.16991821204063315), np.float64(-0.1308996433830509), np.float64(0.1308996433819804), np.float64(-0.13089970112433666), np.float64(0.16991844130278222), np.float64(0.13089970112843038), np.float64(0.16992697065219867), np.float64(-0.13090128911804455), np.float64(0.1309006785193028), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.521058\n",
      "         Iterations: 17\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 54\n",
      "\n",
      "Current energy: -25.52105751489578\n",
      "(change of -0.12875267007067137)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.4373098166617787e-08\n",
      "Operator 1: -2.4231469180606735e-08\n",
      "Operator 2: 1.2358855596320567e-07\n",
      "Operator 3: -1.0184050441889012e-08\n",
      "Operator 4: 9.271016828488618e-08\n",
      "Operator 5: -7.184156614048476e-08\n",
      "Operator 6: 9.193820416086851e-08\n",
      "Operator 7: -9.194010885948956e-08\n",
      "Operator 8: -4.8459239176956714e-08\n",
      "Operator 9: 1.8368688559089463e-07\n",
      "Operator 10: -2.0798735287884203e-08\n",
      "Operator 11: -4.034294686283092e-08\n",
      "Operator 12: -1.274460615442753e-08\n",
      "Operator 14: -1.2358856489310655e-07\n",
      "Operator 15: -1.0429014422434218e-08\n",
      "Operator 16: -1.490261140790202e-07\n",
      "Operator 17: 5.1228495223654136e-08\n",
      "Operator 18: -9.193820871278291e-08\n",
      "Operator 19: 9.194010530677588e-08\n",
      "Operator 20: 4.8459240620246646e-08\n",
      "Operator 21: -1.816976791246816e-07\n",
      "Operator 22: 2.5922016556916196e-08\n",
      "Operator 23: 3.9620808684136024e-08\n",
      "Operator 24: 3.616721681838473e-08\n",
      "Operator 25: 3.167820563629577e-08\n",
      "Operator 26: 1.1228690861086373e-07\n",
      "Operator 27: -1.1057714110128249e-07\n",
      "Operator 28: 5.667459296396597e-05\n",
      "Operator 29: 1.82135551263729\n",
      "Operator 30: -1.8213672041117044\n",
      "Operator 31: 1.2341433252552164e-07\n",
      "Operator 32: -1.234162107736071e-07\n",
      "Operator 33: 1.8213672041116102\n",
      "Operator 34: -1.9999999999999936\n",
      "Operator 35: 1.9999999999999911\n",
      "Operator 36: -1.816675935207345\n",
      "Operator 37: 9.358909103933075e-08\n",
      "Operator 38: 0.043296715098788505\n",
      "Operator 39: 1.7573573249556238\n",
      "Operator 40: -1.9346954372442202\n",
      "Operator 41: 0.00012066704272097967\n",
      "Operator 42: 1.821360933436623\n",
      "Operator 43: -1.8213672041116018\n",
      "Operator 44: 2.524367640471836e-07\n",
      "Operator 45: 2.2907638575475077e-07\n",
      "Operator 46: 1.821367231376963\n",
      "Operator 47: -2.000000000000004\n",
      "Operator 48: 1.9999999999999942\n",
      "Operator 49: -1.8166759388845168\n",
      "Operator 50: 5.161255532743857e-08\n",
      "Operator 51: 0.07534044791379416\n",
      "Operator 52: 2.2787189386425606\n",
      "Operator 53: 1.4438833394844884e-08\n",
      "Operator 54: -2.147715411432929e-08\n",
      "Operator 55: -2.4937773700869165e-05\n",
      "Operator 56: -1.8213555126372882\n",
      "Operator 57: 1.8213672313769558\n",
      "Operator 58: 2.2907638539031837e-07\n",
      "Operator 59: 2.52436768655133e-07\n",
      "Operator 60: -1.821367204111612\n",
      "Operator 61: 1.9999999999999922\n",
      "Operator 62: -1.9999999999999911\n",
      "Operator 63: 1.817953328257303\n",
      "Operator 64: -3.471029941239658e-08\n",
      "Operator 65: 5.161255693736998e-08\n",
      "Operator 66: 0.13119786403420475\n",
      "Operator 67: 2.5069058740104695\n",
      "Operator 68: -3.396129046068211e-05\n",
      "Operator 69: 8.310525960628515e-06\n",
      "Operator 70: 1.8213542264899547\n",
      "Operator 71: -1.8213672313769582\n",
      "Operator 72: -2.2907588240356612e-07\n",
      "Operator 73: -2.52435392285067e-07\n",
      "Operator 74: 1.8213672041117168\n",
      "Operator 75: -2.0\n",
      "Operator 76: 1.9999999999999913\n",
      "Operator 77: -1.8179533282572988\n",
      "Operator 78: 1.7418915298771077e-08\n",
      "Operator 79: -9.821906995974069e-08\n",
      "Operator 80: -0.13119786572733122\n",
      "Operator 81: -2.5069058740104655\n",
      "Operator 82: -0.00012675309748438098\n",
      "Operator 84: 1.6586786237026754\n",
      "Operator 85: -0.48803393139540574\n",
      "Operator 86: -2.2907588480135345e-07\n",
      "Operator 88: 1.8213672041116111\n",
      "Operator 90: 1.8179533282572986\n",
      "Operator 91: -0.49538986652610006\n",
      "Operator 92: 1.74189174031583e-08\n",
      "Operator 93: -0.12030387744227658\n",
      "Operator 94: -0.13119786403420378\n",
      "Operator 95: 1.1228690898514928e-07\n",
      "Operator 97: 1.3361912887613749e-08\n",
      "Operator 98: -8.227584579199842e-08\n",
      "Operator 99: -1.0429013794397239e-08\n",
      "Operator 101: 7.184156736173009e-08\n",
      "Operator 102: 8.231402499262845e-08\n",
      "Operator 103: 8.351269187301114e-08\n",
      "Operator 104: 2.0500140529566835e-08\n",
      "Operator 105: -1.7576483635206586e-07\n",
      "Operator 107: 3.7222580131905013e-08\n",
      "Operator 108: -3.396129046021466e-05\n",
      "Operator 109: -1.821354226489953\n",
      "Operator 110: 1.821367204111603\n",
      "Operator 111: -1.234162066049737e-07\n",
      "Operator 112: 1.2341433201378817e-07\n",
      "Operator 113: -1.8213672041117133\n",
      "Operator 114: 2.0000000000000018\n",
      "Operator 115: -1.9999999999999942\n",
      "Operator 116: 1.8166759388845168\n",
      "Operator 117: -2.957028560689046e-08\n",
      "Operator 118: -0.04329665253365616\n",
      "Operator 119: -1.7573573213985152\n",
      "Operator 120: 1.9346954372442182\n",
      "Operator 121: -1.4383907270327305e-07\n",
      "Operator 123: 1.5373981663139633e-08\n",
      "Operator 124: 8.227584889293104e-08\n",
      "Operator 125: -1.0184051423196788e-08\n",
      "Operator 127: -5.122849644489946e-08\n",
      "Operator 128: -8.231402499505176e-08\n",
      "Operator 129: -8.351269187301114e-08\n",
      "Operator 130: -2.05001412512118e-08\n",
      "Operator 131: 1.776890888871563e-07\n",
      "Operator 133: -4.232110897993831e-08\n",
      "Operator 134: 8.310525961179037e-06\n",
      "Operator 136: -1.658678598872818\n",
      "Operator 137: 0.48803382964010356\n",
      "Operator 138: -2.5243539223664043e-07\n",
      "Operator 140: -1.8213672041116094\n",
      "Operator 142: -1.8166759352073396\n",
      "Operator 143: 0.4906814340245824\n",
      "Operator 144: 0.03152049093903313\n",
      "Operator 146: -2.277117791181548\n",
      "Operator 147: 0.00012066704272446112\n",
      "Operator 149: -1.6586774524293728\n",
      "Operator 150: 0.48803393139540485\n",
      "Operator 151: 2.2907638895249287e-07\n",
      "Operator 153: -1.8213672041117164\n",
      "Operator 155: -1.817953328257298\n",
      "Operator 156: 0.49538986652609895\n",
      "Operator 157: -3.4710299301594694e-08\n",
      "Operator 158: 0.12030388591694195\n",
      "Operator 159: 0.13119786572732994\n",
      "Operator 160: -3.3961290464699225e-05\n",
      "Operator 162: 1.658683535505884\n",
      "Operator 163: -0.48803382964009956\n",
      "Operator 164: 1.234143294852881e-07\n",
      "Operator 166: 1.8213672313769664\n",
      "Operator 168: 1.81667593520734\n",
      "Operator 169: -0.4906814340245815\n",
      "Operator 170: -0.043296652533656554\n",
      "Operator 172: 1.7585930046511773\n",
      "Operator 173: 8.31052596604483e-06\n",
      "Operator 174: 0.4880342423265325\n",
      "Operator 175: -1.6586774275994367\n",
      "Operator 177: -2.5243539172747865e-07\n",
      "Operator 178: 0.4880338296401018\n",
      "Operator 179: -1.8213672041117146\n",
      "Operator 181: -1.8166759388845182\n",
      "Operator 183: 0.031520490939034154\n",
      "Operator 184: 0.47465956577390517\n",
      "Operator 185: -2.2771177865723806\n",
      "Operator 186: -1.438390729636054e-07\n",
      "Operator 187: -0.00012675309748410828\n",
      "Operator 188: -1.8213609334366252\n",
      "Operator 189: 1.821367204111705\n",
      "Operator 190: -2.524353920333099e-07\n",
      "Operator 191: -2.2907588515462424e-07\n",
      "Operator 192: -1.821367231376967\n",
      "Operator 193: 2.000000000000002\n",
      "Operator 194: -1.9999999999999925\n",
      "Operator 195: 1.8166759352073394\n",
      "Operator 196: -9.821906911381703e-08\n",
      "Operator 197: -0.07534046464057367\n",
      "Operator 198: -2.2787189386425615\n",
      "Operator 199: -1.4438832745012448e-08\n",
      "Operator 200: -0.00012675309748411741\n",
      "Operator 201: 0.00012066704272527993\n",
      "Operator 202: 0.48805927219912576\n",
      "Operator 203: -1.6586835355058824\n",
      "Operator 205: 2.2907638908995041e-07\n",
      "Operator 206: 0.4880339313954074\n",
      "Operator 207: -1.8213672313769682\n",
      "Operator 209: -1.8166759352073392\n",
      "Operator 211: 0.07534044791379516\n",
      "Operator 212: 0.6209478831597721\n",
      "Operator 213: 0.08382191978144526\n",
      "Operator 214: -2.4937773704507853e-05\n",
      "Operator 216: 1.6586774275994332\n",
      "Operator 217: -0.4880338296397062\n",
      "Operator 218: 2.524367639714618e-07\n",
      "Operator 220: 1.8213672041117133\n",
      "Operator 222: 1.8166759388845206\n",
      "Operator 223: -0.49068144763878796\n",
      "Operator 224: -0.03152053689829283\n",
      "Operator 226: 2.2771177865723793\n",
      "Operator 227: 0.00012066704272070038\n",
      "Total gradient norm: 15.889584155159712\n",
      "Operators under consideration (1):\n",
      "[81]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.5069058740104655)]\n",
      "Operator(s) added to ansatz: [81]\n",
      "Gradients: [np.float64(-2.5069058740104655)]\n",
      "Initial energy: -25.52105751489578\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81]...\n",
      "Starting point: [np.float64(0.7853981569271488), np.float64(0.7853981718611446), np.float64(0.7853981815417711), np.float64(0.7853981737681368), np.float64(-0.7853981787991398), np.float64(-0.7853981438289298), np.float64(-0.7853981820608833), np.float64(-0.7853981623403902), np.float64(-0.17186462038723443), np.float64(-0.13310880756404858), np.float64(0.13181337523589487), np.float64(-0.13089968318757197), np.float64(0.16991846357078488), np.float64(0.13089968318768033), np.float64(0.16992685308342087), np.float64(-0.13090190895236764), np.float64(0.13090059127740264), np.float64(-0.12812404203655028), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.740324\n",
      "         Iterations: 18\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 55\n",
      "\n",
      "Current energy: -25.740324264230622\n",
      "(change of -0.21926674933484236)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.9215830306875075e-08\n",
      "Operator 1: 1.1765232330150607e-07\n",
      "Operator 2: 1.6738033361365954e-07\n",
      "Operator 3: -2.0035737400352152e-07\n",
      "Operator 4: 1.015167555931591e-07\n",
      "Operator 5: 4.563987517913759e-07\n",
      "Operator 6: -5.0046803923287086e-08\n",
      "Operator 7: -2.629714792590221e-07\n",
      "Operator 8: 5.816732728813889e-08\n",
      "Operator 9: 2.200040072342091e-08\n",
      "Operator 11: 1.43533925715668e-07\n",
      "Operator 13: -2.3388904350955745e-07\n",
      "Operator 14: -1.6738034211178498e-07\n",
      "Operator 15: 2.632810330670488e-07\n",
      "Operator 16: 7.039387322511722e-08\n",
      "Operator 17: -3.93475087068218e-07\n",
      "Operator 18: 5.0046799371372686e-08\n",
      "Operator 19: 2.629714792590221e-07\n",
      "Operator 20: -5.816732662200508e-08\n",
      "Operator 22: 8.086481885810203e-08\n",
      "Operator 23: -1.1667895459244014e-07\n",
      "Operator 25: 6.515649339641749e-08\n",
      "Operator 26: 5.096238790058492e-07\n",
      "Operator 27: -3.667736133553413e-07\n",
      "Operator 28: 5.659121796002817e-05\n",
      "Operator 29: 1.8213555225841866\n",
      "Operator 30: -1.8213671589495182\n",
      "Operator 31: -2.451986792220017e-07\n",
      "Operator 32: 2.451623977596311e-07\n",
      "Operator 33: 1.8213671589474876\n",
      "Operator 34: -2.00000000000002\n",
      "Operator 35: 2.0000000000000067\n",
      "Operator 36: -1.8059820366912676\n",
      "Operator 37: -3.3154449042161197e-07\n",
      "Operator 38: 0.08105244726973498\n",
      "Operator 39: -0.8657281818436773\n",
      "Operator 40: -2.0775237264345012\n",
      "Operator 41: 0.0001207519243120601\n",
      "Operator 42: 1.8213609581924977\n",
      "Operator 43: -1.8213671589474862\n",
      "Operator 44: -2.137633195649805e-07\n",
      "Operator 45: 3.3098254871969244e-07\n",
      "Operator 46: 1.8213671655849586\n",
      "Operator 47: -2.0000000000000138\n",
      "Operator 48: 2.0000000000000093\n",
      "Operator 49: -1.8035934962089546\n",
      "Operator 50: -5.730224806629822e-08\n",
      "Operator 51: 0.20956723396778343\n",
      "Operator 52: 1.8328794936421977\n",
      "Operator 53: 0.7553225900814531\n",
      "Operator 54: -6.515649447456198e-08\n",
      "Operator 55: -2.4761133147991915e-05\n",
      "Operator 56: -1.8213555225841853\n",
      "Operator 57: 1.8213671655849275\n",
      "Operator 58: 3.3098254907296326e-07\n",
      "Operator 59: -2.1376331468613087e-07\n",
      "Operator 60: -1.8213671589474894\n",
      "Operator 61: 2.00000000000002\n",
      "Operator 62: -2.0000000000000067\n",
      "Operator 63: 1.8059820207456352\n",
      "Operator 64: 1.4361536379580606e-07\n",
      "Operator 65: -0.08105251527237456\n",
      "Operator 66: 0.8657281847156636\n",
      "Operator 67: 2.077523731281407\n",
      "Operator 68: -3.379136724143064e-05\n",
      "Operator 69: 8.070527827305113e-06\n",
      "Operator 70: 1.8213542315361213\n",
      "Operator 71: -1.8213671655849424\n",
      "Operator 72: -3.3097283228101653e-07\n",
      "Operator 73: 2.1378987500976196e-07\n",
      "Operator 74: 1.8213671589495428\n",
      "Operator 75: -2.000000000000014\n",
      "Operator 76: 2.0000000000000058\n",
      "Operator 77: -1.8059820207456336\n",
      "Operator 78: -4.7662827566110105e-08\n",
      "Operator 79: 0.1373403470231631\n",
      "Operator 80: -0.18310782812672874\n",
      "Operator 81: 3.390203046552649e-08\n",
      "Operator 82: -0.00012686116971755818\n",
      "Operator 84: 1.6586785728458118\n",
      "Operator 85: -0.48803393210952317\n",
      "Operator 86: -3.3097283144018503e-07\n",
      "Operator 88: 1.821367158947492\n",
      "Operator 90: 1.805982020745634\n",
      "Operator 91: -0.5064185336504676\n",
      "Operator 92: -0.05556590233759573\n",
      "Operator 93: -0.12254905325576423\n",
      "Operator 94: -0.18054653461684972\n",
      "Operator 95: 5.096238792449929e-07\n",
      "Operator 97: -1.1845635673745542e-07\n",
      "Operator 98: 8.412234710093444e-08\n",
      "Operator 99: 2.632810324390118e-07\n",
      "Operator 101: -4.563987517913759e-07\n",
      "Operator 103: 2.3714682129138964e-07\n",
      "Operator 104: 3.7351214399183874e-08\n",
      "Operator 107: -1.4362066235325888e-07\n",
      "Operator 108: -3.37913672417195e-05\n",
      "Operator 109: -1.8213542315361235\n",
      "Operator 110: 1.8213671589474876\n",
      "Operator 111: 2.4516240349181776e-07\n",
      "Operator 112: -2.451986788294786e-07\n",
      "Operator 113: -1.8213671589495528\n",
      "Operator 114: 2.0000000000000115\n",
      "Operator 115: -2.0000000000000093\n",
      "Operator 116: 1.8035934962089542\n",
      "Operator 117: -1.5583114943455298e-08\n",
      "Operator 118: -0.20956741487291144\n",
      "Operator 119: -1.8328795091913057\n",
      "Operator 120: -0.7553225767499748\n",
      "Operator 121: -6.258562449756029e-07\n",
      "Operator 123: 2.243112698649909e-07\n",
      "Operator 124: -8.412234388224489e-08\n",
      "Operator 125: -2.0035737400352152e-07\n",
      "Operator 127: 3.93475087068218e-07\n",
      "Operator 129: -2.3714682129138964e-07\n",
      "Operator 130: -3.735121456571733e-08\n",
      "Operator 131: 1.929842230463663e-08\n",
      "Operator 133: 1.1676569546791362e-07\n",
      "Operator 134: 8.070527827544041e-06\n",
      "Operator 136: -1.6586785668030957\n",
      "Operator 137: 0.4880339073458385\n",
      "Operator 138: 2.1378987516677119e-07\n",
      "Operator 140: -1.8213671589474905\n",
      "Operator 142: -1.8059820366912671\n",
      "Operator 143: 0.5064185905155335\n",
      "Operator 144: 0.05556615321846767\n",
      "Operator 145: 0.12254907611637943\n",
      "Operator 146: 0.18054652936769616\n",
      "Operator 147: 0.00012075192432847641\n",
      "Operator 149: -1.6586773971095377\n",
      "Operator 150: 0.48803393210952106\n",
      "Operator 151: 3.309825547171217e-07\n",
      "Operator 153: -1.8213671589495424\n",
      "Operator 155: -1.8059820207456332\n",
      "Operator 156: 0.5064185336504667\n",
      "Operator 157: 0.0768526510553709\n",
      "Operator 158: 0.11569478593137128\n",
      "Operator 159: 0.8612117080904587\n",
      "Operator 160: -3.379136727636011e-05\n",
      "Operator 162: 1.6586835169223266\n",
      "Operator 163: -0.4880339073458485\n",
      "Operator 164: -2.4519867217515794e-07\n",
      "Operator 166: 1.8213671655849473\n",
      "Operator 168: 1.8059820366912662\n",
      "Operator 169: -0.5064185905155362\n",
      "Operator 170: -0.07685283268639931\n",
      "Operator 171: -0.11569485432659524\n",
      "Operator 172: -0.8612117090433183\n",
      "Operator 173: 8.070527869203823e-06\n",
      "Operator 174: 0.4880341984621269\n",
      "Operator 175: -1.6586773910649568\n",
      "Operator 177: 2.1378987705088213e-07\n",
      "Operator 178: 0.48803390734585644\n",
      "Operator 179: -1.8213671589495415\n",
      "Operator 181: -1.8035934962089537\n",
      "Operator 183: 0.18819869052080873\n",
      "Operator 184: 0.4976628627078982\n",
      "Operator 185: 0.11599829344618998\n",
      "Operator 186: -6.258562447362884e-07\n",
      "Operator 187: -0.00012686116971704733\n",
      "Operator 188: -1.8213609581924923\n",
      "Operator 189: 1.8213671589495348\n",
      "Operator 190: 2.137898765798544e-07\n",
      "Operator 191: -3.309728308121481e-07\n",
      "Operator 192: -1.8213671655849484\n",
      "Operator 193: 2.000000000000017\n",
      "Operator 194: -2.0000000000000058\n",
      "Operator 195: 1.805982036691267\n",
      "Operator 196: 3.0758549350039776e-07\n",
      "Operator 197: -0.13734009256768584\n",
      "Operator 198: 0.18310783209144083\n",
      "Operator 199: -5.054652432801092e-08\n",
      "Operator 200: -0.00012686116971312255\n",
      "Operator 201: 0.0001207519243282345\n",
      "Operator 202: 0.4880593018905699\n",
      "Operator 203: -1.658683516922325\n",
      "Operator 205: 3.309825538535709e-07\n",
      "Operator 206: 0.4880339321095224\n",
      "Operator 207: -1.8213671655849495\n",
      "Operator 209: -1.8059820366912658\n",
      "Operator 211: 0.13597697456479385\n",
      "Operator 212: 0.06357765234950519\n",
      "Operator 213: 0.7981412564430721\n",
      "Operator 214: -2.4761133183509803e-05\n",
      "Operator 216: 1.6586773910649568\n",
      "Operator 217: -0.4880339073381725\n",
      "Operator 218: -2.137633195649805e-07\n",
      "Operator 220: 1.8213671589495528\n",
      "Operator 222: 1.8035934962089573\n",
      "Operator 223: -0.49783347771663844\n",
      "Operator 224: -0.13079815493818936\n",
      "Operator 225: 0.08177419441745302\n",
      "Operator 226: 1.8558099553124396\n",
      "Operator 227: 0.00012075192431258659\n",
      "Total gradient norm: 14.839815985215326\n",
      "Operators under consideration (1):\n",
      "[40]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.0775237264345012)]\n",
      "Operator(s) added to ansatz: [40]\n",
      "Gradients: [np.float64(-2.0775237264345012)]\n",
      "Initial energy: -25.740324264230622\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40]...\n",
      "Starting point: [np.float64(0.7853981474010936), np.float64(0.7853981803407033), np.float64(0.7853981886705501), np.float64(0.7853981906078695), np.float64(-0.7853981321557502), np.float64(-0.7853981653159808), np.float64(-0.7853982321485529), np.float64(-0.7853981628873773), np.float64(-0.1788981402343349), np.float64(-0.13686735175566525), np.float64(0.13448894473648665), np.float64(-0.13089970618793514), np.float64(0.16991848120588976), np.float64(0.1308997061900394), np.float64(0.16992685439216434), np.float64(-0.13090190205562025), np.float64(0.13090057935970137), np.float64(-0.1743556074073025), np.float64(0.1743556068510513), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.883035\n",
      "         Iterations: 20\n",
      "         Function evaluations: 178\n",
      "         Gradient evaluations: 159\n",
      "\n",
      "Current energy: -25.883034572918632\n",
      "(change of -0.1427103086880095)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40]\n",
      "On iteration 20.\n",
      "\n",
      "*** ADAPT-VQE Iteration 21 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 1: -4.8693506514209205e-08\n",
      "Operator 3: 1.0347612766705939e-07\n",
      "Operator 4: -1.2772183654117354e-08\n",
      "Operator 5: -2.2915661312961078e-07\n",
      "Operator 7: 2.2206826244541844e-07\n",
      "Operator 8: -3.491301303615302e-08\n",
      "Operator 9: -1.1710631286909745e-07\n",
      "Operator 11: 1.4207188868646637e-08\n",
      "Operator 12: 1.63382705493337e-08\n",
      "Operator 13: 4.9494593235591216e-08\n",
      "Operator 15: -1.314392882193032e-07\n",
      "Operator 16: -6.362458733302855e-08\n",
      "Operator 17: 2.0119345844893388e-07\n",
      "Operator 19: -2.2206826244541844e-07\n",
      "Operator 20: 3.491301403535374e-08\n",
      "Operator 21: 1.1564540436381776e-07\n",
      "Operator 22: -1.1679331130008355e-08\n",
      "Operator 23: -1.461752848229761e-08\n",
      "Operator 24: -1.37783652588639e-08\n",
      "Operator 25: 1.2247422739732738e-08\n",
      "Operator 26: -1.513812931228439e-08\n",
      "Operator 28: 5.66247624020644e-05\n",
      "Operator 29: 1.8213555145544031\n",
      "Operator 30: -1.8213671945703531\n",
      "Operator 31: -5.907162109318505e-08\n",
      "Operator 32: 5.9480127950996896e-08\n",
      "Operator 33: 1.8213671945934897\n",
      "Operator 34: -2.000000000000043\n",
      "Operator 35: 2.0000000000000275\n",
      "Operator 36: -1.8006598497243367\n",
      "Operator 38: 0.17947188072367373\n",
      "Operator 39: -0.17947189160523278\n",
      "Operator 41: 0.0001207061689880592\n",
      "Operator 42: 1.8213609419617605\n",
      "Operator 43: -1.8213671945934822\n",
      "Operator 44: -5.5917103371630445e-08\n",
      "Operator 45: 6.880597772779834e-08\n",
      "Operator 46: 1.8213671953233184\n",
      "Operator 47: -2.0000000000000373\n",
      "Operator 48: 2.0000000000000324\n",
      "Operator 49: -1.8006598481647298\n",
      "Operator 50: 2.6855153655212487e-08\n",
      "Operator 51: 0.17947191266035914\n",
      "Operator 52: -0.1794719019126393\n",
      "Operator 54: -1.1242285014546365e-08\n",
      "Operator 55: -2.486020414493877e-05\n",
      "Operator 56: -1.8213555145544018\n",
      "Operator 57: 1.8213671953233082\n",
      "Operator 58: 6.880597766126018e-08\n",
      "Operator 59: -5.591710490599785e-08\n",
      "Operator 60: -1.8213671945934924\n",
      "Operator 61: 2.0000000000000404\n",
      "Operator 62: -2.0000000000000275\n",
      "Operator 63: 1.8006598504950628\n",
      "Operator 64: 1.0891451404415526e-08\n",
      "Operator 65: -0.17947187733563147\n",
      "Operator 66: 0.17947189658388918\n",
      "Operator 68: -3.392463276129007e-05\n",
      "Operator 69: 8.24259956443818e-06\n",
      "Operator 70: 1.8213542291530183\n",
      "Operator 71: -1.8213671953233082\n",
      "Operator 72: -6.891543675114367e-08\n",
      "Operator 73: 5.56180542013588e-08\n",
      "Operator 74: 1.8213671945703607\n",
      "Operator 75: -2.000000000000036\n",
      "Operator 76: 2.0000000000000275\n",
      "Operator 77: -1.8006598504950584\n",
      "Operator 79: 0.17947189461700785\n",
      "Operator 80: -0.17947189939760694\n",
      "Operator 81: -1.0058796082854988e-08\n",
      "Operator 82: -0.0001267886940477318\n",
      "Operator 84: 1.6586785926152716\n",
      "Operator 85: -0.4880338837740902\n",
      "Operator 86: -6.891543696408923e-08\n",
      "Operator 88: 1.8213671945934928\n",
      "Operator 90: 1.8006598504950588\n",
      "Operator 91: -0.5076831447091321\n",
      "Operator 92: -0.10900284151880692\n",
      "Operator 93: -0.02766945830508288\n",
      "Operator 94: -0.15960365251115266\n",
      "Operator 95: -1.5138129644371055e-08\n",
      "Operator 97: 4.618501091283299e-08\n",
      "Operator 98: -6.384249443891281e-08\n",
      "Operator 99: -1.31439285358791e-07\n",
      "Operator 101: 2.2915661312961078e-07\n",
      "Operator 102: -3.162665927922263e-08\n",
      "Operator 103: -1.999347070125168e-07\n",
      "Operator 105: 1.0369027048096058e-07\n",
      "Operator 107: -1.306619795586863e-08\n",
      "Operator 108: -3.392463276065389e-05\n",
      "Operator 109: -1.8213542291530191\n",
      "Operator 110: 1.8213671945934835\n",
      "Operator 111: 5.9480125521106214e-08\n",
      "Operator 112: -5.9071620291629194e-08\n",
      "Operator 113: -1.8213671945703647\n",
      "Operator 114: 2.0000000000000355\n",
      "Operator 115: -2.000000000000033\n",
      "Operator 116: 1.80065984816473\n",
      "Operator 117: -2.339327092225436e-08\n",
      "Operator 118: -0.17947190389972814\n",
      "Operator 119: 0.17947190407817795\n",
      "Operator 120: 1.242632122684875e-08\n",
      "Operator 121: 1.593947349668145e-08\n",
      "Operator 123: -4.69145525055957e-08\n",
      "Operator 124: 6.384249775082659e-08\n",
      "Operator 125: 1.0347612732605491e-07\n",
      "Operator 127: -2.0119345822688928e-07\n",
      "Operator 128: 3.162665928783458e-08\n",
      "Operator 129: 1.999347070125168e-07\n",
      "Operator 131: -1.0500557134918154e-07\n",
      "Operator 133: 1.5286451281286007e-08\n",
      "Operator 134: 8.242599564646398e-06\n",
      "Operator 136: -1.6586785919295726\n",
      "Operator 137: 0.48803388096402367\n",
      "Operator 138: 5.561805435736724e-08\n",
      "Operator 140: -1.8213671945934906\n",
      "Operator 142: -1.8006598497243336\n",
      "Operator 143: 0.5076831419755083\n",
      "Operator 144: 0.10900283024801977\n",
      "Operator 145: 0.02766945697280973\n",
      "Operator 146: 0.1596036456691183\n",
      "Operator 147: 0.0001207061689862593\n",
      "Operator 149: -1.6586774220213152\n",
      "Operator 150: 0.4880338837740909\n",
      "Operator 151: 6.880597852354937e-08\n",
      "Operator 153: -1.8213671945703598\n",
      "Operator 155: -1.800659850495058\n",
      "Operator 156: 0.5076831447091312\n",
      "Operator 157: 0.10900284781398178\n",
      "Operator 158: 0.027669454488504247\n",
      "Operator 159: 0.15960364799179877\n",
      "Operator 160: -3.392463276123044e-05\n",
      "Operator 162: 1.6586835345804285\n",
      "Operator 163: -0.48803388096402267\n",
      "Operator 164: -5.9071618728050525e-08\n",
      "Operator 166: 1.8213671953233128\n",
      "Operator 168: 1.8006598497243325\n",
      "Operator 169: -0.5076831419755075\n",
      "Operator 170: -0.10900283974072802\n",
      "Operator 171: -0.027669451270150386\n",
      "Operator 172: -0.159603642472413\n",
      "Operator 173: 8.242599562967928e-06\n",
      "Operator 174: 0.48803423159390374\n",
      "Operator 175: -1.658677421356682\n",
      "Operator 177: 5.561805650336896e-08\n",
      "Operator 178: 0.48803388096402645\n",
      "Operator 179: -1.821367194570359\n",
      "Operator 181: -1.8006598481647296\n",
      "Operator 183: 0.15960364650165726\n",
      "Operator 184: 0.027669452204946092\n",
      "Operator 185: 0.10900282777316586\n",
      "Operator 186: 1.593947177196196e-08\n",
      "Operator 187: -0.00012678869404814946\n",
      "Operator 188: -1.8213609419617605\n",
      "Operator 189: 1.821367194570351\n",
      "Operator 190: 5.561805620149438e-08\n",
      "Operator 191: -6.891543733507851e-08\n",
      "Operator 192: -1.821367195323314\n",
      "Operator 193: 2.000000000000038\n",
      "Operator 194: -2.000000000000027\n",
      "Operator 195: 1.8006598497243334\n",
      "Operator 197: -0.1794719067656813\n",
      "Operator 198: 0.1794718922534083\n",
      "Operator 200: -0.00012678869404843729\n",
      "Operator 201: 0.0001207061689860062\n",
      "Operator 202: 0.4880592833434201\n",
      "Operator 203: -1.6586835345804274\n",
      "Operator 205: 6.880597829034587e-08\n",
      "Operator 206: 0.48803388377409285\n",
      "Operator 207: -1.821367195323315\n",
      "Operator 209: -1.8006598497243318\n",
      "Operator 211: 0.1596036654108553\n",
      "Operator 212: 0.027669455689398423\n",
      "Operator 213: 0.10900283006394612\n",
      "Operator 214: -2.486020415164492e-05\n",
      "Operator 216: 1.6586774213566815\n",
      "Operator 217: -0.48803388105035117\n",
      "Operator 218: -5.5917103827232904e-08\n",
      "Operator 220: 1.8213671945703636\n",
      "Operator 222: 1.8006598481647353\n",
      "Operator 223: -0.5076831364438806\n",
      "Operator 224: -0.10900281232878274\n",
      "Operator 225: -0.027669452286901104\n",
      "Operator 226: -0.15960365440935528\n",
      "Operator 227: 0.00012070616898722988\n",
      "Total gradient norm: 14.015325265108094\n",
      "Operators under consideration (1):\n",
      "[61]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.0000000000000404)]\n",
      "Operator(s) added to ansatz: [61]\n",
      "Gradients: [np.float64(2.0000000000000404)]\n",
      "Initial energy: -25.883034572918632\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61]...\n",
      "Starting point: [np.float64(0.7853981663187042), np.float64(0.7853981467625301), np.float64(0.7853981682576703), np.float64(0.7853981562564735), np.float64(-0.7853981786078968), np.float64(-0.7853981707887351), np.float64(-0.7853981629236478), np.float64(-0.7853981654407949), np.float64(-0.180661248784978), np.float64(-0.13740469213494397), np.float64(0.13740468983988247), np.float64(-0.1308996970101905), np.float64(0.16991846050345233), np.float64(0.13089969698649165), np.float64(0.16992685377794284), np.float64(-0.1309019053076336), np.float64(0.13090058839686883), np.float64(-0.13740469115063614), np.float64(0.18066124847154455), np.float64(0.1374046916131458), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.011787\n",
      "         Iterations: 13\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "\n",
      "Current energy: -26.01178724298368\n",
      "(change of -0.12875267006504743)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61]\n",
      "On iteration 21.\n",
      "\n",
      "*** ADAPT-VQE Iteration 22 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.509469816681123e-08\n",
      "Operator 2: -1.8028262943614592e-07\n",
      "Operator 3: 2.2331025240074354e-07\n",
      "Operator 4: -1.338511136442233e-07\n",
      "Operator 5: -3.6239907474922006e-07\n",
      "Operator 6: 9.480575879905473e-08\n",
      "Operator 7: 1.0976397035733498e-07\n",
      "Operator 8: 1.8814558910840162e-08\n",
      "Operator 9: 1.2298079665740147e-08\n",
      "Operator 11: -3.8926743350997016e-08\n",
      "Operator 12: 1.302832243659278e-07\n",
      "Operator 13: 5.747525376464946e-08\n",
      "Operator 14: 1.802826208041619e-07\n",
      "Operator 15: -2.7789260859112375e-07\n",
      "Operator 16: -1.326801718493087e-08\n",
      "Operator 17: 3.095989444812375e-07\n",
      "Operator 18: -9.480576168563459e-08\n",
      "Operator 19: -1.0976397035733498e-07\n",
      "Operator 20: -1.881455835572865e-08\n",
      "Operator 21: -3.435473971968861e-08\n",
      "Operator 22: -6.104168883237703e-08\n",
      "Operator 23: 2.775639722326429e-08\n",
      "Operator 24: -1.0570718374507433e-07\n",
      "Operator 25: 1.6420428339990064e-07\n",
      "Operator 26: -2.0480711933105835e-07\n",
      "Operator 27: 1.8109422110917e-07\n",
      "Operator 28: 5.649884994610697e-05\n",
      "Operator 29: 1.8213555200634708\n",
      "Operator 30: -1.8166758923877786\n",
      "Operator 31: -7.47567366555422e-08\n",
      "Operator 32: 0.07534065514166527\n",
      "Operator 33: 2.2787188924353288\n",
      "Operator 34: 2.3300720571147244e-08\n",
      "Operator 35: 2.0000000000000275\n",
      "Operator 36: -1.8006598952523087\n",
      "Operator 37: -2.738565638081061e-07\n",
      "Operator 38: 0.1794715379619166\n",
      "Operator 39: -0.17947193064569367\n",
      "Operator 41: 0.00012080679651591602\n",
      "Operator 42: 1.8213609645238291\n",
      "Operator 43: -1.817953286630911\n",
      "Operator 44: -8.449113505997509e-08\n",
      "Operator 45: 3.1047515706379325e-07\n",
      "Operator 46: -0.1311978923599289\n",
      "Operator 47: -2.5069058805777775\n",
      "Operator 48: 1.934695435523594\n",
      "Operator 49: -1.8006598544551151\n",
      "Operator 50: 1.9150733261116737e-07\n",
      "Operator 51: 0.17947169092720494\n",
      "Operator 52: -0.1794718891128933\n",
      "Operator 53: 1.372985341036683e-08\n",
      "Operator 54: -1.5455458109638762e-07\n",
      "Operator 55: -2.4659680264610256e-05\n",
      "Operator 56: -1.82135552006347\n",
      "Operator 57: 1.8166759109855772\n",
      "Operator 58: 3.104751564706411e-07\n",
      "Operator 59: -0.0753405705438242\n",
      "Operator 60: -2.27871889243533\n",
      "Operator 61: -2.3300720571147244e-08\n",
      "Operator 62: -2.0000000000000275\n",
      "Operator 63: 1.8006598530615654\n",
      "Operator 64: -2.14275013173717e-07\n",
      "Operator 65: -0.17947171161009298\n",
      "Operator 66: 0.17947190074806296\n",
      "Operator 68: -3.382141913806273e-05\n",
      "Operator 69: 8.058692294283608e-06\n",
      "Operator 70: 1.821354235947489\n",
      "Operator 71: -1.816675910985591\n",
      "Operator 72: -3.367499955806206e-07\n",
      "Operator 73: 0.04329676057230113\n",
      "Operator 74: 1.7573572784141922\n",
      "Operator 75: -1.9346954355235992\n",
      "Operator 76: 1.9346954355235908\n",
      "Operator 77: -1.8006598530615636\n",
      "Operator 78: 3.995191205502762e-07\n",
      "Operator 79: 0.1794721702611575\n",
      "Operator 80: -0.17947186803553244\n",
      "Operator 81: 1.888894017979182e-08\n",
      "Operator 82: -0.00012688323935569878\n",
      "Operator 84: 1.6544063493199435\n",
      "Operator 85: -0.490681519746037\n",
      "Operator 86: -0.04329707700348778\n",
      "Operator 88: 1.7585929628199497\n",
      "Operator 90: 1.8006598530615627\n",
      "Operator 91: -0.5076830731928723\n",
      "Operator 92: -0.10900246557732726\n",
      "Operator 93: -0.02766949442791846\n",
      "Operator 94: -0.15960364276553846\n",
      "Operator 95: -2.0480712035779458e-07\n",
      "Operator 97: 1.8065244244969136e-08\n",
      "Operator 98: -6.024480359023698e-08\n",
      "Operator 99: -2.778926070368115e-07\n",
      "Operator 100: 3.356433349533996e-08\n",
      "Operator 101: 3.6239907064139487e-07\n",
      "Operator 102: -2.1689012556176703e-08\n",
      "Operator 103: -9.882379359282822e-08\n",
      "Operator 104: -5.0470840562422126e-08\n",
      "Operator 105: -3.50467250775921e-08\n",
      "Operator 107: 4.703629839801679e-08\n",
      "Operator 108: -3.3821419137948646e-05\n",
      "Operator 109: -1.821354235947487\n",
      "Operator 110: 1.8179532866309125\n",
      "Operator 111: 1.7194452915541798e-07\n",
      "Operator 112: -7.475673557274102e-08\n",
      "Operator 113: 0.1311979009231375\n",
      "Operator 114: 2.5069058805777757\n",
      "Operator 115: -1.934695435523594\n",
      "Operator 116: 1.8006598544551151\n",
      "Operator 117: -3.831706219398811e-07\n",
      "Operator 118: -0.1794721650115818\n",
      "Operator 119: 0.179471859045495\n",
      "Operator 120: -2.2812166852446387e-08\n",
      "Operator 121: 2.58245566819835e-07\n",
      "Operator 123: -6.664232065495268e-08\n",
      "Operator 124: 6.024480742050642e-08\n",
      "Operator 125: 2.2331025503752322e-07\n",
      "Operator 127: -3.0959894348203676e-07\n",
      "Operator 128: 2.1689012562627004e-08\n",
      "Operator 129: 9.882379359282822e-08\n",
      "Operator 130: 5.047084067344443e-08\n",
      "Operator 131: 1.5188451607528464e-08\n",
      "Operator 132: 1.0686430225619926e-08\n",
      "Operator 133: -1.8490116078946077e-08\n",
      "Operator 134: 8.058692294521792e-06\n",
      "Operator 136: -1.6544063323833464\n",
      "Operator 137: 0.4906814508904287\n",
      "Operator 138: 0.04329676057230036\n",
      "Operator 140: -1.7585929628199475\n",
      "Operator 142: -1.8006598952523065\n",
      "Operator 143: 0.5076832228357722\n",
      "Operator 144: 0.10900309986318192\n",
      "Operator 145: 0.027669544899332925\n",
      "Operator 146: 0.15960368403540887\n",
      "Operator 147: 0.00012080679652311212\n",
      "Operator 149: -1.6544051829086626\n",
      "Operator 150: 0.49068151974603397\n",
      "Operator 151: 0.03152089261180714\n",
      "Operator 153: -2.2771177388653934\n",
      "Operator 155: -1.741864199324369\n",
      "Operator 156: 0.5076830731928716\n",
      "Operator 157: 0.10900263283950959\n",
      "Operator 158: 0.027669394591142517\n",
      "Operator 159: 0.15960363086077012\n",
      "Operator 160: -3.3821419144965074e-05\n",
      "Operator 162: 1.654411277793289\n",
      "Operator 163: -0.49068145089043536\n",
      "Operator 164: -0.03152066016694989\n",
      "Operator 166: 2.2771177621768537\n",
      "Operator 168: 1.7418642401374884\n",
      "Operator 169: -0.5076832228357713\n",
      "Operator 170: -0.1090030942414425\n",
      "Operator 171: -0.027669548309560366\n",
      "Operator 172: -0.159603688204579\n",
      "Operator 173: 8.058692304286462e-06\n",
      "Operator 174: 0.4880342019567724\n",
      "Operator 175: -1.6555684596799107\n",
      "Operator 177: 1.2966669818416422e-08\n",
      "Operator 178: 0.05510270609774566\n",
      "Operator 179: -1.7573572784141906\n",
      "Operator 181: -1.7418642006724188\n",
      "Operator 183: 0.15960392751313815\n",
      "Operator 184: 0.027669488986519997\n",
      "Operator 185: 0.10900283074676306\n",
      "Operator 186: 2.582455665866436e-07\n",
      "Operator 187: -0.00012688323935563844\n",
      "Operator 188: -1.821360964523828\n",
      "Operator 189: 1.8166758923877944\n",
      "Operator 190: 1.2966668936118411e-08\n",
      "Operator 191: -0.04329707700348602\n",
      "Operator 192: -1.757357296404726\n",
      "Operator 193: 1.934695435523601\n",
      "Operator 194: -1.9346954355235908\n",
      "Operator 195: 1.8006598952523065\n",
      "Operator 196: 2.8027574705537425e-07\n",
      "Operator 197: -0.1794715225286071\n",
      "Operator 198: 0.17947192800056394\n",
      "Operator 200: -0.00012688323935041313\n",
      "Operator 201: 0.00012080679652297936\n",
      "Operator 202: 0.4880593125501144\n",
      "Operator 203: -1.6544112777932876\n",
      "Operator 205: 0.03152089261180663\n",
      "Operator 206: 0.4746596482742096\n",
      "Operator 207: -2.2771177621768555\n",
      "Operator 209: -1.7418642401374884\n",
      "Operator 211: 0.15960344195883402\n",
      "Operator 212: 0.02766943695259253\n",
      "Operator 213: 0.10900285324660214\n",
      "Operator 214: -2.4659680281016376e-05\n",
      "Operator 216: 1.6555684596799067\n",
      "Operator 217: -0.4953898874997875\n",
      "Operator 218: -8.449113484034143e-08\n",
      "Operator 219: -0.12030384766244603\n",
      "Operator 220: -0.13119790092313705\n",
      "Operator 222: 1.7418642006724214\n",
      "Operator 223: -0.5076830781355479\n",
      "Operator 224: -0.10900265450144345\n",
      "Operator 225: -0.027669395334664382\n",
      "Operator 226: -0.15960361851951743\n",
      "Operator 227: 0.00012080679651550309\n",
      "Total gradient norm: 13.724197182874692\n",
      "Operators under consideration (1):\n",
      "[47]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.5069058805777775)]\n",
      "Operator(s) added to ansatz: [47]\n",
      "Gradients: [np.float64(-2.5069058805777775)]\n",
      "Initial energy: -26.01178724298368\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47]...\n",
      "Starting point: [np.float64(0.7853981762824264), np.float64(0.7853981576769999), np.float64(0.7853981390041616), np.float64(0.7853981330635854), np.float64(-0.785398188170196), np.float64(-0.7853981642591188), np.float64(-0.785398131788285), np.float64(-0.7853981830143435), np.float64(-0.18066125822777046), np.float64(-0.1374046732827254), np.float64(0.13740467465518857), np.float64(-0.13310881618520007), np.float64(0.17186464957187475), np.float64(0.13181337894975687), np.float64(0.1699268555398778), np.float64(-0.13090189602171914), np.float64(0.13090058042778607), np.float64(-0.1374046912177209), np.float64(0.18066125007975048), np.float64(0.1374046903980121), np.float64(-0.12812404373377936), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.231054\n",
      "         Iterations: 17\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 89\n",
      "\n",
      "Current energy: -26.23105399231835\n",
      "(change of -0.21926674933467183)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47]\n",
      "On iteration 22.\n",
      "\n",
      "*** ADAPT-VQE Iteration 23 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.1160060900027155e-08\n",
      "Operator 1: -7.953788918046989e-07\n",
      "Operator 2: -1.1261349242008833e-08\n",
      "Operator 3: 7.229616745263456e-07\n",
      "Operator 4: -1.433158545727231e-07\n",
      "Operator 5: -1.4739607598901472e-06\n",
      "Operator 6: -1.966775013961808e-07\n",
      "Operator 7: 7.190509947463397e-07\n",
      "Operator 8: -1.5193924374212742e-07\n",
      "Operator 9: -6.44590626430741e-08\n",
      "Operator 11: 7.635420841182644e-08\n",
      "Operator 12: 4.615868497366019e-07\n",
      "Operator 13: 1.044565003938125e-06\n",
      "Operator 14: 1.1261340748802695e-08\n",
      "Operator 15: -9.23509572814396e-07\n",
      "Operator 16: -3.7959657453456863e-07\n",
      "Operator 17: 1.1649501876487278e-06\n",
      "Operator 18: -4.0089793706954424e-07\n",
      "Operator 19: -9.027592327015554e-07\n",
      "Operator 20: 1.5193924540746195e-07\n",
      "Operator 21: 3.08252262604114e-08\n",
      "Operator 22: -7.998927860240869e-08\n",
      "Operator 23: -7.414314648766154e-08\n",
      "Operator 24: -3.696729723009877e-07\n",
      "Operator 25: 6.800882655485115e-07\n",
      "Operator 26: -1.2684382317208224e-06\n",
      "Operator 27: 7.119444152663332e-07\n",
      "Operator 28: 5.564238995320407e-05\n",
      "Operator 29: 1.8213555649479023\n",
      "Operator 30: -1.8035933632158594\n",
      "Operator 31: -2.672998946073868e-07\n",
      "Operator 32: 0.20956778125466474\n",
      "Operator 33: 1.8328793026524326\n",
      "Operator 34: 0.7553228537807306\n",
      "Operator 35: 1.8796276832251353\n",
      "Operator 36: -1.8006597236169042\n",
      "Operator 37: 9.52826459237119e-07\n",
      "Operator 38: 0.17947297606796003\n",
      "Operator 39: -0.1794712813564962\n",
      "Operator 40: 5.590237788384793e-07\n",
      "Operator 41: 0.0001215127142215661\n",
      "Operator 42: 1.8213611295390868\n",
      "Operator 43: -1.80598190812472\n",
      "Operator 44: -2.2381311737262877e-07\n",
      "Operator 45: 0.13734148971723703\n",
      "Operator 46: -0.18310806810332558\n",
      "Operator 47: 1.5777845263631463e-07\n",
      "Operator 48: 1.8796276796259486\n",
      "Operator 49: -1.8006598507420621\n",
      "Operator 50: -5.00067822366513e-07\n",
      "Operator 51: 0.17947241446850581\n",
      "Operator 52: -0.1794721614808264\n",
      "Operator 53: 2.3092214664193158e-07\n",
      "Operator 54: -6.439986031238165e-07\n",
      "Operator 55: -2.3244121228329492e-05\n",
      "Operator 56: -1.821355564947901\n",
      "Operator 57: 1.805981981871506\n",
      "Operator 58: 1.425954909067828e-06\n",
      "Operator 59: -0.13734031295665172\n",
      "Operator 60: 0.18310808527104905\n",
      "Operator 61: -2.3598092191434716e-07\n",
      "Operator 62: -1.879627683225135\n",
      "Operator 63: 1.8006598507498992\n",
      "Operator 64: 5.001008269621886e-07\n",
      "Operator 65: -0.17947241413681714\n",
      "Operator 66: 0.17947216553556217\n",
      "Operator 67: -2.2666328933703577e-07\n",
      "Operator 68: -3.2999857577328365e-05\n",
      "Operator 69: 6.668680245770041e-06\n",
      "Operator 70: 1.821354282808645\n",
      "Operator 71: -1.8059819818714775\n",
      "Operator 72: -1.5371741237668601e-06\n",
      "Operator 73: 0.08105242771071292\n",
      "Operator 74: -0.8657284789687092\n",
      "Operator 75: -2.0775236633981145\n",
      "Operator 76: 1.7665001103905476\n",
      "Operator 77: -1.800659850749902\n",
      "Operator 78: -1.070578765075867e-06\n",
      "Operator 79: 0.1794709677824734\n",
      "Operator 80: -0.17947254461061085\n",
      "Operator 81: -6.167403330585496e-07\n",
      "Operator 82: -0.00012757980559481913\n",
      "Operator 84: 1.6446676664386026\n",
      "Operator 85: -0.5064188440942562\n",
      "Operator 86: -0.07685401631261754\n",
      "Operator 87: -0.11569480409716404\n",
      "Operator 88: -0.8612119993942856\n",
      "Operator 90: 1.6922850517707486\n",
      "Operator 91: -0.5076833283541107\n",
      "Operator 92: -0.10900383527865906\n",
      "Operator 93: -0.02766936479027246\n",
      "Operator 94: -0.15960422830064092\n",
      "Operator 95: -1.2684382315342799e-06\n",
      "Operator 97: 7.36697285841359e-07\n",
      "Operator 98: -4.094479513216953e-07\n",
      "Operator 99: -8.679270777633974e-07\n",
      "Operator 100: 4.0331208414623454e-08\n",
      "Operator 101: 1.3433794243677966e-06\n",
      "Operator 103: -6.473831319908641e-07\n",
      "Operator 104: -1.3107567420345134e-08\n",
      "Operator 105: 1.2358666312550781e-08\n",
      "Operator 106: 2.996876709027257e-08\n",
      "Operator 107: -4.668709440715391e-08\n",
      "Operator 108: -3.299985757707348e-05\n",
      "Operator 109: -1.8213542828086815\n",
      "Operator 110: 1.8059819081247208\n",
      "Operator 111: 6.680023201255381e-07\n",
      "Operator 112: -0.08105274216864869\n",
      "Operator 113: 0.8657284931247549\n",
      "Operator 114: 2.077523686140772\n",
      "Operator 115: -1.7665001103905489\n",
      "Operator 116: 1.8006598507420626\n",
      "Operator 117: 1.0705550668829787e-06\n",
      "Operator 118: -0.1794709679590697\n",
      "Operator 119: 0.1794725416419417\n",
      "Operator 120: 6.155396079146306e-07\n",
      "Operator 121: 1.5176199177653324e-06\n",
      "Operator 123: -9.617101616732349e-07\n",
      "Operator 124: 4.0944795459685324e-07\n",
      "Operator 125: 6.794493838668814e-07\n",
      "Operator 126: 8.727236162908714e-08\n",
      "Operator 127: -1.2854836450859608e-06\n",
      "Operator 129: 8.12781159709175e-07\n",
      "Operator 130: 1.3107567364833983e-08\n",
      "Operator 131: -4.264020470046148e-08\n",
      "Operator 133: 1.0946129070029149e-07\n",
      "Operator 134: 6.6686802460706755e-06\n",
      "Operator 136: -1.6424924044980962\n",
      "Operator 137: 0.49783345271840806\n",
      "Operator 138: 0.13079807909911362\n",
      "Operator 139: -0.08177422620845977\n",
      "Operator 140: -1.8558097538945524\n",
      "Operator 142: -1.692284932289403\n",
      "Operator 143: 0.5076828774364619\n",
      "Operator 144: 0.10900198062462126\n",
      "Operator 145: 0.027669139203097853\n",
      "Operator 146: 0.15960301324434162\n",
      "Operator 147: 0.00012151271420513812\n",
      "Operator 149: -1.6446665086784018\n",
      "Operator 150: 0.506418844094259\n",
      "Operator 151: 0.05556724420969129\n",
      "Operator 152: 0.12254908367839078\n",
      "Operator 153: 0.18054679212795938\n",
      "Operator 155: -1.5904329125627372\n",
      "Operator 156: 0.5076833283541096\n",
      "Operator 157: 0.10900330733741845\n",
      "Operator 158: 0.02766967588441236\n",
      "Operator 159: 0.15960398825428473\n",
      "Operator 160: -3.299985767961361e-05\n",
      "Operator 162: 1.642497422627916\n",
      "Operator 163: -0.4978334527183789\n",
      "Operator 164: -0.13079837617235085\n",
      "Operator 165: 0.08177424840846653\n",
      "Operator 166: 1.8558098510362355\n",
      "Operator 168: 1.692284929048947\n",
      "Operator 169: -0.5076828774364697\n",
      "Operator 170: -0.10900198068305271\n",
      "Operator 171: -0.027669139183912356\n",
      "Operator 172: -0.15960301432183988\n",
      "Operator 173: 6.668680378720783e-06\n",
      "Operator 174: 0.488033971849393\n",
      "Operator 175: -1.6446664415189218\n",
      "Operator 177: 0.058121961690736246\n",
      "Operator 178: -0.12746982304732982\n",
      "Operator 179: -1.852797951810762\n",
      "Operator 181: -1.5904329125558128\n",
      "Operator 183: 0.15960270089657014\n",
      "Operator 184: 0.02766921556497577\n",
      "Operator 185: 0.10900232751652741\n",
      "Operator 186: 1.5176199180619254e-06\n",
      "Operator 187: -0.00012757980559523126\n",
      "Operator 188: -1.8213611295390417\n",
      "Operator 189: 1.8035933632158214\n",
      "Operator 190: -6.979479305613268e-08\n",
      "Operator 191: -0.2095686178991955\n",
      "Operator 192: -1.8328793742706777\n",
      "Operator 193: -0.755322791226798\n",
      "Operator 194: -1.8796276796259472\n",
      "Operator 195: 1.80065972361691\n",
      "Operator 196: -9.528357665103667e-07\n",
      "Operator 197: -0.17947297622305802\n",
      "Operator 198: 0.17947128027043247\n",
      "Operator 199: -5.620819027935116e-07\n",
      "Operator 200: -0.00012757980561973957\n",
      "Operator 201: 0.00012151271420557631\n",
      "Operator 202: 0.48805952338771985\n",
      "Operator 203: -1.6424974226279154\n",
      "Operator 205: 0.18819980576940576\n",
      "Operator 206: 0.497663093037276\n",
      "Operator 207: 0.11599844104164195\n",
      "Operator 208: 0.6422441700834711\n",
      "Operator 209: -1.6922849290489472\n",
      "Operator 211: 0.15960420902323277\n",
      "Operator 212: 0.027669620310059553\n",
      "Operator 213: 0.1090030467823131\n",
      "Operator 214: -2.3244121337117416e-05\n",
      "Operator 216: 1.6446664415189602\n",
      "Operator 217: -0.5064185810998023\n",
      "Operator 218: -0.05556608384727779\n",
      "Operator 219: -0.12254897800753986\n",
      "Operator 220: -0.1805468175737867\n",
      "Operator 222: 1.5904329125558139\n",
      "Operator 223: -0.5076833283262973\n",
      "Operator 224: -0.10900330743088905\n",
      "Operator 225: -0.027669675600760536\n",
      "Operator 226: -0.1596039841766894\n",
      "Operator 227: 0.00012151271422127939\n",
      "Total gradient norm: 12.291471797337087\n",
      "Operators under consideration (1):\n",
      "[114]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.077523686140772)]\n",
      "Operator(s) added to ansatz: [114]\n",
      "Gradients: [np.float64(2.077523686140772)]\n",
      "Initial energy: -26.23105399231835\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114]...\n",
      "Starting point: [np.float64(0.7853981811218832), np.float64(0.785398089089067), np.float64(0.7853981853934172), np.float64(0.7853981064824795), np.float64(-0.7853982644561727), np.float64(-0.7853981578885456), np.float64(-0.7853980160100448), np.float64(-0.7853982367642387), np.float64(-0.18066121178819924), np.float64(-0.13740473918673773), np.float64(0.137404739179017), np.float64(-0.13686737354417036), np.float64(0.17889822248090878), np.float64(0.13448894629231062), np.float64(0.16992686769045165), np.float64(-0.13090183197906194), np.float64(0.13090051840967257), np.float64(-0.1374047174550754), np.float64(0.1806612226173114), np.float64(0.13740471719481284), np.float64(-0.1743556284688181), np.float64(0.17435562583542333), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.373764\n",
      "         Iterations: 17\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "Current energy: -26.373764301006318\n",
      "(change of -0.14271030868796686)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114]\n",
      "On iteration 23.\n",
      "\n",
      "*** ADAPT-VQE Iteration 24 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.645635141584284e-07\n",
      "Operator 1: 8.715883512006029e-07\n",
      "Operator 2: -2.1478191643276023e-07\n",
      "Operator 3: -9.385141831186328e-07\n",
      "Operator 4: 5.560847008562186e-07\n",
      "Operator 5: -6.499581179930658e-07\n",
      "Operator 6: 4.1433140252067346e-08\n",
      "Operator 7: 2.2395370180561258e-07\n",
      "Operator 8: -5.1020120905853616e-08\n",
      "Operator 9: 4.074719370805724e-07\n",
      "Operator 10: -2.660298570379916e-08\n",
      "Operator 11: -2.0533901644107651e-07\n",
      "Operator 12: 4.057007207363817e-07\n",
      "Operator 13: -6.897952078283032e-07\n",
      "Operator 14: 2.1478190791179852e-07\n",
      "Operator 15: 8.125097977207396e-07\n",
      "Operator 16: -8.769949402936961e-07\n",
      "Operator 17: 5.365126245493457e-07\n",
      "Operator 18: -4.143303539150267e-08\n",
      "Operator 19: -2.2395366272576211e-07\n",
      "Operator 20: 5.102012190505434e-08\n",
      "Operator 21: -4.792508752071711e-07\n",
      "Operator 22: -1.562049411720105e-07\n",
      "Operator 23: 1.6891833640085157e-07\n",
      "Operator 24: -3.259182020644904e-07\n",
      "Operator 25: 5.651853927017978e-07\n",
      "Operator 26: -5.898051248127465e-07\n",
      "Operator 27: 6.612134082358083e-07\n",
      "Operator 28: 5.587634137890277e-05\n",
      "Operator 29: 1.821355552573671\n",
      "Operator 30: -1.800659859731009\n",
      "Operator 31: -1.707590744308111e-06\n",
      "Operator 32: 0.17947087568217596\n",
      "Operator 33: -0.17947196530547538\n",
      "Operator 34: 3.2847490777233824e-07\n",
      "Operator 35: 1.800659887420627\n",
      "Operator 36: -1.8006598761326367\n",
      "Operator 37: -1.8119823711323128e-07\n",
      "Operator 38: 0.17947162013379403\n",
      "Operator 39: -0.17947257308631345\n",
      "Operator 40: -9.558983363305878e-07\n",
      "Operator 41: 0.00012131960549592015\n",
      "Operator 42: 1.8213610842911683\n",
      "Operator 43: -1.8006596863032203\n",
      "Operator 44: 8.191513368998441e-07\n",
      "Operator 45: 0.17947279646856662\n",
      "Operator 46: -0.17947246604179\n",
      "Operator 47: -7.892200379058113e-07\n",
      "Operator 48: 1.8006598040630033\n",
      "Operator 49: -1.8006598507772758\n",
      "Operator 50: 8.616527118141202e-08\n",
      "Operator 51: 0.17947178031889385\n",
      "Operator 52: -0.1794714072550569\n",
      "Operator 53: -4.886542305684122e-07\n",
      "Operator 54: -5.33859009202877e-07\n",
      "Operator 55: -2.363155389445958e-05\n",
      "Operator 56: -1.8213555525736704\n",
      "Operator 57: 1.8006597971904483\n",
      "Operator 58: 9.98562662468589e-07\n",
      "Operator 59: -0.1794711644153211\n",
      "Operator 60: 0.17947127778226002\n",
      "Operator 61: -5.942662152270238e-07\n",
      "Operator 62: -1.8006598874206263\n",
      "Operator 63: 1.8006598507698217\n",
      "Operator 64: -8.629280482716359e-08\n",
      "Operator 65: -0.17947178041715428\n",
      "Operator 66: 0.1794714075579606\n",
      "Operator 67: 4.889820366741934e-07\n",
      "Operator 68: -3.3226065376366596e-05\n",
      "Operator 69: 7.050444330697153e-06\n",
      "Operator 70: 1.8213542699958003\n",
      "Operator 71: -1.8006597971903264\n",
      "Operator 72: -2.0767359557565598e-07\n",
      "Operator 73: 0.1794731054114686\n",
      "Operator 74: -0.17947127927260081\n",
      "Operator 75: 2.777661462311351e-07\n",
      "Operator 76: 1.8006598638905045\n",
      "Operator 77: -1.8006598507698088\n",
      "Operator 78: 1.9128633777303508e-07\n",
      "Operator 79: 0.17947209128006059\n",
      "Operator 80: -0.17947097095598336\n",
      "Operator 81: 7.004043785193545e-07\n",
      "Operator 82: -0.00012738877197754966\n",
      "Operator 84: 1.63982085995441\n",
      "Operator 85: -0.5076832597719256\n",
      "Operator 86: -0.10900307897351115\n",
      "Operator 87: -0.02766955906536185\n",
      "Operator 88: -0.1596031165629077\n",
      "Operator 90: 1.6211879820849604\n",
      "Operator 91: -0.507683102256375\n",
      "Operator 92: -0.10900264597154978\n",
      "Operator 93: -0.02766945110386139\n",
      "Operator 94: -0.15960273825672106\n",
      "Operator 95: -5.898051249268426e-07\n",
      "Operator 97: -8.426327493260555e-07\n",
      "Operator 98: -1.5276775838790613e-07\n",
      "Operator 99: 7.291644525397878e-07\n",
      "Operator 100: -2.0412510975958263e-07\n",
      "Operator 101: 6.461919577205322e-07\n",
      "Operator 103: -2.0163222202995712e-07\n",
      "Operator 104: -1.2456603992738735e-07\n",
      "Operator 105: -4.448462880279891e-07\n",
      "Operator 106: 4.345778958117778e-08\n",
      "Operator 107: 2.3000502398826447e-07\n",
      "Operator 108: -3.322606537619465e-05\n",
      "Operator 109: -1.8213542699958913\n",
      "Operator 110: 1.8006596863032214\n",
      "Operator 111: -1.0975489468191526e-06\n",
      "Operator 112: -0.17947351376125256\n",
      "Operator 113: 0.17947218693145223\n",
      "Operator 114: 1.1229789251548593e-07\n",
      "Operator 115: -1.8006598638904892\n",
      "Operator 116: 1.8006598507772762\n",
      "Operator 117: -1.9119476212636773e-07\n",
      "Operator 118: -0.17947209126052222\n",
      "Operator 119: 0.17947097073153534\n",
      "Operator 120: -7.004967782363844e-07\n",
      "Operator 121: 7.715869620810151e-07\n",
      "Operator 123: 6.789591744521539e-07\n",
      "Operator 124: 1.52767761274486e-07\n",
      "Operator 125: -8.426099733366277e-07\n",
      "Operator 126: 2.8173880919386726e-07\n",
      "Operator 127: -5.327464636106782e-07\n",
      "Operator 129: 2.0163218117374981e-07\n",
      "Operator 130: 1.2456603970534275e-07\n",
      "Operator 131: 3.8022155868233654e-07\n",
      "Operator 133: -1.3717625248964214e-07\n",
      "Operator 134: 7.050444330798144e-06\n",
      "Operator 136: -1.6398209169087392\n",
      "Operator 137: 0.5076834815918488\n",
      "Operator 138: 0.10900397332096179\n",
      "Operator 139: 0.0276696934168233\n",
      "Operator 140: 0.15960406119454684\n",
      "Operator 142: -1.621188004919861\n",
      "Operator 143: 0.507683192213673\n",
      "Operator 144: 0.10900294542732854\n",
      "Operator 145: 0.027669587664244565\n",
      "Operator 146: 0.15960433925827344\n",
      "Operator 147: 0.00012131960561191201\n",
      "Operator 149: -1.639819705211142\n",
      "Operator 150: 0.507683259771971\n",
      "Operator 151: 0.10900378657788001\n",
      "Operator 152: 0.027669134662436694\n",
      "Operator 153: 0.1596029254057208\n",
      "Operator 155: -1.6211879609000857\n",
      "Operator 156: 0.5076831022563735\n",
      "Operator 157: 0.10900276005974319\n",
      "Operator 158: 0.02766938905560909\n",
      "Operator 159: 0.1596031471291655\n",
      "Operator 160: -3.3226065516043006e-05\n",
      "Operator 162: 1.639825897279445\n",
      "Operator 163: -0.5076834815920097\n",
      "Operator 164: -0.10900441894398077\n",
      "Operator 165: -0.027669422059805725\n",
      "Operator 166: -0.15960365932375187\n",
      "Operator 168: 1.6211879298705094\n",
      "Operator 169: -0.5076831922136638\n",
      "Operator 170: -0.1090029453987684\n",
      "Operator 171: -0.0276695876825254\n",
      "Operator 172: -0.1596043393449006\n",
      "Operator 173: 7.050444414813049e-06\n",
      "Operator 174: 0.48803403513018107\n",
      "Operator 175: -1.6398196042288715\n",
      "Operator 177: 0.15960481448336789\n",
      "Operator 178: 0.02766945352835975\n",
      "Operator 179: 0.10900307083512184\n",
      "Operator 180: 0.5076831818584749\n",
      "Operator 181: -1.6211879609068116\n",
      "Operator 183: 0.15960385754555334\n",
      "Operator 184: 0.027669701220700307\n",
      "Operator 185: 0.10900347963712236\n",
      "Operator 186: 7.715869618722133e-07\n",
      "Operator 187: -0.00012738877197739865\n",
      "Operator 188: -1.8213610842911057\n",
      "Operator 189: 1.8006598597309187\n",
      "Operator 190: 1.1950992978782403e-06\n",
      "Operator 191: -0.17947209938569442\n",
      "Operator 192: 0.17947224590618882\n",
      "Operator 193: 6.649473003997906e-07\n",
      "Operator 194: -1.8006598040630193\n",
      "Operator 195: 1.8006598761326216\n",
      "Operator 196: 1.8123419226259387e-07\n",
      "Operator 197: -0.17947162005508593\n",
      "Operator 198: 0.17947257300786398\n",
      "Operator 199: 9.556629330109048e-07\n",
      "Operator 200: -0.00012738877189422953\n",
      "Operator 201: 0.00012131960561215951\n",
      "Operator 202: 0.48805946562234315\n",
      "Operator 203: -1.6398258972794442\n",
      "Operator 205: 0.15960455179223715\n",
      "Operator 206: 0.02766937707080678\n",
      "Operator 207: 0.10900214049752344\n",
      "Operator 208: 0.5076829696612355\n",
      "Operator 209: -1.6211879298705092\n",
      "Operator 211: 0.1596034772866136\n",
      "Operator 212: 0.027669302991405815\n",
      "Operator 213: 0.10900236967193212\n",
      "Operator 214: -2.36315538760745e-05\n",
      "Operator 216: 1.6398196042289714\n",
      "Operator 217: -0.5076828664757038\n",
      "Operator 218: -0.10900204919017867\n",
      "Operator 219: -0.027669093299440582\n",
      "Operator 220: -0.1596041715564968\n",
      "Operator 222: 1.6211879609067972\n",
      "Operator 223: -0.5076831022828555\n",
      "Operator 224: -0.10900276018836277\n",
      "Operator 225: -0.027669389043273422\n",
      "Operator 226: -0.1596031468208987\n",
      "Operator 227: 0.00012131960549581264\n",
      "Total gradient norm: 10.971038777030616\n",
      "Operators under consideration (1):\n",
      "[42]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.8213610842911683)]\n",
      "Operator(s) added to ansatz: [42]\n",
      "Gradients: [np.float64(1.8213610842911683)]\n",
      "Initial energy: -26.373764301006318\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42]...\n",
      "Starting point: [np.float64(0.7853981939801831), np.float64(0.7853981477175122), np.float64(0.7853980341266944), np.float64(0.7853982253267517), np.float64(-0.7853982530788142), np.float64(-0.7853981259493737), np.float64(-0.7853980558705561), np.float64(-0.7853982270807697), np.float64(-0.18066125515367576), np.float64(-0.13740468102541675), np.float64(0.13740468103277087), np.float64(-0.13740461972657977), np.float64(0.1806612943069271), np.float64(0.13740479052959786), np.float64(0.16992686437123483), np.float64(-0.13090184948568725), np.float64(0.13090053546709596), np.float64(-0.13740464833903857), np.float64(0.18066127230927576), np.float64(0.1374046483191772), np.float64(-0.18066127167384777), np.float64(0.13740464596715402), np.float64(-0.13740470488923368), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.470832\n",
      "         Iterations: 29\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 63\n",
      "\n",
      "Current energy: -26.470832476960606\n",
      "(change of -0.09706817595428774)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42]\n",
      "On iteration 24.\n",
      "\n",
      "*** ADAPT-VQE Iteration 25 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.85317343106167e-07\n",
      "Operator 1: -6.783068082114554e-07\n",
      "Operator 2: 9.969577804337403e-07\n",
      "Operator 3: -7.136571106672598e-07\n",
      "Operator 4: 6.51965888492335e-07\n",
      "Operator 5: 1.0683728637831727e-06\n",
      "Operator 6: -3.629496916015884e-08\n",
      "Operator 7: -4.221257924440919e-07\n",
      "Operator 8: -3.884868688373844e-08\n",
      "Operator 9: -1.5700504946494775e-08\n",
      "Operator 10: -2.224973352782378e-08\n",
      "Operator 11: 2.4548415210323693e-07\n",
      "Operator 12: -3.018958186044862e-07\n",
      "Operator 13: 5.811874079974323e-07\n",
      "Operator 14: -9.969577875216878e-07\n",
      "Operator 15: 8.748057686975252e-07\n",
      "Operator 16: -2.3629944626362574e-07\n",
      "Operator 17: -9.199651085194451e-07\n",
      "Operator 18: 3.6294952987367725e-08\n",
      "Operator 19: 4.221257854982591e-07\n",
      "Operator 20: 3.884868770946681e-08\n",
      "Operator 21: 9.747192549099115e-08\n",
      "Operator 22: 2.3050665356582484e-07\n",
      "Operator 23: -1.909565283536585e-07\n",
      "Operator 24: 2.4788451145549284e-07\n",
      "Operator 25: -3.195113683629458e-07\n",
      "Operator 26: 3.067682783380833e-07\n",
      "Operator 27: -5.632322263749243e-07\n",
      "Operator 28: 7.107761878005448e-07\n",
      "Operator 29: 2.2215149986470264\n",
      "Operator 30: -0.28857717611882944\n",
      "Operator 32: 0.18186877121750508\n",
      "Operator 33: -0.18194904740058418\n",
      "Operator 35: 1.8002863794481596\n",
      "Operator 36: -1.8006596785034814\n",
      "Operator 37: -9.094260096092613e-07\n",
      "Operator 38: 0.17947281934062373\n",
      "Operator 39: -0.1794714232191866\n",
      "Operator 40: 3.597040883605416e-08\n",
      "Operator 41: 1.9320633948218084e-06\n",
      "Operator 43: -2.148016153204461\n",
      "Operator 44: -0.05487640125350484\n",
      "Operator 45: 0.1810366275764619\n",
      "Operator 46: -0.18310452640054387\n",
      "Operator 47: -1.0451070347373774e-07\n",
      "Operator 48: 1.8002640188832615\n",
      "Operator 49: -1.8006596809550772\n",
      "Operator 50: -9.219653971647828e-07\n",
      "Operator 51: 0.17947279798348972\n",
      "Operator 52: -0.17947190521242262\n",
      "Operator 53: -1.6927127281207006e-07\n",
      "Operator 54: 2.983040163773154e-07\n",
      "Operator 55: -3.5977124186032964e-08\n",
      "Operator 56: -2.221514998647026\n",
      "Operator 57: 0.2885771784281066\n",
      "Operator 58: 4.4406720379824005e-08\n",
      "Operator 59: -0.18186875824203128\n",
      "Operator 60: 0.18194897822321468\n",
      "Operator 61: -1.9517723217652293e-08\n",
      "Operator 62: -1.800286379448159\n",
      "Operator 63: 1.800659682042643\n",
      "Operator 64: 9.39860815021916e-07\n",
      "Operator 65: -0.17947278212964762\n",
      "Operator 66: 0.17947189062936172\n",
      "Operator 67: 1.5261923235226961e-07\n",
      "Operator 68: 0.15619229691630557\n",
      "Operator 69: -5.293242480694693e-07\n",
      "Operator 70: 1.7918907006710865\n",
      "Operator 71: -1.7562654199555425\n",
      "Operator 72: -0.031838567511007373\n",
      "Operator 73: 0.18105087637442896\n",
      "Operator 74: -0.18259580417137736\n",
      "Operator 76: 1.8002640251514552\n",
      "Operator 77: -1.8006596820426402\n",
      "Operator 78: -9.467654565570307e-07\n",
      "Operator 79: 0.17947274380262113\n",
      "Operator 80: -0.17947206620981201\n",
      "Operator 81: -6.448798788750503e-07\n",
      "Operator 82: -0.008667802172482822\n",
      "Operator 83: 0.13257293752911886\n",
      "Operator 84: 1.6105237540830637\n",
      "Operator 85: -0.1527956292741105\n",
      "Operator 86: -0.10988093328969449\n",
      "Operator 87: -0.027748111776665797\n",
      "Operator 88: -0.1617529434924758\n",
      "Operator 90: 1.6208515498014089\n",
      "Operator 91: -0.5076832986334135\n",
      "Operator 92: -0.10900371585123526\n",
      "Operator 93: -0.027669235788526123\n",
      "Operator 94: -0.15960380447781464\n",
      "Operator 95: 3.0676827966005566e-07\n",
      "Operator 96: -4.84939121696541e-08\n",
      "Operator 97: 5.728811579744985e-07\n",
      "Operator 98: 6.283524552258061e-08\n",
      "Operator 99: 7.913322485867701e-07\n",
      "Operator 100: -2.1952727800625672e-07\n",
      "Operator 101: -1.0718356051641109e-06\n",
      "Operator 103: 3.800524438334757e-07\n",
      "Operator 104: 1.7618752341796995e-07\n",
      "Operator 105: 9.680299020362049e-08\n",
      "Operator 107: -2.6357353859196575e-07\n",
      "Operator 108: 0.15619229691630568\n",
      "Operator 109: -0.22082322007116725\n",
      "Operator 110: 2.1480161532044617\n",
      "Operator 111: 0.05487641972518768\n",
      "Operator 112: -0.18103658451371574\n",
      "Operator 113: 0.18310450301617687\n",
      "Operator 114: 3.1038374920839186e-08\n",
      "Operator 115: -1.8002640251514705\n",
      "Operator 116: 1.8006596809550772\n",
      "Operator 117: 9.339155237180926e-07\n",
      "Operator 118: -0.17947274770311214\n",
      "Operator 119: 0.1794720772581789\n",
      "Operator 120: 6.495748147197965e-07\n",
      "Operator 121: -3.9639531992392595e-07\n",
      "Operator 122: 1.0480297033348317e-07\n",
      "Operator 123: -4.919195844886569e-07\n",
      "Operator 124: -6.283524291686977e-08\n",
      "Operator 125: -6.462753828518493e-07\n",
      "Operator 126: 1.192656451501305e-07\n",
      "Operator 127: 9.234278352801324e-07\n",
      "Operator 129: -3.8005243679396783e-07\n",
      "Operator 130: -1.761875235168492e-07\n",
      "Operator 131: -2.3181729708575405e-08\n",
      "Operator 132: -5.206300440194224e-08\n",
      "Operator 133: 1.7085864478794122e-07\n",
      "Operator 134: -5.293242476660982e-07\n",
      "Operator 135: -0.0975528094410304\n",
      "Operator 136: -1.610523750466076\n",
      "Operator 137: 0.15279562311264966\n",
      "Operator 138: 0.10988086807720639\n",
      "Operator 139: 0.027748113716125185\n",
      "Operator 140: 0.16175303850655526\n",
      "Operator 142: -1.6208515466156592\n",
      "Operator 143: 0.507683286080663\n",
      "Operator 144: 0.10900369580084691\n",
      "Operator 145: 0.027669188522665887\n",
      "Operator 146: 0.1596031624626433\n",
      "Operator 147: 0.0015195403868579218\n",
      "Operator 148: -0.1321241289031025\n",
      "Operator 149: -1.9953602126843282\n",
      "Operator 150: 0.498069571697114\n",
      "Operator 151: 0.12788499189887073\n",
      "Operator 152: 0.027252893702694226\n",
      "Operator 153: 0.1622172155152061\n",
      "Operator 155: -1.6208314235610097\n",
      "Operator 156: 0.5076832986334125\n",
      "Operator 157: 0.1090037015729719\n",
      "Operator 158: 0.027669241787943787\n",
      "Operator 159: 0.15960363157855215\n",
      "Operator 160: 5.537205320171893e-08\n",
      "Operator 162: 2.03547211906417\n",
      "Operator 163: -0.4980695577889138\n",
      "Operator 164: -0.12788494318658508\n",
      "Operator 165: -0.02725288586083615\n",
      "Operator 166: -0.16221728270226754\n",
      "Operator 168: 1.6208314147318459\n",
      "Operator 169: -0.5076832860806658\n",
      "Operator 170: -0.1090037001523943\n",
      "Operator 171: -0.0276691858612355\n",
      "Operator 172: -0.15960315773857545\n",
      "Operator 173: -0.20567526487864568\n",
      "Operator 174: 0.722301728744013\n",
      "Operator 175: -1.6107981132365736\n",
      "Operator 177: 0.1857792024139175\n",
      "Operator 178: 0.026098442744949008\n",
      "Operator 179: 0.11106986412342039\n",
      "Operator 180: 0.5080060729244289\n",
      "Operator 181: -1.620831422582061\n",
      "Operator 183: 0.1596044783650892\n",
      "Operator 184: 0.027669392649343885\n",
      "Operator 185: 0.10900231768542647\n",
      "Operator 186: -3.963953187604624e-07\n",
      "Operator 187: -0.00866780217248271\n",
      "Operator 188: -1.8012427842245664\n",
      "Operator 189: 1.7562654160112392\n",
      "Operator 190: 0.03183850192269378\n",
      "Operator 191: -0.18105093251216403\n",
      "Operator 192: 0.18259589671342852\n",
      "Operator 193: 9.839521180790139e-08\n",
      "Operator 194: -1.8002640188832488\n",
      "Operator 195: 1.800659678503481\n",
      "Operator 196: 9.043805263467474e-07\n",
      "Operator 197: -0.17947283129398878\n",
      "Operator 198: 0.17947142675388267\n",
      "Operator 199: -2.4013303047211034e-08\n",
      "Operator 200: -2.124593036150448e-06\n",
      "Operator 201: 0.0015195403868578788\n",
      "Operator 202: 0.4843329948687218\n",
      "Operator 203: -2.035472119064169\n",
      "Operator 205: 0.1776206251417362\n",
      "Operator 206: 0.02615337253457503\n",
      "Operator 207: 0.1108769542868979\n",
      "Operator 208: 0.5080060507112503\n",
      "Operator 209: -1.620831414731846\n",
      "Operator 211: 0.15960456086662808\n",
      "Operator 212: 0.027669548447919357\n",
      "Operator 213: 0.10900276644147529\n",
      "Operator 214: 0.185400032624002\n",
      "Operator 215: -0.6937978883119051\n",
      "Operator 216: 0.12811370534942884\n",
      "Operator 217: -0.6130944002248225\n",
      "Operator 218: -0.15731934676564086\n",
      "Operator 219: -0.027276776127786447\n",
      "Operator 220: -0.16292101167583056\n",
      "Operator 222: 1.6208314225820721\n",
      "Operator 223: -0.5076832947760287\n",
      "Operator 224: -0.10900368437988199\n",
      "Operator 225: -0.02766924158091487\n",
      "Operator 226: -0.15960364674843286\n",
      "Operator 227: 1.9320633946054673e-06\n",
      "Total gradient norm: 10.714153639620635\n",
      "Operators under consideration (1):\n",
      "[29]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.2215149986470264)]\n",
      "Operator(s) added to ansatz: [29]\n",
      "Gradients: [np.float64(2.2215149986470264)]\n",
      "Initial energy: -26.470832476960606\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29]...\n",
      "Starting point: [np.float64(0.7853981155784222), np.float64(0.7853981908235318), np.float64(0.7853983033199075), np.float64(0.7853982888434756), np.float64(-0.7853981055103865), np.float64(-0.7853981600251708), np.float64(-0.7853982487879555), np.float64(-0.785398120284936), np.float64(-0.18066133305797516), np.float64(-0.13740474385531642), np.float64(0.13740474278421383), np.float64(-0.13909010547416847), np.float64(0.18218476826053587), np.float64(0.13809575926752785), np.float64(0.16620667965628466), np.float64(-0.12331742386028978), np.float64(0.12252940971317801), np.float64(-0.1374046939644136), np.float64(0.1806612154747323), np.float64(0.13740469495787638), np.float64(-0.18086855319013645), np.float64(0.13751647682538812), np.float64(-0.1375164829948094), np.float64(-0.10648166928970423), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.625866\n",
      "         Iterations: 30\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 73\n",
      "\n",
      "Current energy: -26.625865800416165\n",
      "(change of -0.15503332345555876)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29]\n",
      "On iteration 25.\n",
      "\n",
      "*** ADAPT-VQE Iteration 26 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 1: -8.557749819404116e-08\n",
      "Operator 2: 1.954928742607781e-08\n",
      "Operator 3: 1.7223965785703046e-07\n",
      "Operator 4: -7.18557984560562e-08\n",
      "Operator 5: 3.985147305042298e-08\n",
      "Operator 6: 2.5830074060056387e-08\n",
      "Operator 7: -2.766404768456132e-07\n",
      "Operator 8: 6.735923435696733e-08\n",
      "Operator 9: 2.677063779543687e-07\n",
      "Operator 11: -2.751396259714767e-07\n",
      "Operator 12: -3.5241395579532764e-08\n",
      "Operator 13: 9.605567600014155e-08\n",
      "Operator 14: 3.7965251996420026e-08\n",
      "Operator 15: -1.756025411038596e-07\n",
      "Operator 16: 3.326147207610725e-08\n",
      "Operator 17: -5.410040550372074e-08\n",
      "Operator 18: -2.5825950639535498e-08\n",
      "Operator 19: 2.7663750323814185e-07\n",
      "Operator 20: -6.735923363532237e-08\n",
      "Operator 21: -3.1181116066925085e-07\n",
      "Operator 22: -1.0883138043538132e-07\n",
      "Operator 23: 2.319733893862974e-07\n",
      "Operator 24: 2.5461301067508657e-08\n",
      "Operator 25: -1.1588190673705867e-07\n",
      "Operator 27: -1.5611402836515527e-08\n",
      "Operator 30: -0.2668892521667698\n",
      "Operator 31: -0.09399744526251066\n",
      "Operator 32: 0.18638509546241688\n",
      "Operator 33: -0.19020276248496826\n",
      "Operator 35: 1.7993034326600728\n",
      "Operator 36: -1.800659848717821\n",
      "Operator 37: 1.8133878950526983e-08\n",
      "Operator 38: 0.17947190367218346\n",
      "Operator 39: -0.17947190675764765\n",
      "Operator 42: -0.5633361630525715\n",
      "Operator 43: -1.8934881898897573\n",
      "Operator 44: -0.14558169990588724\n",
      "Operator 45: 0.18498454252201585\n",
      "Operator 46: -0.1913271887430672\n",
      "Operator 48: 1.7992642339966833\n",
      "Operator 49: -1.8006598498489812\n",
      "Operator 51: 0.1794718987693515\n",
      "Operator 52: -0.17947190145481523\n",
      "Operator 54: 1.1204178207435689e-07\n",
      "Operator 55: 0.023219394814673416\n",
      "Operator 56: -1.985374829126249\n",
      "Operator 57: -0.24571585757985098\n",
      "Operator 58: 0.0554904552552066\n",
      "Operator 59: -0.186410876239042\n",
      "Operator 60: 0.18931598766084817\n",
      "Operator 62: -1.7993034326600719\n",
      "Operator 63: 1.8006598497991382\n",
      "Operator 65: -0.17947189932905305\n",
      "Operator 66: 0.1794719053198933\n",
      "Operator 68: 0.45382880172360635\n",
      "Operator 70: 1.9595615989600645\n",
      "Operator 71: 0.2426347452787197\n",
      "Operator 72: -0.05550451055052239\n",
      "Operator 73: 0.18641049557248657\n",
      "Operator 74: -0.1893162934667118\n",
      "Operator 76: 1.7993034220766067\n",
      "Operator 77: -1.800659849799128\n",
      "Operator 79: 0.17947188673344433\n",
      "Operator 80: -0.17947190427063675\n",
      "Operator 82: 0.030030120520055867\n",
      "Operator 83: 0.07968727371681902\n",
      "Operator 84: 1.8239657351936858\n",
      "Operator 85: -0.017830154337170404\n",
      "Operator 86: -0.14302346886442302\n",
      "Operator 87: -0.027031072444464614\n",
      "Operator 88: -0.16796889162477163\n",
      "Operator 90: 1.6199667243983384\n",
      "Operator 91: -0.5076831422677894\n",
      "Operator 92: -0.10900283789546769\n",
      "Operator 93: -0.027669453194500898\n",
      "Operator 94: -0.15960365572618446\n",
      "Operator 97: 9.56618475462737e-08\n",
      "Operator 98: -1.4851481702772522e-08\n",
      "Operator 99: -1.6479986147204947e-07\n",
      "Operator 100: 3.040581808555793e-08\n",
      "Operator 101: -3.8538192851923536e-08\n",
      "Operator 103: 2.4906769512111904e-07\n",
      "Operator 104: -1.1143466277507841e-07\n",
      "Operator 105: -2.7909489674904364e-07\n",
      "Operator 107: 2.715571576494824e-07\n",
      "Operator 108: 0.21702357289148463\n",
      "Operator 109: -0.242921563550073\n",
      "Operator 110: 0.26717547994307067\n",
      "Operator 111: 0.09402186607060317\n",
      "Operator 112: -0.18638470760489464\n",
      "Operator 113: 0.19020330876595695\n",
      "Operator 115: -1.7993034220766133\n",
      "Operator 116: 1.800659849848981\n",
      "Operator 118: -0.17947188666873376\n",
      "Operator 119: 0.17947190142174363\n",
      "Operator 123: -7.450088952502124e-08\n",
      "Operator 124: 2.6071278946579126e-08\n",
      "Operator 125: 1.4110286090603076e-07\n",
      "Operator 126: -2.1194058236762163e-08\n",
      "Operator 127: 5.2770634611992e-08\n",
      "Operator 129: -2.4906501759625055e-07\n",
      "Operator 130: 1.114346625530338e-07\n",
      "Operator 131: 2.393860383254065e-07\n",
      "Operator 132: 2.5974655374488842e-08\n",
      "Operator 133: -2.3530567649920897e-07\n",
      "Operator 134: -0.1748352203295287\n",
      "Operator 135: -0.08076853179198709\n",
      "Operator 136: 0.6061868715493205\n",
      "Operator 137: 0.16804000582580303\n",
      "Operator 138: 0.19259955384624497\n",
      "Operator 139: 0.027073801308453823\n",
      "Operator 140: 0.16919658018814893\n",
      "Operator 142: -1.6199667234255277\n",
      "Operator 143: 0.5076831384325544\n",
      "Operator 144: 0.10900282148197518\n",
      "Operator 145: 0.027669452104960475\n",
      "Operator 146: 0.15960365769572443\n",
      "Operator 147: -0.03188327500115196\n",
      "Operator 148: -0.34298008893091725\n",
      "Operator 149: -1.7366292458071895\n",
      "Operator 150: 0.018754780606015063\n",
      "Operator 151: 0.14303128395285497\n",
      "Operator 152: 0.027030842319023324\n",
      "Operator 153: 0.16796911155673439\n",
      "Operator 155: -1.6199667148697265\n",
      "Operator 156: 0.5076831422677879\n",
      "Operator 157: 0.10900283330545701\n",
      "Operator 158: 0.027669455962651912\n",
      "Operator 159: 0.1596036580125359\n",
      "Operator 160: 0.24638301608952493\n",
      "Operator 161: -0.8734226041884956\n",
      "Operator 162: 0.17544668161336313\n",
      "Operator 163: -0.5375120794878527\n",
      "Operator 164: -0.22138027434004945\n",
      "Operator 165: -0.026221227024694635\n",
      "Operator 166: -0.17000283564595453\n",
      "Operator 168: 1.6199314316958753\n",
      "Operator 169: -0.5076831384325444\n",
      "Operator 170: -0.10900282130329021\n",
      "Operator 171: -0.027669452226962907\n",
      "Operator 172: -0.15960365876643154\n",
      "Operator 173: -0.2781899848278263\n",
      "Operator 174: 0.6867832576429131\n",
      "Operator 175: 0.46324455226542005\n",
      "Operator 176: 0.10478886095998485\n",
      "Operator 177: 0.20706860356800433\n",
      "Operator 178: 0.024029098008879743\n",
      "Operator 179: 0.11507213065556156\n",
      "Operator 180: 0.5089581304254518\n",
      "Operator 181: -1.6199667149145793\n",
      "Operator 183: 0.15960363885434667\n",
      "Operator 184: 0.027669451816995656\n",
      "Operator 185: 0.1090028417140254\n",
      "Operator 187: 0.24678275821328638\n",
      "Operator 188: 0.30316088330466695\n",
      "Operator 189: 1.8845726305852002\n",
      "Operator 190: 0.14557133520464915\n",
      "Operator 191: -0.18498454971548853\n",
      "Operator 192: 0.19132694826746122\n",
      "Operator 194: -1.7992642339966762\n",
      "Operator 195: 1.800659848717808\n",
      "Operator 196: -1.7881791103402378e-08\n",
      "Operator 197: -0.17947190317719092\n",
      "Operator 198: 0.17947190574146366\n",
      "Operator 200: 0.2693870162698207\n",
      "Operator 201: -0.3079009865874099\n",
      "Operator 202: 0.7497419416278112\n",
      "Operator 203: -1.8160977030310645\n",
      "Operator 204: -0.05190466716515957\n",
      "Operator 205: 0.23307202297167134\n",
      "Operator 206: 0.022188928490312357\n",
      "Operator 207: 0.1159033745267835\n",
      "Operator 208: 0.5088195727064998\n",
      "Operator 209: -1.6199314316958748\n",
      "Operator 211: 0.15960365062067106\n",
      "Operator 212: 0.02766945176759872\n",
      "Operator 213: 0.10900284010543339\n",
      "Operator 214: 0.2648754189872527\n",
      "Operator 215: -0.7013602078683109\n",
      "Operator 216: -0.37938284035302\n",
      "Operator 217: -0.16816297230951904\n",
      "Operator 218: -0.19262063104287858\n",
      "Operator 219: -0.027073582681402454\n",
      "Operator 220: -0.1691971330110581\n",
      "Operator 222: 1.6199667149145838\n",
      "Operator 223: -0.5076831424446178\n",
      "Operator 224: -0.10900283426066526\n",
      "Operator 225: -0.027669455755324705\n",
      "Operator 226: -0.1596036540997593\n",
      "Operator 227: -0.3186363632691546\n",
      "Total gradient norm: 9.501801885664607\n",
      "Operators under consideration (1):\n",
      "[56]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.985374829126249)]\n",
      "Operator(s) added to ansatz: [56]\n",
      "Gradients: [np.float64(-1.985374829126249)]\n",
      "Initial energy: -26.625865800416165\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56]...\n",
      "Starting point: [np.float64(0.7853981839219414), np.float64(0.7853981911031175), np.float64(0.7853981726054842), np.float64(0.7853981494671215), np.float64(-0.7853981682332506), np.float64(-0.7853981465334285), np.float64(-0.7853981643486636), np.float64(-0.7853981555908551), np.float64(-0.18066124810710044), np.float64(-0.13740469138481323), np.float64(0.13740469143391437), np.float64(-0.14152111208333643), np.float64(0.1859897817640204), np.float64(0.14152064857484534), np.float64(0.14739719774588933), np.float64(-0.11940688723474839), np.float64(0.10956022480929424), np.float64(-0.13740469161663726), np.float64(0.180661249520714), np.float64(0.13740469136560046), np.float64(-0.18136470584958406), np.float64(0.1377955950690193), np.float64(-0.1378340986438089), np.float64(-0.13998420606860923), np.float64(-0.13996600335239504), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.743485\n",
      "         Iterations: 19\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 54\n",
      "\n",
      "Current energy: -26.7434853572667\n",
      "(change of -0.11761955685053493)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56]\n",
      "On iteration 26.\n",
      "\n",
      "*** ADAPT-VQE Iteration 27 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 7.144685901588708e-08\n",
      "Operator 1: -9.558183423119001e-08\n",
      "Operator 2: 4.2472617613972996e-08\n",
      "Operator 3: -2.727674531893879e-08\n",
      "Operator 4: 9.493338347899529e-08\n",
      "Operator 5: -2.3540083766030764e-08\n",
      "Operator 7: 1.0921515039563623e-08\n",
      "Operator 8: -7.94493128924989e-08\n",
      "Operator 9: 5.452032939001761e-08\n",
      "Operator 10: -1.8103434043514888e-08\n",
      "Operator 11: 7.014229108648351e-08\n",
      "Operator 12: -5.6584970098416676e-08\n",
      "Operator 13: 8.31235647568962e-08\n",
      "Operator 14: -4.278913911704814e-08\n",
      "Operator 15: 2.6281998987596082e-08\n",
      "Operator 16: -9.747841530995905e-08\n",
      "Operator 17: 2.259228913216496e-08\n",
      "Operator 19: -1.0921035437094773e-08\n",
      "Operator 20: 7.944931358638829e-08\n",
      "Operator 21: -3.096452200201938e-08\n",
      "Operator 22: 7.809579548323509e-08\n",
      "Operator 23: -5.1912762422112735e-08\n",
      "Operator 24: 4.815958715154167e-08\n",
      "Operator 25: -2.813517241406771e-08\n",
      "Operator 26: 6.358192071323736e-08\n",
      "Operator 27: -1.1070354499264111e-07\n",
      "Operator 28: -0.010623822605091619\n",
      "Operator 29: 2.588478708370998e-06\n",
      "Operator 30: -0.22068078465950514\n",
      "Operator 31: -0.13499836908069607\n",
      "Operator 32: 0.18847486677977363\n",
      "Operator 33: -0.19496419273982513\n",
      "Operator 34: -0.00016584616436884732\n",
      "Operator 35: 1.798700636653551\n",
      "Operator 36: -1.800659846931622\n",
      "Operator 38: 0.1794719103018468\n",
      "Operator 39: -0.17947192228463577\n",
      "Operator 40: -2.6535375167304283e-08\n",
      "Operator 41: -9.743963661413486e-05\n",
      "Operator 42: -0.04105903839606753\n",
      "Operator 43: -0.19268354194261123\n",
      "Operator 44: -0.1358027267841534\n",
      "Operator 45: 0.18858670577149744\n",
      "Operator 46: -0.19452505477066012\n",
      "Operator 47: 0.00019841686829515575\n",
      "Operator 48: 1.7987186171623892\n",
      "Operator 49: -1.8006598477514872\n",
      "Operator 51: 0.17947190852803244\n",
      "Operator 52: -0.17947189072079006\n",
      "Operator 53: -1.3569859951228356e-08\n",
      "Operator 54: 2.4826970701496975e-08\n",
      "Operator 55: -0.004233455125963781\n",
      "Operator 56: 4.026677865745341e-05\n",
      "Operator 57: 0.2190855811494849\n",
      "Operator 58: 0.1349215146074769\n",
      "Operator 59: -0.18853003403415405\n",
      "Operator 60: 0.19478503143880882\n",
      "Operator 61: 9.460410142328056e-05\n",
      "Operator 62: -1.7987006366535505\n",
      "Operator 63: 1.8006598477545932\n",
      "Operator 65: -0.1794719084790779\n",
      "Operator 66: 0.17947189074995773\n",
      "Operator 67: 1.3596854332043995e-08\n",
      "Operator 68: 0.6038268595358074\n",
      "Operator 69: -0.1415568155470986\n",
      "Operator 70: 0.17055343730402608\n",
      "Operator 71: -0.20593207014602322\n",
      "Operator 72: -0.13544682762221516\n",
      "Operator 73: 0.18868387434907552\n",
      "Operator 74: -0.19465088438022363\n",
      "Operator 75: 0.0002957466440014774\n",
      "Operator 76: 1.7987345566643564\n",
      "Operator 77: -1.8006598477545905\n",
      "Operator 79: 0.1794718999931333\n",
      "Operator 80: -0.1794718796160546\n",
      "Operator 81: 1.93745992989665e-08\n",
      "Operator 82: 0.20537475608223632\n",
      "Operator 83: 0.04448564279243882\n",
      "Operator 84: -0.050129552860956675\n",
      "Operator 85: -0.12915501945881583\n",
      "Operator 86: -0.21364851015092406\n",
      "Operator 87: -0.026377010211564203\n",
      "Operator 88: -0.17287043610497416\n",
      "Operator 90: 1.619424007315939\n",
      "Operator 91: -0.5076831437519707\n",
      "Operator 92: -0.1090028470719018\n",
      "Operator 93: -0.027669449919589383\n",
      "Operator 94: -0.1596036310801526\n",
      "Operator 95: 5.72259067547533e-08\n",
      "Operator 96: -2.5692342615581855e-08\n",
      "Operator 97: 7.92825519193217e-08\n",
      "Operator 98: -2.05870734131057e-08\n",
      "Operator 99: 2.2858715344191717e-08\n",
      "Operator 100: -3.5046310226915946e-08\n",
      "Operator 101: 2.2715266488090302e-08\n",
      "Operator 104: 7.488425313173508e-08\n",
      "Operator 105: -2.646716085186185e-08\n",
      "Operator 106: 1.098608149895652e-08\n",
      "Operator 107: -7.347665864632091e-08\n",
      "Operator 108: 0.19572020820337352\n",
      "Operator 109: -0.19604674696060206\n",
      "Operator 110: 0.19473168557459605\n",
      "Operator 111: 0.1358107779866481\n",
      "Operator 112: -0.18870970119011143\n",
      "Operator 113: 0.19445538054489708\n",
      "Operator 114: -0.00037873159396201286\n",
      "Operator 115: -1.798734556664356\n",
      "Operator 116: 1.800659847751488\n",
      "Operator 118: -0.17947190000679328\n",
      "Operator 119: 0.1794718795957154\n",
      "Operator 120: -1.938220829254288e-08\n",
      "Operator 121: -6.980343538831407e-08\n",
      "Operator 122: 3.6868693836941895e-08\n",
      "Operator 123: -6.922205559356993e-08\n",
      "Operator 124: 2.0649367078085472e-08\n",
      "Operator 125: -2.37276969128925e-08\n",
      "Operator 126: 3.565124685611046e-08\n",
      "Operator 127: -2.1766532592214535e-08\n",
      "Operator 130: -7.488425352031314e-08\n",
      "Operator 131: 4.7675157030524495e-08\n",
      "Operator 132: -2.568021723606504e-08\n",
      "Operator 133: 4.9290190262674116e-08\n",
      "Operator 134: -0.11377240439095558\n",
      "Operator 135: 0.01691304136001951\n",
      "Operator 136: 0.05660947560338865\n",
      "Operator 137: 0.1296350590130641\n",
      "Operator 138: 0.21374323589025362\n",
      "Operator 139: 0.026407090768508043\n",
      "Operator 140: 0.17311670068982016\n",
      "Operator 142: -1.6194240065758008\n",
      "Operator 143: 0.5076831408330473\n",
      "Operator 144: 0.10900283240893666\n",
      "Operator 145: 0.027669451907979647\n",
      "Operator 146: 0.15960367438972017\n",
      "Operator 147: -0.2545388964652467\n",
      "Operator 148: 0.389221582667049\n",
      "Operator 149: -0.05446021852910353\n",
      "Operator 150: 0.12834671553154595\n",
      "Operator 151: 0.2140330815411161\n",
      "Operator 152: 0.026332224360725263\n",
      "Operator 153: 0.1730104108442818\n",
      "Operator 155: -1.619454546516687\n",
      "Operator 156: 0.5076831437519703\n",
      "Operator 157: 0.10900284399766969\n",
      "Operator 158: 0.027669451925922024\n",
      "Operator 159: 0.15960364311068984\n",
      "Operator 160: 0.1760439037387105\n",
      "Operator 161: -0.7172545109241231\n",
      "Operator 162: 0.12850294718917005\n",
      "Operator 163: -0.12568860243601498\n",
      "Operator 164: -0.21405341424247154\n",
      "Operator 165: -0.02638922119342606\n",
      "Operator 166: -0.1731975399753289\n",
      "Operator 168: 1.6194401949659474\n",
      "Operator 169: -0.507683140833046\n",
      "Operator 170: -0.10900283242182063\n",
      "Operator 171: -0.02766945190038258\n",
      "Operator 172: -0.15960367439515302\n",
      "Operator 173: -0.2438028993575129\n",
      "Operator 174: 0.7487274969212059\n",
      "Operator 175: 0.011091796239398523\n",
      "Operator 176: 0.042570968534539844\n",
      "Operator 177: 0.229964383403078\n",
      "Operator 178: 0.02172623149248746\n",
      "Operator 179: 0.11799839874719661\n",
      "Operator 180: 0.5094818056912147\n",
      "Operator 181: -1.6194545465138952\n",
      "Operator 183: 0.15960365210630056\n",
      "Operator 184: 0.02766945729699838\n",
      "Operator 185: 0.10900285714166161\n",
      "Operator 186: -7.575209173148433e-08\n",
      "Operator 187: 0.24196175367175557\n",
      "Operator 188: -0.19281959603388815\n",
      "Operator 189: 0.19660232173134304\n",
      "Operator 190: 0.1355156093572852\n",
      "Operator 191: -0.18850570807536943\n",
      "Operator 192: 0.19489971865234568\n",
      "Operator 193: -4.418964841825913e-05\n",
      "Operator 194: -1.7987186171623888\n",
      "Operator 195: 1.8006598469316208\n",
      "Operator 197: -0.1794719103371409\n",
      "Operator 198: 0.17947192227580816\n",
      "Operator 199: 2.651599325032068e-08\n",
      "Operator 200: 0.41428010100088153\n",
      "Operator 201: -0.2862953022769118\n",
      "Operator 202: 0.6326404599193114\n",
      "Operator 203: -0.05876752536316432\n",
      "Operator 204: 0.04266708725568216\n",
      "Operator 205: 0.22965301601932325\n",
      "Operator 206: 0.021612268954222316\n",
      "Operator 207: 0.11790592414985376\n",
      "Operator 208: 0.5094255281377623\n",
      "Operator 209: -1.619440194965948\n",
      "Operator 211: 0.15960365889332412\n",
      "Operator 212: 0.0276694479648891\n",
      "Operator 213: 0.10900282538233347\n",
      "Operator 214: 0.2458945066415672\n",
      "Operator 215: -0.5410461980764042\n",
      "Operator 216: 0.08704468146697734\n",
      "Operator 217: -0.12508602604242572\n",
      "Operator 218: -0.214508399568512\n",
      "Operator 219: -0.026308606717950794\n",
      "Operator 220: -0.17274181870278765\n",
      "Operator 222: 1.6194545465138956\n",
      "Operator 223: -0.5076831437409617\n",
      "Operator 224: -0.10900284395229082\n",
      "Operator 225: -0.02766945192052861\n",
      "Operator 226: -0.15960364308273606\n",
      "Operator 227: -0.4944683128872461\n",
      "Total gradient norm: 8.025341177420191\n",
      "Operators under consideration (1):\n",
      "[195]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.8006598469316208)]\n",
      "Operator(s) added to ansatz: [195]\n",
      "Gradients: [np.float64(1.8006598469316208)]\n",
      "Initial energy: -26.7434853572667\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195]...\n",
      "Starting point: [np.float64(0.7853981467034196), np.float64(0.7853981570229585), np.float64(0.7853981838668445), np.float64(0.7853981772072293), np.float64(-0.7853981680172502), np.float64(-0.7853981520733849), np.float64(-0.7853981782451488), np.float64(-0.7853981566722059), np.float64(-0.180661249217562), np.float64(-0.13740469191870425), np.float64(0.137404691915648), np.float64(-0.14246573092859727), np.float64(0.18917147099544854), np.float64(0.1424606044635935), np.float64(0.13710473483652), np.float64(-0.1186088841440853), np.float64(0.10327992108401683), np.float64(-0.13740469036815847), np.float64(0.18066124999177122), np.float64(0.13740469036644257), np.float64(-0.18163391579174223), np.float64(0.13799294572317305), np.float64(-0.13800858944408584), np.float64(-0.14914387493108228), np.float64(-0.11865878397699285), np.float64(0.11874643939165652), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.838171\n",
      "         Iterations: 16\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 58\n",
      "\n",
      "Current energy: -26.838171168820786\n",
      "(change of -0.09468581155408629)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195]\n",
      "On iteration 27.\n",
      "\n",
      "*** ADAPT-VQE Iteration 28 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.2225898599852987e-06\n",
      "Operator 1: 1.6247593501892174e-06\n",
      "Operator 2: -7.304395767207501e-07\n",
      "Operator 3: 4.408661363800778e-07\n",
      "Operator 4: -1.6739414181448086e-06\n",
      "Operator 5: 3.392714213137791e-07\n",
      "Operator 7: -1.465633196318485e-07\n",
      "Operator 8: 5.647993243601632e-07\n",
      "Operator 9: -6.352919852087129e-07\n",
      "Operator 10: 1.906189977305241e-07\n",
      "Operator 11: -7.452743599860312e-07\n",
      "Operator 12: 5.65255826276076e-07\n",
      "Operator 13: -1.4182675567180178e-06\n",
      "Operator 14: 7.358349285061609e-07\n",
      "Operator 15: -3.811326814874765e-07\n",
      "Operator 16: 1.8284445162861439e-06\n",
      "Operator 17: -2.817539815842911e-07\n",
      "Operator 19: 1.2471640554285405e-07\n",
      "Operator 20: -6.979074333837753e-07\n",
      "Operator 21: 2.7272923924196325e-07\n",
      "Operator 22: -7.796066790234386e-07\n",
      "Operator 23: 5.820723423194743e-07\n",
      "Operator 24: -4.803676868798262e-07\n",
      "Operator 25: 2.853007884801029e-07\n",
      "Operator 26: -1.0529038752145343e-06\n",
      "Operator 27: 1.8748729289854238e-06\n",
      "Operator 28: -0.010511498792877164\n",
      "Operator 29: -0.0004784689353467857\n",
      "Operator 30: -0.2202065883883752\n",
      "Operator 31: -0.13471714719493647\n",
      "Operator 32: 0.1916786210999285\n",
      "Operator 33: -0.19606722650406042\n",
      "Operator 34: 0.031420904116245804\n",
      "Operator 35: 1.7555198183185583\n",
      "Operator 36: -1.7849010923851436\n",
      "Operator 37: -0.009346282485551555\n",
      "Operator 38: -0.024812700713888535\n",
      "Operator 39: -0.16984450277587715\n",
      "Operator 40: -1.0417947496821755e-05\n",
      "Operator 41: -0.00025915363935101807\n",
      "Operator 42: -0.04125853152615058\n",
      "Operator 43: -0.19184087637150396\n",
      "Operator 44: -0.13589053469775483\n",
      "Operator 45: 0.19197100084072682\n",
      "Operator 46: -0.1965067107651764\n",
      "Operator 47: 0.05348389093051557\n",
      "Operator 48: 2.1378400381871434\n",
      "Operator 49: -0.23546560524089774\n",
      "Operator 50: 0.15564808002627395\n",
      "Operator 51: 0.04931998485687732\n",
      "Operator 52: -0.16393751095879092\n",
      "Operator 53: -6.406367632162824e-05\n",
      "Operator 54: -2.5204201772877077e-07\n",
      "Operator 55: -0.004560129292319264\n",
      "Operator 56: 0.0006282590745166014\n",
      "Operator 57: 0.2186703300121178\n",
      "Operator 58: 0.13484511023786414\n",
      "Operator 59: -0.19098709995092317\n",
      "Operator 60: 0.19692372339437778\n",
      "Operator 61: -0.00026381982826108433\n",
      "Operator 62: -0.3187165812768219\n",
      "Operator 63: 2.193579125656163\n",
      "Operator 64: 9.826030054081348e-05\n",
      "Operator 65: -0.0227759287029849\n",
      "Operator 66: 0.1647911093005087\n",
      "Operator 67: -3.104874048984086e-05\n",
      "Operator 68: 0.6035978549617378\n",
      "Operator 69: -0.14099259018085386\n",
      "Operator 70: 0.17011552653744097\n",
      "Operator 71: -0.2052964598558917\n",
      "Operator 72: -0.13549159591702642\n",
      "Operator 73: 0.1907793873375528\n",
      "Operator 74: -0.19705905237240695\n",
      "Operator 75: -4.731214695322489e-05\n",
      "Operator 76: 0.31870011093751566\n",
      "Operator 77: -2.193579125656182\n",
      "Operator 78: -6.46591411768008e-05\n",
      "Operator 79: 0.17573226444222353\n",
      "Operator 80: -0.15789883916461211\n",
      "Operator 81: 6.887720993195409e-05\n",
      "Operator 82: 0.20571449218675275\n",
      "Operator 83: 0.044511901057349826\n",
      "Operator 84: -0.050767793792589645\n",
      "Operator 85: -0.12911326529206146\n",
      "Operator 86: -0.21481917889582086\n",
      "Operator 87: -0.02591462718430437\n",
      "Operator 88: -0.17492653761621785\n",
      "Operator 89: -0.09513815528920111\n",
      "Operator 90: 0.07141120545548929\n",
      "Operator 91: -0.4695140199336007\n",
      "Operator 92: -0.10190433260629189\n",
      "Operator 93: -0.06552537093814342\n",
      "Operator 94: -0.14556181289067657\n",
      "Operator 95: -9.469470631859444e-07\n",
      "Operator 96: 4.432433862696134e-07\n",
      "Operator 97: -1.3441735313065622e-06\n",
      "Operator 98: 3.6467566886233636e-07\n",
      "Operator 99: -3.363458598378362e-07\n",
      "Operator 100: 6.114977024162506e-07\n",
      "Operator 101: -3.1931981430392425e-07\n",
      "Operator 103: 1.1194957009547308e-07\n",
      "Operator 104: -6.824672529311213e-07\n",
      "Operator 105: 3.584341594016797e-07\n",
      "Operator 106: -1.036058872337906e-07\n",
      "Operator 107: 7.781874861315444e-07\n",
      "Operator 108: 0.19527064723276094\n",
      "Operator 109: -0.19591071456426318\n",
      "Operator 110: 0.19398072959436688\n",
      "Operator 111: 0.13597700853813258\n",
      "Operator 112: -0.19138421554748558\n",
      "Operator 113: 0.1965232254051455\n",
      "Operator 114: -0.03099150311809275\n",
      "Operator 115: -1.7554920503749836\n",
      "Operator 116: 1.7743855917699225\n",
      "Operator 117: -9.811836169494165e-05\n",
      "Operator 118: -0.17579345838448604\n",
      "Operator 119: 0.15755125654371638\n",
      "Operator 120: -4.219688360614834e-05\n",
      "Operator 121: 1.1556219458252723e-06\n",
      "Operator 122: -6.310716986054437e-07\n",
      "Operator 123: 1.1784442533169383e-06\n",
      "Operator 124: -3.6341673371700425e-07\n",
      "Operator 125: 3.894733947240328e-07\n",
      "Operator 126: -6.485900660169087e-07\n",
      "Operator 127: 2.6304487403668426e-07\n",
      "Operator 128: -1.4825227849015406e-08\n",
      "Operator 129: -1.3312469869899198e-07\n",
      "Operator 130: 2.2937134360015736e-07\n",
      "Operator 131: -4.449375048420391e-07\n",
      "Operator 132: 2.6681493799671045e-07\n",
      "Operator 133: -5.552667829231833e-07\n",
      "Operator 134: -0.11317667009082714\n",
      "Operator 135: 0.016897837374010374\n",
      "Operator 136: 0.05713863629778664\n",
      "Operator 137: 0.12951935672977014\n",
      "Operator 138: 0.215046580655793\n",
      "Operator 139: 0.024819516494551063\n",
      "Operator 140: 0.1899674323577258\n",
      "Operator 142: -2.01241532383602\n",
      "Operator 143: 0.5049646682504557\n",
      "Operator 144: 0.10635920028301671\n",
      "Operator 145: 0.07913506833838091\n",
      "Operator 146: 0.15254070043659\n",
      "Operator 147: -0.2548024288281423\n",
      "Operator 148: 0.3893096607579543\n",
      "Operator 149: -0.05380766855007451\n",
      "Operator 150: 0.12825246044648828\n",
      "Operator 151: 0.21506803102435224\n",
      "Operator 152: 0.025945049648223754\n",
      "Operator 153: 0.17483348206378083\n",
      "Operator 154: 0.0951788219011323\n",
      "Operator 155: -0.07139871420282054\n",
      "Operator 156: 0.4695140199336041\n",
      "Operator 157: 0.1019760085927448\n",
      "Operator 158: 0.032158029622096136\n",
      "Operator 159: 0.14062809296638526\n",
      "Operator 160: 0.1755858409655556\n",
      "Operator 161: -0.717375881723152\n",
      "Operator 162: 0.12784094804764942\n",
      "Operator 163: -0.12549926461531433\n",
      "Operator 164: -0.21547857066002424\n",
      "Operator 165: -0.024780511717758774\n",
      "Operator 166: -0.19781007579549778\n",
      "Operator 168: 1.6028227259525962\n",
      "Operator 169: 0.1383733354502884\n",
      "Operator 170: -0.10735353019773172\n",
      "Operator 171: -0.07432481742350308\n",
      "Operator 172: -0.15213503395535985\n",
      "Operator 173: -0.24324354206849777\n",
      "Operator 174: 0.7489894836887002\n",
      "Operator 175: 0.011657081448922123\n",
      "Operator 176: 0.04257038619081172\n",
      "Operator 177: 0.23192512173279364\n",
      "Operator 178: 0.021134898182935912\n",
      "Operator 179: 0.1358142969850149\n",
      "Operator 180: 0.49997853841180734\n",
      "Operator 181: -1.9681231922256401\n",
      "Operator 182: -0.1381658916481985\n",
      "Operator 183: 0.15642044751741493\n",
      "Operator 184: 0.04108803103172414\n",
      "Operator 185: 0.09603827093755192\n",
      "Operator 186: 1.2548554474304428e-06\n",
      "Operator 187: 0.24230561576036358\n",
      "Operator 188: -0.19243156195538025\n",
      "Operator 189: 0.1958147754938862\n",
      "Operator 190: 0.1352771316396025\n",
      "Operator 191: -0.19205769701033584\n",
      "Operator 192: 0.1961864944735413\n",
      "Operator 193: -0.053605312792475615\n",
      "Operator 194: -2.137840038187314\n",
      "Operator 195: 9.529272472877105e-05\n",
      "Operator 196: -3.939799589954552e-05\n",
      "Operator 197: 0.00547339175945267\n",
      "Operator 198: 0.16933868851195286\n",
      "Operator 199: 7.881723807246948e-05\n",
      "Operator 200: 0.41468645748831745\n",
      "Operator 201: -0.2865559934645359\n",
      "Operator 202: 0.6325239439430236\n",
      "Operator 203: -0.057918054515209164\n",
      "Operator 204: 0.04274726968129225\n",
      "Operator 205: 0.23246016310958212\n",
      "Operator 206: 0.021238280250393114\n",
      "Operator 207: 0.1642549539580526\n",
      "Operator 208: 0.6127079015322727\n",
      "Operator 209: 0.07630507690719214\n",
      "Operator 210: 0.04372372176809933\n",
      "Operator 211: 0.03136479296751252\n",
      "Operator 212: 0.02906037765898754\n",
      "Operator 213: 0.1026176620267835\n",
      "Operator 214: 0.24564067313709143\n",
      "Operator 215: -0.5411015716118749\n",
      "Operator 216: 0.08675937171178574\n",
      "Operator 217: -0.12494182312825773\n",
      "Operator 218: -0.21607613947476242\n",
      "Operator 219: -0.024900126371795627\n",
      "Operator 220: -0.19031148819535212\n",
      "Operator 222: 1.9681231922254843\n",
      "Operator 223: -0.476926383446802\n",
      "Operator 224: -0.10129594368019293\n",
      "Operator 225: -0.032129968552155656\n",
      "Operator 226: -0.14014706757713163\n",
      "Operator 227: -0.49481563770235937\n",
      "Total gradient norm: 7.264273245418388\n",
      "Operators under consideration (1):\n",
      "[77]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.193579125656182)]\n",
      "Operator(s) added to ansatz: [77]\n",
      "Gradients: [np.float64(-2.193579125656182)]\n",
      "Initial energy: -26.838171168820786\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77]...\n",
      "Starting point: [np.float64(0.7853983296490423), np.float64(0.785398210962421), np.float64(0.7853978148212791), np.float64(0.7853979285262003), np.float64(-0.7853980596136317), np.float64(-0.7853982630142589), np.float64(-0.7853979133539345), np.float64(-0.7853982308389458), np.float64(-0.17645835749245242), np.float64(-0.12933902298865058), np.float64(0.12847159950938), np.float64(-0.14258582109995577), np.float64(0.18937228681339943), np.float64(0.1425490572528964), np.float64(0.13709010718300194), np.float64(-0.1186158679900923), np.float64(0.10330271897956175), np.float64(-0.13675216557753062), np.float64(0.17977539656186067), np.float64(0.1367402109971884), np.float64(-0.18314700294822528), np.float64(0.13963369498282263), np.float64(-0.13865741732477818), np.float64(-0.149180879773346), np.float64(-0.11869231263864928), np.float64(0.1187878340171256), np.float64(-0.10506897572152442), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.989145\n",
      "         Iterations: 19\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "\n",
      "Current energy: -26.989145478665325\n",
      "(change of -0.15097430984453908)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77]\n",
      "On iteration 28.\n",
      "\n",
      "*** ADAPT-VQE Iteration 29 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 5.795427420039745e-06\n",
      "Operator 1: -7.858584277500857e-06\n",
      "Operator 2: 3.440411213005007e-06\n",
      "Operator 3: -2.06046894996037e-06\n",
      "Operator 4: 7.655618344053824e-06\n",
      "Operator 5: -1.0595225325476443e-06\n",
      "Operator 6: -9.090507809824759e-08\n",
      "Operator 7: 1.840585104240713e-06\n",
      "Operator 8: -2.728821697655815e-06\n",
      "Operator 9: 1.7877944247645529e-06\n",
      "Operator 10: -9.80529511861224e-07\n",
      "Operator 11: 2.570401416437279e-06\n",
      "Operator 12: -1.9192250410441443e-06\n",
      "Operator 13: 6.845133035462969e-06\n",
      "Operator 14: -3.4662478401925254e-06\n",
      "Operator 15: 1.9695526193193313e-06\n",
      "Operator 16: -7.887934129250556e-06\n",
      "Operator 17: 9.731098363112242e-07\n",
      "Operator 18: 9.076451835623828e-08\n",
      "Operator 19: -1.8406141257094285e-06\n",
      "Operator 20: 2.728833942096355e-06\n",
      "Operator 21: -1.0408095731379524e-06\n",
      "Operator 22: 2.9399972546732017e-06\n",
      "Operator 23: -2.0211630009046175e-06\n",
      "Operator 24: 1.6402221802855145e-06\n",
      "Operator 25: -8.71729646559786e-07\n",
      "Operator 26: 5.145576268850149e-06\n",
      "Operator 27: -8.989960988921297e-06\n",
      "Operator 28: -0.010799218655732416\n",
      "Operator 29: 6.377965681836669e-05\n",
      "Operator 30: -0.21920534025088623\n",
      "Operator 31: -0.13531013746377824\n",
      "Operator 32: 0.1986947271893293\n",
      "Operator 33: -0.20192179194474352\n",
      "Operator 34: 0.05431522397683137\n",
      "Operator 35: -0.20582938321013475\n",
      "Operator 36: -1.971131268438753\n",
      "Operator 37: 0.03186142051345989\n",
      "Operator 38: -0.3008229445402136\n",
      "Operator 39: -0.14208877612401172\n",
      "Operator 40: 5.827077264792562e-06\n",
      "Operator 41: -4.959521407819593e-06\n",
      "Operator 42: -0.04120363053805781\n",
      "Operator 43: -0.191399223171742\n",
      "Operator 44: -0.13639578394098\n",
      "Operator 45: 0.20062196506618118\n",
      "Operator 46: -0.2005134475870485\n",
      "Operator 47: 0.14245525952721505\n",
      "Operator 48: 1.893988280287018\n",
      "Operator 49: 0.29176312313899844\n",
      "Operator 50: 0.21301441700779117\n",
      "Operator 51: -0.05408799955567945\n",
      "Operator 52: -0.1301036371174274\n",
      "Operator 53: 3.4013180090871783e-06\n",
      "Operator 54: 7.640687595605618e-07\n",
      "Operator 55: -0.004048480912896183\n",
      "Operator 56: -6.33186757833181e-05\n",
      "Operator 57: 0.21760721078938497\n",
      "Operator 58: 0.13536348697489606\n",
      "Operator 59: -0.19868255613587263\n",
      "Operator 60: 0.2020329612953241\n",
      "Operator 61: -0.05426932950604816\n",
      "Operator 62: 0.20585511036137044\n",
      "Operator 63: 1.971133680904214\n",
      "Operator 64: -0.03190588715832551\n",
      "Operator 65: 0.3008076355020505\n",
      "Operator 66: 0.14208292922822494\n",
      "Operator 67: -7.465708616252371e-06\n",
      "Operator 68: 0.6040876325536588\n",
      "Operator 69: -0.14165032491240112\n",
      "Operator 70: 0.17059482491851338\n",
      "Operator 71: -0.20457991830312158\n",
      "Operator 72: -0.1359936981632429\n",
      "Operator 73: 0.1994728375398015\n",
      "Operator 74: -0.20205440629783963\n",
      "Operator 75: 0.0918082631566821\n",
      "Operator 76: 0.29294161123367807\n",
      "Operator 77: -1.526814074562394e-05\n",
      "Operator 78: 4.505631025536504e-05\n",
      "Operator 79: -0.11020447975041096\n",
      "Operator 80: -0.13531759853863923\n",
      "Operator 81: 8.780453461415413e-06\n",
      "Operator 82: 0.20542733040305566\n",
      "Operator 83: 0.04446367092863134\n",
      "Operator 84: -0.05073988458900275\n",
      "Operator 85: -0.12898751493025876\n",
      "Operator 86: -0.21877714993769043\n",
      "Operator 87: -0.02264616434212206\n",
      "Operator 88: -0.20535914056748042\n",
      "Operator 89: -0.10688908508676219\n",
      "Operator 90: 0.10827451121576372\n",
      "Operator 91: -0.43671971639687557\n",
      "Operator 92: -0.06130106461074596\n",
      "Operator 93: -0.14563013764095556\n",
      "Operator 94: -0.1269010725040971\n",
      "Operator 95: 4.628869502587563e-06\n",
      "Operator 96: -2.0721986783173857e-06\n",
      "Operator 97: 6.520933893694458e-06\n",
      "Operator 98: -1.6819292632933892e-06\n",
      "Operator 99: 1.7196078724371344e-06\n",
      "Operator 100: -2.8658320707708527e-06\n",
      "Operator 101: 9.807940261495938e-07\n",
      "Operator 102: -1.7898149718998095e-07\n",
      "Operator 103: -1.7396527788100077e-06\n",
      "Operator 104: 5.244088774485639e-07\n",
      "Operator 105: -9.800441942390718e-07\n",
      "Operator 106: 5.640568643719577e-07\n",
      "Operator 107: -2.6789030613265654e-06\n",
      "Operator 108: 0.19581991087186104\n",
      "Operator 109: -0.19593526290675253\n",
      "Operator 110: 0.19341884993960587\n",
      "Operator 111: 0.136453758743506\n",
      "Operator 112: -0.20060155270183716\n",
      "Operator 113: 0.20055704495972088\n",
      "Operator 114: -0.1423435467136967\n",
      "Operator 115: -1.8939686517189875\n",
      "Operator 116: -0.29174595794830455\n",
      "Operator 117: -0.21302574488041046\n",
      "Operator 118: 0.05404487172607678\n",
      "Operator 119: 0.13009991064344276\n",
      "Operator 120: -7.647869166455395e-06\n",
      "Operator 121: -5.652419615698179e-06\n",
      "Operator 122: 2.98412170605597e-06\n",
      "Operator 123: -5.702859165422625e-06\n",
      "Operator 124: 1.686515211637117e-06\n",
      "Operator 125: -1.7987618515982046e-06\n",
      "Operator 126: 2.9226848565824793e-06\n",
      "Operator 127: -9.008179376287665e-07\n",
      "Operator 128: 1.79098625853219e-07\n",
      "Operator 129: 1.7700859676662018e-06\n",
      "Operator 130: -4.930476630326878e-07\n",
      "Operator 131: 1.614254789321915e-06\n",
      "Operator 132: -1.0422749336584205e-06\n",
      "Operator 133: 1.9301657699890427e-06\n",
      "Operator 134: -0.11385598141910322\n",
      "Operator 135: 0.01696119289823568\n",
      "Operator 136: 0.05727572460691749\n",
      "Operator 137: 0.12945255125177507\n",
      "Operator 138: 0.21871260288648017\n",
      "Operator 139: 0.022633389435533363\n",
      "Operator 140: 0.20520927488899754\n",
      "Operator 141: 0.1068760650055726\n",
      "Operator 142: -0.1083100384565975\n",
      "Operator 143: 0.43673415837871854\n",
      "Operator 144: 0.061355874485003986\n",
      "Operator 145: 0.14563504576034125\n",
      "Operator 146: 0.12690909838104042\n",
      "Operator 147: -0.25456496737312573\n",
      "Operator 148: 0.389140392759588\n",
      "Operator 149: -0.053794794896026836\n",
      "Operator 150: 0.12820418305560405\n",
      "Operator 151: 0.21940227272035817\n",
      "Operator 152: 0.022557377556284206\n",
      "Operator 153: 0.21887290336627585\n",
      "Operator 154: 0.10360779229458063\n",
      "Operator 155: 0.4251125583608214\n",
      "Operator 156: -0.30467118831016027\n",
      "Operator 157: 0.09539441067218267\n",
      "Operator 158: 0.10325527140392562\n",
      "Operator 159: 0.12201869672118096\n",
      "Operator 160: 0.17608638229724338\n",
      "Operator 161: -0.7173454147358335\n",
      "Operator 162: 0.12777803737594504\n",
      "Operator 163: -0.12552838805144365\n",
      "Operator 164: -0.21931591085355173\n",
      "Operator 165: -0.022541905667738636\n",
      "Operator 166: -0.2187620313911788\n",
      "Operator 167: -0.10357505155371942\n",
      "Operator 168: -0.42508144943319015\n",
      "Operator 169: 0.3046724446098516\n",
      "Operator 170: -0.0954376250131307\n",
      "Operator 171: -0.10326946976355343\n",
      "Operator 172: -0.12202681720727496\n",
      "Operator 173: -0.24398852982702793\n",
      "Operator 174: 0.7488075915626015\n",
      "Operator 175: 0.012173205493585526\n",
      "Operator 176: 0.042535362032454296\n",
      "Operator 177: 0.2393370783994269\n",
      "Operator 178: 0.019941784000646508\n",
      "Operator 179: 0.22614298337791247\n",
      "Operator 180: 0.5426139091721376\n",
      "Operator 181: -0.1351957944222136\n",
      "Operator 182: 0.6880435873552272\n",
      "Operator 183: -0.11904918063881961\n",
      "Operator 184: 0.0449160487778179\n",
      "Operator 185: 0.08046826949222481\n",
      "Operator 186: -6.135891248648662e-06\n",
      "Operator 187: 0.24204492282613202\n",
      "Operator 188: -0.19282012582300598\n",
      "Operator 189: 0.19528271814717751\n",
      "Operator 190: 0.13588236833052866\n",
      "Operator 191: -0.19950541999703805\n",
      "Operator 192: 0.20189996366938026\n",
      "Operator 193: -0.09196785089749739\n",
      "Operator 194: -0.29298247839426084\n",
      "Operator 195: -1.1471935699479444e-05\n",
      "Operator 196: 1.5016752559315862e-05\n",
      "Operator 197: 0.11026303380078628\n",
      "Operator 198: 0.135327183502127\n",
      "Operator 199: -2.8971437062874344e-06\n",
      "Operator 200: 0.4144573821054105\n",
      "Operator 201: -0.28635222260112725\n",
      "Operator 202: 0.6326589814980154\n",
      "Operator 203: -0.05775828810212048\n",
      "Operator 204: 0.04270242815142987\n",
      "Operator 205: 0.23846100884084195\n",
      "Operator 206: 0.02079099761306393\n",
      "Operator 207: 0.19872290772149345\n",
      "Operator 208: 0.17430860695171946\n",
      "Operator 209: 0.5934231149896937\n",
      "Operator 210: -0.11242331981844403\n",
      "Operator 211: -0.05150073654699255\n",
      "Operator 212: 0.044429280404713556\n",
      "Operator 213: 0.08242694593229868\n",
      "Operator 214: 0.24619770392159543\n",
      "Operator 215: -0.5411408955401384\n",
      "Operator 216: 0.08615741423904155\n",
      "Operator 217: -0.12490316708319454\n",
      "Operator 218: -0.22067735245194303\n",
      "Operator 219: -0.0206968349288055\n",
      "Operator 220: -0.24336687650193733\n",
      "Operator 221: 0.050280140081606445\n",
      "Operator 222: 1.7217994882658596\n",
      "Operator 223: -0.7519983765074721\n",
      "Operator 224: 0.16513500184165092\n",
      "Operator 225: -0.08735398916688439\n",
      "Operator 226: -0.11482306985742627\n",
      "Operator 227: -0.4946197992583921\n",
      "Total gradient norm: 5.169655906961599\n",
      "Operators under consideration (1):\n",
      "[63]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.971133680904214)]\n",
      "Operator(s) added to ansatz: [63]\n",
      "Gradients: [np.float64(1.971133680904214)]\n",
      "Initial energy: -26.989145478665325\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77, 63]...\n",
      "Starting point: [np.float64(0.7853975771901507), np.float64(0.7853978488049645), np.float64(0.7853998329408135), np.float64(0.7853992704375616), np.float64(-0.7853985411858783), np.float64(-0.7853977820585877), np.float64(-0.7853993674981521), np.float64(-0.7853979379117474), np.float64(-0.15715742424371196), np.float64(-0.12532836695650415), np.float64(0.11366516481751486), np.float64(-0.14288645623040763), np.float64(0.18992160469456906), np.float64(0.14289887788161676), np.float64(0.13709120498681332), np.float64(-0.11859023615144397), np.float64(0.10326764291090554), np.float64(-0.1358347571418553), np.float64(0.17796719114798185), np.float64(0.13561448986134847), np.float64(-0.18687959222680303), np.float64(0.14203187669891562), np.float64(-0.1420217075794575), np.float64(-0.14921925909076203), np.float64(-0.11870129780795068), np.float64(0.11878625138278281), np.float64(-0.13804371786629258), np.float64(0.13804208602246565), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -27.104701\n",
      "         Iterations: 26\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "Current energy: -27.104700529219556\n",
      "(change of -0.11555505055423154)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77, 63]\n",
      "On iteration 29.\n",
      "\n",
      "*** ADAPT-VQE Iteration 30 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 6.277380057599795e-05\n",
      "Operator 1: -8.676029840107057e-05\n",
      "Operator 2: 3.704346325779896e-05\n",
      "Operator 3: -2.2355376627682213e-05\n",
      "Operator 4: 8.004253380824766e-05\n",
      "Operator 5: -6.4093247976609905e-06\n",
      "Operator 6: -2.5023792101222173e-06\n",
      "Operator 7: 2.457275352972521e-05\n",
      "Operator 8: -3.505092325560566e-05\n",
      "Operator 9: 2.736866876554278e-05\n",
      "Operator 10: -1.67409714287442e-05\n",
      "Operator 11: 3.841951812322275e-05\n",
      "Operator 12: -2.661440171514151e-05\n",
      "Operator 13: 7.544581129442309e-05\n",
      "Operator 14: -3.732771621681181e-05\n",
      "Operator 15: 2.351465674649533e-05\n",
      "Operator 16: -7.701520811556944e-05\n",
      "Operator 17: 7.532972016635135e-06\n",
      "Operator 18: 2.499854688530769e-06\n",
      "Operator 19: -2.2788739639303945e-05\n",
      "Operator 20: 4.0335312314732086e-05\n",
      "Operator 21: -1.3886329845808065e-05\n",
      "Operator 22: 4.715932571340564e-05\n",
      "Operator 23: -2.9172890160023002e-05\n",
      "Operator 24: 2.2807333075025582e-05\n",
      "Operator 25: -1.1409324958577827e-05\n",
      "Operator 26: 5.722284214400123e-05\n",
      "Operator 27: -9.839711016642537e-05\n",
      "Operator 28: -0.010696610982647988\n",
      "Operator 29: 5.5144262010936445e-05\n",
      "Operator 30: -0.21856643849662893\n",
      "Operator 31: -0.13563982162333457\n",
      "Operator 32: 0.20430975957920788\n",
      "Operator 33: -0.2041211227863425\n",
      "Operator 34: 0.13293363610861683\n",
      "Operator 35: 0.2205427885622776\n",
      "Operator 36: -0.20582522537195613\n",
      "Operator 37: 0.24243176776239675\n",
      "Operator 38: -0.3710639167954823\n",
      "Operator 39: -0.12661244584161782\n",
      "Operator 40: 6.810360884948027e-05\n",
      "Operator 41: 0.00017356166414883735\n",
      "Operator 42: -0.04131580013887526\n",
      "Operator 43: -0.19091157673506315\n",
      "Operator 44: -0.13685468237840098\n",
      "Operator 45: 0.2040068575750365\n",
      "Operator 46: -0.20418425670616341\n",
      "Operator 47: 0.1335209901711782\n",
      "Operator 48: 0.21866513895110973\n",
      "Operator 49: -0.20852328010725202\n",
      "Operator 50: 0.19500765741233578\n",
      "Operator 51: -0.09034300404928645\n",
      "Operator 52: -0.11445162152744819\n",
      "Operator 53: 1.901574434234187e-05\n",
      "Operator 54: 9.953502122073132e-06\n",
      "Operator 55: -0.0041655102392630285\n",
      "Operator 56: -7.82260256012186e-05\n",
      "Operator 57: 0.2169507115037641\n",
      "Operator 58: 0.13553187560261004\n",
      "Operator 59: -0.20434560877683622\n",
      "Operator 60: 0.20422282413460693\n",
      "Operator 61: -0.13229888732258743\n",
      "Operator 62: -0.2461853524045814\n",
      "Operator 63: -1.313382626522566e-06\n",
      "Operator 64: -0.01094938186860437\n",
      "Operator 65: 0.424559991492818\n",
      "Operator 66: 0.1309904913094784\n",
      "Operator 67: -4.367329892729515e-05\n",
      "Operator 68: 0.6040487829433981\n",
      "Operator 69: -0.14149667460900467\n",
      "Operator 70: 0.17060342687118857\n",
      "Operator 71: -0.20399753459770104\n",
      "Operator 72: -0.13617616214346312\n",
      "Operator 73: 0.20421527903355297\n",
      "Operator 74: -0.204259087081118\n",
      "Operator 75: 0.13230284498012695\n",
      "Operator 76: 0.24447074030208732\n",
      "Operator 77: 1.6001489876730304e-05\n",
      "Operator 78: -0.004645580002568142\n",
      "Operator 79: -0.3173005706129537\n",
      "Operator 80: -0.12727881400919466\n",
      "Operator 81: -3.776774269086601e-05\n",
      "Operator 82: 0.2053277062413399\n",
      "Operator 83: 0.0445016171003086\n",
      "Operator 84: -0.05103915524349893\n",
      "Operator 85: -0.12886693558683937\n",
      "Operator 86: -0.2214213486668697\n",
      "Operator 87: -0.02006294088864878\n",
      "Operator 88: -0.24032500450759728\n",
      "Operator 89: -0.03678669275051429\n",
      "Operator 90: 0.07366125274366743\n",
      "Operator 91: 0.18626031988478714\n",
      "Operator 92: -0.08873355826427719\n",
      "Operator 93: -0.1763790298874208\n",
      "Operator 94: -0.11818926316644326\n",
      "Operator 95: 5.1482752495820924e-05\n",
      "Operator 96: -2.2146604992869605e-05\n",
      "Operator 97: 7.221867354439916e-05\n",
      "Operator 98: -1.768348187831208e-05\n",
      "Operator 99: 2.0375763294971886e-05\n",
      "Operator 100: -3.0664022873915635e-05\n",
      "Operator 101: 5.956021172297309e-06\n",
      "Operator 102: -2.3049878863411546e-06\n",
      "Operator 103: -2.3094875376539897e-05\n",
      "Operator 104: 1.5065018807103314e-05\n",
      "Operator 105: -1.4861742977351211e-05\n",
      "Operator 106: 1.003289335363422e-05\n",
      "Operator 107: -3.987252085690951e-05\n",
      "Operator 108: 0.19568867924830813\n",
      "Operator 109: -0.1958725177986845\n",
      "Operator 110: 0.1929054741591857\n",
      "Operator 111: 0.1368481532590995\n",
      "Operator 112: -0.2041351471014967\n",
      "Operator 113: 0.20421100720033292\n",
      "Operator 114: -0.13298000493545914\n",
      "Operator 115: -0.23065911743518608\n",
      "Operator 116: 0.18140785132535425\n",
      "Operator 117: -0.14112615556074784\n",
      "Operator 118: 0.2403019382795326\n",
      "Operator 119: 0.12010541305934416\n",
      "Operator 120: 4.463272457482962e-05\n",
      "Operator 121: -6.291826546863199e-05\n",
      "Operator 122: 3.225246426546168e-05\n",
      "Operator 123: -6.30358487623956e-05\n",
      "Operator 124: 1.7850866026308436e-05\n",
      "Operator 125: -1.9321787375505206e-05\n",
      "Operator 126: 2.9909163238088273e-05\n",
      "Operator 127: -7.002648521543852e-06\n",
      "Operator 128: 2.4134616113599683e-06\n",
      "Operator 129: 2.534555041609765e-05\n",
      "Operator 130: -1.4819832676239708e-05\n",
      "Operator 131: 2.3014033969731562e-05\n",
      "Operator 132: -1.632186317972184e-05\n",
      "Operator 133: 2.7871934031198364e-05\n",
      "Operator 134: -0.11369648429178214\n",
      "Operator 135: 0.0169430123266166\n",
      "Operator 136: 0.05759795121449958\n",
      "Operator 137: 0.12935758877374195\n",
      "Operator 138: 0.22157405805029562\n",
      "Operator 139: 0.02004765139088698\n",
      "Operator 140: 0.24049981980234192\n",
      "Operator 141: 0.043233922939027\n",
      "Operator 142: -0.07176534215863907\n",
      "Operator 143: 0.6395761341106201\n",
      "Operator 144: -0.179893660186895\n",
      "Operator 145: 0.16123234263145275\n",
      "Operator 146: 0.11216212443816745\n",
      "Operator 147: -0.2544545657508578\n",
      "Operator 148: 0.38909321268575403\n",
      "Operator 149: -0.05349372936626552\n",
      "Operator 150: 0.12809686201401732\n",
      "Operator 151: 0.22170493457400964\n",
      "Operator 152: 0.020078113091246498\n",
      "Operator 153: 0.24033978437611705\n",
      "Operator 154: 0.03678919722326702\n",
      "Operator 155: -0.07119561982287853\n",
      "Operator 156: -0.1779884745793117\n",
      "Operator 157: 0.07462440384064563\n",
      "Operator 158: 0.15223693163422433\n",
      "Operator 159: 0.11545543953419266\n",
      "Operator 160: 0.17592859298280497\n",
      "Operator 161: -0.7174014792455627\n",
      "Operator 162: 0.12746233764509182\n",
      "Operator 163: -0.1254475758769557\n",
      "Operator 164: -0.22178379618385363\n",
      "Operator 165: -0.02009465765923414\n",
      "Operator 166: -0.24071379431874312\n",
      "Operator 167: -0.043183828732072126\n",
      "Operator 168: 0.017505245690364733\n",
      "Operator 169: 0.27532937758292975\n",
      "Operator 170: -0.08767214535424611\n",
      "Operator 171: -0.10775797192816419\n",
      "Operator 172: -0.1053833604642612\n",
      "Operator 173: -0.24388725868525762\n",
      "Operator 174: 0.7489131773979378\n",
      "Operator 175: 0.012624246840586894\n",
      "Operator 176: 0.042517160345646045\n",
      "Operator 177: 0.24218576499353545\n",
      "Operator 178: 0.019978675679215244\n",
      "Operator 179: 0.21950431447515942\n",
      "Operator 180: 0.13500233760080826\n",
      "Operator 181: -0.06484064347795183\n",
      "Operator 182: 0.38807662141511856\n",
      "Operator 183: -0.11145689179643636\n",
      "Operator 184: 0.03963014861572562\n",
      "Operator 185: 0.07481838339774155\n",
      "Operator 186: -6.828424399952466e-05\n",
      "Operator 187: 0.2419624864168416\n",
      "Operator 188: -0.19280205086561647\n",
      "Operator 189: 0.19474287254566067\n",
      "Operator 190: 0.13629060196410703\n",
      "Operator 191: -0.20405113404889758\n",
      "Operator 192: 0.20413063647340435\n",
      "Operator 193: -0.13347858797127743\n",
      "Operator 194: -0.21643796833786888\n",
      "Operator 195: -0.043458599123876424\n",
      "Operator 196: -7.592658341736789e-05\n",
      "Operator 197: 0.12437417420514854\n",
      "Operator 198: 0.11724669319589465\n",
      "Operator 199: -5.02661318076026e-05\n",
      "Operator 200: 0.41441373297506456\n",
      "Operator 201: -0.28625854778988874\n",
      "Operator 202: 0.6327042253475853\n",
      "Operator 203: -0.05733678985363656\n",
      "Operator 204: 0.04265161667981459\n",
      "Operator 205: 0.24178671514522243\n",
      "Operator 206: 0.01998806373709196\n",
      "Operator 207: 0.22015189627467704\n",
      "Operator 208: 0.1309277373972389\n",
      "Operator 209: 0.1327613005807635\n",
      "Operator 210: -0.2860188577025259\n",
      "Operator 211: -0.05634325718781788\n",
      "Operator 212: 0.05260698835192022\n",
      "Operator 213: 0.0718150972657432\n",
      "Operator 214: 0.24615508787964843\n",
      "Operator 215: -0.5411885302358757\n",
      "Operator 216: 0.08578923873900726\n",
      "Operator 217: -0.12489076103223477\n",
      "Operator 218: -0.22258103193966955\n",
      "Operator 219: -0.020077991360799685\n",
      "Operator 220: -0.24061573144930876\n",
      "Operator 221: -0.04313814425503434\n",
      "Operator 222: -0.0047657123939398\n",
      "Operator 223: -0.5884953991487726\n",
      "Operator 224: 0.09896782307795196\n",
      "Operator 225: -0.1304366288403829\n",
      "Operator 226: -0.10557518260880774\n",
      "Operator 227: -0.4945692910646195\n",
      "Total gradient norm: 2.781951387961096\n",
      "Operators under consideration (1):\n",
      "[174]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.7489131773979378)]\n",
      "Operator(s) added to ansatz: [174]\n",
      "Gradients: [np.float64(0.7489131773979378)]\n",
      "Initial energy: -27.104700529219556\n",
      "Optimizing energy with indices [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77, 63, 174]...\n",
      "Starting point: [np.float64(0.7853897461481425), np.float64(0.7853935361019285), np.float64(0.7854164264340725), np.float64(0.7854101255981398), np.float64(-0.7854009992551804), np.float64(-0.7853934987899529), np.float64(-0.785411388825296), np.float64(-0.785395049144507), np.float64(-0.1450639575885314), np.float64(-0.12439909679100877), np.float64(0.10798087914267761), np.float64(-0.14310270428440827), np.float64(0.19021918584605949), np.float64(0.1430716724209787), np.float64(0.13709729802755577), np.float64(-0.11859263507008233), np.float64(0.10327189055649291), np.float64(-0.135432500600151), np.float64(0.17709551899782686), np.float64(0.13513206849759268), np.float64(-0.1900284323471734), np.float64(0.14298144338418323), np.float64(-0.1429528330142486), np.float64(-0.14925188293971678), np.float64(-0.11871931861593311), np.float64(0.11880235176171651), np.float64(-0.14730655834729303), np.float64(0.11761890937745534), np.float64(-0.11752501058546429), np.float64(0.0)]\n",
      "         Current function value: -27.117443\n",
      "         Iterations: 17\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 72\n",
      "\n",
      "Current energy: -27.11744326829137\n",
      "(change of -0.012742739071814668)\n",
      "Current ansatz: [226, 222, 216, 218, 127, 131, 186, 54, 118, 92, 117, 218, 205, 177, 227, 214, 173, 199, 81, 40, 61, 47, 114, 42, 29, 56, 195, 77, 63, 174]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 4 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(30):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "23\n",
      "30\n",
      "37\n",
      "44\n",
      "49\n",
      "54\n",
      "61\n",
      "68\n",
      "75\n",
      "82\n",
      "89\n",
      "96\n",
      "103\n",
      "110\n",
      "117\n",
      "124\n",
      "131\n",
      "138\n",
      "145\n",
      "152\n",
      "159\n",
      "166\n",
      "173\n",
      "180\n",
      "187\n",
      "194\n",
      "201\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGwCAYAAAC+Qv9QAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQsNJREFUeJzt3Qd4leX9//HvySZ7kkDIJOwthL1luKtiFbUVt/39/bXWVq3ipCq4fnXUWqxYB22tWsVdUSQgCLJlJqwkJEBCFskJCdnnf913yDEhQRLOSZ4z3q/rOlee5zknJzc3Dzkf7mmyWCwWAQAAcGIeRhcAAADAVgQaAADg9Ag0AADA6RFoAACA0yPQAAAAp0egAQAATo9AAwAAnJ6XuLiGhgY5evSoBAUFiclkMro4AACgHdQyeeXl5dKzZ0/x8Dh7+4vLBxoVZuLi4owuBgAAOAe5ubnSq1evs77O5QONaplpqpDg4GCjiwMAANrBbDbrBommz3Fx90DT1M2kwgyBBgAA59Le4SIMCgYAAE6PQAMAAJwegQYAADg9Ag0AAHB6hgeampoauf/++8XLy0uys7Ot1+vq6mTJkiUybdo0mT59uowcOVJuvfVWKSoqMrS8AADA8RgaaFSAmTJliuTl5Ul9fX2L5/Lz8+XXv/61vPjii7Jy5UpZt26dZGVlyVVXXWVYeQEAgGMyNNCcOHFCli5dKjfddFOr53x8fOTmm2+WoUOH6nNfX1/5n//5H1m9erUOQAAAAA6xDs3gwYP118OHD7d6rnv37vKXv/ylxTU/Pz/9tbq6uotKCAAAnIFTLay3fv16SU1NlcTExDO+RoWd5oFHrTQIAABcm+GDgttLDQZ+/fXX5eWXX/7J1y1atEhCQkKsD/ZxAgDA9TlFoFEznq699lp54oknZPTo0T/52gceeEDKysqsD7WHEwAAcG0O3+XU0NAg8+bNkxkzZuhp22ejBg+rBwAAcB8O30Jz5513Snx8vPzhD3/Q5ytWrJDMzEyjiyUWi0XSMgqkocFidFEAAHB7Dh1o1IJ7GRkZMmfOHNm8ebN+vPfee5KTk2N00eTX72yTm97cJP/ccMjoogAA4Pa8jF4leNasWVJaWqrP586dqwfxvv/++7J79255+umn9XU1s6m56667ToyWmhgun+3Ik2e+3CszB8ZITEjjlHIAAND1TBbVd+LC1LRtNdtJDRAODg622/vWN1hkzl/XyQ+5pXLBoBhZ/MuRdntvAADcnbmDn98O3eXkyDw9TLLoyiHi5WGSL3fny1e7840uEgAAbotAY4MBPYLl1knJ+vjRT3bLieo6e/29AACADiDQ2Oiu8/tIfLi/5JVVyXPL99r6dgAA4BwQaGzUzcdTnryicU+qt9Zn6zE1AACgaxFo7GBSnyi5YkSsqOHVD3y4U2rrG+zxtgAAoJ0INHby0MUDJNTfW9LzzPL3tVn2elsAANAOBBo7iQj0lfkXDdDHz6/YJ7kllfZ6awAAcBYEGjv6+cheMjY5XKpqG+TBj3bp7REAAEDnI9DYkclkkoVXDBEfLw/5dl+hfLL9qD3fHgAAnAGBxs6SowLl19NS9PEfP90jpZU19v4RAADgNASaTnDHlN7Sp3ugFFfUyKIvMjrjRwAAgGYINJ1AdTmpbRGUdzfnyveZxZ3xYwAAwCkEmk4yKjFcrhsTr4/nL9spVbX1nfWjAABwewSaTvSHC/pLVJCvZBZWyCurDrr9zQYAQGch0HSikG7e8tilg/TxX1cdkAMF5Z354wAAcFsEmk520ZAYmd6/u9TWW/S2CA0NrE0DAIC9EWi6YG2aP/5skPj7eMqm7ON6kDAAALAvAk0X6BXmL7+b2VcfL/wiXQrKq7rixwIA4DYINF3kxvGJMiQ2RMqr6vSCewAAwH4INF3Ey7NxbRoPk8hnO/IkLaOgq340AAAuj0DThQbHhsjNE5L08UMf7ZLKmrqu/PEAALgsAk0Xu3tmX4kN7SZHSk/K81/v6+ofDwCASyLQdLEAXy954vLB+vj1tVmy60hZVxcBAACXQ6AxwLT+3eWSoT1ELUmj1qapq28wohgAALgMAo1BHrl0oAT7ecnOI2Xy5rpso4oBAIBLINAYpHuQnzxw0QB9/Kev9+kxNQAA4NwQaAx0zag4SU0Mk8qaenn4o11isbAtAgAA54JAYyAPD5Nem8bb0yQrMwrki535RhYHAACnRaAxWEr3IPmfqSn6+LFPd0vZyVqjiwQAgNMh0DiA/ze1tyRHBkhhebU8/WWG0cUBAMDpEGgcgJ+3pyy8cog+/teGHNmUXWJ0kQAAcCoEGgcxNjlCrh7VSx/P/3Cn1NSxNg0AAO1FoHEg8y8aIBEBPrK/4IS8uvqg0cUBAMBpEGgcSKi/j15wT/lz2gHJLDxhdJEAAHAKBBoHc9mwnjKpT6TucnpwGWvTAADQHgQaB2MymeTJy4eIn7eHrM8slv9sOWx0kQAAcHgEGgcUH+Evv53RVx8/+UW6FJ+oNrpIAAA4NAKNg7plYpIM6BEspZW18sTn6UYXBwAAh0agcVDenh56WwSTSWTZtiOyZn+h0UUCAMBhEWgc2PC4UJk3LlEfqwHCJ2vqjS4SAAAOiUDj4O6Z3U96hPhJTkmlvLRyv9HFAQDAIRFoHFygr5csuGyQPv7bt5mSnmc2ukgAADgcAo0TmDUoRi4YFCP1DRZ54MOd+isAAPgRgcZJPHbZIN1a80Nuqfzj+0NGFwcAAIdCoHESMSF+ct8F/fTxs8v3Sl7ZSaOLBACAwyDQOJHrxyTIiPhQOVFdJ49+vNvo4gAA4DAINE7E08Ok16bx8jDJV3uOyfLd+UYXCQAAh2B4oKmpqZH7779fvLy8JDs7u9Xzr776qowcOVImTJggF198sRw5ckTcWf+YYLltcrI+fvLzdKmuY20aAAAMDTQqwEyZMkXy8vKkvr71B/OHH34oCxYskOXLl8t3330nY8aMkUsuuUQaGhrEnf3vtBSJCvLVa9MsXc8AYQAADA00J06ckKVLl8pNN93U5vNPPPGEzJs3TyIjI/X5XXfdJbt27ZLPP/9c3FmAr5fcM6tx88qXvtkvxytqjC4SAADuG2gGDx4sKSkpbT5XUlIi27Ztk1GjRlmvhYSESN++fWXFihXi7q4aGSf9Y4LEXFXHCsIAALdn+BiaM8nKytJfo6OjW1yPiYmxPteW6upqMZvNLR6uOkD4wYsH6GPV7ZRVVGF0kQAAMIzDBprKykr91dfXt8V1dd70XFsWLVqkW3KaHnFxceKqJvWJkqn9oqSuwSJP/Tfd6OIAAGAYhw00/v7+1haX5tR503NteeCBB6SsrMz6yM3NFVc2/6IB4mESWb77mGzILDa6OAAAGMJhA01ycuPU5GPHjrW4np+fb32uLaoFJzg4uMXDlfWNDpK5o+P18ZNfpEsD+zwBANyQwwaasLAwGTFihGzZssV6TY2H2bdvn8yYMcPQsjmau2f0lQAfT9lxuEw+2X7U6OIAANDlHDbQKA899JC89dZbUlzc2JXy0ksv6ZlRF110kdFFcyhqTZr/N61xttgzX2ZIVS2L7QEA3IuX0asEz5o1S0pLS/X53Llz9SDe999/X59feeWVUlBQIDNnzhQ/Pz/davPpp5+Kh4dD5zBD3DIxSf75/SE5WlYlr6/NkjtPBRwAANyByWKxWMSFqW4qNdtJDRB29fE0y7Ydlrvf3a67n1bdO0233AAA4A6f3zR1uJCfDYuVob1CpKKmXp5fsc/o4gAA0GUINC7EQy22d1HjYnv/3pgj+46VG10kAAC6BIHGxYxJjpBZA6NFzd5e+AWL7QEA3AOBxgXdf2F/8fIwyaq9hbJmf6HRxQEAoNMRaFxQclSg/GJsgj5+8vN0qWexPQCAiyPQuKi7zu8jwX5ekpFfLv/Z4trbPwAAQKBxUWEBPvLr6X308XNf7ZOK6jqjiwQAQKch0LiwG8YnSHy4vxSWV8ur32YaXRwAADoNgcaF+Xp56gHCyt++PSj5ZVVGFwkAgE5BoHFxFw6OkVEJYVJV2yDPfbXX6OIAANApCDQuzmQyyYMXNy6298HWw7L7aJnRRQIAwO4ING5gRHyYXDqsp6hdu9Q0bhffvgsA4IYING7ivtn9xMfLQ9YdLJaVGQVGFwcAALsi0LiJuHB/uWlCoj5+8ot0qa1vMLpIAADYDYHGjdw5LUXCA3wks7BC3tmYY3RxAACwGwKNGwn285bfzmhcbO+FFfvlBIvtAQBcBIHGzVw7Ol6SIgOkpKJG3tlAKw0AwDUQaNyMt6eH3DE5WR+/vjZLauoYSwMAcH4EGjd0xXmx0j3IV/LNVfLRD0eMLg4AADYj0Ljplgg3T0zSx6+uPigNDaxLAwBwbgQaN3XdmHgJ8vWSg4UVsiL9mNHFAQDAJgQaN57xdP3YBH28ePVBVg8GADg1Ao0bu3lCovh4esjWnFLZlH3c6OIAAHDOCDRurHuwn8wZGWttpQEAwFkRaNzcbZOSxWQSvb/T3vxyo4sDAMA5IdC4ueSoQLlgUIw+fvVbWmkAAM6JQAP51ZTeuhY++eGoHCk9SY0AAJwOgQYyLC5UxiVHSF2DRV5fk0WNAACcDoEG2q+mNrbS/HtTjpRW1lArAACnQqCBNrlPpAzsESyVNfXy9vpD1AoAwKkQaKCZTCa5Y0rjppVvrsuWkzX11AwAwGkQaGB18ZAe0iusm5RU1Mj7W3KpGQCA0yDQwMrL00OvS6P87dtMqatvoHYAAE6BQIMWrh4VJ+EBPnL4+En5fGcetQMAcAoEGrTQzcdT5o1L1MeLV2eyaSUAwCkQaNDKDeMSpJu3p6TnmeXb/UXUEADA4RFo0EpYgI/MHR2njxevYjsEAIDjI9CgTbdOShYvD5OszyyW7bml1BIAwKERaNCm2NBuctmwnvp48WpaaQAAjo1AgzO649SmlV/uzpfMwhPUFADAYRFocEb9YoJkev/uYrGIvLYmk5oCADgsAg1+0q9OtdJ8sOWIFJirqC0AgEMi0OAnpSaGyXnxoVJT3yB//y6b2gIAOCQCDc66aWVTK80/vz8k5qpaagwA4HAINDirGQOiJaV7oJRX18m/NuRQYwAAh0OgwdlvEg+T3D65cdPKv6/Nkuq6emoNAOBQCDRol8uHx0pMsJ8UlFfLsq1HqDUAgEMh0KBdfLw85JaJSfr4b99mSn2DhZoDADgMhw801dXVcvfdd8uwYcNkypQpMmbMGFm2bJnRxXJL146Jl2A/L8ksqpCv9+QbXRwAAJwn0DzxxBPy0UcfybfffiurV6+WxYsXy9y5c2X79u1GF83tBPp6yS/HJejjxaszxaJW3AMAwAE4fKD54YcfJDU1VUJCQvT5iBEj9PHKlSuNLppbunF8kvh4esgPuaWy5dBxo4sDAIBzBJo5c+bImjVrJCencbrw8uXLpbCwUKKjo40umluKCvKVK0bE6mO2QwAAOAovcXA33nijVFZWytChQ6VHjx6yb98+ueqqq+Tqq68+45gb9WhiNpu7sLTu4dZJSfLu5lz5as8xySqqkKTIAKOLBABwcw7fQrNkyRJ56qmnZMuWLZKeni5bt26VsWPHiodH20VftGiR7pJqesTFxXV5mV1dn+gfN618fS2bVgIAjOfQgUYNOr3vvvvkjjvukN69G5ffV7OdvvjiC1m4cGGb3/PAAw9IWVmZ9ZGbm9vFpXYPt01qXGjv/c2HpaSixujiAADcnEMHGjVW5vjx45KYmNjielJSknzwwQdtfo+vr68EBwe3eMD+xiaHy+DYYKmua5B/fH+IKgYAGMqhA01kZKQOKHl5eS2uq3N/f3/DyoXGTSubWmneWpctVbVshwAAMI5DBxo1TmbevHl6HI1qqVHUGJqvv/76jIOC0XUuGtJDYkO7SXFFjSzbxnYIAADjOHSgUZ5//nm57LLL5Pzzz5eJEyfKTTfdpAcJ/+Y3vzG6aG7P29NDbprQ2B24ZE2mNLAdAgDAICaLiy/3qqZtq9lOaoAw42nsr7yqVsYvWinl1XXy+rxRcv4A1gcCAHT957fDt9DAsQX5ect1Y+Ktm1YCAGAEAg1sduOERPHyMMmGrBLZcbiUGgUAOH6g2bFjh+zevbtzSgOn1COkm1w2rKc+fm1NltHFAQC4oQ4HmuHDh+uBukBzt56awv3FzjzJLamkcgAAjh1o1EwjNY0aaG5gz2CZmBIp9Q0WeeO7bCoHAODYgWbw4MFy9OjRNp9T06vhvm6b3NhK8+6mHCk7WWt0cQAAbqTDu20HBQXJ+PHj9bowvXr1Ek9PT+tzu3btsnf54EQm94mUftFBsvdYubyzMUd+NaVx/y0AABxuHZqwsDA9jqYt27dvl5KSEnEkrEPTtd7fnCv3/meHRAf7ypr7pouPFxPpAACd//ntdS5jaD799NM2n7v22ms7+nZwMZcN7ynPLt8rx8zV8un2ozJnZC+jiwQAcAMd/u/zmcKM8s4779haHjg5Xy9PvS6N8tqaTHHxhagBAA7inPoDDh06pPdSmjZtmn6oY3UNUK4fnSD+Pp6SkV8uaw8UUSkAAMcLNKtWrZL+/fvLmjVrJDIyUj/Wrl0rAwYMkNWrV3dOKeFUQvy95epRcfqY7RAAAF2hw2No5s+fL5988onMnDmzxfUVK1bI/fffL+vXr7dn+eCkbpmYJG+vz5Y1+4skPc8sA3qcfUAXAABd1kKjxkScHmaUGTNmMF4CVnHh/nLhkB76eAnbIQAAHC3QVFRUSFFR63ERhYWFUlnJkvf40e2ntkP4ZPsRyS+romoAAI7T5TRv3jwZOXKk3HTTTdK7d+PCaQcOHJC33npLDw4GmgyLC5XRieGyMbtE3lyXLfdf2J/KAQA4RqD5/e9/r1cLXrhwoeTk5Ohr8fHx8uCDD8ptt93WGWWEk2+HoALNvzYckv+dniKBvh2+5QAAsP9KwWrlPpPJpEPNiRMn9LXAwEBxVKwUbKyGBovM+NNqySyqkEcuGSg3T0wyuEQAAGfQ0c/vDo+hCQ0NlTlz5liDjCOHGRjPw8Mkt54aS/P62iypq28wukgAABfU4UCTmpoqX331VeeUBi7pyvNiJSLAR46UnpT/7so3ujgAABfU4UDTr18/KS8vb/O522+/3R5lgovx8/aUX45L0MdL2A4BANAJOjxCc+jQoTJ16lS5/PLLpVevXuLp6Wl9Tq0YDLTll2MT5K+rDsr2w2WyMatExiRHUFEAAOMGBXfr1k1iYmLafO7YsWMOtxYNg4Idx4PLdso/N+TIjAHdZcm8VKOLAwBwYB39/O5wC83YsWMlLS2tzefURpXAT22H8K+NObIivUCyiyokMTKAygIAGDOG5tZbb5UvvviizefOFHQAJTkqUCamROrjz3fmUSkAAOMCjVoheMuWLfYrAdzKRaf2d/pqN7OdAAAGBprJkyfLww8/3OZzjjZ+Bo5nxoBoMZlEDw5W07gBADBsHZqdO3e2+dwll1xijzLBhUUF+UpqQrg+ppUGAGAvHR4UfPToUT1te/jw4a2mbWdkZNitYHBdswfH6P2dlu/Ol5smsBUCAMCAFhq1SvBll12mN6T08PAQNeu76QG0x6yB0fqrWo+m+EQ1lQYA6PoWGtWt9Nprr7X53N133217ieDy4sL9ZXBssOw6YpYV6cfkmtR4o4sEAHC3FpozhRnl+eeft7U8cBMXDGpcnHH57mNGFwUA4I6BRnn33XdlypQpMmHCBH3++OOPy9KlS+1dNriw2acCzdr9RVJeVWt0cQAA7hZoXn31Vbnnnntk2LBhcvJk47TbK6+8UpYtWyYvvvhiZ5QRLiile6AkRwVITX2DpO0tNLo4AAB3CzSqJWb79u3y0ksv6T0WlEGDBulWmw8++KAzyggXZDKZmnU7scgeAKCLA42a2RQeHm79UGri7e0tNTU1NhYH7tjtlJZRIFW19UYXBwDgToGmurpadu3a1er6ihUrpL6eDyW039BeIdIjxE8qa+r1WBoAALos0Dz22GN6x221Fs3+/fv13k7jx4/X07kXLlx4zgWB+1EtfE2tNF/S7QQA6MpAc+GFF8qGDRt0t1N0dLTeBqFv376ybds2mTlzpi1lgRtqCjRqPZq6+gajiwMAcJeF9ZoGAb/55pv2Lw3cTmpimIQH+EhJRY1eOXh8SqTRRQIAuMs6NIC9eHl6yIwB3fUx3U4AgHNFoIHhLhjc2O301e5j0tDAnmAAgI4j0MBw43tHSqCvl+Sbq2T74VKjiwMAcEIEGhjOz9tTpvaL0sd0OwEAuiTQTJ48+Zx+ENCebqflu/LFYqHbCQDQyYFmz549Mnr0aFmwYIEcOnSoo98OtGlqv+7i4+Uh2cWVsu/YCWoJANC5geaWW26RdevWydChQ+Wuu+6S2bNnyz/+8Q+pqqrq6FsBVmoMzaRTU7a/3MXeTgCATg40Tz/9tHh5eckVV1whH330kd6scvPmzdKjRw+544475Pvvvxd7y8zMlDlz5si0adP0GjhqpWL1M+FaZjd1O7FqMACgswPN+++/r7/W1tbKe++9J/PmzZOXX35ZIiIiJDY2Vt544w2ZOHGirFq1SuyhsLBQzj//fN0alJaWpnf69vf3lwMHDtjl/eE4ZgyIFk8Pk+zJM0tOcaXRxQEAuPJKwWrszJo1a+Sf//yn3l37qquukpUrV7YYLFxaWiqzZs2SjRs32lxA1SI0btw46/ur1qG//e1vOtTAtagVg0cnhsv6zGLdSnPb5GSjiwQAcOVBwaqV5LnnnpP8/HzdInP6zKf09HQ5evSoXQr44Ycftnr/lJQU6dmzp13eHw4624luJwBAZwaa6667TlavXq132Q4ICGjzNarl5pVXXhFbVVRUSFZWltTX18v1118vEyZM0IOQ//vf/57xe6qrq8VsNrd4wHnMGhStv27JOS4FZgaaAwA6KdAkJ5+9G2DKlCly2WWXia1U15Xy8MMPy3333Sffffed/nrppZfK119/3eb3LFq0SEJCQqyPuLg4m8uBrtMjpJsMiwsVtRTNV3uOUfUAgHYxWTq4illSUpLcfPPNbS5+5u3tLYmJiXLhhRdKaGio2Ep1aanZUzfccIO89dZb1utqfI6Pj4989tlnbbbQqEcT1UKjQk1ZWZkEBwfbXCZ0vr+uOihPf5khk/pEytJbxlDlAOCGzGazbpho7+d3hwcFJyQkyB//+EcdNOLj48VkMklOTo4UFxfLqFGjJC8vT89IWr58uYwYMUJsERUVJb6+vnr21OllUGvhtEW9Xj3gvGYPitaBZv3BYimrrJUQf2+jiwQAcLUuJzXj6J133tEhZu3atXrGk1oxWLWgXHDBBbJ371690N69995rc+E8PT31uBkVkpo7duyYDlNwTclRgdI3OlDqGizyTQbdTgCATgg0aiq2mqp9OrXwnZq+3dQlpAYG28Mf/vAH+fjjj3WAappl9dVXX8mdd95pl/eHY7pgELOdAADt1+Eup4MHD+rBuqePkSkpKdGtM/amwtFLL70kP/vZzyQwMFDq6up0a9All1xi958Fx1o1+KWVB2T1vkKprKkTf58O36oAADfS4U8JNcNo5MiReoVgNUC4aWuCt99+W2+HoFYQVjON7DmO5Re/+IV+wH0M7BEsvcK6yeHjJ+XbfYVyweAeRhcJAOBKgeaFF17Qg3T//Oc/W8e2qAHCv/nNb+See+6RkydP6m0QVKgBzpUabK66nZaszdKbVRJoAAB2nbatplGpD5ugoCDronWOPB26o9O+4Dg2Z5fIVYvXS5Cfl2x5aKb4eHV4yBcAwEl19PO7w58QauyMGgCsqB9ASEBnOS8+TCIDfaW8qk7v7wQAgN0CTWpqqp5lBHQ2Dw+TdSsE1e0EAIDdAk2/fv2kvLy8zeduv/32jr4d0K7p21/vOSb1DR3qHQUAuJEODwoeOnSoTJ06VS6//HLp1auXXvyuiVpoD7CnsckRegxN0Ylq2ZpzXFITw6lgAIDtgUZtFBkTEyN///vfWz2nVvAF7EkNBJ4xIFqWbTuiu50INAAAu3Q5jR07VrKystp8jBnDRoKwv9nNVg3u4KQ8AICb6HCgaWuH6yZpaWm2lgdoZUrfKPHz9tCL7O0+2rhUAAAANgWagIAAyc3NlUcffVR+97vf6WvLli2T/fv3d/StgHbp5uOpQ01TKw0AADYHGjXwV810UiHmyy+/1NfUdgdq24Nvvvmmo28HtMsFg9msEgBgx0CjBgWr4LJjxw6Jjm5cI+Tqq6/W3U1PPvlkR98OaJfp/aLFy8Mk+46dkIOFJ6g1AIBtgUYNyhw3bpw+VlsgNImKipL6+vqOvh3QLiH+3jKud4Q+ptsJAGBzoFF7KrS1sJ4aV1NUVNTRtwM63u3EqsEAAFsDzXXXXaenZ//pT3+SwsJCefvtt2X+/Pl6Ovdtt93W0bcD2m3mwGhRjYLbD5dJflkVNQcAOPeF9e699169++XChQslJydHbrzxRomPj5fHHnuMQINO1T3IT0bEhcrWnFL5ek++/HJcIjUOADi3FpqmPZuys7P11t7qoY5pnUFXmGVdZI9VqQEANgaaJoGBgfrRvPUG6IpVg7/PLJayyloqGwBwbl1Oas2Zf/3rX/LDDz/o1pnmS9GrdWmeffbZjr4l0G5JkQHSp3ug7C84IWl7C+TyEbHUHgCg4y008+bNk4ceekiPn1HTtFWgaXoAXb23EwAA59RCo1pm1DYHfn5+rZ5Ts52AzjZrULS8nHZAVu8rlKraevHz9qTSAcDNdbiFpn///m2GGeWGG26wR5mAnzQkNkR6hPhJZU29rN3P2kcAgHMINHPnzpX//d//lXXr1klWVpbuemp63HzzzdQpOp1aoXrWwMZtN77aQ7cTAEDEZOng4BcPjx8zUPOtD9TbqHNH2/5ADVxW6+aoFY6Dg4ONLg7s5LsDRXL9kg0SHuAjG+efL16eNk3YAwA4mI5+fnd4DI1aJfjf//53q+sq0Fx77bUdfTvgnIxOCpeQbt5SUlEjWw4dlzHJjfs8AQDcU4cDzXPPPScJCQltPrd48WJ7lAk4K29PDzm/f3f5cNsR+WrPMQINALi5DrfTT5gw4YzPDRs2zNbyAOewanA+ywYAgJtrV6BJSkqS5ORkWbNmTZvPv/fee/o1/v7+9i4fcEaT+0aKr5eHHD5+UtLzWu8ADwBwH+3qckpMTJS0tDR9vGDBghaDgR955BG5+uqr9WPcuHGdV1LgNP4+XjK5b5R8veeYbqUZ2JNB3wDgrtrVQtM8wKhwo8bQqIHB6vhMrwO6wo/Tt9msEgDcmde5bH2gvPnmmyykB8OdPyBaPEwi6XlmyS2plLhwuj0BwB2d8+IdtMbAEah1aNQUboW9nQDAfbWrhSYvL0+WLl3aYiZJfn5+q2uFhYWdU0rgJ8waGCPfZ5bobqdbJyVTVwDghtq1UnDz1YF/8s1YKRgGOHy8UiY+naa7njY9OEMiAn35ewAAN1spuF1JZcqUKdLQ0HDWx+jRo+3xZwA6pFeYvwzqGSwNFpFv0guoPQBwQ+0KNM8880y73uyFF16wtTzAOZndbJE9AID7aVegSU1Nbfc+T4ARZg1qnL695kCRVFTX8ZcAAG6GLYrhEvpFB0l8uL/U1DXIt/sYnA4A7oZAA5egBqTPPtVKQ7cTALgfAg1cbrPKbzIKpLa+wejiAAC6EIEGLuO8+DCJDPSR8qo6+T6z2OjiAAC6EIEGLsPTwyQzBpza22k3ezsBgDsh0MAlZzt9tSdfGtTCNAAAt0CggUsZ3ztSAnw85Zi5WnYcKTO6OACALkKggUvx8/aUqf266+OvWGQPANwGgQYu2+3E9G0AcB8EGricaf27i7enSQ4WVsiBghNGFwcA0AUINHA5wX7eMq53pHVwMADA9TlVoHn55Zf1irCrVq0yuihwcLMGMn0bANyJ0wSao0ePyrPPPmt0MeAkZp4KND/klkp+WZXRxQEAdDKnCTS//vWvZf78+UYXA04iOthPRsSH6uOv01lkDwBcnVMEmk8//VS8vb1l9uzZRhcFTmTWwMa9nZi+DQCuz+EDTUVFhTz44IPy/PPPt+v11dXVYjabWzzgnpp2315/sFjKTtYaXRwAgDsHmocfflh+9atfSY8ePdr1+kWLFklISIj1ERcX1+llhGNKjgqUlO6BUtdgkVV7C4wuDgDAXQPN1q1bZcOGDTrQtNcDDzwgZWVl1kdubm6nlhHO0UrDInsA4Nq8xIF9/vnncvLkSZk+fbo+r6pqnK3y29/+VkJDQ2XJkiWSkpLS4nt8fX31A2gaR/OXtIOyam+hVNXW660RAACux2SxWJxmS+Ls7GxJSkqStLQ0mTp1aru+R42hUV1PqrUmODi408sIx6J23B7/1ErJN1fJ6/NGyfkDGltsAACOraOf3w7d5QTYysPDZN3b6avdTN8GAFflNIFGdTPNnTu31THQ3unbK9KPSX2D0zRIAgBcZQxNcy+88ILRRYCTGpMcLsF+XlJcUSNbDh2X0UnhRhcJAOCuLTTAufL29LCOnWGRPQBwTQQauNf07T354kTj4AEA7USggVuY3DdKfL08JLfkpNz5r63y9vpsSc8zM6YGAFyE04yhAWzh7+MlFw6OkY9+OCpf7MzXDyXIz0tGJYTJqMRwSU0Ml6G9QlirBgCckFOtQ3MuWIcGTerqG2Rjdolszj4um7JLZOuh41JRU9+ignw8PXSoaQw4YTIqIVxC/L2pRABw8M9vAg3cOuBk5JfLxqwS2XyoRDZmHZeiE9WtXtcvOkhGJYbp2VETUyIlIpCVqAGgsxFobKwQuC/VWHmouFK33jS14mQWVbR4jbenSWYOjJZrUuN1uPH0MBlWXgBwZWZaaGyrEKA51WLTFG7WHyyWPXlm63Oxod3k56N6yc9HxeljAID9EGhsrBDgp6iZUe9uypVl245I2clafc1kEpncJ0rmpsbp9W58vJg8CAC2ItDYWCFAe6idu5fvztfhZt3BYuv1yEAfmXNeL7k6NU56RwVSmQBwjgg0NlYI0FHZRRXy3uZceX/LYSks/3FQ8ejEcLkmNU4uGtJDuvl4UrEA0AEEGhsrBLBl1lTa3kJ5d1OOrMwokKZ9MIN8veRnI3rK1aPiZEhsiJhUHxUA4CcRaGysEMAe8suq5IOth3WXVE5JpfV6ZKCvjO8dIRNSImR870iJC/enwgGgDQQaGysEsKeGBot8n1ks/96UK1/tyZeq2oYWz8eH+1vDjQo6rHEDAI0INKch0MBRVNfVy9ZDpbLuYJF8d6BIth8ua7WXVP+YIJmQEqlDzuikCAn0ZXcSAO7JzDo0tlUI0FXKq2r1KsXfHSjWIUetWtycl4dJhsWFyoTeETI+JVJGxIeKrxeDiwG4BzOBxrYKAYxcxE9NAV93oEi+O1ikdwZvTu0WrgKO3mMqMVxGJoRJsB/7TAFwTQQaGysEcBS5JZW6a+q7g8Wy/mCRFJ2oafG8mizVPyZYRif+uFt4TIifYeUFAHsi0NhYIYCj7jOl9pXanN24iabaTFPtO3W6uPBukpoQrgPO6KQwvbgf08QBOCMCjY0VAjiLAnOVbDq1z5QKOHuOmq1r3zQJ8/eWkQnhMiYpXC4cEiO9wpgmDsA5EGhsrBDAmQcZb8sp1a04Kuhsyz3eYpo4e04BcCYEGhsrBHAVNXUNsvtomd4tXK1cvD7zxz2nIgJ8ZM7IXnr14pTu7DkFwPEQaGysEMBVHSo+tefU5sNS0GzPKTVr6prUeLloSIz4+7DuDQDHQKCxsUIAd9hzatXeQr16cdreAuvifmrPqcuG95S5qfEyODaYwcQADEWgsbFCAHdyzFwl/9nSes+pgT2CZe7oOPnZsFgJ8WetGwBdj0BjY4UAbrvnVFaxDjb/3ZWvx980LeZ30ZAecsWIWBmdFC5+3qxUDKBrEGhsrBDA3ZVW1shH247oLqnm2zH4eHnIyPiwxs00UyJlaGyIeHl6GFpWAK7LzNYHtlUIgB8X89txuKxxrE1GgeSbq1pUjRpzMyY5XO8UrjbU7BvNIn4A7IdAY2OFADjzSsV6n6lTm2maq+pavCYy0FfG946QiSmRMj4lgkX8ANiEQGNjhQA4OzUzSq1x0xRu1GrFzRfxUxIi/E+13kTo8Tfdg9hnCkD7EWhsrBAAHVddVy9bD5XqcKM21Nx+uMw6HbxJYoR/4x5TiWqvqTBJigxgajiAMyLQ2FghAOyzDcPGrBLdgqNWKM7IN4vltH2mIgN9ZJTeSDNM7xQ+qGcwg4wBWBFoTkOgAYxXdrJWtuYcb9xnKuu4/HC41Do1vIm/j6eMiA/VIUcFHHUc4MvKxYC7MjPLybYKAdA1XVS7jpTJxqzGkLP50HEdeprz9DDpVhs1DueqkbGS0j2IvxrAjZgJNLZVCABjFvbbX3BCDy5WD7Wh5pHSky1eMypB7TkVJxcP7cGeU4AbMBNobKsQAI5BBZpNWSXy2Y68FntOBVr3nIqTIbEhDCwGXJSZQGNbhQBwnj2nBqg9p1Lj5PLh7DkFuBoCjY0VAsA595y6cHCMzB0dL2OSwmm1AVwAgcbGCgHg3HtOqfVurkmNlzkjY1nMD3BiBBobKwSA8+459ckPR6Sipt46S+r8/t3lwiExeqZUdDArFQPOhEBjY4UAcF4V1XXy+c483SW15dDxFs+ldA+UCb0bdwofmxwhId28DSsngLMj0NhYIQBcw/5j5fLhtiOydn+R7Dpa1mKlYg+T6BlSKtxM6B2pVyv28/Y0srgATkOgsbFCALjmeJvvM4v1VgzfHSySzMKKFs/7eHnIyPgwvZGmCjlDY0PYhgEwGIHGxgoB4Pryyk7KulPhRn3NN1e1eD7I10vGJIfrsTfq64CYYPFQzToAugyBxsYKAeB+g4oPFlZYdwpff7BYzFV1LV6jxtuo/aXGJofLmKQIGdgzWA86BtB5CDQ2VggA96ZWJN59tEx3T6luKrXXVNPMqeYtOGrczZjkCD3AeDA7hQN2R6CxsUIAoLm6+gbZddQsGzKLZUOW2i28RMqrW7bgBPh4yshE1XrT2IozJDZUj8sBcO4INDZWCACcrQUnPc+sW29UwNmYVdJqp3A/bw8ZmRAmswfFyJXn9dL7TwFw80Dz3nvvyZIlS6S+vl7/4RITE+XZZ5/VX9uDQAOgs7dj2Hus3NqCox4lFTXW51WYufK8WLlhXIKkdA/iLwNw10Dj4+Mjn376qcyePVsaGhrkxhtvlI0bN8r27dvF19f3rN9PoAHQldSv1AMFJ+Tb/UXyrw2H9IDjJuN7R8gN4xJlxoDuTAsH3C3Q/PznP5f333/fer5582ZJTU2VdevWybhx4876/QQaAEZRv17XHSyWt9dny9d7jknDqd+2PUP85PqxCXJNapxEBp79P2aAOzJ3MNA4fMdu8zCj+Pk17sdSXV3d5uvV9ebPqQoBACOYTCaZoFYjTomUI6UndYvNvzfmytGyKnl2+V55ccV+uWhIjNwwPlFGxIWySzhgA4dvoTnda6+9Jo899phkZ2eLt3frvVjUcwsWLGh1nUHBABxBdV29fLEzT95ad0h+yC21Xh8cG6y7oy4b1pNtGABxwS6n5lTLy5AhQ+Tpp5+WK664ot0tNHFxcQQaAA5nx+FSeXv9Iflk+1GpqWvQ10L9veXqUXHyizEJEh/hb3QRAcO4dKBRA4JVOHn88cfb/T2MoQHg6NSsqPc258o/vj8kh4+ftF7vFx0kk/pEyqS+UTI6MVy6+bCBJtyH2VUDzf3336//cK+88kqHvo9AA8CZ1rhJyyiQt78/JGv2F7bYIVwt1JeaGCaT+kTpkMP+UnB1ZlcMNE899ZTs3LlTli5dKh4eHrJlyxZ9feTIkWf9XgINAGd0vKJGz5BSwebbfYV6IHFzkYE+MjEl0hpwugc3TpgAXIXLBZrFixfLyy+/rBfX8/JqnJT12Wef6YX1VBfU2RBoADg79Ws6s6hC1uwrlDX7i2R9ZrFUnra/VP+YU91TfaJkdFI4A4vh9Fwq0JSXl0toaKheUO90b7zxBoEGgFtSA4i35hzXrTcq4Ow8Utaqe2pkvNo8U+0tFSHD40IJOHA6LhVo7IEWGgDuMKj4uwNF1oCTd1r3lAo4KtSMTQrXO4SfFx/GAGM4PAKNjRUCAM5M/R/1YOEJWZ9ZYt1fqrC85UKk3p4mGdorVO8MPiYpQm+kGcAGmnAwBBobKwQAXHH8zQYVcLKK9dd8c8sWHC8PkwyODbF2UY1KCJMgv9YLlwJdiUBjY4UAgKsHnJySSh1svj8VcNS2DKcHnNmDYvQO4WqAsdrCAehqBBobKwQA3M3h46cCzqkuKhV4mi/u98txCXLFiFi6pdClCDQ2VggAuLv0PLPekuGjbUfkZG3j9PAgXy+ZM7KXDje9owKNLiLcgJlZTrZVCACgUdnJWvnPlsN6S4asogprtaj1btRGmtP7dxdPD7qj0DkINDZWCACgpYYGi6w5UCRL12fLNxkF1jVvYkO7yS/GJsg1qXESHuBDtcGuCDQ2VggA4MxySyrlHxsOybubcqW0sta6zs2lQ3vqQcTD4kKpPtgFgcbGCgEAnF1Vbb18uv2oHmujVipuMqxXiFydGidT+kZJrzB/qhLnjEBjY4UAADo2DfyH3FJZuv6QfLYjT2rqf9yqJjkywLq/1NjeERLI4n3oAAKNjRUCADg3RSeq5f3Nh+Wb9GOyLbdU6hssLda2OS8hTCb3iZSJfaJkSGwIA4rxkwg0NlYIAMB25qpaWX+w2Lq/1KHiH9e2UUK6ecvEFNV6EymT+kbpAcZAcwSa0xBoAMB4OcWVsuZAoazZVyTfHSyS8qq6Fs8nRwXI5D5ROuSogcVRQb6GlRWOgUBjY4UAADpXXX2DbD9cZm29+eG07ilFBZoBPYJlYI9gGdAjSAb1DJbEiADx8vTgr8dNmFlYz7YKAQB0/QJ+Td1T6zOL9SJ+TWvdNOfr5SH9Y4Iag05PFXSC9TkbabomAo2NFQIAMFZlTZ3szS+XPXlmvQ3DnqNmycgvl8qaxm0YThcf7n+qJSdYhseHypikcPHz9uzycsO+CDQ2VggAwDFXKz5UUqnDjQ45p8JOXllVq9d28/aUiX0i5fz+3WVa/+4SHexnSJlhGwKNjRUCAHAexytqrAFHhZ11B4sl39wy5Kgp4mrfqfMHdJfBPUPEg/2nnAKBxsYKAQA490J/KtysTC/Q+05tP1zaYjxOZKCvTO8fJdP7R+tWHBb7c1wEGhsrBADgOgrLq2XV3gJZmVEg3+4rlIpm43B8PD1kTHK47ppSASc+gq0aHAmBxsYKAQC4puq6etmUdVy+yTgm36QXSE5Jy8X+UroHyrjkCBmZECbnxYdJXHg3MZlMhpXX3ZmZtm1bhQAA3KNr6mBhhaw8FW42Hzreai0c1T01MiFUhxsVcgbHhjB7qgsRaGysEACA+ymrrJW1B4pky6HjsiXnuOw5Wia19S0DjrenSQb1DNHhpqkVJyaEGVSdhUBjY4UAAFBVWy87j5TJVhVwDh2XrTnHpehETauKUXtQjYgP1QFnRHyY9IsOkm4+rIFjDwQaGysEAIC2uqhyS07KlpwS2XqoVIecjHyznNZLJWrITVJEgHUVY/21R5AOPozH6RgCjY0VAgBAe5yorpMduaXWbqqdh8ukuKJ1K44S5OclA2Iaw01T2OkXEyT+Pl5U9hkQaGysEAAAbJkmrhb6U6036Xnl+vhg4YlW43GaWnPUhptNLTmjk8J115U3G3BqBJrTEGgAAEaqqWvQoaZ5yFF7U6nwc7ogXy+Z1DdSpvbrLlP7Rkl3N962wcy0bdsqBACArlB0oloyTgUcNQBZzbIqOa3LanBssExT4aZfdxkeFyqebrRtg5lAY1uFAABg1AacO46USVpGgazaVyg7Ttu2IdTfW6b0jZKp/aJkcp8oiQj0dem/KDOBxrYKAQDAUVpwVu8tlLS9jds2mKvqWoy/GdYrVG+6qVpwBvUMdrlNNwk0NlYIAACOpq6+QbbllurWm7S9hbqbqrmIAB+9Ds55CaEyIi5MhsWFOP0MKgKNjRUCAICjyy+rktX7CiQto1CPvTlR/WPrjaLG2qjZU2rRP7WisQo7iRH+TrUWDoHGxgoBAMDZZlHtPFIm23KOy7acUr2qcV5ZVavXhatWnLhQa8gZGhcqgb6O24pDoLGxQgAAcHZ5ZScbw82h47qrSgUeFXyaU0Nu+kYHyXkJYdIj2E+8vTz0Gjg+Xh7i42nSxz+eNx6r/azUedP1pq9h/t527+Ii0NhYIQAAuJrqunrZc9RsbcFRX4+UnrTb+z9++WD55dgEMfLz23HbmgAAgF34ennqcTTqcbMk6WvHzFW6m+qH3DIpO1kjNXUWqa1v0C05+mt9Q7NzS9vX6hqv+TrA6sYEGgAA3FB0sJ9cMLiHfrgC4yMVAACAjQg0AADA6RFoAACA0yPQAAAAp0egAQAATo9AAwAAnB6BBgAAOD0CDQAAcHoEGgAA4PScItAsW7ZMUlNTZdKkSTJlyhTZvXu30UUCAAAOxOG3Pti4caPMmzdPtmzZIn369JG3335bZs+eLenp6RIUFGR08QAAgANw+Baap556Si6++GIdZpRf/OIXUldXJ2+++abRRQMAAA7C4QPNN998I6NGjbKee3h4yMiRI2XFihWGlgsAADgOh+5yKi4uFrPZLNHR0S2ux8TEyKZNm9r8nurqav1oor4fAAC4NocONJWVlfqrr69vi+vqvOm50y1atEgWLFjQ6jrBBgAA59H0uW2xWJw/0Pj7++uvzVtcms6bnjvdAw88IL/73e+s50eOHJGBAwdKXFxcJ5cWAADYW3l5uYSEhDh3oImIiNB/iGPHjrW4np+fL8nJyW1+j2q9ad6iExgYKLm5uXpGlMlksmtyVCFJvXdwcLDd3tfVUW/UG/ebY+PfKPXmKPebaplRYaZnz57tei+HDjTK9OnT9ZTt5n/ArVu3yoMPPtiu71eDiHv16tVp5VN/AQQa6q2rcL9Rb9xrjo1/o/att/a0zDjNLKf7779fPv/8czlw4IA+/+c//ymenp56bRoAAACnaKEZPXq0XnNm7ty50q1bN93isnz5chbVAwAAzhNolCuuuEI/HIkap/Poo4+2moEF6o37zXHw75Q6415zn3+jJkt750MBAAA4KIcfQwMAAHA2BBoAAOD0CDQAAMDpOcWgYEe0bNkyWbhwofj5+emZV6+88ooMGjTI6GI5rMcee0w++ugjCQ0NtV4LDw+XDz/80NByOaKamhp55JFH5LnnntPLFSQmJrZ4/tVXX5W//e1v+t5T9amOY2Njxd39VL3deOONkpGRoeusiVpBXP27dWfvvfeeLFmyROrr6/UCZ6rOnn32WWvdqSGWjz/+uP636+XlJX379pW//OUvHVobxN3qbOrUqW2up6buTXf18ccfy+LFi/W/UbXSv9q66N5775Vrr73W+hq73GtqUDA6ZsOGDZagoCDLvn379Plbb71liY2NtZjNZqryDB599FFLWloa9XMWWVlZlrFjx1puuOEGNVhfnzf3wQcfWHr06GEpLCzU5wsWLLAMHz7cUl9f79Z1e7Z6mzdvXqtrsFi8vb0tX375pa4KdQ/98pe/tPTr189SVVWlr/3f//2fZejQoZbKykp9ftNNN1kuvfRSt666s9XZlClTDC6h45k9e7b+nGzyySefWEwmk2X79u3Wa/a41wg05+CKK66wzJ0713quburo6GjLSy+9dC5v5xYINO2zc+dOy/79+3X4a+uDecSIEZb777/fel5aWmrx8vLSvyDc2dnqjUDTtquuuqrF+aZNm3T9rVu3zlJXV2eJioqyLF682Pr87t279fM7duywuKufqjOFQNPa5s2bLbW1tdZz9Z9/VWfLli3T5/a61xhDcw6++eYbGTVqlPVcdTmNHDlSVqxYcS5vB1gNHjxYUlJS2qyRkpIS2bZtW4t7TzXHqqZZd7/3fqrecGbvv/9+i/OmLjnVLbBjxw4pLCxscb8NGDBAAgIC3Pp++6k6Q9vU56PqRlJqa2t1t7Dq8p0xY4a+Zq97jUDTQcXFxbrfNDo6usX1mJgYycrK6ujbuZW///3vun95woQJeuuKgwcPGl0kp9J0f3HvnZtFixbp+2/ixIly5513ttr0FiLr16/XGwGqf6OZmZmt7je1wa8653dd23XW5K677pIpU6bI5MmT9fY9aoNFiP53FxUVpUOKWvFfbR6t2OteI9B0kBrMpJy+qqE6b3oOrcXHx8uIESP0jbxmzRpJSkrSqf3IkSNUF/dep1OtWOrDZeXKlZKWlqb/Nz127Fg5ceIE998pqk7U4NaXX35ZvL29+V13DnWmDB8+XC6++GJZvXq1fPHFF7Jz506ZOXOmHkTs7v7yl79IUVGR9T+2eXl5dv1cJdB0kL+/f5vNi+q86Tm0dvPNN8vdd9+tmx1VF93DDz+sm2rdfZZJR3Dvnbv58+fL9ddfr+899cHzpz/9SXJycuSdd96x49+Qc7vjjjvkmmuusW4zw/3W8TpTXnjhBZk1a5Y+Vi0QzzzzjGzYsEGHaYj+DFCzmRoaGvS/Q3veawSaDoqIiNDjFk5vrs7Pz5fk5GTu13ZSO6araY50O7Vf0/3FvWe74OBg3fTN/ddIdYuoDw71QXO2+02d87uu7TprS+/evfVXd77XampqWpyr/1ioVtM9e/bY9V4j0JwDtabAli1brOdqttjWrVutA5zQmupTPt3Ro0d1VxTaJywsTHfbNb/31Hiuffv2ce918P5T//NT4+G4/0Seeuopyc3N1d0mirq/1GPo0KE69DW/39LT06WiosLt77cz1VlBQYE8+eSTLe61pm51d77XzjvvvFbXVHeTGnuk2O1eO4dZa25PrUMTHBysp4kqS5cuZR2as0hMTLR8/PHH1vPXXnvN4ufnZ0lPT3f7+6ktZ5p+rNah6dmzp6WoqEifP/7446xD04568/Hx0dNrmzz00EN6mmhBQYFb339//etfLYMGDbKsX79e1496qCUW3njjDevaIMOGDbOuDXLLLbe4/To0P1Vn6r4LDw+33n9qOrJaMqB///6WkydPWtyVyWSyfPbZZ9Zz9Znp4eFhWbNmjfWaPe41Vgo+B6NHj5Y333xT5s6dK926ddPNZ2rEdlBQ0Lm8nVtQ/2tRfcuqz1Q1P6rBXmqAcP/+/Y0umkNRdaP630tLS/W5usfi4uKsU0WvvPJK/b9ANchQjUFSrTaffvqpvgfd2dnqTU0TbRrDpQYZqv8NqsHB6qu7UjNv1KwTNZZh3LhxLZ5744039FdVZ2rgtBrAqequT58+8vbbb4u7Oludqdmuv//97/UKuOp3nGphUHWmPh+ar1Ltbl588UX9GaBmGqq6UzOYPvnkEz3jsIk97jWTSjWdUH4AAIAu497/rQMAAC6BQAMAAJwegQYAADg9Ag0AAHB6BBoAAOD0CDQAAMDpEWgAAIDTI9AAAACnR6ABYBcbN26UqVOn6lVA1QrQf/zjH/XKvY899ph1Bd+ukJ2drX/m6S6//HJ5/vnnu6wcALoWKwUDsO8vFZNJLwN/44036nCRlJQkWVlZenf1rrBq1SqZNm2a3jS2ObW0utq2RC1LD8D1sJcTALdA6wzg2uhyAtAp9uzZozeJVNRX1R21bNkyfa42obvttttkxIgRMmXKFN0dlJOTo59bu3atjB07Vrf0qM0lf/azn0lKSooMHz5cP//KK6/ImDFjdCtMamqq3vSuqTVm5cqV8tvf/lYfq5+nHuvXr5f77rtPtxCp8+aWLl2q31e9nypL02aWyq233qo3G7zhhhvkD3/4gy5nv3799EaDAByQ/TYIBwCdLCxvvPGGroqsrCx9rr42d+211+pHfX29Pl+4cKFl4MCBlrq6uhbfd/PNN+vXlJeXW6ZOnaqfS01NtezcuVMfnzhxwjJ06FDLW2+9ZX3vtLQ0/b2ne/TRRy1Tpkyxni9fvtwSGBhoycjI0Oc7duyw+Pn5Wb777jvra+bNm2cJCwuzpKen6/MXX3zREh8fz18z4IBooQHQpTIzM+Xf//63/O53vxMPj8ZfQbfffrtu0VHjX5pTrSPqNYGBgZKWlqavqVaUwYMH6+OAgAC56KKL5L///W+Hy6FadlTLkGp1UYYMGSKzZ8+WhQsXtnidarlRg5wV1cKjWpKOHz9+jn96AJ2FMTQAutTu3bt1F9Fdd90l3t7e1usJCQlSWFjY4rW9evVq9f2HDx+W3/zmN1JUVKS/v2ngcUft2rVLpk+f3uKa6tpq3u2k9OzZ03ocFBSkv5rNZgkLC+vwzwTQeQg0AAzxj3/846xBxNPTs8X5oUOHZObMmXpK+D333KOvqSnap7fs2FPzMqhxPcrpM6gAGI8uJwCd9wvmVJeS0tDQIBUVFTJo0CB9vnfv3havfeSRRyQjI+Mn32/z5s1y8uRJueaaa6zXampqzvgz6+rq9OvborqtDhw40OLawYMHddcTAOdDoAHQaSIiInTAUGNOVBhRa9MkJyfrtWCeeeYZqaqq0q9bt26dfPDBB7rL56eosSyqleSbb77R5yqsnD5+JioqSn9VP/PDDz/UQaktDz74oHz88ceyf/9+a1fYl19+KfPnz7fLnx1AFzN6VDIA17BhwwY9i0j9WunXr59lwYIF+vp9991nGTRokGXMmDGWtWvX6mtq1tLtt9+uX6dmL1166aWW/fv36+e2bdumX6veR33985//3OLnLF682JKYmGiZNGmS5aqrrrLMmTPHEhISYrnuuuusr1HHw4cPt4wbN07PYrr33nstCQkJ+nUXX3yx9XVqdtSwYcMso0eP1q9/9913rc/dddddlujoaP1Q36/ep3m51KwoAI6DlYIBAIDTo8sJAAA4PQINAABwegQaAADg9Ag0AADA6RFoAACA0yPQAAAAp0egAQAATo9AAwAAnB6BBgAAOD0CDQAAcHoEGgAAIM7u/wOmDQVT2DEw7AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9b18ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_shots = 30_000\n",
    "sampler = Sampler(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'x': 7, 'cx': 4, 'rx': 2, 'h': 2, 'barrier': 2, 'rz': 1})\n",
      "On circuit 1/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'cx': 8, 'x': 7, 'rx': 4, 'h': 4, 'barrier': 3, 'rz': 2})\n",
      "On circuit 2/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'cx': 12, 'x': 7, 'rx': 6, 'h': 6, 'barrier': 4, 'rz': 3})\n",
      "On circuit 3/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 16, 'measure': 16, 'rx': 8, 'h': 8, 'x': 7, 'barrier': 5, 'rz': 4})\n",
      "On circuit 4/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 22, 'measure': 16, 'rx': 10, 'h': 10, 'x': 7, 'barrier': 6, 'rz': 5})\n",
      "On circuit 5/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 28, 'measure': 16, 'rx': 12, 'h': 12, 'x': 7, 'barrier': 7, 'rz': 6})\n",
      "On circuit 6/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 30, 'measure': 16, 'rx': 14, 'h': 14, 'barrier': 8, 'x': 7, 'rz': 7})\n",
      "On circuit 7/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 32, 'rx': 16, 'h': 16, 'measure': 16, 'barrier': 9, 'rz': 8, 'x': 7})\n",
      "On circuit 8/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'rx': 18, 'h': 18, 'measure': 16, 'barrier': 10, 'rz': 9, 'x': 7})\n",
      "On circuit 9/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 34, 'rx': 16, 'h': 16, 'measure': 16, 'barrier': 11, 'x': 7, 'rz': 7, 'unitary': 3})\n",
      "On circuit 10/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 28, 'measure': 16, 'rx': 14, 'barrier': 12, 'h': 10, 'unitary': 8, 'x': 7, 'rz': 3})\n",
      "On circuit 11/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 40, 'rx': 20, 'h': 18, 'measure': 16, 'barrier': 13, 'rz': 8, 'x': 7, 'unitary': 4})\n",
      "On circuit 12/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 48, 'rx': 24, 'h': 24, 'measure': 16, 'barrier': 14, 'rz': 11, 'x': 7, 'unitary': 2})\n",
      "On circuit 13/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 56, 'rx': 28, 'h': 28, 'measure': 16, 'barrier': 15, 'rz': 14, 'x': 7})\n",
      "On circuit 14/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 44, 'rx': 22, 'h': 18, 'barrier': 16, 'measure': 16, 'unitary': 8, 'x': 7, 'rz': 7})\n",
      "On circuit 15/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 56, 'rx': 28, 'h': 26, 'barrier': 17, 'measure': 16, 'rz': 12, 'x': 7, 'unitary': 4})\n",
      "On circuit 16/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 68, 'rx': 34, 'h': 34, 'barrier': 18, 'rz': 17, 'measure': 16, 'x': 7})\n",
      "On circuit 17/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 64, 'rx': 34, 'h': 28, 'barrier': 19, 'measure': 16, 'rz': 14, 'x': 7, 'unitary': 4})\n",
      "On circuit 18/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 72, 'h': 36, 'rx': 34, 'barrier': 20, 'rz': 17, 'measure': 16, 'x': 7, 'unitary': 2})\n",
      "On circuit 19/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 68, 'rx': 34, 'h': 30, 'barrier': 21, 'measure': 16, 'rz': 14, 'x': 7, 'unitary': 6})\n",
      "On circuit 20/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 80, 'rx': 40, 'h': 40, 'barrier': 22, 'rz': 19, 'measure': 16, 'x': 7, 'unitary': 2})\n",
      "On circuit 21/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 86, 'h': 44, 'rx': 42, 'barrier': 23, 'rz': 21, 'measure': 16, 'x': 7, 'unitary': 1})\n",
      "On circuit 22/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 92, 'rx': 46, 'h': 46, 'barrier': 24, 'rz': 23, 'measure': 16, 'x': 7})\n",
      "On circuit 23/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 94, 'h': 48, 'rx': 46, 'barrier': 25, 'rz': 23, 'measure': 16, 'x': 7, 'unitary': 1})\n",
      "On circuit 24/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 92, 'rx': 44, 'h': 44, 'barrier': 26, 'rz': 21, 'measure': 16, 'x': 7, 'unitary': 4})\n",
      "On circuit 25/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 98, 'rx': 48, 'h': 48, 'barrier': 27, 'rz': 23, 'measure': 16, 'x': 7, 'unitary': 3})\n",
      "On circuit 26/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 108, 'rx': 54, 'h': 54, 'barrier': 28, 'rz': 27, 'measure': 16, 'x': 7})\n",
      "On circuit 27/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 112, 'rx': 56, 'h': 56, 'barrier': 29, 'rz': 28, 'measure': 16, 'x': 7})\n",
      "On circuit 28/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 116, 'rx': 58, 'h': 58, 'barrier': 30, 'rz': 29, 'measure': 16, 'x': 7})\n",
      "On circuit 29/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 120, 'rx': 60, 'h': 60, 'barrier': 31, 'rz': 30, 'measure': 16, 'x': 7})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sim.run(to_run)\n",
    "    # counts = job.result().data()['counts']\n",
    "    # bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    # counts1 = bit_array.get_counts()\n",
    "    job = sampler.run((circuit,), shots=num_shots)\n",
    "    data = job.result()[0].data\n",
    "    bit_array = data['meas']\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    all_counts = collections.Counter()\n",
    "    tuple_of_counts = tuple(counts_list[:i])\n",
    "    assert len(tuple_of_counts) == i\n",
    "    for counts in tuple_of_counts:\n",
    "        for bitstring, count in counts.items():\n",
    "            all_counts[bitstring] += count\n",
    "\n",
    "    bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPtJJREFUeJzt3Qd8VFXe//HfTCrplQRSqUF6L4IEUGDtBQu6rqhref7rs7q66tr7gqvu6ro+/tX1r2JZ24Oo2FAQpArSOwQSIJAEkkAyKaTP/3VOSEwgQCbt3jvzeb9e9zV37p1Mjocx+eZUm9PpdAoAAIAF2Y0uAAAAQEsRZAAAgGURZAAAgGURZAAAgGURZAAAgGURZAAAgGURZAAAgGV5i5urqamRrKwsCQ4OFpvNZnRxAABAM6hl7oqKiqRr165it9s9N8ioEJOQkGB0MQAAQAtkZmZKfHy85wYZ1RJTVxEhISFGFwcAADSDw+HQDRF1v8c9NsjUdSepEEOQAQDAWs40LITBvgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMgAAwLIIMi3kdDpl0c7DUl3jbNt/EQAA0GwEmRb644fr5aa3f5H/rN7f0rcAAACtRJBpoRHJEfrxue92yOGistb+OwAAgBYgyLTQ9aOTZGB8qBSVVckzX21v6dsAAIBWIMi0kJfdJn+9bIDYbSJfbsySpWm5rfl3AAAALUCQaYUB8aFyw5hkff7o51ukrLK6NW8HAABcRJBppT9P6S0xIX6yN79UXl28p7VvBwAAXECQaaVgfx95/OJ++vy1xXtkT25xa98SAAA0E0GmDZzfP1YmpkRLRXWN7mJSa8wAAID2R5BpAzabTZ66tL/4+9hlxZ58+XzDwbZ4WwAAcAYEmTaSEBEgd57bS5+r6dgFpRVt9dYAAOAUCDJt6JZx3aVX5yDJL6mQv323sy3fGgAANIEg04Z8ve3y18sH6PMPV++XtfuOtOXbAwCAExBk2tjIbhFy9fB4ff7w3C1SWV3T1t8CAAAcR5BpBw+cf5aEB/jIjpwieXt5Rnt8CwAAQJBpHxGBvvLQBWfp8xd/SJMDR0v5sAEA0A5okWknVw6L191Mxyqr5Ykvt7XXtwEAwKMRZNpxbZmZl/cXHy+bLNh+SOZvzWmvbwUAgMciyLSjnp2D5bbx3fX5E19ulZLyqvb8dgAAeByCTDv746RekhgRINmFZfLiD7va+9sBAOBRCDLtzN/HS566tHZTybdX7JWtWYXt/S0BAPAYBJkOMCGls1w4oItU1zj12jI1NWwqCQBAWyDIdJDHLu4rQX7esiGzQP6zen9HfVsAANwaQaaDxIT4y71Teuvzv323Q3KLyjvqWwMA4LYIMh3od2OSZUBcqBSVVckzX7O2DAAArUWQ6UBedrW2zACx20S+2JAly9LyOvLbAwDgdggyHWxAfKjcMCZZnz/6xRYpq6zu6CIAAOA2CDIG+POU3tI52E8y8krk30vSjSgCAABuwfAgU1FRIQ888IB4e3vL3r17T7r/+uuvy7Bhw2Ts2LFy4YUXysGDB8Xqgv195JGL+urz15eky9GSCqOLBACAJRkaZFRwSU1NlezsbKmuPrmL5bPPPpMnn3xS5s+fL8uXL5dRo0bJRRddJDU1NWJ1Fw3oIv26hkhxeZW89tMeo4sDAIAlGRpkiouL5b333pObbrqpyfvPPPOMzJgxQ6KiovTzu+66S7Zs2SJff/21WJ3dbpN7p6bo83dW7JWcwjKjiwQAgOUYGmT69+8vPXv2bPLekSNHZP369TJ8+PD6a6GhodK7d29ZsGCBuIMJvaNlRHK4lFfVyL9+TDO6OAAAWI7hY2ROJSMjQz/GxMQ0uh4bG1t/rynl5eXicDgaHWZls9nkvql99PnHv2TKvvwSo4sEAIClmDbIlJaW6kc/P79G19XzuntNmTVrlm65qTsSEhLEzEZ2i5AJKdFSVeOUlxbQKgMAgFsEmYCAgPoWlobU87p7TXnwwQelsLCw/sjMzBSzu3dK7ViZzzcclB055m1BAgDAbEwbZLp3764fDx061Oh6Tk5O/b2mqBabkJCQRofZ9Y8L1btjO50if/9+l9HFAQDAMkwbZMLDw2XIkCGydu3a+mtqvMuuXbvkvPPOE3dz9+TeeuuCH7YdkvX7jxpdHAAALMG0QUZ55JFHZPbs2ZKfn6+fv/zyy3qm0wUXXCDupmfnIJk2NF6fv/D9TqOLAwCAJXgbvarvlClTpKCgQD+fPn26Hpz76aef6udXXHGFHD58WCZPniz+/v66lWbevHlit5s6f7XYXef10ptJLt+dL8t358nYnrXr5wAAgKbZnE41MsN9qe4oNXtJDfy1wniZJ77cqhfIG5QQJp//4Ww9RRsAAE/jaObvb/ds2rCwOyb2lE4+XrIxs0CPlwEAAKdGkDGZ6GA/uXlcsj5XM5iqa9y6wQwAgFYhyJjQbef0kBB/b9l5qEjmbcwyujgAAJgWQcaEQgN85PbUHvr8Hz/skspq6+/2DQBAeyDImNRNY5MlKshP9h8p1fswAQCAkxFkTCrA11v+OKl2Z/CXF6ZJWWW10UUCAMB0CDImNn1kgsSFdZLDReXy7sq9RhcHAADTIciYmJ+3l/zpvF76/NXFe6SorNLoIgEAYCoEGZO7fEic9IgOlILSSnlzaYbRxQEAwFQIMibn7WWXe6ek6PM3l6ZLfnG50UUCAMA0CDIW8Jv+sTIgLlRKKqrl/y7eY3RxAAAwDYKMBaj9lu6dWtsq8+7P+yS78JjRRQIAwBQIMhYxvleUjOwWIRVVNfLywt1GFwcAAFMgyFioVea+460yn6zJlIy8EqOLBACA4QgyFjIiOUImpkTrjSRf/GGX0cUBAMBwBBmLqRsr8+XGLNmW5TC6OAAAGIogYzH9uobKRQO76PN//LDT6OIAAGAogowF3TO5t3jZbbJg+2FZu++o0cUBAMAwBBkL6h4dJFcOjdfnr/yYZnRxAAAwDEHGov5rQg/9+NOuXMkqYF0ZAIBnIshYVLeoQBndPUJqnLXTsQEA8EQEGQu7dmSifvx0zQE9JRsAAE9DkLGwqf1iJbSTjxwsOCZL03KNLg4AAB2OIGNh/j5ecvmQOH3+0Wq6lwAAnocgY3HTRyboxwXbD0luUbnRxQEAoEMRZCyuT2yIDE4Ik6oap8xZd8Do4gAA0KEIMm7g2uOtMh//kilOJ4N+AQCegyDjBi4a2FUCfb30jtirMo4YXRwAADoMQcYNBPp5yyWDu+rzj1bvN7o4AAB0GIKMm5g+onZNmW+25EhBaYXRxQEAoEMQZNzEwPhQOatLiFRU1cjn6w8aXRwAADoEQcZN2Gw2mT6idtDvRwz6BQB4CIKMG7lscJz4edtlR06RbMgsMLo4AAC0O4KMGwkN8JELBnSpn4oNAIC7I8i4mbrupS83ZklxeZXRxQEAoF0RZNzMyG4R0j0qUEorqmXexiyjiwMAQLsiyLjhoN9rGgz6BQDAnRFk3NC0YfHi42WTjZkFsj3bYXRxAABoNwQZNxQV5CeT+8boc1b6BQC4M4KMm7rm+Eq/c9cflLLKaqOLAwBAuyDIuKlzekZJXFgncZRVybdbso0uDgAA7YIg46bs9gaDflcz6BcA4J4IMm7squHxYreJrMo4Ium5xUYXBwCANkeQcWNdQjvJhJTO+pyVfgEA7ogg4+bqupfmrDugd8YGAMCdEGTc3KQ+nSU62E/yiitk4fZDRhcHAIA2RZBxcz5edrlqWLw+/5CVfgEAboYg40HdS0vTcuXA0VKjiwMAQJshyHiApMhAObtHpDidIp+sOWB0cQAAaDMEGQ8xfWTtSr+frsmU6hqn0cUBAKBNEGQ8xJS+MRIW4CPZhWWyZFeu0cUBAMAzgkx5ebncfffdMmjQIElNTZVRo0bJ3LlzjS6W5fj7eMkVQ44P+l293+jiAADQJrzF5J555hn5/PPPZcOGDRIaGirr16+X0aNHy+rVq3W4QfNNH5kgby3PkIU7DsthR5l0DvGn+gAAlmb6FhkVYEaMGKFDjDJkyBB9/uOPPxpdNMvpHRMsQxPD9BiZ/13HoF8AgPWZPshMmzZNli5dKvv313aHzJ8/X3JzcyUmJsbooll60K/asqCGQb8AAIszfdfSjTfeKKWlpTJw4EDp0qWL7Nq1S6688kq5+uqrTzmmRh11HA5HB5bW/C4a2EWemrdN9uWXys/p+XJ2zyijiwQAgPu2yLz55pvy7LPPytq1a2X79u2ybt06PUbGbm+66LNmzdJdT3VHQkLtYnCoFeDrLZcM7qrPP2KlXwCAxZk6yDidTrn//vvl9ttvlx49euhraoDvN998IzNnzmzyax588EEpLCysPzIzMzu41OZ37Yja7qXvtuTI0ZIKo4sDAIB7Bhk1Fubo0aOSnJzc6Hq3bt1kzpw5TX6Nn5+fhISENDrQWP+4EOnbJUQqqmvks/UHqR4AgGWZOshERUXpYJKdnd3ounoeEBBgWLmszmazybUja7vcPvklU7d8AQBgRaYOMmoczIwZM/Q4GdUyo6gxMj/88MMpB/uieS4ZFCe+3nbZeahItmYxIBoAYE2mn7X04osvyhNPPCHnnnuuboUpKirSg3/vvPNOo4tmaaEBPjL5rBj5enO2fLbuoPSPq12nBwAAK7E53bxfQU2/VrOX1MBfxss0tnD7Ifn97DUSFeQrKx88V3y8TN1ABwDwII5m/v7mN5cHG987WiIDfSWvuEKWprGRJADAeggyHky1wFw6OE6fz1nL7CUAgPUQZDzcFUNrg8wP2w9JYWml0cUBAMAlBBkP169riKTEBEtFVY0e+AsAgJUQZDycWlOmrlXmM3bEBgBYDEEGctmQOLHbRNbsOyp780qoEQCAZRBkIDEh/jKuV7SuCbYsAABYCUEG2rQG3Us1NW69tBAAwI0QZKBN6RsrQX7ecuDoMd3FBACAFRBkoHXy9ZILBsTq8zlrD1ArAABLIMig3hVD4/WjmoZdVllNzQAATI8gg3ojkyMkLqyTFJdXyffbDlEzAADTI8jg1w+DnTVlAADWQpBBk91LS3blymFHGbUDADA1ggwa6RYVKEMTw0TNwP5iQxa1AwAwNYIMTtkqM4ctCwAAJkeQwUkuGthFfL3ssiOnSLZlOaghAIBpEWRwkrAAXzmvb2d9zkaSAAAzI8igSVcMqe1e+nxDllRV11BLAABTIsigSakp0RIR6Ct5xeWyNC2PWgIAmBJBBk3y8bLLJYO66nMG/QIAzIogg1O6clht95Ja5bfwWCU1BQCwfpDZtGmTbN26tX1KA1Pp1zVEescESUVVjXyzOdvo4gAA0PogM3jwYHnxxRdd/TJYkM2mtiyobZVh9hIAwC2CzLhx4+TNN99sn9LAdC4fEid2m8gve4/KvvwSo4sDAEDrgkz//v0lK6vppesvueQSV98OJhcT4i9je0bp87nrDxpdHAAAGvEWFwUHB8vZZ58t5557rsTHx4uXl1f9vS1btrj6drCAaUPj9RTsz9YdlLvO7aW7nAAAsGSQeeONN/Q4mfT0dH00VFBQ0JZlg0lM6Rcjgb5esv9IqazZd1RGJEcYXSQAAFoWZNQYmXnz5jV579prr3X17WABAb7ecsGALvLp2gN60C9BBgBg2TEypwoxyocfftja8sCk6mYvfbUpW8oqq40uDgAALV8Qb9++fXLnnXfKxIkT9aHO1TW4r1HdIiQurJMUlVXJD9sOGV0cAABaFmQWL14sffr0kaVLl0pUVJQ+li1bJmeddZb89NNPrr4dLMJut+mp2AprygAALDtG5qGHHpIvv/xSJk+e3Oj6ggUL5IEHHpCVK1e2ZflgIlcMjZNXFu2WJWl5crioTDoH+xtdJACAh3O5RcbpdJ4UYpTzzjtP34P76h4dJEMSw6S6xilfbmh6LSEAAEwdZEpKSiQvL++k67m5uVJaWtpW5YLJB/3OWcfieAAAC3YtzZgxQ4YNGyY33XST9OjRQ1/bvXu3zJ49Ww/6hXu7eGAXeXreNtme7ZBtWQ7p2zXE6CIBADyYy0Hmz3/+s17dd+bMmbJ//359LTExUR5++GG59dZb26OMMJGwAF8596zO8u2WHJm7/oD07drX6CIBADyYzeniwBaHw6GXqFdhpri4WF8LCgoSs1LlDQ0NlcLCQgkJofWgLajp17e+u0aig/1k5QOTxNurRbP4AQBo9e9vl38DhYWFybRp0+oDjJlDDNpHau9oiQj0ldyiclm2++TxUgAAdBSXg8yIESPk+++/b5/SwBJ8ve1yyaCu+pxBvwAASwWZlJQUKSoqavLebbfd1hZlgkXWlFEWbDsk5VVsWQAAsMhg34EDB8qECRPksssuk/j4ePHy8qq/p1b4hWcYEBcqUUF+kldcLmv3HZWze0QZXSQAgAdyOcg8+uijEhsbK2+99dZJ9w4dYg8eT6EGfI/vFSWfrT8oS3blEWQAANboWho9erRkZGQ0eYwaNap9SglTGt87Wj8u2ZVrdFEAAB7K5SBzyy23yDfffNPkvUWLFrVFmWAR43rVdidty3boGUwAAJg+yKgVfdeuXds+pYGlqDEy/Y6v7LtsN60yAAALBJnx48frcTJNYa8lT+5eYj0ZAIBF1pHZvHlzk/cuuuiitigTLGR8r9ogszQtT2pq2P0cAGDyWUtZWVl6+vXgwYNPmn69Y8eOti4fTG5YUrgE+HrpadjbcxzSr2uo0UUCAHgQl1tk1Kq+l1xyid4o0m63i9qqqe6AZ67yO6Z7pD6newkAYPoWGdV99O9//7vJe3fffXdblAkWHCezcMdhPQ37/0zoYXRxAAAexOUWmVOFGOXFF1+U9pCenq43qpw4caL069dPr2WzZs2advlecN05x6dhr9l3REorqqhCAIB5g4zy8ccfS2pqqowdO1Y/f/rpp+W9996T9pCbmyvnnnuu3HXXXXqdmo0bN0pAQIDs3r27Xb4fXNctKlDiwztJZbVTfk7PpwoBAOYNMq+//rrce++9MmjQIDl27Ji+dsUVV8jcuXPln//8Z5sX8G9/+5uMGTNGT/tWvL295Y033qh/DpNsV8A0bACAFYKManlRrSIvv/yyhIbWzlBR3T2qlWbOnDltXsDPPvvspNDSs2dP6dq1a5t/L7R+GvaSNBbGAwCYOMiomUoRERH1f4nX8fHxkYqKijYtXElJid7Dqbq6Wn7729/qrqypU6fKt99+e8qvKS8vF4fD0ehA+zu7Z6R42W2SnlsiB46WUuUAAHMGGRUUtmzZctL1BQsW6MDRlgoKCvSjWkn4/vvvl+XLl+vHiy++WH744Ycmv2bWrFm6pajuSEhIaNMyoWkh/j4yJCFMnzMNGwBg2iDzxBNP6FlDai2ZtLQ0vffS2Wefradlz5w5s00LV7fYngouakyOogb+Tpo06ZTjcR588EEpLCysPzIzM9u0TDi1unEyS+leAgCYNcicf/75smrVKt29FBMTo7cr6N27t6xfv14mT57cpoWLjo4WPz8/iYuLa3Q9KSlJdzk1Rb0+JCSk0YGOnYa9bHeeVFXXUO0AAPMtiFc3uPedd96R9qZaZNS4mOzs7EbXDx06pFcWhrkMjA+TsAAfKSitlI0HCmRYUu1YKgAATLWOTEf6y1/+Il988YXs379fP9+2bZveJuGOO+4wumg4gRrsO7ZnbasM42QAAKZtkelIU6ZM0VO9L730UgkKCpKqqiqZPXs2O22b1PheUfL1pmw9Dfvuyb2NLg4AwM2ZPsgo119/vT5gnQG/GzMLpLC0UkIDfIwuEgDAjZm+awnW0iW0k/TqHCQ1TpHle/KMLg4AwM25HGTYGgBnck7dKr+7WOUXAGCyIKMG244cOVKefPJJ2bdvX/uUCpY2vnfdgN9ccTqdRhcHAODGXA4yv//972XFihUycOBAvSO12jLg/fffl7KysvYpISxnVLdI8fW2S1ZhmezJLTa6OAAAN2ZvyW7Uagfqyy+/XD7//HO9ieSaNWukS5cucvvtt8vPP//cPiWFZXTy9ZKRybVryDANGwBgqiDz6aef6sfKykr55JNPZMaMGfLKK69IZGSkXoH37bfflnHjxsnixYvbo7ywWvcS2xUAAMw0/VqNjVm6dKl88MEHerfrK6+8Un788cdGg4DVZo9q/ZfVq1e3dXlhoWnYM7/ZIT+n50tZZbX4+9TumwUAgKFBRg32Va0vL7zwglx99dUSGBh40mu2b98uWVlZbVVGWFBKTLB0DvaTw0Xlsnbf0foVfwEAMLRr6brrrpOffvpJ73rdVIhRVEvNq6++2hblg0XZbDamYQMAzBdkunfvfsbXpKamyiWXXNLSMsHNxsn8xHoyAACzdC2pWUo+Pj5Nrg+iricnJ8v5558vYWFhbVVGWHhhPJtNZEdOkRx2lEnnEH+jiwQA8PQgk5SUJE899ZSebp2YmKi7ENTO1Pn5+TJ8+HDJzs7W68vMnz9fhgwZ0j6lhiVEBPpK/66hsvlgoSxNy5Npw+KNLhIAwNO7lsaMGSMffvihDi/Lli3TM5jUCr9qR+rf/OY3snPnTr1A3n333dc+JYalMA0bAGCqIKOmVKsp1yeaNm2anoatqKnXasAvMP74vkvL0vKkRu0kCQCAkUFmz549ep2YEx05ckS3xgANDUkMl0BfL8kvqZBt2Q4qBwBg7BiZiy++WIYNG6ZX9O3WrZu+lp6eLu+++67etkCt+Dtr1izx8/Nr25LCktSeS2N6RMmC7Yf07KX+caFGFwkA4MlB5qWXXtJbEfzrX//SA3sVNfD3zjvvlHvvvVeOHTumF8xTYQZQUnvXBhm1G/YdE3tSKQCANmNzNjWP+jQcDoeeqRQcHKzPlZCQEDErVcbQ0FApLCw0dTnd2d68EpnwwmLx8bLJ+semSJCfy/kZAOBhHM38/e3yGBm1Powa2KuoNyYc4EySowIlMSJAKqud8vOefCoMANBmXA4yI0aMkO+//77tSgCPwDRsAIApgkxKSooUFRU1ee+2225rizLBTVf5VdTCeAAAtBWXBysMHDhQJkyYIJdddpnEx8eLl5dX/T21QB7QlLN7RIq33SYZeSWSeaRUEiICqCgAQMcP9u3UqZPExsY2ee/QoUNSWloqZsJgX/O4+rWVsnrvEXnmsv5y/egko4sDADCx5v7+drlFZvTo0bJo0aIm702cONHVt4OHjZNRQWZpWi5BBgBgzBiZr7766pT3ThVwgIbjZFbszpfK6hoqBQDQ8UEmMDBQMjMz5fHHH5d77rlHX5s7d66kpaW1vjRwa2pV3/AAHykqr5INmSdvcwEAQLsHGTWgV81cUuHlu+++09fUtgRqe4KFCxe6XAB4Di+7TcbVzV7alWt0cQAAnhhkHn30UR1YNm3aJDExMfra1VdfrbuV/vrXv7ZHGeFGzukVpR9/Yho2AMCIIKMmOY0ZM0afq60K6kRHR0t1dXVblAlubPzxFplNBwrkaEmF0cUBAHhakFHToJpaEE+Nm8nLY7EznF5sqL+kxASLmvS/bDefFwBABweZ6667TkaNGiX/+Mc/JDc3V95991156KGH9LTsW2+9tZXFgSd1L6lp2AAAtIbL68jcd999eoGamTNnyv79++XGG2+UxMREeeKJJwgyaJbxvaPlzWUZsmRXnu6qbNhFCQBAuwaZuj2V1FFcXKyfBwUFteRt4KFGdosQP2+75DjKJO1wsfSOCTa6SAAAT+laakgFmIYhRrXWAGfi7+Olw4yyhGnYAICObJFRa8b85z//kQ0bNuh9EBpu1aTWlXn++edbUx54iNTe0Xon7CVpeXLLOd2NLg4AwFNaZGbMmCGPPPKIHh+jplurIFN3AK6Mk1FWpedLWSXT9gEAHdQio1pi1HYE/v7+J91Ts5eA5ujVOUhiQ/z1OJlf9h6p34cJAIB2bZHp06dPkyFGueGGG1x9O3goNVPp12nYrCcDAOigIDN9+nT57//+b1mxYoVkZGToLqa64+abb25hMeCJzjnevcSAXwBAh3UtqSCjvPrqq43W/2A9ELhqXM8oUR+hHTlFcthRJp1Dmm7pAwCgzVpk1Kq+qiVGHenp6Y2OkSNHuvp28GARgb7Sv2uoPqd7CQDQIS0yL7zwgiQlJTV577XXXmtRIeC5xveOks0HC/V2BdOGxRtdHACAu7fIjB079pT3Bg0a1NrywMPUzVZSG0jW1DCFHwDQDkGmW7du0r17d1m6dGmT9z/55BP9moCAABe/PTzd0MRwCfD1krziCtme4zC6OAAAd+xaSk5OlkWLFunzJ598stEg38cee0yuvvpqfYwZM6b9Sgq35OttlzHdI2XhjsN6E8l+x8fMAADQZi0yDYOLCjVqjMxHH32kz0/1OsDVVX7VOBkAANp1sK/aokB55513WAAPbaJuYbw1e49KaUWVBPi2aFN2AIAHavHu17S+oK10iwqUuLBOUlFdI6syjlCxAIBma9afvtnZ2fLee+812hgyJyfnpGu5uXQNoGWhWE3D/nB1pl7ld2JKZ6oRANAsNmcztq222+3N/oWkdsQ2E4fDIaGhoVJYWCghISFGFwen8M3mbPnDB+ukZ+cgWXBPKvUEAB7O0czf381KKKmpqVJTU3PGg5V90VJje0SJ3Say+3CxZBUcoyIBAM3SrCDz3HPPNevNXnrpJWlPr7zyim71Wbx4cbt+H3S80AAfGZQQps+XsRs2AKAtg8yIESOavQ9Te8nKypLnn3++3d4f5lnl9yemYQMA2nvWUkf74x//KA899JDRxUA7Gn98Gvby3XlSzXYFAIBmsMSCHfPmzRMfHx+ZOnXqGV9bXl6uj4aDhWANgxPCJNjPWwpKK2XLwcL6riYAACzbIlNSUiIPP/ywvPjii816/axZs/Qo57ojISGh3cuItuHtZZeze0bqc1b5BQC4RZB59NFH5b/+67+kS5cuzXr9gw8+qKdq1R2ZmZntXka0/TiZJQz4BQBYvWtp3bp1smrVKnnhhRea/TV+fn76gDWNPx5k1u07KkVllRLs72N0kQAAJmbqIPP111/LsWPHZNKkSfp5WVmZfvzTn/4kYWFh8uabb0rPnj0NLiXaUmJkgCRHBsje/FL5Of2ITO4bQwUDAFq3sq9Z7N27V7p16yaLFi2SCRMmNOtrWNnXeh79fIu89/M+uWFMkjx1aX+jiwMAsPrKvoARu2EvZZwMAOAMLBNkVHfS9OnTTzqH+xnTI1K87TbJyCuRzCOlRhcHAGBiph4j05HbH8A81ADfoYnhsnrvEVmSliu/HZVkdJEAACZlmRYZeGj30q48o4sCADAxggxM6ZzetdOwl+/Jk6rqGqOLAwAwKYIMTGlAXKiEBfhIUVmVbDxQYHRxAAAmRZCBKXnZbTK2Z2330hK6lwAAp0CQgel3w2bfJQDAqRBkYFrjjm9XsCGzQAqPVRpdHACACRFkYFpxYZ2kR3Sg1DhFVuxm9hIA4GQEGZgau2EDAE6HIANTSz0+DXvJrlyx0LZgAIAOQpCBqY3qHiE+XjY5WHBM74gNAEBDBBmYWoCvtwxPiqhvlQEAoCGCDEzvnN5MwwYANI0gA9Mbf3wa9so9+VJRxXYFAIBfEWRgen27hEhkoK+UVFTL+v1HjS4OAMBECDIwPbvdJuOOr/K7JI1xMgCAXxFkYKn1ZJamsTAeAOBXBBlYat+lzQcL5UhJhdHFAQCYBEEGltA5xF/6xAaLWhNvOdsVAACOI8jAMs6pGyfDejIAgOMIMrDkOBm2KwAAKAQZWMbIbhHi522XHEeZ7D5cbHRxAAAmQJCBZfj7eOkwoyxh9hIAgCADq67yyzgZAIBCiwwsue/Sqox8KausNro4AACDEWRgKSkxwdI52E/KKmtk7T62KwAAT0eQgaXYbLb62UtsVwAAIMjAcsYf715auovtCgDA0xFkYDlje9YGmW3ZDsktKje6OAAAAxFkYDlRQX7Sr2uIPl+2m92wAcCTEWRgSeN7H1/ll+4lAPBoBBlYe9+ltFw5cLTU6OIAAAxCkIElDUsKl/AAH8krrpDU5xfLPR9vkJ05RUYXCwDQwQgysCQ/by959+ZRMq5nlFTXOOWz9Qdl6ktL5Pfv/CK/7D1idPEAAB3E5nTzbYQdDoeEhoZKYWGhhITUDhCFe9l0oEBe+2mPfLslR+o+zcOTwuX/TOghE1M6i91uM7qIAIB2+v1NkIHbSM8tln8vTZc5aw9KRXWNvtY7JkhuH99DLhncVXy8aIAEAKsgyLhYEXAfhx1l8v+WZ8gHP++X4vIqfS0urJPcck43uWZEggT4ehtdRADAGRBkXKwIuJ/CY5Xywap98tayvZJXXLtwnhogPOPsZJkxJlnCA32NLiIA4BQIMi5WBNyX2iV7zroD8saSdNmXXztVu5OPl26d+f24bpIQEWB0EQEAJyDIuFgRcH9qdtO3W7Ll/y7eI1uzHPXX48M7ydDEcBmaGCZDk8LlrC4hjKcBAIMRZFysCHgONVFv2e48PdNpxZ78+plOdfx97DIwPqxRuFHbIgAAOg5BxsWKgGcqKquUjZmFsm7/UX2s31+gx9acKDEioD7UqIDTJzZYvJkFBQDthiDjYkUASk2NU9LzimXdvoL6cJN2uPikVpsAXy8ZGB9qeDeUv7ddpg2Ll6TIQMPKAADtgSDjYkUAp6JaaDZmFsjafbXBZsP+Aik6Pq3bDPy87XLHxJ5ye2p3veIxALgDgoyLFQG40mqzO1e12hyVjLwScRq8qvHP6bVbMnSPCpRnLusvZ/es3VATAKyMIONiRQBWHbj85cYseebr7ZJbVLtWzqWDu8rDF54lnYP9jS4eALT772/WbAcszGazyaWD42Thn1NlxpgksdlEvtiQJef+/Sd5d+VePeUcANwZey0BbkR1NT08d4tsPlion6sByX+9bIAMiA81umgA4BJaZAAPpNa/+fyOsfLUpf0k2M9bNh0olEv/Z5k8/sUWcZSdPK0cAKyOriXAzXjZbXLDmGTd3XTJoK6iepdmr9ynu5u+2HBQj6sBAHdBkAHcVOcQf3n52iHy/u9H6RlNajDwXR9tkN/9v9WSnltsdPEAoE0QZAA3N65XlHz7p3Pknsm9xdfbrrdn+M1LS+UfP+zSG2oCgJWZfrDvJ598Im+++aZUV1frgT/Jycny/PPP68fmYPo18Ku9eSXy2JdbZcmuXP08KTJAzu4RKcH+PnpMTZC/d+25fvSWkPrz2kcjVzEG4FkczZx+bfog4+vrK/PmzZOpU6dKTU2N3HjjjbJ69WrZuHGj+PmdeSM/ggzQmPpf/pvNOfLUV1vlkKN27ZnmUhtq1oUeHXQ6+cjo7pEybWi8xIaybg2AtuM2Qeaqq66STz/9tP75mjVrZMSIEbJixQoZM2bMGb+eIAOcesPMrzZl67Ez6ryorEofjvrzSikur71WWnH6Lii7TSS1d7RcPTxBzj0rRndhAUBrNPf3t7eYXMMQo/j71/7VV17e9F+S6nrDe6oiAJxMtaxcOzKxWVVTVV1TH2rqgk5xWZXkOMr0ysKrM47Iop25+ogI9JXLBsfJNSMSJCU2mKoH0K5MH2ROtHLlSunatauMHTu2yfuzZs2SJ598ssPLBbgzby+7hAX46uNE149O0ntO/e/aTPnftQd0d9VbyzP0MSg+VK4aniAXD+oqoZ18DCk7APdm+q6lhlRLy4ABA+Rvf/ubXH755c1ukUlISGCvJaADqJabpWl58smaTFmw/ZBUVjvrd+g+v3+sXD0iQUZ3ixS76osCAE8YI9OQGuirQsnTTz/d7K9hjAxgjPzicpm7/qAONbsO/bpuTUJEJ7lqWIJMGxYvcWGd+OcB4BlB5oEHHtD/Ua+++qpLX0eQAYylfsSorRI+XpMp8zZkSVF5lb6uNrgc1zNKftM/VoYlhUuvzsF6VWIAcLsg8+yzz8rmzZvlvffeE7vdLmvXrtXXhw0bdsavJcgA5nGsolq+25otn/xyQFam5ze6F+TnLYMTwmRoUrgMTQyTIYnhjKsBPJjDXYLMa6+9Jq+88opeFM/bu3Zs8ldffaUXxFNdTWdCkAHMaV9+ie56UjOeNmQWNDnFu1fnIBmaGK5bbIYmhUn3qCDG1wAewuEOQaaoqEjCwsL0QngnevvttwkygBsNEt55qEjW7S+QdfuOyrr9R2VffulJr1Mzn4YkhtWHm0EJYbolB4D7cYsg0xZokQGsKa+4/HioqQ03Gw8USHlV4z9q1JCa6GA/CfT1lkA/bwnw9dKP+qg79/WSgAbXAny9dfgJ8PPSXxcX3okwBJgQQcbFigBgbpXVNbI92yFrG4SbgwXHWv2+amr4hQO66AX8RnaLEJsahQzAcAQZFysCgPUcdpTJ4aJyKSmv3UZBrT5cWlElxeXVUlpeJSUV1fpeSUWVlJZX68eGr1UrFBceq6x/v+7RgTJ9RILeOyoy6Mx7uQFoPwQZFysCgOdRPetqoPFHqzNl3qas+gHHPl42mdI3VqaPTJCxPaIYYAwYgCDjYkUA8GyqhebLDVny0S/79bo3DRfwu2Z4gt5qISaEHb6BjkKQcbEiAKDO1qxC+fiXTD09XHU/KWqxvokpneXakQl6p2+1/xSA9kOQcbEiAKCpBfy+2ZytW2l+2Xu0/npsiL9cPTxet9IkRARQcUA7IMi4WBEAcDq7DxfpsTRz1h2Qo6W1A4TVBKcx3SPlvLNiZFKfzpIcFUglAm2EIONiRQBAc5RXVcsP2w7pULNsd16je92jAmVin8461IxIjhBfb7qfgJYiyLhYEQDgqswjpTJ/a478uOOw3mqhqubX9UXVontqU8xJZ3WWCSnR0jmYgcKAKwgyLlYEALSGo6xSlqXl6VCzeOdhySuuaHR/YHyoHiysWmsGxIUypRs4A4KMixUBAG2lpsYpmw8W6lCzaOfhRtO5laggP91Ko0KNWk040Ndbd0OpmVEAahFkXKwIAGgvh4vKZPHOXFm047AsTcvTa9Y0xdtu04FGbZugHmvPvcTXq+7810d9XT0/fs/Hyy4+3jbxU4+Nrtlrr3nbxNfLSy/29+s1u4T4+0jvmCC2ZoDpEGRcrAgA6AgVVTWyZu8R3Vrz487Dkp5bYnjFD00Mk7/8po+M6h5pdFGAegQZFysCAIwKNhXVNbWPVTV6VlTtY+3R8Jp6XXll3WP1r19X7dSPamNNddS9trLaKZX15w2v10hllVOfZxUcq99VXHV33T+1j/Ttys9KWOf3t3eHlgoA0EhdF5L4Gbfx5j8XpslHv2Tq7q+fduXKpYO6yj2TUyQxksX+YH42p9o1zY3RIgMAZ5aRVyJ//36nfLUpWz9XY2muG5ko/z2pl0QHsxM4Oh5dSy5WBABAZMvBQvnbdzv0oGQlwNdLbhnXTW4d312C/X2oInQYgoyLFQEA+NWK3Xk60Gw8PnU8PMBH7pjYU64fnST+Pl5UFdodQcbFigAANKZGHny3JUee/35n/eyquLBO8qfzeskVQ+NZ9wbtiiDjYkUAAJpWVV0j/7v2gLy0IE1yHGX6Wq/OQXLf1BSZ3DeGNWjQLggyLlYEAOD0yiqrZfaKvfLq4j1SeKx2B/AhiWEyobfa+TtAkiIDJTkyQMICfKlKtBpBxsWKAAA0jwoxr/+0R95aniFllbVr0DQU2slHB5q6YKMfjwedyEBfWnDQLAQZFysCAOCaQ44ymbv+oKTnFsve/FLZl18ihxzlp/0atSt4UmSAJEcG6nVqkiICdPDp5OslAb7e0snH6/i5V/252pLBZmMfKk/jaObvb9aRAQC0mdKKKtl/pFT25tUGm31Hah/V86zCY9KSlcvUXpq1oca7UcAJOH4MT46Qq4bFS2QQ6924E4KMixUBAGhfaquFzCPHaoPN8RYcFXqKyqrkWEW1HKus1o8qDKlztcVCc6nNM6f2j5XfjkqUUd0iaMFxAwQZFysCAGAuak8oFWjKdLipPerCjnpUgSe/uEK+2HCwfr0bpUd0oFw3KkmmDY1j4LGFEWRcrAgAgLVXJP5g1X4dalTgUdTYmgsHdtGtNEMTw2mlsRiCjIsVAQCwvqKySvliQ5YONduzHfXX+8QGy3WjEuWyIXESwlYLlkCQcbEiAADutSrxhswCHWi+2pRVP01cDRS+ZFBX+e3oRBkYH2Z0MXEaBBkXKwIA4L7r3sxdd0CHmrTDxfXX+8eFyHUjk+T8/rESHsgifmZDkHGxIgAA7t9Ks2bfUfng533yzeYcqaj+dTG/+PBO0r9rqA43/eJC9Xl0MNO5jUSQcbEiAACe40hJhcxZe0A+WZPZqJWmoZgQPx1oaoNNiPSPC5Uuof4MGu4gBBkXKwIA4LldT9uyHLI1q1DPftqS5ZA9ucVNLt4XEegr/Y6HmroWnMSIAMJNOyDIuFgRAADUKSmv0rOe6oKNelQtN9U1ziZbbqb2i5Xf9I+VkckR4u1lpyLbAEHGxYoAAOBMu3/vzCmSLbrlprYFZ0d2UaOxNmpTzCn9YuQ3/bvI2T0ixYdQ02IEGRcrAgCAlmy7sHx3nny7OUe+33ZId1PVCfH3lvP6xsgF/bvIuF5R4u/jRQW7gCDjYkUAANDaLRV+Ts+Xb7fkyPdbcySvuKL+XqCvl0w6S4WaWElNidY7feP0CDIuVgQAAG1FjaVZs/eIDjXfbcmRHEdZ/T1/H7tM6N1Zzh8QK5P6dJZgVhpuEkHGxYoAAKA91NQ4ZcOBAh1ovt2SrXcAb7hr99k9I3WgmZjSWRIiAvhHOI4g42JFAADQEYvybc1y6FDzzZZsSc8taXRf7dw9IaU21IzoFi5+3p47rsbRzN/fNqeqVTdGkAEAmJH69aumdC/cflgW7zysVx1uOL07wNdLzu4RJRP7ROtwExfWSTyJgyDjWkUAAGAkNeNJzYBSoWbRzlzJLSpvdD8lJlgmpNSGmuHJ4W4/tdtBkHGtIgAAMNO4mm3ZDh1qFu/MlXX7j0rDtfiC/bxlbM/a1ppR3SIlNtTf7aZ3E2RcrAgAAMyqoLRClqTVttb8tDNX8kt+ndrdMNyojS6jgv30Y3SQn3QOqX3Uz48fkYF+4mW3idkRZFysCAAArNJas/lgoSw63lqzLdshFVW/ri58JirDRAT+GmwiAnx0a46ft73+0e+Ex/rr+vDSU8jVo59P7b3wAJ82XxuHIONiRQAAYNVBw0XlVXpMzWFHueQWl+vz+qPB8/yS8iY3w2ytpy/tJ78bk2zI72+WFgQAwMJsNpuE+Pvoo0d00GlfW1VdI0dKKxoFnYLSSr3VQllljX4sr6rR+0qpx/Lj1xrea3y/9tHI8TkEGQAAPIS3l106B/vrw12499wtAADg1ggyAADAsggyAADAsiwRZObOnSsjRoyQc845R1JTU2Xr1q1GFwkAAJiA6Qf7rl69WmbMmCFr166VXr16ybvvvitTp06V7du3S3BwsNHFAwAABjJ9i8yzzz4rF154oQ4xyvXXXy9VVVXyzjvvGF00AABgMNMHmYULF8rw4cPrn9vtdhk2bJgsWLCgydeXl5frRXQaHgAAwD2ZOsjk5+frIBITE9PoemxsrGRkZDT5NbNmzdIrAdYdCQkJHVRaAADQ0UwdZEpLS/Wjn59fo+vqed29Ez344IN6OeO6IzMzs0PKCgAAOp6pB/sGBATUdxc1pJ7X3TuRCjknBh8AAOCeTN0iExkZqbuHDh061Oh6Tk6OdO/e3bByAQAAczB1kFEmTZqkp1433OVz3bp1ct555xlaLgAAYDzTB5kHHnhAvv76a9m9e7d+/sEHH4iXl5deWwYAAHg2U4+RUUaOHKnXjJk+fbp06tRJT7+eP38+i+EBAACxOVVfjRtTM5fCwsL07KWQkBCjiwMAAJpBLb+illApKCjQ42Ut2yLTWkVFRfqR9WQAALDm7/HTBRm3b5GpqamRrKws3RVls9naPCnS0kMdGoXPIHVoBnwOqb/2ouKJCjFdu3bVw0o8tkVG/cfHx8e32/ur7iq6rKhDI/EZpA7NgM8h9dceTtcSY5lZSwAAAKdCkAEAAJZFkGkhtQ3C448/znYIrUAdtg7113rUIXVoND6Dref2g30BAID7okUGAABYFkEGAABYFkEGAABYltuvI9Ne5s6dKzNnzhR/f3+9Vs2rr74q/fr1M7pYlvDEE0/I559/rreOqBMRESGfffaZoeUys4qKCnnsscfkhRde0BuoJicnN7r/+uuvyxtvvKE/j6pe1XlcXJxh5bVaHd54442yY8cOXX91+vbtq/+/Rq1PPvlE3nzzTamurtaL4Kn6e/755+vrUQ23fPrpp/X/297e3tK7d2/5n//5n2atA+IJzlR/EyZMOOlrJk2apD+zOAM12BeuWbVqlTM4ONi5a9cu/Xz27NnOuLg4p8PhoCqb4fHHH3cuWrSIumqmjIwM5+jRo5033HCDGpivnzc0Z84cZ5cuXZy5ubn6+ZNPPukcPHiws7q6mjpuZh3OmDHjpGtozMfHx/ndd9/pc/XZ+t3vfudMSUlxlpWV6Wt///vfnQMHDnSWlpbq5zfddJPz4osvphqbWX+pqanUVQvRtdQCzz77rFx44YXSq1cv/fz666+XqqoqvUs30NaKi4vlvffek5tuuqnJ+88884zMmDFDoqKi9PO77rpLtmzZIl9//TX/GM2sQ5zZpZdeKlOnTtXnqhX6zjvvlJ07d8q6det0K4P6ufiHP/xBOnXqpF9z7733yrx582Tz5s1U7xnqD61DkGmBhQsXyvDhw3+tRLtdhg0bJgsWLGjlPwdwsv79+0vPnj2brJojR47I+vXrG30eVVO+atbn89i8OkTzfPrpp42e13XDlZeXy6ZNmyQ3N7fR5/Css86SwMBAPofNqD+0DkHGRfn5+bp/MyYmptH12NhYycjIaOU/h+d46623dJ/w2LFjdWvCnj17jC6SJdV95vg8tt6sWbP0Z3LcuHFyxx13yKFDh9rgXd3XypUr9WZ+6v/h9PT0kz6HapNe9Zyfi2euvzqqNTU1NVXGjx8vDzzwgN4wEWdGkHFRaWlp/WqMDanndfdweomJiTJkyBD9l9rSpUulW7duukXr4MGDVB2fR0OoFiz1y+PHH3+URYsW6b+SR48erbukcDJVP2qg6iuvvCI+Pj78XGxl/SmDBw/WQxZ++ukn+eabb3SX3OTJk3W3HU6PIOOigICAJpsD1fO6ezi9m2++We6++249s0F1yz366KO6mZUZIq7j89g2HnroIfntb3+rP4/qF8s//vEP2b9/v3z44Ydt9B3cy+233y7XXHONXH755fo5n8PW1Z/y0ksvyZQpU/R5UFCQPPfcc7Jq1SodrnF6BBkXRUZG6jEIJzY75+TkSPfu3V19O4iIl5eXnoJI95Lr6j5zfB7bVkhIiERHR/OZbILq8lDBRU21PtPnUD3n5+KZ668pPXr00I/8XDwzgkwLqLn9a9eurX+u1k9QI8/PO++8lrydx1H9wCfKysrSXU5wTXh4uO6ma/h5VGO4du3axeexFZ9J1cKqxsPxmWxMzUzKzMzUXSKK+typY+DAgTr4Nfwcbt++XUpKSvgcNqP+Dh8+LH/9618b1XVdVzufwTMjyLQwUauprWpRLeWDDz7QrQpq0CrO7Msvv9RHHbVIlJrxoLqc4LpHHnlEZs+erX/xKi+//LKepXPBBRdQnc302muvyZo1axpNaVch8aqrrqIOG9TR+++/L3/84x/1H26qvuqmV6uff+rnouoePnbsmH793//+d7n44ov1ZxGnrz81vlJ1Z+7du1dXlRoXo1ps+vTpo/9wxumxsm8LjBw5Uq8ZM336dL1mgupXnz9/vgQHB7fk7TyO+stD9Qer/3HVaqtqoLQa+Kv+p8XJVB2pvvOCggL9XH3uEhIS6qdzXnHFFfovOjUwUI01Ur+A1Q9I9blE8+pQrfZbN25L/VJRrQtq0K96hOjZM2omV01NjYwZM6ZRlbz99tv6UdWfGhytZuGoelTrbL377rtUXzPqT816/fOf/yzXXnut/nmoWrJU/anfKw1Xm0bTbGpVvFPcAwAAMDX+ZAMAAJZFkAEAAJZFkAEAAJZFkAEAAJZFkAEAAJZFkAEAAJZFkAEAAJZFkAEAAJZFkAHQJlavXi0TJkwQm82mV2l+6qmn9Eq6TzzxRP2Kuh1BLfOuvueJLrvsMnnxxRc7rBwAOgYr+wJo2x8qNptedv3GG2/UoaJbt26SkZGhdzjvCIsXL5aJEyfqzVwbUkvoq+1F1DLwANwHey0B8Ai0xgDuia4lAO1i27ZtenNGRT2qbqe5c+fq52pzwVtvvVWGDBkiqamputtn//79+t6yZctk9OjRumVHbep46aWXSs+ePWXw4MH6vtphedSoUbrVZcSIEXoT0rrWlx9//FH+9Kc/6XP1/dSxcuVKuf/++3WLkHre0HvvvaffV72fKkvdJpLKLbfcojfzu+GGG+Qvf/mLLmdKSoreyA+AiahNIwGgragfK2+//bY+z8jI0M/VY0PXXnutPqqrq/XzmTNnOvv27eusqqpq9HU333yzfk1RUZFzwoQJ+t6IESOcmzdv1ufFxcXOgQMHOmfPnl3/3osWLdJfe6LHH3/cmZqaWv98/vz5zqCgIOeOHTv0802bNjn9/f2dy5cvr3/NjBkznOHh4c7t27fr5//85z+diYmJfFgAE6FFBkCHSk9Pl48++kjuuecesdtrfwTddtttugVHjW9pSLWGqNcEBQXJokWL9DXVatK/f399HhgYKBdccIF8++23LpdDteSoliDVyqIMGDBApk6dKjNnzmz0OtVSowYvK6pFR7UcHT16tIX/9QDaGmNkAHSorVu36q6gu+66S3x8fOqvJyUlSW5ubqPXxsfHn/T1Bw4ckDvvvFPy8vL019cNKHbVli1bZNKkSY2uqS6sht1LSteuXevPg4OD9aPD4ZDw8HCXvyeAtkeQAWCI999//4wBxMvLq9Hzffv2yeTJk/XU7nvvvVdfU1OtT2zJaUsNy6DG7SgnzogCYBy6lgC03w+Y411HSk1NjZSUlEi/fv308507dzZ67WOPPSY7duw47futWbNGjh07Jtdcc039tYqKilN+z6qqKv36pqjuqd27dze6tmfPHt3FBMA6CDIA2k1kZKQOFmpMiQoham2Z7t2767VcnnvuOSkrK9OvW7FihcyZM0d37ZyOGquiWkUWLlyon6uQcuL4mOjoaP2ovudnn32mA1JTHn74Yfniiy8kLS2tvsvru+++k4ceeqhN/tsBdBCjRxsDcA+rVq3Ss4LUj5WUlBTnk08+qa/ff//9zn79+jlHjRrlXLZsmb6mZiHddttt+nVqNtLFF1/sTEtL0/fWr1+vX6veRz3+61//avR9XnvtNWdycrLznHPOcV555ZXOadOmOUNDQ53XXXdd/WvU+eDBg51jxozRs5Luu+8+Z1JSkn7dhRdeWP86Ndtp0KBBzpEjR+rXf/zxx/X37rrrLmdMTIw+1Ner92lYLjXLCYDxWNkXAABYFl1LAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAADAsggyAABArOr/A8wLUwbBqmvFAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1649111d0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"SQD\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"iSQD\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
