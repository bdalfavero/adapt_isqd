{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "268707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.0894916426871495)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 108]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150727975\n",
      "(change of -0.13682546025281095)\n",
      "Current ansatz: [244, 74, 228, 210, 108]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875515157731\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 228]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [244, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981639978813), np.float64(-0.7853981625399111), np.float64(0.12248927961411445), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233392)\n",
      "Current ansatz: [244, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964044667907\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089491640981468)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981417276958), np.float64(-0.7853981819240415), np.float64(0.1635702866854681), np.float64(0.16356997269745452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071121\n",
      "(change of -0.13682546025114384)\n",
      "Current ansatz: [244, 31, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.000136128583548991\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044667907 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474280784)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 198]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 26, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053169\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.0894929267350784)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [244, 26, 228, 198, 135]...\n",
      "Starting point: [np.float64(0.7853981509462123), np.float64(0.7853981644537731), np.float64(0.1635701974083778), np.float64(-0.1635696366828293), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016146468395\n",
      "(change of -0.13682545982712568)\n",
      "Current ansatz: [244, 26, 228, 198, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002921629235857861\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053169 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819709\n",
      "(change of -0.20417052920205858)\n",
      "Current ansatz: [244, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580530028\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089492926737891)]\n",
      "Initial energy: -6.327276154819709\n",
      "Optimizing energy with indices [244, 79, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981700346647), np.float64(0.7853981900506334), np.float64(-0.1635701974085294), np.float64(0.16356963668219085), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614835283\n",
      "(change of -0.13682546001557316)\n",
      "Current ansatz: [244, 79, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00027271975089248316\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580530028 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617646\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327169\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071048479704)]\n",
      "Initial energy: -6.123105625617646\n",
      "Optimizing energy with indices [241, 74, 225, 198]...\n",
      "Starting point: [np.float64(-0.785398163997711), np.float64(-0.7853981625399766), np.float64(-0.1224892796141143), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819995\n",
      "(change of -0.20417052920234902)\n",
      "Current ansatz: [241, 74, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964035952707\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894916486344886)]\n",
      "Initial energy: -6.327276154819995\n",
      "Optimizing energy with indices [241, 74, 225, 198, 210]...\n",
      "Starting point: [np.float64(-0.7853981669996093), np.float64(-0.7853981627952086), np.float64(-0.16357028615401242), np.float64(-0.16356997069742418), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072774\n",
      "(change of -0.13682546025277897)\n",
      "Current ansatz: [241, 74, 225, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013505199547288632\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964035952707 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [225, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.9999999999993725)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [225, 79, 216]...\n",
      "Starting point: [np.float64(0.7853985607314237), np.float64(0.7853989420959421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625475226\n",
      "(change of -0.12310562547703796)\n",
      "Current ansatz: [225, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526155297296\n",
      "Operator(s) added to ansatz: [180]\n",
      "Gradients: [np.float64(2.48507102185107)]\n",
      "Initial energy: -6.123105625475226\n",
      "Optimizing energy with indices [225, 79, 216, 180]...\n",
      "Starting point: [np.float64(0.7854038546599069), np.float64(0.7854037259371959), np.float64(-0.12248927291404064), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154553884\n",
      "(change of -0.20417052907865774)\n",
      "Current ansatz: [225, 79, 216, 180]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240988012366919\n",
      "Operator(s) added to ansatz: [156]\n",
      "Gradients: [np.float64(-2.089491894997428)]\n",
      "Initial energy: -6.327276154553884\n",
      "Optimizing energy with indices [225, 79, 216, 180, 156]...\n",
      "Starting point: [np.float64(0.7853981604388565), np.float64(0.7853981095762331), np.float64(-0.16357577727297534), np.float64(-0.16357144252341946), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615061043\n",
      "(change of -0.1368254605071595)\n",
      "Current ansatz: [225, 79, 216, 180, 156]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00014295794031607994\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240988012366919 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474288453)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 198]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819708\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [241, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531757\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734934)]\n",
      "Initial energy: -6.327276154819708\n",
      "Optimizing energy with indices [241, 79, 228, 198, 210]...\n",
      "Starting point: [np.float64(-0.7853981618473267), np.float64(0.7853981651745618), np.float64(0.16357019740836948), np.float64(-0.16356963668286137), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134318\n",
      "(change of -0.1368254603146104)\n",
      "Current ansatz: [241, 79, 228, 198, 210]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 2.9260899740755612e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531757 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474280784)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 198]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 26, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926735078)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [244, 26, 228, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981509462123), np.float64(0.7853981644537731), np.float64(0.1635701974083778), np.float64(-0.1635696366828293), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614646871\n",
      "(change of -0.13682545982715677)\n",
      "Current ansatz: [244, 26, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002921624091770451\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053169 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113274\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 26, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257758), np.float64(0.7853981815917098), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617638\n",
      "(change of -0.12310562561762772)\n",
      "Current ansatz: [241, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441738\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710474288693)]\n",
      "Initial energy: -6.123105625617638\n",
      "Optimizing energy with indices [241, 26, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853981634447681), np.float64(0.7853981634524064), np.float64(0.12248927934332132), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819988\n",
      "(change of -0.2041705292023499)\n",
      "Current ansatz: [241, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138126\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643867201)]\n",
      "Initial energy: -6.327276154819988\n",
      "Optimizing energy with indices [241, 26, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853981604483139), np.float64(0.7853981623217249), np.float64(0.16357028648498867), np.float64(0.16356997194327816), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071929\n",
      "(change of -0.13682546025194053)\n",
      "Current ansatz: [241, 26, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013555110317705658\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138126 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.000000000000004)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 210]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617644\n",
      "(change of -0.12310562561763749)\n",
      "Current ansatz: [244, 31, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.4850710484797025)]\n",
      "Initial energy: -6.123105625617644\n",
      "Optimizing energy with indices [244, 31, 210, 201]...\n",
      "Starting point: [np.float64(0.7853981648267889), np.float64(-0.785398161917778), np.float64(0.12248927961411428), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233303)\n",
      "Current ansatz: [244, 31, 210, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042410177\n",
      "Operator(s) added to ansatz: [75]\n",
      "Gradients: [np.float64(2.08949164296387)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 210, 201, 75]...\n",
      "Starting point: [np.float64(0.7853981644220853), np.float64(-0.7853981646437237), np.float64(0.163570286547772), np.float64(0.1635699721793674), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150727815\n",
      "(change of -0.13682546025280473)\n",
      "Current ansatz: [244, 31, 210, 201, 75]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350442666523217\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042410177 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441852\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047428929)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 31, 228, 198]...\n",
      "Starting point: [np.float64(-0.7853981609748782), np.float64(-0.785398165605737), np.float64(0.12248927934333585), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819712\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [241, 31, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053182\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.089492926734877)]\n",
      "Initial energy: -6.327276154819712\n",
      "Optimizing energy with indices [241, 31, 228, 198, 45]...\n",
      "Starting point: [np.float64(-0.7853981510104416), np.float64(-0.785398174208182), np.float64(0.163570197408367), np.float64(-0.16356963668287464), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614754331\n",
      "(change of -0.13682545993461925)\n",
      "Current ansatz: [241, 31, 228, 198, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00026578794938720884\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053182 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.0894916426871495)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 108]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150727975\n",
      "(change of -0.13682546025281095)\n",
      "Current ansatz: [244, 74, 228, 210, 108]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875515157731\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [228, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897114057\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.000000000002479)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [228, 31, 198]...\n",
      "Starting point: [np.float64(-0.7853985607314264), np.float64(-0.7853989420959467), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056251565095\n",
      "(change of -0.12310562515832135)\n",
      "Current ansatz: [228, 31, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752464806151\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.48507018232123)]\n",
      "Initial energy: -6.1231056251565095\n",
      "Optimizing energy with indices [228, 31, 198, 216]...\n",
      "Starting point: [np.float64(-0.7854083594299881), np.float64(-0.7854119606470162), np.float64(-0.12248905661368972), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761540477624\n",
      "(change of -0.20417052889125298)\n",
      "Current ansatz: [228, 31, 198, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409327186585335\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.089515136466004)]\n",
      "Initial energy: -6.3272761540477624\n",
      "Optimizing energy with indices [228, 31, 198, 216, 225]...\n",
      "Starting point: [np.float64(-0.7854062333533177), np.float64(-0.7854112677521524), np.float64(-0.16356354211914162), np.float64(-0.16356761613239554), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145145045\n",
      "(change of -0.13682546046674204)\n",
      "Current ansatz: [228, 31, 198, 216, 225]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0003146268345607649\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409327186585335 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072774\n",
      "(change of -0.13682546025278697)\n",
      "Current ansatz: [244, 79, 228, 210, 57]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.000135056081442826\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474280784)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 198]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 26, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926735078)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [244, 26, 228, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981509462123), np.float64(0.7853981644537731), np.float64(0.1635701974083778), np.float64(-0.1635696366828293), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614646871\n",
      "(change of -0.13682545982715677)\n",
      "Current ansatz: [244, 26, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002921624091770451\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053169 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916426871504)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072783\n",
      "(change of -0.13682546025279674)\n",
      "Current ansatz: [244, 74, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350387552286998\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072685\n",
      "(change of -0.13682546025269726)\n",
      "Current ansatz: [244, 79, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013508605784767053\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562482\n",
      "(change of -0.1231056256329639)\n",
      "Current ansatz: [228, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526201775814\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484682814)]\n",
      "Initial energy: -6.123105625562482\n",
      "Optimizing energy with indices [228, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853947065772693), np.float64(0.7853985308794288), np.float64(0.1224892796166995), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765011\n",
      "(change of -0.20417052920252932)\n",
      "Current ansatz: [228, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069117\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643863324)]\n",
      "Initial energy: -6.327276154765011\n",
      "Optimizing energy with indices [228, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.78539470657713), np.float64(0.7853982468601272), np.float64(0.16357028648716854), np.float64(0.16356997194353076), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615018015\n",
      "(change of -0.13682546025300368)\n",
      "Current ansatz: [228, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016418668721381277\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069117 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474288453)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 147]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819707\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [241, 79, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409625805317495\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894929267349442)]\n",
      "Initial energy: -6.327276154819707\n",
      "Optimizing energy with indices [241, 79, 228, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981622864769), np.float64(0.7853981648066234), np.float64(0.1635701974083699), np.float64(-0.16356963668285893), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.464101615134799\n",
      "(change of -0.13682546031509268)\n",
      "Current ansatz: [241, 79, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.784121640913492e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409625805317495 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916426871495)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072771\n",
      "(change of -0.1368254602527843)\n",
      "Current ansatz: [244, 74, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875513833517\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000005)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 216]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200768382\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.4850710481682663)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 79, 216, 201]...\n",
      "Starting point: [np.float64(0.7853981633974504), np.float64(0.7853981633974519), np.float64(-0.1224892795338588), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819708\n",
      "(change of -0.20417052920206213)\n",
      "Current ansatz: [244, 79, 216, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531913\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0894929267348763)]\n",
      "Initial energy: -6.327276154819708\n",
      "Optimizing energy with indices [244, 79, 216, 201, 225]...\n",
      "Starting point: [np.float64(0.7853981633974537), np.float64(0.7853981633975231), np.float64(-0.16357019740839485), np.float64(0.16356963668288216), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135399\n",
      "(change of -0.13682546031569132)\n",
      "Current ansatz: [244, 79, 216, 201, 225]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887658808884167e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531913 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 228]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [244, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981639978813), np.float64(-0.7853981625399111), np.float64(0.12248927961411445), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233392)\n",
      "Current ansatz: [244, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964044667907\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916409814677)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981417276958), np.float64(-0.7853981819240415), np.float64(0.1635702866854681), np.float64(0.16356997269745452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071115\n",
      "(change of -0.1368254602511385)\n",
      "Current ansatz: [244, 31, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0001361285843518749\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044667907 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048479704)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481971\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [244, 31, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531929\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267349482)]\n",
      "Initial energy: -6.32727615481971\n",
      "Optimizing energy with indices [244, 31, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981631072092), np.float64(-0.7853981620731963), np.float64(-0.1635701974084101), np.float64(0.16356963668286922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135289\n",
      "(change of -0.13682546031557852)\n",
      "Current ansatz: [244, 31, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.615543193896931e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531929 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916426871504)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072802\n",
      "(change of -0.1368254602528154)\n",
      "Current ansatz: [244, 74, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875522059898\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474292024)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 147]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819699\n",
      "(change of -0.20417052920205947)\n",
      "Current ansatz: [244, 79, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531783\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734875)]\n",
      "Initial energy: -6.327276154819699\n",
      "Optimizing energy with indices [244, 79, 228, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981646904773), np.float64(0.7853981643651435), np.float64(0.16357019740836662), np.float64(-0.1635696366828739), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132633\n",
      "(change of -0.1368254603129344)\n",
      "Current ansatz: [244, 79, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.372040497021057e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531783 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.99999999999442)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610642\n",
      "(change of -0.12310562561245764)\n",
      "Current ansatz: [228, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955728\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868942)]\n",
      "Initial energy: -6.123105625610642\n",
      "Optimizing energy with indices [228, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853983869831797), np.float64(-0.7853991695302367), np.float64(-0.12248869758310571), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816355\n",
      "(change of -0.20417052920571255)\n",
      "Current ansatz: [228, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096256714494\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929485632674)]\n",
      "Initial energy: -6.327276154816355\n",
      "Optimizing energy with indices [228, 74, 225, 210, 198]...\n",
      "Starting point: [np.float64(-0.785398438893094), np.float64(-0.7853991591701955), np.float64(-0.1635701984907769), np.float64(0.1635696317030657), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132105\n",
      "(change of -0.13682546031574994)\n",
      "Current ansatz: [228, 74, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.185632046620665e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096256714494 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000005)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 216]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200768382\n",
      "Operator(s) added to ansatz: [30]\n",
      "Gradients: [np.float64(2.4850710481682663)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 79, 216, 30]...\n",
      "Starting point: [np.float64(0.7853981633974504), np.float64(0.7853981633974519), np.float64(-0.1224892795338588), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819986\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [244, 79, 216, 30]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041381381\n",
      "Operator(s) added to ansatz: [99]\n",
      "Gradients: [np.float64(-2.0894916438671935)]\n",
      "Initial energy: -6.327276154819986\n",
      "Optimizing energy with indices [244, 79, 216, 30, 99]...\n",
      "Starting point: [np.float64(0.7853981633975555), np.float64(0.7853981633964132), np.float64(-0.16357028648501604), np.float64(-0.16356997194328732), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.46410161507281\n",
      "(change of -0.13682546025282427)\n",
      "Current ansatz: [244, 79, 216, 30, 99]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503877285039854\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041381381 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072685\n",
      "(change of -0.13682546025269726)\n",
      "Current ansatz: [244, 79, 228, 210, 147]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605784767053\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "Pool will be tiled from 22 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 8\n",
      "Got DMRG energy -1.34997e+01\n",
      "Tiled pool has 88 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -6.999999999999975\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.99999999999997\n",
      "Operator 1: 3.999999999999984\n",
      "Operator 2: -3.9999999999999902\n",
      "Operator 3: 3.999999999999996\n",
      "Operator 4: -3.9999999999999947\n",
      "Operator 5: 3.9999999999999862\n",
      "Operator 6: -3.999999999999985\n",
      "Operator 7: 3.99999999999997\n",
      "Operator 8: -3.999999999999984\n",
      "Operator 9: 3.9999999999999902\n",
      "Operator 10: 3.99999999999997\n",
      "Operator 11: 3.999999999999984\n",
      "Operator 12: 3.9999999999999902\n",
      "Operator 13: 3.999999999999996\n",
      "Operator 14: 3.9999999999999947\n",
      "Operator 15: -3.999999999999985\n",
      "Operator 16: -3.99999999999997\n",
      "Operator 17: -3.999999999999984\n",
      "Operator 18: -3.9999999999999902\n",
      "Operator 19: -3.999999999999996\n",
      "Operator 25: 3.9999999999999862\n",
      "Operator 26: -3.999999999999985\n",
      "Operator 27: 3.99999999999997\n",
      "Operator 28: -3.999999999999984\n",
      "Operator 29: 3.9999999999999902\n",
      "Operator 30: 3.999999999999985\n",
      "Operator 31: -3.9999999999999862\n",
      "Operator 32: 3.999999999999985\n",
      "Operator 33: -3.99999999999997\n",
      "Operator 34: 3.999999999999984\n",
      "Operator 35: -3.9999999999999902\n",
      "Operator 36: 3.999999999999985\n",
      "Operator 37: 3.99999999999997\n",
      "Operator 38: 3.999999999999984\n",
      "Operator 39: 3.9999999999999902\n",
      "Operator 40: 3.999999999999996\n",
      "Operator 41: -3.999999999999985\n",
      "Operator 42: -3.99999999999997\n",
      "Operator 43: -3.999999999999984\n",
      "Operator 44: -3.9999999999999902\n",
      "Operator 45: -3.999999999999996\n",
      "Operator 46: -3.9999999999999862\n",
      "Operator 47: 3.999999999999985\n",
      "Operator 48: -3.9999999999999947\n",
      "Operator 49: -3.999999999999996\n",
      "Operator 50: 3.9999999999999947\n",
      "Operator 56: -3.99999999999997\n",
      "Operator 57: -3.999999999999984\n",
      "Operator 58: -3.9999999999999902\n",
      "Operator 59: -3.999999999999996\n",
      "Operator 60: -3.9999999999999947\n",
      "Operator 71: -3.9999999999999862\n",
      "Operator 82: -3.9999999999999862\n",
      "Operator 83: -3.999999999999985\n",
      "Operator 84: -3.99999999999997\n",
      "Operator 85: -3.999999999999984\n",
      "Operator 86: -3.9999999999999902\n",
      "Total gradient norm: 30.199337741082893\n",
      "Operators under consideration (1):\n",
      "[85]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.999999999999984)]\n",
      "Operator(s) added to ansatz: [85]\n",
      "Gradients: [np.float64(-3.999999999999984)]\n",
      "Initial energy: -6.999999999999975\n",
      "Optimizing energy with indices [85]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -7.828427\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -7.828427124746158\n",
      "(change of -0.8284271247461827)\n",
      "Current ansatz: [85]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.414213562369139\n",
      "Operator 2: -3.414213562369156\n",
      "Operator 3: 3.999999999999994\n",
      "Operator 4: -3.999999999999992\n",
      "Operator 5: 2.828427124738323\n",
      "Operator 6: -3.9999999999999805\n",
      "Operator 7: 3.4142135623691385\n",
      "Operator 9: 3.414213562369156\n",
      "Operator 10: 3.414213562369139\n",
      "Operator 12: 3.4142135623691563\n",
      "Operator 13: 2.828427124738331\n",
      "Operator 14: 3.999999999999992\n",
      "Operator 15: -2.828427124738322\n",
      "Operator 16: -3.4142135623691385\n",
      "Operator 18: -3.414213562369156\n",
      "Operator 19: -3.999999999999994\n",
      "Operator 25: 3.9999999999999822\n",
      "Operator 26: -3.9999999999999805\n",
      "Operator 27: 3.4142135623691385\n",
      "Operator 29: 3.4142135623691563\n",
      "Operator 30: 3.9999999999999805\n",
      "Operator 31: -2.828427124738323\n",
      "Operator 32: 3.9999999999999805\n",
      "Operator 33: -3.414213562369139\n",
      "Operator 35: -3.4142135623691563\n",
      "Operator 36: 2.828427124738322\n",
      "Operator 37: 3.414213562369139\n",
      "Operator 39: 3.4142135623691563\n",
      "Operator 40: 3.999999999999994\n",
      "Operator 41: -3.9999999999999805\n",
      "Operator 42: -3.4142135623691385\n",
      "Operator 44: -3.414213562369156\n",
      "Operator 45: -2.828427124738331\n",
      "Operator 46: -3.9999999999999822\n",
      "Operator 47: 3.9999999999999805\n",
      "Operator 48: -3.999999999999992\n",
      "Operator 49: -3.999999999999994\n",
      "Operator 50: 3.999999999999992\n",
      "Operator 52: 1.4142135623770091\n",
      "Operator 53: -1.4142135623770171\n",
      "Operator 56: -3.414213562369139\n",
      "Operator 58: -3.414213562369156\n",
      "Operator 59: -2.828427124738331\n",
      "Operator 60: -2.8284271247383286\n",
      "Operator 62: -1.4142135623770111\n",
      "Operator 63: 1.4142135623770165\n",
      "Operator 67: -1.4142135623770111\n",
      "Operator 68: 1.4142135623770165\n",
      "Operator 71: -3.9999999999999822\n",
      "Operator 82: -2.828427124738323\n",
      "Operator 83: -2.828427124738322\n",
      "Operator 84: -3.4142135623691385\n",
      "Operator 86: -3.4142135623691563\n",
      "Total gradient norm: 24.436388542272944\n",
      "Operators under consideration (1):\n",
      "[71]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999822)]\n",
      "Operator(s) added to ansatz: [71]\n",
      "Gradients: [np.float64(-3.9999999999999822)]\n",
      "Initial energy: -7.828427124746158\n",
      "Optimizing energy with indices [85, 71]...\n",
      "Starting point: [np.float64(0.39269908170011203), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -9.064495\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "\n",
      "Current energy: -9.064495102245951\n",
      "(change of -1.2360679774997934)\n",
      "Current ansatz: [85, 71]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135622890684\n",
      "Operator 2: -3.4142135622890843\n",
      "Operator 3: 4.0\n",
      "Operator 4: -3.9999999999999987\n",
      "Operator 6: -2.894427191102209\n",
      "Operator 7: 3.4142135622890675\n",
      "Operator 9: 3.4142135622890843\n",
      "Operator 10: 1.5268827231706519\n",
      "Operator 12: 3.4142135622890843\n",
      "Operator 13: 2.828427124578178\n",
      "Operator 14: 3.9999999999999987\n",
      "Operator 15: -2.046669094357529\n",
      "Operator 16: -3.4142135622890675\n",
      "Operator 18: -3.4142135622890843\n",
      "Operator 19: -4.0\n",
      "Operator 26: -2.894427191102209\n",
      "Operator 27: 3.414213562289068\n",
      "Operator 29: 3.4142135622890843\n",
      "Operator 30: 2.894427191102209\n",
      "Operator 32: 2.894427191102209\n",
      "Operator 33: -3.4142135622890684\n",
      "Operator 35: -3.4142135622890843\n",
      "Operator 36: 2.0466690943575285\n",
      "Operator 37: 3.4142135622890684\n",
      "Operator 39: 3.4142135622890843\n",
      "Operator 40: 4.0\n",
      "Operator 41: -2.894427191102209\n",
      "Operator 42: -1.5268827231706517\n",
      "Operator 44: -3.4142135622890843\n",
      "Operator 45: -2.828427124578178\n",
      "Operator 47: 2.894427191102209\n",
      "Operator 48: -3.9999999999999987\n",
      "Operator 49: -4.0\n",
      "Operator 50: 3.9999999999999987\n",
      "Operator 52: 1.4142135624570915\n",
      "Operator 53: -1.4142135624571002\n",
      "Operator 56: -1.5268827231706519\n",
      "Operator 58: -3.4142135622890843\n",
      "Operator 59: -2.828427124578178\n",
      "Operator 60: -2.8284271245781762\n",
      "Operator 62: -0.6324555321435787\n",
      "Operator 63: 1.4142135624570982\n",
      "Operator 67: -1.4142135624570935\n",
      "Operator 68: 1.4142135624570984\n",
      "Operator 83: -2.0466690943575285\n",
      "Operator 84: -3.414213562289068\n",
      "Operator 86: -3.4142135622890843\n",
      "Operator 87: -1.7888543819486749\n",
      "Total gradient norm: 21.014125023727647\n",
      "Operators under consideration (1):\n",
      "[50]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.9999999999999987)]\n",
      "Operator(s) added to ansatz: [50]\n",
      "Gradients: [np.float64(3.9999999999999987)]\n",
      "Initial energy: -9.064495102245951\n",
      "Optimizing energy with indices [85, 71, 50]...\n",
      "Starting point: [np.float64(0.392699081728425), np.float64(0.5535743588684507), np.float64(0.0)]\n",
      "         Current function value: -10.300563\n",
      "         Iterations: 9\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 60\n",
      "\n",
      "Current energy: -10.30056307974575\n",
      "(change of -1.2360679774997987)\n",
      "Current ansatz: [85, 71, 50]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135781261294\n",
      "Operator 1: 6.301222245700484e-08\n",
      "Operator 2: -3.4142135781261453\n",
      "Operator 3: 2.894427136241872\n",
      "Operator 4: 1.3689511391135056e-07\n",
      "Operator 5: -6.672195190939939e-08\n",
      "Operator 6: -2.8944271532562675\n",
      "Operator 7: 3.414213578126129\n",
      "Operator 8: -2.8179919706359752e-08\n",
      "Operator 9: 3.4142135781261453\n",
      "Operator 10: 1.5268826656461367\n",
      "Operator 11: 6.301222245700484e-08\n",
      "Operator 12: 3.4142135781261453\n",
      "Operator 13: 2.04666907848502\n",
      "Operator 14: -1.3689511391135056e-07\n",
      "Operator 15: -2.0466690905160143\n",
      "Operator 16: -3.414213578126129\n",
      "Operator 17: -6.301222245700484e-08\n",
      "Operator 18: -1.5268826366007373\n",
      "Operator 19: -2.8944271362418728\n",
      "Operator 25: -9.435908795811788e-08\n",
      "Operator 26: -2.8944271532562675\n",
      "Operator 27: 3.414213578126129\n",
      "Operator 28: -6.301222196250423e-08\n",
      "Operator 29: 3.4142135781261453\n",
      "Operator 30: 2.8944271532562675\n",
      "Operator 31: 6.672195194809044e-08\n",
      "Operator 32: 2.8944271532562675\n",
      "Operator 33: -3.4142135781261294\n",
      "Operator 34: 2.817991938696279e-08\n",
      "Operator 35: -3.4142135781261453\n",
      "Operator 36: 2.046669090516014\n",
      "Operator 37: 3.4142135781261294\n",
      "Operator 38: 6.301222196250423e-08\n",
      "Operator 39: 1.526882636600737\n",
      "Operator 40: 2.894427136241872\n",
      "Operator 41: -2.894427153256267\n",
      "Operator 42: -1.5268826656461363\n",
      "Operator 43: -6.301222196250423e-08\n",
      "Operator 44: -3.4142135781261453\n",
      "Operator 45: -2.04666907848502\n",
      "Operator 46: 9.435908803040006e-08\n",
      "Operator 47: 2.8944271532562675\n",
      "Operator 48: 1.3689511391135056e-07\n",
      "Operator 49: -2.8944271362418728\n",
      "Operator 50: -1.3689511391135056e-07\n",
      "Operator 52: 1.4142135466200325\n",
      "Operator 53: -1.41421354662004\n",
      "Operator 55: 1.7888544093788556\n",
      "Operator 56: -1.5268826656461367\n",
      "Operator 57: -2.8179921332441555e-08\n",
      "Operator 58: -3.4142135781261453\n",
      "Operator 59: -2.04666907848502\n",
      "Operator 60: 9.679946488816427e-08\n",
      "Operator 62: -0.6324554982999087\n",
      "Operator 63: 1.4142135466200387\n",
      "Operator 65: -1.2649110975172044\n",
      "Operator 67: -1.4142135466200338\n",
      "Operator 68: 1.4142135466200387\n",
      "Operator 70: -1.7888544093788548\n",
      "Operator 71: 9.435908803040006e-08\n",
      "Operator 82: 6.672195194809044e-08\n",
      "Operator 83: -2.046669090516014\n",
      "Operator 84: -3.414213578126129\n",
      "Operator 85: -2.817991938696279e-08\n",
      "Operator 86: -1.526882636600737\n",
      "Operator 87: -1.7888544008716467\n",
      "Total gradient norm: 17.541219911199356\n",
      "Operators under consideration (1):\n",
      "[84]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.414213578126129)]\n",
      "Operator(s) added to ansatz: [84]\n",
      "Gradients: [np.float64(-3.414213578126129)]\n",
      "Initial energy: -10.30056307974575\n",
      "Optimizing energy with indices [85, 71, 50, 84]...\n",
      "Starting point: [np.float64(0.39269907612917887), np.float64(0.553574369446713), np.float64(-0.553574374202384), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -10.972066\n",
      "         Iterations: 10\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 15\n",
      "\n",
      "Current energy: -10.972065833846274\n",
      "(change of -0.6715027541005245)\n",
      "Current ansatz: [85, 71, 50, 84]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.042778505634707\n",
      "Operator 2: -3.6894586733081054\n",
      "Operator 3: 2.8944271910874337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator 6: -2.4775948466145925\n",
      "Operator 9: 3.6894586733081054\n",
      "Operator 10: -0.7031172305868252\n",
      "Operator 11: 1.3681172484456208\n",
      "Operator 12: 2.6269289608062825\n",
      "Operator 13: 2.4450075611207454\n",
      "Operator 15: -2.092897051278248\n",
      "Operator 18: -1.6499760789000617\n",
      "Operator 19: -2.8944271910874337\n",
      "Operator 20: 1.1175787285947885\n",
      "Operator 26: -2.094789138544006\n",
      "Operator 28: -1.36811724844562\n",
      "Operator 29: 3.6894586733081054\n",
      "Operator 30: 2.4775948466145925\n",
      "Operator 32: 2.4775948466145916\n",
      "Operator 33: -1.042778505634707\n",
      "Operator 34: 0.6118406338027392\n",
      "Operator 35: -3.6894586733081054\n",
      "Operator 36: 2.092897051278247\n",
      "Operator 37: 1.042778505634707\n",
      "Operator 38: 1.36811724844562\n",
      "Operator 39: 1.6499760789000617\n",
      "Operator 40: 2.8944271910874337\n",
      "Operator 41: -2.4775948466145925\n",
      "Operator 42: 1.2463752755690747\n",
      "Operator 44: -2.6269289608062825\n",
      "Operator 45: -2.4450075611207454\n",
      "Operator 47: 2.094789138544005\n",
      "Operator 49: -2.8944271910874337\n",
      "Operator 51: 1.1862872200547296\n",
      "Operator 52: -1.64588169960396\n",
      "Operator 53: -1.07038749580888\n",
      "Operator 55: 1.7888543819560774\n",
      "Operator 56: 0.7031172305868252\n",
      "Operator 58: -2.6269289608062825\n",
      "Operator 59: -1.7408681707605207\n",
      "Operator 61: -1.1862872200547283\n",
      "Operator 62: -0.4652760261626224\n",
      "Operator 63: 0.7621258729276026\n",
      "Operator 65: -1.5110977754404553\n",
      "Operator 66: -1.4043400277224591\n",
      "Operator 67: -1.3872882812921583\n",
      "Operator 68: 0.7621258729276026\n",
      "Operator 70: -1.788854381956077\n",
      "Operator 71: 0.9289724383382694\n",
      "Operator 72: -1.3230021500395417\n",
      "Operator 73: 0.7515940027690093\n",
      "Operator 74: 0.7515940027690104\n",
      "Operator 77: 1.323002150039541\n",
      "Operator 82: 0.7847302716073862\n",
      "Operator 83: -1.7695298394323955\n",
      "Operator 85: -0.6118406338027392\n",
      "Operator 86: -1.6499760789000617\n",
      "Operator 87: -1.8841621315674746\n",
      "Total gradient norm: 14.85737198738544\n",
      "Operators under consideration (1):\n",
      "[35]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.6894586733081054)]\n",
      "Operator(s) added to ansatz: [35]\n",
      "Gradients: [np.float64(-3.6894586733081054)]\n",
      "Initial energy: -10.972065833846274\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35]...\n",
      "Starting point: [np.float64(0.2823685457008476), np.float64(0.6143910960168902), np.float64(-0.5535743588725847), np.float64(0.3892203336194868), np.float64(0.0)]\n",
      "         Current function value: -12.000000\n",
      "         Iterations: 22\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 51\n",
      "\n",
      "Current energy: -11.999999999883759\n",
      "(change of -1.0279341660374843)\n",
      "Current ansatz: [85, 71, 50, 84, 35]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.0040373084156537925\n",
      "Operator 2: -0.0040373052229336626\n",
      "Operator 3: 0.012111654324133636\n",
      "Operator 4: -4.971909817186315e-08\n",
      "Operator 5: 3.151504844518484e-08\n",
      "Operator 6: -0.008074490929631608\n",
      "Operator 7: 9.656448061367336e-08\n",
      "Operator 8: -0.004037188070382225\n",
      "Operator 9: 0.0040373052229336626\n",
      "Operator 10: -1.999963326866205\n",
      "Operator 11: 1.9999796261042633\n",
      "Operator 12: -1.9999511025855214\n",
      "Operator 13: 2.000012224328594\n",
      "Operator 14: -1.999963326886078\n",
      "Operator 15: -2.0000122242823006\n",
      "Operator 16: 1.9999511025630885\n",
      "Operator 17: -1.9999796260899516\n",
      "Operator 18: 1.9999633268008334\n",
      "Operator 19: -2.000016299152438\n",
      "Operator 20: 1.9999755513536317\n",
      "Operator 22: 1.9999592522987593\n",
      "Operator 23: -8.149524969686042e-06\n",
      "Operator 24: 1.9999755513233353\n",
      "Operator 25: 3.151511299542719e-08\n",
      "Operator 26: -0.012111641676734607\n",
      "Operator 27: 9.656448084863246e-08\n",
      "Operator 28: -0.01614868338218931\n",
      "Operator 29: 9.337176298807037e-08\n",
      "Operator 30: 2.0000162991061456\n",
      "Operator 31: -3.1515048278147214e-08\n",
      "Operator 32: 0.008074490929631844\n",
      "Operator 33: -0.0040373084156531645\n",
      "Operator 34: 0.004037188070410869\n",
      "Operator 35: -9.337176298807037e-08\n",
      "Operator 36: 2.0000122242822997\n",
      "Operator 37: -1.9999307287896677\n",
      "Operator 38: 1.9999796261042633\n",
      "Operator 39: -1.999975551116442\n",
      "Operator 40: 2.0000162991524393\n",
      "Operator 41: -2.0000162991061465\n",
      "Operator 42: 1.9999755511610524\n",
      "Operator 43: -1.9999796260899514\n",
      "Operator 44: 1.9999307288168713\n",
      "Operator 45: -2.0000122243285916\n",
      "Operator 46: -3.151511316179179e-08\n",
      "Operator 47: 0.012111641676732844\n",
      "Operator 48: 1.999963326886078\n",
      "Operator 49: -0.012111654324136967\n",
      "Operator 50: 4.971909817186315e-08\n",
      "Operator 51: 1.9999796261004135\n",
      "Operator 52: -1.999991850233993\n",
      "Operator 53: 1.9999918502356684\n",
      "Operator 54: -1.9999796260908747\n",
      "Operator 55: 1.999995925190951\n",
      "Operator 56: 1.9999633268662045\n",
      "Operator 58: 1.999930728816871\n",
      "Operator 59: -4.889720906334997e-05\n",
      "Operator 60: 1.9999592521701568\n",
      "Operator 61: -1.9999796261004126\n",
      "Operator 62: -1.629907196300359e-05\n",
      "Operator 63: -1.9999307290239998\n",
      "Operator 64: 1.6299014834743678e-05\n",
      "Operator 65: -1.999991850408617\n",
      "Operator 66: -1.9999837008578423\n",
      "Operator 67: 1.9999307290192303\n",
      "Operator 68: -1.9999307290239998\n",
      "Operator 69: 1.999983700848303\n",
      "Operator 70: -1.9999959251909527\n",
      "Operator 71: 1.9999633269994126\n",
      "Operator 72: -1.9999796261027614\n",
      "Operator 73: 1.6298982784259652e-05\n",
      "Operator 74: -1.9999592523147478\n",
      "Operator 76: -1.9999796260724616\n",
      "Operator 77: 1.99997962610276\n",
      "Operator 79: 1.9999592522987593\n",
      "Operator 80: -1.6298978012740488e-05\n",
      "Operator 81: 1.9999796260724643\n",
      "Operator 82: 1.9999592522834904\n",
      "Operator 83: -4.889717231644769e-05\n",
      "Operator 84: 1.9999511025630876\n",
      "Operator 85: -3.259796802555261e-05\n",
      "Operator 86: 1.999975551116442\n",
      "Operator 87: -1.999995925211707\n",
      "Total gradient norm: 14.422063835834402\n",
      "Operators under consideration (1):\n",
      "[40]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.0000162991524393)]\n",
      "Operator(s) added to ansatz: [40]\n",
      "Gradients: [np.float64(2.0000162991524393)]\n",
      "Initial energy: -11.999999999883759\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40]...\n",
      "Starting point: [np.float64(0.0010093057046464448), np.float64(0.7843888579835763), np.float64(-0.7843888554126287), np.float64(0.7833795522230771), np.float64(0.7833795516321658), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -12.141616\n",
      "         Iterations: 19\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "\n",
      "Current energy: -12.141616390445943\n",
      "(change of -0.14161639056218434)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.45988707783701716\n",
      "Operator 2: -0.45137100515952533\n",
      "Operator 3: 0.6765461947719622\n",
      "Operator 4: -0.2998317851346316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator 6: -1.0297000920742923\n",
      "Operator 8: -0.5048489811931155\n",
      "Operator 9: 0.6521802234624059\n",
      "Operator 10: -1.5946377604822302\n",
      "Operator 11: 1.7793330282670845\n",
      "Operator 12: -1.4497409133076185\n",
      "Operator 13: -0.05973161634925864\n",
      "Operator 14: -1.9081489655464878\n",
      "Operator 15: -2.1124765588077428\n",
      "Operator 16: 1.4071285652794498\n",
      "Operator 17: -1.6042844778395304\n",
      "Operator 18: 1.2995025213804\n",
      "Operator 19: -2.024239587469195\n",
      "Operator 20: 1.726208220459713\n",
      "Operator 22: 1.5013282808776283\n",
      "Operator 23: -0.6531630524722879\n",
      "Operator 24: -0.43871550046442487\n",
      "Operator 26: -1.252613424166736\n",
      "Operator 28: -1.555963646240567\n",
      "Operator 29: 0.22188360753367367\n",
      "Operator 30: 2.1730673125540276\n",
      "Operator 32: 1.0297000920742925\n",
      "Operator 33: -0.4402898408565843\n",
      "Operator 34: 0.5299509172102582\n",
      "Operator 36: 2.1124765588077423\n",
      "Operator 37: -1.1499783412718547\n",
      "Operator 38: 1.6725724707521104\n",
      "Operator 39: -2.0685163423739374\n",
      "Operator 41: -2.1730673125540285\n",
      "Operator 42: 1.7377204875270456\n",
      "Operator 43: -1.6950521970084629\n",
      "Operator 44: 1.0749965981667382\n",
      "Operator 45: -1.9633160184780825\n",
      "Operator 47: 1.252613424166735\n",
      "Operator 48: 1.2753873910841977\n",
      "Operator 49: -0.991052331969228\n",
      "Operator 51: 1.7639558871209755\n",
      "Operator 52: -1.9065006354973628\n",
      "Operator 53: 1.8711965419078687\n",
      "Operator 54: 0.41362411592324494\n",
      "Operator 55: 2.3791182115505283\n",
      "Operator 56: 1.5946377604822302\n",
      "Operator 58: 1.2194999128571322\n",
      "Operator 59: -0.270250931020696\n",
      "Operator 60: 1.2398262821862513\n",
      "Operator 61: -1.7639558871209746\n",
      "Operator 62: -0.1697083804959411\n",
      "Operator 63: -1.1833254378515803\n",
      "Operator 64: 0.7280171873975857\n",
      "Operator 65: 0.35760396382572196\n",
      "Operator 66: -1.8145502552952564\n",
      "Operator 67: 1.1470732568332622\n",
      "Operator 68: -1.1019626411226748\n",
      "Operator 69: 2.2875853457830635\n",
      "Operator 70: 0.3092045869938663\n",
      "Operator 71: 1.6110664966015569\n",
      "Operator 72: -1.7757198975311606\n",
      "Operator 73: 0.20162275481719646\n",
      "Operator 74: -1.443627812844551\n",
      "Operator 75: -0.00803884942067068\n",
      "Operator 76: -1.609640581144133\n",
      "Operator 77: 1.7757198975311588\n",
      "Operator 79: 1.4373519538102038\n",
      "Operator 80: -0.31503631961131334\n",
      "Operator 81: -0.39264259164565407\n",
      "Operator 82: 1.5661457834692303\n",
      "Operator 83: -0.5769967320386942\n",
      "Operator 84: 1.3471663848124245\n",
      "Operator 85: -0.4413536724514243\n",
      "Operator 86: 1.470504816702348\n",
      "Operator 87: -1.9572011217096061\n",
      "Total gradient norm: 11.940242953738073\n",
      "Operators under consideration (1):\n",
      "[55]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.3791182115505283)]\n",
      "Operator(s) added to ansatz: [55]\n",
      "Gradients: [np.float64(2.3791182115505283)]\n",
      "Initial energy: -12.141616390445943\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55]...\n",
      "Starting point: [np.float64(0.11834935441664085), np.float64(0.681773358532676), np.float64(-0.6341852400058826), np.float64(0.5683792920819821), np.float64(0.5385713891908049), np.float64(-0.1464910980356101), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -12.334748\n",
      "         Iterations: 14\n",
      "         Function evaluations: 18\n",
      "         Gradient evaluations: 18\n",
      "\n",
      "Current energy: -12.334747953210826\n",
      "(change of -0.19313156276488286)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.37782972011209415\n",
      "Operator 2: -0.3495171551621382\n",
      "Operator 3: 0.39529016345986856\n",
      "Operator 4: 0.13834604302668205\n",
      "Operator 6: -0.7922597189848107\n",
      "Operator 8: -0.3860646105767956\n",
      "Operator 9: 0.5052711114879483\n",
      "Operator 10: -1.720764746737806\n",
      "Operator 11: 1.8495914212849807\n",
      "Operator 12: -1.5477034952417554\n",
      "Operator 13: -0.6301619211070628\n",
      "Operator 14: -1.7664088283623551\n",
      "Operator 15: -2.080485890635364\n",
      "Operator 16: 1.6126905511364011\n",
      "Operator 17: -1.5899740921976986\n",
      "Operator 18: 1.6879690762564972\n",
      "Operator 19: 0.5462375691291707\n",
      "Operator 20: 1.8125560625805492\n",
      "Operator 22: 1.5859355453647548\n",
      "Operator 23: -0.6932589206660409\n",
      "Operator 24: -0.8288169814840869\n",
      "Operator 26: -1.040554973209203\n",
      "Operator 28: -1.323522219843361\n",
      "Operator 29: 0.15260200051941608\n",
      "Operator 30: 2.1197016088228424\n",
      "Operator 32: 0.7922597189848112\n",
      "Operator 33: -0.33727752260941457\n",
      "Operator 34: 0.4653319861075059\n",
      "Operator 35: 0.005702038179291773\n",
      "Operator 36: 2.0804858906353636\n",
      "Operator 37: -1.4416371898636808\n",
      "Operator 38: 1.6105448325617249\n",
      "Operator 39: -1.8717349829379406\n",
      "Operator 40: -0.594578316360874\n",
      "Operator 41: -2.1197016088228438\n",
      "Operator 42: 1.8192962599159528\n",
      "Operator 43: -1.8137126670281154\n",
      "Operator 44: 1.329235638248136\n",
      "Operator 45: 0.5605805287023874\n",
      "Operator 47: 1.0405549732092019\n",
      "Operator 48: 1.7802053525020667\n",
      "Operator 49: 0.017434306100247345\n",
      "Operator 51: 1.839879685490383\n",
      "Operator 52: -1.9368521250539832\n",
      "Operator 53: 1.8214957347504028\n",
      "Operator 54: 0.2122063453104352\n",
      "Operator 56: 1.7207647467378078\n",
      "Operator 58: 1.3952855348412276\n",
      "Operator 59: -0.13201128420759892\n",
      "Operator 60: 1.7399888174978422\n",
      "Operator 61: -1.8398796854903816\n",
      "Operator 62: -0.11810720984998141\n",
      "Operator 63: -1.3738501554358997\n",
      "Operator 64: 0.7415455751903347\n",
      "Operator 65: 0.04087883985067563\n",
      "Operator 66: -1.8745601433439196\n",
      "Operator 67: 1.4428590054246073\n",
      "Operator 68: -1.3461399243232615\n",
      "Operator 69: 0.08500996905622257\n",
      "Operator 70: -0.0032729362651878363\n",
      "Operator 71: 1.7308952501016315\n",
      "Operator 72: -1.846721489065351\n",
      "Operator 73: 0.1341472953964176\n",
      "Operator 74: -1.5714502803810335\n",
      "Operator 75: 0.4527357919043642\n",
      "Operator 76: 0.6768146422659721\n",
      "Operator 77: 1.8467214890653496\n",
      "Operator 79: 1.5736446036432807\n",
      "Operator 80: -0.09958838529020192\n",
      "Operator 81: -0.8190556412585703\n",
      "Operator 82: 1.6988726767085183\n",
      "Operator 83: -0.3817226417730345\n",
      "Operator 84: 1.4396016111742818\n",
      "Operator 85: -0.21478133220055728\n",
      "Operator 86: 1.7910325705158865\n",
      "Operator 87: -1.9702984677473125\n",
      "Total gradient norm: 11.489232766489595\n",
      "Operators under consideration (1):\n",
      "[41]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.1197016088228438)]\n",
      "Operator(s) added to ansatz: [41]\n",
      "Gradients: [np.float64(-2.1197016088228438)]\n",
      "Initial energy: -12.334747953210826\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41]...\n",
      "Starting point: [np.float64(0.09632733939816022), np.float64(0.6991205013559425), np.float64(-0.6926132196225817), np.float64(0.6073718287480427), np.float64(0.5938683901529344), np.float64(-0.17239822705138058), np.float64(-0.1611726213797103), np.float64(0.0)]\n",
      "         Current function value: -12.498723\n",
      "         Iterations: 14\n",
      "         Function evaluations: 58\n",
      "         Gradient evaluations: 48\n",
      "\n",
      "Current energy: -12.498722988584312\n",
      "(change of -0.16397503537348612)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.5999992182359202\n",
      "Operator 1: 4.2761739814845625e-08\n",
      "Operator 2: -0.5726423347514638\n",
      "Operator 3: 0.5127588329779547\n",
      "Operator 4: 0.1697501267390696\n",
      "Operator 5: -1.0025138053038063e-08\n",
      "Operator 6: 7.687283293256186e-08\n",
      "Operator 7: 1.5249805085382206e-07\n",
      "Operator 8: -0.5163390181502923\n",
      "Operator 9: 0.7995114946557439\n",
      "Operator 10: -0.9668298402436672\n",
      "Operator 11: 1.4323615822876141\n",
      "Operator 12: -1.0553165719096267\n",
      "Operator 13: -0.5717480281041564\n",
      "Operator 14: -1.3904457036970557\n",
      "Operator 15: 0.11437173245499982\n",
      "Operator 16: 1.134296500219285\n",
      "Operator 17: -1.333282629925927\n",
      "Operator 18: 1.1917328754541088\n",
      "Operator 19: 0.37163404646949677\n",
      "Operator 20: 1.3694513540380702\n",
      "Operator 21: 0.11499350858453611\n",
      "Operator 22: 1.1979673630913341\n",
      "Operator 23: -0.7876357702684254\n",
      "Operator 24: -0.9632962523503881\n",
      "Operator 25: -1.0568547814671092e-08\n",
      "Operator 26: -1.1653410999673133\n",
      "Operator 27: 0.2851173671508182\n",
      "Operator 28: -1.7734461146064555\n",
      "Operator 29: 0.2340729838270408\n",
      "Operator 30: 2.040058315239946\n",
      "Operator 31: -0.3728662006460466\n",
      "Operator 32: 1.1266925323788073\n",
      "Operator 33: -0.754217313313787\n",
      "Operator 34: 0.6024715968906347\n",
      "Operator 35: 0.0009805408644188729\n",
      "Operator 36: 1.9219475179367835\n",
      "Operator 37: -0.5511856215549669\n",
      "Operator 38: 1.2535264090305196\n",
      "Operator 39: -1.6050964623606703\n",
      "Operator 40: -0.4659842380852641\n",
      "Operator 41: 1.3919064689726923e-07\n",
      "Operator 42: 1.862292140607007\n",
      "Operator 43: -1.4469782310633637\n",
      "Operator 44: 0.5200368196963467\n",
      "Operator 45: 0.423110307903335\n",
      "Operator 46: -0.39307728748872073\n",
      "Operator 47: 0.740320042465188\n",
      "Operator 48: 1.416483020201873\n",
      "Operator 49: 0.0026992441558384\n",
      "Operator 50: 2.75062227367868e-07\n",
      "Operator 51: -0.6117336008311487\n",
      "Operator 52: -1.7980856390588407\n",
      "Operator 53: 1.7192004649528507\n",
      "Operator 54: 0.4691502765027537\n",
      "Operator 55: 8.307649675571448e-08\n",
      "Operator 56: 1.5486867295511704\n",
      "Operator 57: 0.041232388615012786\n",
      "Operator 58: 0.6356227640886735\n",
      "Operator 59: -0.2580101075660365\n",
      "Operator 60: 1.3247910858283798\n",
      "Operator 61: -2.007233277167062\n",
      "Operator 62: -0.4188578492378057\n",
      "Operator 63: -0.558824695784856\n",
      "Operator 64: 0.8791146439126333\n",
      "Operator 65: 0.1141360853118972\n",
      "Operator 66: -2.1749874360964823\n",
      "Operator 67: 0.5948864089316309\n",
      "Operator 68: -0.5491155438113947\n",
      "Operator 69: 0.16587724871264756\n",
      "Operator 70: -0.0007623439211890385\n",
      "Operator 71: 1.6310558674028348\n",
      "Operator 72: 0.5561629856302975\n",
      "Operator 73: 0.5137464011723746\n",
      "Operator 74: -1.0629567302209584\n",
      "Operator 75: 0.23342360077722007\n",
      "Operator 76: 0.644363595111443\n",
      "Operator 77: 1.44368200351856\n",
      "Operator 78: 0.023059231307323708\n",
      "Operator 79: 1.072854273132396\n",
      "Operator 80: -0.23114352349607098\n",
      "Operator 81: -0.9362255475759726\n",
      "Operator 82: 1.5471909053951407\n",
      "Operator 83: -0.5875729229506328\n",
      "Operator 84: 0.8143254426623028\n",
      "Operator 85: -0.4029152510936843\n",
      "Operator 86: 1.4364646474550617\n",
      "Operator 87: -2.3172835065025774\n",
      "Total gradient norm: 9.386124134561785\n",
      "Operators under consideration (1):\n",
      "[87]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.3172835065025774)]\n",
      "Operator(s) added to ansatz: [87]\n",
      "Gradients: [np.float64(-2.3172835065025774)]\n",
      "Initial energy: -12.498722988584312\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87]...\n",
      "Starting point: [np.float64(0.16103471909590175), np.float64(0.5996096427126083), np.float64(-0.6477627516349949), np.float64(0.4772821642952995), np.float64(0.49284713021234194), np.float64(-0.18481989634359533), np.float64(-0.1589104193697309), np.float64(0.15985298584964994), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -12.684932\n",
      "         Iterations: 15\n",
      "         Function evaluations: 22\n",
      "         Gradient evaluations: 22\n",
      "\n",
      "Current energy: -12.684931726051246\n",
      "(change of -0.18620873746693434)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.5240759699065302\n",
      "Operator 2: -0.5240759698057018\n",
      "Operator 3: 0.4918824653680484\n",
      "Operator 4: 0.16567522100226562\n",
      "Operator 6: 0.2552575443196594\n",
      "Operator 7: -0.0025718487331164858\n",
      "Operator 8: -0.49281469092063\n",
      "Operator 9: 0.7344314420732214\n",
      "Operator 10: -1.3233579087969576\n",
      "Operator 11: 1.4055053933417008\n",
      "Operator 12: -1.2099631960039716\n",
      "Operator 13: -0.5865396628347288\n",
      "Operator 14: -1.4915704358515804\n",
      "Operator 15: 0.5865396615484892\n",
      "Operator 16: 1.2099631963179867\n",
      "Operator 17: -1.4055053937283932\n",
      "Operator 18: 1.3233579108270037\n",
      "Operator 19: 0.41747078691240647\n",
      "Operator 20: -0.6701660517249197\n",
      "Operator 21: 0.10446340650613933\n",
      "Operator 22: 1.2316113315867303\n",
      "Operator 23: -0.7659441215743634\n",
      "Operator 24: -0.9295778701065247\n",
      "Operator 26: 0.008223879189758901\n",
      "Operator 27: 0.2140842492498727\n",
      "Operator 28: -1.6867569385318848\n",
      "Operator 29: 0.21408424839249318\n",
      "Operator 30: -0.41747078429568624\n",
      "Operator 31: 0.15862398574992748\n",
      "Operator 32: -0.21293553767604262\n",
      "Operator 33: -0.6519968168005024\n",
      "Operator 34: 0.583666218489979\n",
      "Operator 35: 0.002897018211196034\n",
      "Operator 36: -0.4574858454190006\n",
      "Operator 37: -0.7646836310609149\n",
      "Operator 38: 1.3752360269468333\n",
      "Operator 39: -1.6784973887332124\n",
      "Operator 40: -0.5004451989268321\n",
      "Operator 41: 0.5004451975820108\n",
      "Operator 42: 1.6784973878492313\n",
      "Operator 43: -1.3752360276302762\n",
      "Operator 44: 0.7646836309660172\n",
      "Operator 45: 0.45748584794401825\n",
      "Operator 46: 0.16567521808139496\n",
      "Operator 47: 0.4918824673432535\n",
      "Operator 48: 1.514826346985545\n",
      "Operator 49: 0.008223880021572594\n",
      "Operator 51: -0.40207586869373646\n",
      "Operator 52: -1.7484687609915932\n",
      "Operator 53: 1.7484687610613827\n",
      "Operator 54: 0.4020758679886997\n",
      "Operator 56: 1.467674521715118\n",
      "Operator 57: 0.06475070263704205\n",
      "Operator 58: 0.7898740190927042\n",
      "Operator 59: -0.22483279146313695\n",
      "Operator 60: 1.4344045444175348\n",
      "Operator 61: -0.05700358022388276\n",
      "Operator 62: -0.2941473456637124\n",
      "Operator 63: -0.7056469220374484\n",
      "Operator 64: 0.8435074957681128\n",
      "Operator 65: 0.09480536327055455\n",
      "Operator 66: -0.14628376654999203\n",
      "Operator 67: 0.7947881367361611\n",
      "Operator 68: -0.7947881365221221\n",
      "Operator 69: 0.14628376452848008\n",
      "Operator 70: -0.0021343604910211145\n",
      "Operator 71: 1.4915704335530906\n",
      "Operator 72: 0.9063281001479027\n",
      "Operator 73: 0.19479938122307552\n",
      "Operator 74: -1.172267548662267\n",
      "Operator 75: 0.3016288557262236\n",
      "Operator 76: 0.6530014733403724\n",
      "Operator 77: -0.6530014718162793\n",
      "Operator 78: -0.30162885537631895\n",
      "Operator 79: 1.1722675487745755\n",
      "Operator 80: -0.19479938075693426\n",
      "Operator 81: -0.9063281001509417\n",
      "Operator 82: 1.42808834016865\n",
      "Operator 83: 0.030885072504828873\n",
      "Operator 84: 0.9594317399449651\n",
      "Operator 85: -0.3599376662753314\n",
      "Operator 86: 1.5328368575782236\n",
      "Total gradient norm: 7.980553187738889\n",
      "Operators under consideration (1):\n",
      "[53]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.7484687610613827)]\n",
      "Operator(s) added to ansatz: [53]\n",
      "Gradients: [np.float64(1.7484687610613827)]\n",
      "Initial energy: -12.684931726051246\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53]...\n",
      "Starting point: [np.float64(0.1464000480321535), np.float64(0.6584333309099765), np.float64(-0.658433331355469), np.float64(0.5177703484307454), np.float64(0.5177703486220843), np.float64(-0.18134160095356316), np.float64(-0.159538218742822), np.float64(0.18134160096822044), np.float64(0.15953821861727926), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -12.782986\n",
      "         Iterations: 16\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "\n",
      "Current energy: -12.782986198614507\n",
      "(change of -0.09805447256326083)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.554839562277027\n",
      "Operator 1: 0.009782619198188652\n",
      "Operator 2: -0.04309095025553966\n",
      "Operator 3: 0.511729356488638\n",
      "Operator 4: 0.09016238311670932\n",
      "Operator 6: 0.19923437608348465\n",
      "Operator 7: -0.011247198664406478\n",
      "Operator 8: -0.09488429713420699\n",
      "Operator 9: 0.2585403199870132\n",
      "Operator 10: -1.364367775656624\n",
      "Operator 11: 1.429234321376323\n",
      "Operator 12: -1.268630405606863\n",
      "Operator 13: -0.5848689314856408\n",
      "Operator 14: -1.6631050350498797\n",
      "Operator 15: 0.6513155094317421\n",
      "Operator 16: 1.6696179856613276\n",
      "Operator 17: 0.033253351084247706\n",
      "Operator 18: 1.6103803903300662\n",
      "Operator 19: 0.28032674015190573\n",
      "Operator 20: -0.6926146967813425\n",
      "Operator 21: 0.08809056098119689\n",
      "Operator 22: 1.3338667770490404\n",
      "Operator 23: -0.11010844323952625\n",
      "Operator 24: -0.8409740642655096\n",
      "Operator 26: 0.012844945693340593\n",
      "Operator 27: 0.19698043877368732\n",
      "Operator 28: -0.947659829092832\n",
      "Operator 29: 0.17310192081629716\n",
      "Operator 30: -0.46435627531934637\n",
      "Operator 31: 0.15064566736708127\n",
      "Operator 32: -0.18614230058008047\n",
      "Operator 33: -0.6738921163821282\n",
      "Operator 34: 0.5351198422118836\n",
      "Operator 35: -0.042626598673781047\n",
      "Operator 36: -0.5163253392043471\n",
      "Operator 37: -0.9471106822708204\n",
      "Operator 38: 1.4445341150493243\n",
      "Operator 39: -1.696079412593876\n",
      "Operator 40: -0.49896710893022056\n",
      "Operator 41: 0.5334980009468618\n",
      "Operator 42: 1.7348256491549268\n",
      "Operator 43: -0.02187230837683172\n",
      "Operator 44: 1.4648744234808053\n",
      "Operator 45: 0.4806927992995542\n",
      "Operator 46: 0.1601354605078057\n",
      "Operator 47: 0.46373605870017165\n",
      "Operator 48: 1.6182466315377217\n",
      "Operator 49: -0.13743413324582954\n",
      "Operator 51: -0.39683246062108235\n",
      "Operator 52: -2.065687308718206\n",
      "Operator 54: 0.29282790317485435\n",
      "Operator 55: 0.18866497379099056\n",
      "Operator 56: 1.4948353758820523\n",
      "Operator 57: 0.05315511872755776\n",
      "Operator 58: 1.4389864044791716\n",
      "Operator 59: -0.19615062275119738\n",
      "Operator 60: 1.5451103630691199\n",
      "Operator 61: -0.021325890438268765\n",
      "Operator 62: -0.2474327874680075\n",
      "Operator 63: -1.5114935654647768\n",
      "Operator 64: 0.7529848954752805\n",
      "Operator 65: 0.09360118453681188\n",
      "Operator 66: -0.12759012385719462\n",
      "Operator 67: -0.3594529938145406\n",
      "Operator 68: -1.5913818517632454\n",
      "Operator 69: 0.05183975091600937\n",
      "Operator 70: 0.03224676486478728\n",
      "Operator 71: 1.5721532105157807\n",
      "Operator 72: 0.8829801708412747\n",
      "Operator 73: 0.25235037965539564\n",
      "Operator 74: 0.12764376657304718\n",
      "Operator 75: 0.29970223751506664\n",
      "Operator 76: 0.6958965800111832\n",
      "Operator 77: -0.6678493513625451\n",
      "Operator 78: -0.020448664748900343\n",
      "Operator 79: 1.2812060866293162\n",
      "Operator 80: 0.1371604355169725\n",
      "Operator 81: -0.856553830232237\n",
      "Operator 82: 1.47898578397572\n",
      "Operator 83: 0.05141768444173833\n",
      "Operator 84: 1.4269475454297234\n",
      "Operator 85: -0.17206574831517246\n",
      "Operator 86: 1.6210569611548706\n",
      "Total gradient norm: 8.099944545612264\n",
      "Operators under consideration (1):\n",
      "[52]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.065687308718206)]\n",
      "Operator(s) added to ansatz: [52]\n",
      "Gradients: [np.float64(-2.065687308718206)]\n",
      "Initial energy: -12.782986198614507\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52]...\n",
      "Starting point: [np.float64(0.1332910924239194), np.float64(0.6679670477815225), np.float64(-0.6701654784404043), np.float64(0.5393580550685035), np.float64(0.5780455618694108), np.float64(-0.17444733932471435), np.float64(-0.14924755141184223), np.float64(0.18085949501900853), np.float64(0.1614844557410972), np.float64(-0.11161938397819185), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -12.933768\n",
      "         Iterations: 21\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "\n",
      "Current energy: -12.93376757823575\n",
      "(change of -0.15078137962124316)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.011106817173951053\n",
      "Operator 1: 0.010487884043323625\n",
      "Operator 2: -0.011106819455415584\n",
      "Operator 3: 0.25802227412343043\n",
      "Operator 4: 0.03871418744787469\n",
      "Operator 5: 0.05960772295819799\n",
      "Operator 6: 0.08214734960561246\n",
      "Operator 7: 0.019104790138970967\n",
      "Operator 8: 0.01009363284676025\n",
      "Operator 9: 0.10714652838759697\n",
      "Operator 10: -1.8836496312693165\n",
      "Operator 11: -0.5177573010479287\n",
      "Operator 12: -1.8570443020965155\n",
      "Operator 13: -0.6964773113829292\n",
      "Operator 14: -2.0622340970749646\n",
      "Operator 15: 0.6964773151707208\n",
      "Operator 16: 1.8570443020298442\n",
      "Operator 17: 0.5177573006727826\n",
      "Operator 18: 1.8836496307665715\n",
      "Operator 19: 0.34654720922752424\n",
      "Operator 20: -0.41911487389432395\n",
      "Operator 21: 0.03731249727109812\n",
      "Operator 22: -0.5897608447640827\n",
      "Operator 23: -0.02997989591257757\n",
      "Operator 24: -0.7266882670174686\n",
      "Operator 26: -0.0848564413609474\n",
      "Operator 27: 0.07397179690216282\n",
      "Operator 28: -0.07897157052946989\n",
      "Operator 29: 0.07397179978622459\n",
      "Operator 30: -0.34654721076202666\n",
      "Operator 31: 0.007307987102103569\n",
      "Operator 32: -0.05028750029284656\n",
      "Operator 33: -0.09695121365451179\n",
      "Operator 34: 0.023717747489814006\n",
      "Operator 35: -0.02383831043930762\n",
      "Operator 36: -0.6081478041094516\n",
      "Operator 37: -1.874596383125147\n",
      "Operator 38: -0.38238368160082825\n",
      "Operator 39: -1.9174546575411098\n",
      "Operator 40: -0.5840365146562432\n",
      "Operator 41: 0.5840365186217051\n",
      "Operator 42: 1.9174546584925283\n",
      "Operator 43: 0.38238368176627846\n",
      "Operator 44: 1.87459638322796\n",
      "Operator 45: 0.6081478025713221\n",
      "Operator 46: 0.03871418984255133\n",
      "Operator 47: 0.25802227109071446\n",
      "Operator 48: 1.9797173609290653\n",
      "Operator 49: -0.08485644194126007\n",
      "Operator 51: -0.15659698341738776\n",
      "Operator 54: 0.15659698060191918\n",
      "Operator 55: 0.3159515086727048\n",
      "Operator 56: 2.002346021868326\n",
      "Operator 57: 0.048829874105451775\n",
      "Operator 58: 1.706649714152899\n",
      "Operator 59: 0.012747665074790922\n",
      "Operator 60: 1.8086341488001665\n",
      "Operator 61: -0.2812613759865249\n",
      "Operator 62: 0.5375943119567532\n",
      "Operator 63: 0.002247392881918414\n",
      "Operator 64: 0.6214705420963929\n",
      "Operator 65: 0.12065375564648273\n",
      "Operator 66: -0.008257351051291464\n",
      "Operator 67: 0.17542578849423307\n",
      "Operator 68: -0.17542578855050023\n",
      "Operator 69: 0.008257351288658482\n",
      "Operator 70: 0.008882782472108275\n",
      "Operator 71: 2.0622340986730543\n",
      "Operator 72: 0.6811222285940188\n",
      "Operator 73: -0.02832417363753861\n",
      "Operator 74: 0.4181631560635638\n",
      "Operator 75: 0.021320389168479656\n",
      "Operator 76: 0.677323112246757\n",
      "Operator 77: -0.6773231131940034\n",
      "Operator 78: -0.021320389210553146\n",
      "Operator 79: -0.41816315608811505\n",
      "Operator 80: 0.028324173497717042\n",
      "Operator 81: -0.6811222255953957\n",
      "Operator 82: 1.8216072872629194\n",
      "Operator 83: 0.05576424763688499\n",
      "Operator 84: 1.7178996197465688\n",
      "Operator 85: 0.046511352709503975\n",
      "Operator 86: 1.919461082474022\n",
      "Operator 87: -0.31595150907535197\n",
      "Total gradient norm: 8.303599301596638\n",
      "Operators under consideration (1):\n",
      "[71]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.0622340986730543)]\n",
      "Operator(s) added to ansatz: [71]\n",
      "Gradients: [np.float64(2.0622340986730543)]\n",
      "Initial energy: -12.93376757823575\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71]...\n",
      "Starting point: [np.float64(0.05793518207359171), np.float64(0.7332480174127781), np.float64(-0.7332480168746403), np.float64(0.6980645242776152), np.float64(0.6980645238821529), np.float64(-0.16544146213332805), np.float64(-0.14759857537051838), np.float64(0.16544146233242155), np.float64(0.1475985754317749), np.float64(-0.14515415992161282), np.float64(0.14515415994950273), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -13.083378\n",
      "         Iterations: 20\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n",
      "\n",
      "Current energy: -13.08337827462292\n",
      "(change of -0.14961069638716928)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 10: 0.1712189699661627\n",
      "Operator 11: -0.6066413429517423\n",
      "Operator 12: -1.9725061387077623\n",
      "Operator 13: -0.7170265156403195\n",
      "Operator 14: -2.162697609644974\n",
      "Operator 15: 0.10786980826068068\n",
      "Operator 16: 1.8529611698928032\n",
      "Operator 17: 0.5947190602912957\n",
      "Operator 18: 1.9575737869213747\n",
      "Operator 19: 0.3716766895336248\n",
      "Operator 20: 0.2474039337722083\n",
      "Operator 21: 0.13961988125909275\n",
      "Operator 22: -0.7066776848523176\n",
      "Operator 23: -0.009285588338418704\n",
      "Operator 24: -0.69159057485496\n",
      "Operator 30: 0.31629300822582473\n",
      "Operator 36: 0.01975476921002503\n",
      "Operator 37: -1.9094650927784438\n",
      "Operator 38: -0.5202907248021855\n",
      "Operator 39: -1.976070906668821\n",
      "Operator 40: -0.6062207067502057\n",
      "Operator 41: -0.03444872834809717\n",
      "Operator 42: -0.14121733356753025\n",
      "Operator 43: 0.43766799088230934\n",
      "Operator 44: 1.989857373343214\n",
      "Operator 45: 0.6414448446201666\n",
      "Operator 48: 2.073404895893666\n",
      "Operator 51: -0.1894780281707606\n",
      "Operator 52: 0.12701258514038075\n",
      "Operator 54: 0.11449973578342326\n",
      "Operator 55: 0.3405203700047732\n",
      "Operator 56: -0.04586279620868697\n",
      "Operator 57: 0.0043000322541782605\n",
      "Operator 58: 1.826327848017124\n",
      "Operator 59: 0.05431082717396239\n",
      "Operator 60: 1.8829911352619693\n",
      "Operator 61: -0.2569838004340219\n",
      "Operator 62: 0.08592188205277407\n",
      "Operator 63: 0.033650932788353405\n",
      "Operator 64: 0.5854852453569949\n",
      "Operator 65: 0.11716238210669097\n",
      "Operator 67: 0.1493522364056158\n",
      "Operator 68: -0.10307136424911452\n",
      "Operator 72: 0.004948410100410236\n",
      "Operator 73: -0.017099846261391757\n",
      "Operator 74: 0.4760034877743759\n",
      "Operator 75: 0.02269739295874504\n",
      "Operator 76: 0.66803759593037\n",
      "Operator 77: 0.040548702594866134\n",
      "Operator 78: 0.023671031006761226\n",
      "Operator 79: -0.5152004503587427\n",
      "Operator 80: 0.008909714470128088\n",
      "Operator 81: -0.6328120192817459\n",
      "Operator 82: -0.16547827068893622\n",
      "Operator 83: 0.059592920151317376\n",
      "Operator 84: 1.7338701918670716\n",
      "Operator 85: 0.05286711488513457\n",
      "Operator 86: 1.9959025308032068\n",
      "Operator 87: -0.3092836470917625\n",
      "Total gradient norm: 7.17437855817589\n",
      "Operators under consideration (1):\n",
      "[14]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.162697609644974)]\n",
      "Operator(s) added to ansatz: [14]\n",
      "Gradients: [np.float64(-2.162697609644974)]\n",
      "Initial energy: -13.08337827462292\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14]...\n",
      "Starting point: [np.float64(-6.364481799881169e-08), np.float64(0.7853981635953788), np.float64(-0.7853981635456729), np.float64(0.7853981637135172), np.float64(0.7853981636746586), np.float64(-0.16300085681938695), np.float64(-0.14743884489472808), np.float64(0.1738447132154868), np.float64(0.11570202636030062), np.float64(-0.1512269688315772), np.float64(0.15531399736829593), np.float64(-0.13924871869114458), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.234246\n",
      "         Iterations: 15\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 29\n",
      "\n",
      "Current energy: -13.234245879497347\n",
      "(change of -0.15086760487442774)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.02551694722386e-08\n",
      "Operator 1: -2.2226841936076602e-07\n",
      "Operator 2: 2.26933813685104e-08\n",
      "Operator 3: -1.2997699079531078e-07\n",
      "Operator 4: 1.213591342974496e-07\n",
      "Operator 5: 3.8118756426949035e-08\n",
      "Operator 7: -4.703633482705527e-08\n",
      "Operator 8: 3.860149500045651e-08\n",
      "Operator 9: -6.005027494704329e-08\n",
      "Operator 10: 0.18245105502577616\n",
      "Operator 11: -0.65088973295822\n",
      "Operator 12: -1.852637454147271\n",
      "Operator 13: -0.11965114515354995\n",
      "Operator 15: 0.1196511528275708\n",
      "Operator 16: 1.85263745434798\n",
      "Operator 17: 0.650889728032772\n",
      "Operator 18: -0.18245105330918338\n",
      "Operator 19: -0.3301897116897698\n",
      "Operator 20: 0.2533283253819589\n",
      "Operator 21: 0.14415497463163943\n",
      "Operator 22: -0.7643895631337619\n",
      "Operator 23: 0.1742318606887216\n",
      "Operator 24: -0.08071257150213701\n",
      "Operator 25: 4.046159681391978e-08\n",
      "Operator 26: 1.989601158911697e-08\n",
      "Operator 27: -4.8961250810608536e-08\n",
      "Operator 28: 1.875985546247183e-08\n",
      "Operator 29: -1.2562090975776208e-07\n",
      "Operator 30: 0.3301897296158019\n",
      "Operator 31: -3.585737752828688e-08\n",
      "Operator 32: -1.462517398680141e-08\n",
      "Operator 33: -1.5855983470647317e-08\n",
      "Operator 35: 9.40236130619887e-08\n",
      "Operator 36: 0.01886346242222328\n",
      "Operator 37: -1.8868840603685422\n",
      "Operator 38: -0.5520186597347414\n",
      "Operator 39: 0.14788662090987512\n",
      "Operator 40: 0.03590868535328878\n",
      "Operator 41: -0.03590867725909122\n",
      "Operator 42: -0.14788661357947797\n",
      "Operator 43: 0.5520186531093453\n",
      "Operator 44: 1.8868840585273787\n",
      "Operator 45: -0.01886344425476408\n",
      "Operator 46: -3.724310134248232e-08\n",
      "Operator 47: -1.1796727529718443e-08\n",
      "Operator 48: -0.12182797386413752\n",
      "Operator 49: 4.379145342181534e-08\n",
      "Operator 50: -1.5002600828317725e-07\n",
      "Operator 51: -0.20440512481291798\n",
      "Operator 52: 0.1301706587203314\n",
      "Operator 53: -0.13017065283647583\n",
      "Operator 54: 0.20440511219661822\n",
      "Operator 55: 0.3228560617404644\n",
      "Operator 56: -0.0509497139625081\n",
      "Operator 57: 0.013328695760135362\n",
      "Operator 58: 1.7358044999459028\n",
      "Operator 59: 0.06362923348090879\n",
      "Operator 60: -0.21032528525963354\n",
      "Operator 61: -0.26444358784151306\n",
      "Operator 62: 0.08497139082944939\n",
      "Operator 63: 0.08239850564323752\n",
      "Operator 64: 0.4378392691781886\n",
      "Operator 65: 0.16992970707380206\n",
      "Operator 66: -1.151152868846853e-08\n",
      "Operator 67: 0.0499083221825156\n",
      "Operator 68: -0.0499083136539061\n",
      "Operator 69: -1.1497740803977936e-08\n",
      "Operator 70: 0.09477575754902556\n",
      "Operator 72: 0.005524157292813392\n",
      "Operator 73: -0.011692000645671212\n",
      "Operator 74: 0.5744350058896132\n",
      "Operator 75: -0.0454129911031087\n",
      "Operator 76: -0.04251377011134192\n",
      "Operator 77: 0.042513788902377246\n",
      "Operator 78: 0.04541299004456964\n",
      "Operator 79: -0.5744350122000614\n",
      "Operator 80: 0.01169200692928964\n",
      "Operator 81: -0.005524148975104027\n",
      "Operator 82: -0.17465395270954576\n",
      "Operator 83: 0.05815923042988541\n",
      "Operator 84: 1.746583867581592\n",
      "Operator 85: 0.04087081188096469\n",
      "Operator 86: -0.1532009771488203\n",
      "Operator 87: -0.3228560838899716\n",
      "Total gradient norm: 4.89992872086352\n",
      "Operators under consideration (1):\n",
      "[37]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.8868840603685422)]\n",
      "Operator(s) added to ansatz: [37]\n",
      "Gradients: [np.float64(-1.8868840603685422)]\n",
      "Initial energy: -13.234245879497347\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37]...\n",
      "Starting point: [np.float64(-1.7609779627106563e-08), np.float64(0.785398162798295), np.float64(-0.7853981849421807), np.float64(0.7853981796564445), np.float64(0.7853981924886936), np.float64(-0.1743177701916302), np.float64(-0.11511660248605084), np.float64(0.17431777053802755), np.float64(0.11511660121935681), np.float64(-0.15940805356003046), np.float64(0.15940805401041344), np.float64(-0.13975979823373444), np.float64(0.1397597982775777), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.339218\n",
      "         Iterations: 23\n",
      "         Function evaluations: 91\n",
      "         Gradient evaluations: 78\n",
      "\n",
      "Current energy: -13.339217758437652\n",
      "(change of -0.10497187894030446)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -6.682284936498917e-08\n",
      "Operator 1: -2.838859883974587e-08\n",
      "Operator 2: 8.124874806281213e-08\n",
      "Operator 3: 2.808558682843021e-08\n",
      "Operator 4: -7.012943159523388e-08\n",
      "Operator 5: -7.751664758345878e-08\n",
      "Operator 6: 1.3783952059615023e-08\n",
      "Operator 7: 5.278385645235417e-08\n",
      "Operator 9: -7.023496767510906e-08\n",
      "Operator 10: 0.20398868812021154\n",
      "Operator 11: -0.06029311662790207\n",
      "Operator 12: -0.2772512013572112\n",
      "Operator 13: -0.10765948361503544\n",
      "Operator 14: 5.721538415038378e-07\n",
      "Operator 15: -0.11637497330476415\n",
      "Operator 16: 0.21839840454747694\n",
      "Operator 17: 0.13669145902755936\n",
      "Operator 18: -0.2196481355168581\n",
      "Operator 19: -0.25521881825370885\n",
      "Operator 20: 0.25202150235401544\n",
      "Operator 21: 0.47113916349654006\n",
      "Operator 22: -0.20315423309908756\n",
      "Operator 23: 0.17548272356518713\n",
      "Operator 24: -0.10367344056053225\n",
      "Operator 25: -8.307951970126792e-08\n",
      "Operator 27: 4.262520954256832e-08\n",
      "Operator 29: -7.809374175105077e-08\n",
      "Operator 30: 0.34180333018372094\n",
      "Operator 31: 6.218225792262166e-08\n",
      "Operator 32: -1.8730868051042293e-08\n",
      "Operator 33: -4.6102562080430065e-08\n",
      "Operator 35: 9.016667501526854e-08\n",
      "Operator 36: 0.008145470119835792\n",
      "Operator 37: 2.903671052518051e-08\n",
      "Operator 38: -0.02737698177451304\n",
      "Operator 39: 0.2053810455404647\n",
      "Operator 40: 0.030318715800205034\n",
      "Operator 41: -0.22195371695666527\n",
      "Operator 42: -0.11186995596655228\n",
      "Operator 43: 0.029754065467679666\n",
      "Operator 44: 0.3283138104087302\n",
      "Operator 45: 0.0051929898480277134\n",
      "Operator 46: 6.8294371227684e-08\n",
      "Operator 47: -4.933198208267875e-08\n",
      "Operator 48: -0.09434724195455231\n",
      "Operator 50: 8.191192391038271e-08\n",
      "Operator 51: -0.22047110533716444\n",
      "Operator 52: 0.08403791128219996\n",
      "Operator 53: -0.1266540958598118\n",
      "Operator 54: 0.2137117680443242\n",
      "Operator 55: 0.24933299386725588\n",
      "Operator 56: -0.08475936252555918\n",
      "Operator 57: 0.07323033528347198\n",
      "Operator 58: 0.10159471240561029\n",
      "Operator 59: -0.030260382327579775\n",
      "Operator 60: -0.17643999983323722\n",
      "Operator 61: -0.24978211705644074\n",
      "Operator 62: -0.22692964473335786\n",
      "Operator 63: 0.024000001162817128\n",
      "Operator 64: 0.07152038467420484\n",
      "Operator 65: 0.14845431649005913\n",
      "Operator 66: -0.007215486983773533\n",
      "Operator 67: -0.02778870539046549\n",
      "Operator 68: -0.05485188179823209\n",
      "Operator 69: -0.08703010998652265\n",
      "Operator 70: 0.07357400204346004\n",
      "Operator 71: 0.16183920703389604\n",
      "Operator 72: -0.018175997444130308\n",
      "Operator 73: 0.030539115383680284\n",
      "Operator 74: 0.05110686514486167\n",
      "Operator 75: -0.1162733637078173\n",
      "Operator 76: 0.037285862806973574\n",
      "Operator 77: 0.04863253563585784\n",
      "Operator 78: 0.05607404396946614\n",
      "Operator 79: 0.049837135742942525\n",
      "Operator 80: 0.1551263971920198\n",
      "Operator 81: -0.038702516519035605\n",
      "Operator 82: 0.024036790921801634\n",
      "Operator 83: 0.031771839375619615\n",
      "Operator 84: -0.01548627443661252\n",
      "Operator 85: -0.07656530771057214\n",
      "Operator 86: -0.1897143205003996\n",
      "Operator 87: -0.33567601788104573\n",
      "Total gradient norm: 1.2896655125898249\n",
      "Operators under consideration (1):\n",
      "[21]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.47113916349654006)]\n",
      "Operator(s) added to ansatz: [21]\n",
      "Gradients: [np.float64(0.47113916349654006)]\n",
      "Initial energy: -13.339217758437652\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21]...\n",
      "Starting point: [np.float64(-4.172638403464783e-08), np.float64(0.7853981723395802), np.float64(-0.7853981564565224), np.float64(0.7853981628954663), np.float64(0.78539816962949), np.float64(-0.1759612194815905), np.float64(-0.11961642798797487), np.float64(0.15902793344380423), np.float64(0.11545062711827693), np.float64(-0.13289931470719182), np.float64(0.1634066381088286), np.float64(-0.12844795753420937), np.float64(0.1403344873096234), np.float64(0.11165370672512427), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.344008\n",
      "         Iterations: 17\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "Current energy: -13.344007889667786\n",
      "(change of -0.004790131230134165)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -6.228357771614901e-07\n",
      "Operator 1: 1.238672961423326e-08\n",
      "Operator 2: 6.279456753743862e-07\n",
      "Operator 3: -7.612673488532895e-07\n",
      "Operator 4: 9.190482965770741e-07\n",
      "Operator 5: 9.001752932676077e-07\n",
      "Operator 6: -1.3770454728324194e-07\n",
      "Operator 7: 5.764589094438277e-07\n",
      "Operator 8: -6.738429515242006e-08\n",
      "Operator 9: -8.34707799879375e-07\n",
      "Operator 10: 0.22988763268489046\n",
      "Operator 11: -0.20061951865079866\n",
      "Operator 12: -0.23307125670639306\n",
      "Operator 13: -0.10928480486327319\n",
      "Operator 14: 5.391557890945938e-07\n",
      "Operator 15: 0.016441273200319212\n",
      "Operator 16: 0.1740405023935658\n",
      "Operator 17: 0.1635719884610349\n",
      "Operator 18: -0.22531178069175636\n",
      "Operator 19: -0.26149618134955593\n",
      "Operator 20: 0.2201539364352707\n",
      "Operator 21: 1.0504398341277402e-06\n",
      "Operator 22: -0.22092818087893118\n",
      "Operator 23: 0.18741644916630085\n",
      "Operator 24: -0.10502468476849489\n",
      "Operator 25: 1.0005167477220023e-06\n",
      "Operator 26: -1.9164645550292375e-07\n",
      "Operator 27: 7.691030656682696e-07\n",
      "Operator 28: 1.737328901227353e-08\n",
      "Operator 29: -8.229500461920392e-07\n",
      "Operator 30: 0.31318720750260853\n",
      "Operator 31: -7.869753988597877e-07\n",
      "Operator 32: 1.671952225817258e-07\n",
      "Operator 33: -6.973595663272575e-07\n",
      "Operator 34: 5.135603608694139e-08\n",
      "Operator 35: 6.236096683487347e-07\n",
      "Operator 36: -0.02281596593389301\n",
      "Operator 37: -2.5149097397688348e-08\n",
      "Operator 38: -0.050590266051692694\n",
      "Operator 39: 0.2116464133217011\n",
      "Operator 40: 0.030936735181741358\n",
      "Operator 41: -0.20189557030616495\n",
      "Operator 42: -0.14342113359589004\n",
      "Operator 43: 0.06343768012739943\n",
      "Operator 44: 0.29623066943423726\n",
      "Operator 45: 0.0035583580373367532\n",
      "Operator 46: -8.744092158084749e-07\n",
      "Operator 47: 5.561354378957883e-07\n",
      "Operator 48: -0.0966910741170338\n",
      "Operator 49: 1.7851534366009059e-07\n",
      "Operator 50: -1.1228736691570873e-06\n",
      "Operator 51: -0.20267930002708104\n",
      "Operator 52: 0.10885063743520035\n",
      "Operator 53: -0.1630846301896792\n",
      "Operator 54: 0.21823691392408476\n",
      "Operator 55: 0.2554773007961733\n",
      "Operator 56: -0.12581494203672114\n",
      "Operator 57: 0.10046849205855983\n",
      "Operator 58: 0.05497581755846122\n",
      "Operator 59: -0.030030503599904963\n",
      "Operator 60: -0.18023094464204154\n",
      "Operator 61: -0.2028345922912232\n",
      "Operator 62: -0.10991337782951902\n",
      "Operator 63: 0.1562617690941938\n",
      "Operator 64: 0.05894065120459062\n",
      "Operator 65: 0.1513743968840555\n",
      "Operator 66: -0.0029558483716704215\n",
      "Operator 67: -0.01980506238574719\n",
      "Operator 68: -0.02995784910748259\n",
      "Operator 69: -0.0891520615006355\n",
      "Operator 70: 0.07553791029557694\n",
      "Operator 71: 0.13348940236348733\n",
      "Operator 72: 0.11817596600299037\n",
      "Operator 73: 0.05358056244884298\n",
      "Operator 74: 0.08421445258616478\n",
      "Operator 75: -0.12670955761453154\n",
      "Operator 76: 0.038110138117995566\n",
      "Operator 77: 0.013669923331298801\n",
      "Operator 78: 0.019956549502288617\n",
      "Operator 79: -0.1095588757511127\n",
      "Operator 80: 0.1609246703869528\n",
      "Operator 81: -0.04080891917923326\n",
      "Operator 82: -0.001911743239147802\n",
      "Operator 83: 0.03161028340256315\n",
      "Operator 84: -0.06780662800220838\n",
      "Operator 85: -0.06989909728117856\n",
      "Operator 86: -0.19575667193390492\n",
      "Operator 87: -0.3021480145574146\n",
      "Total gradient norm: 1.1840110161839936\n",
      "Operators under consideration (1):\n",
      "[30]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.31318720750260853)]\n",
      "Operator(s) added to ansatz: [30]\n",
      "Gradients: [np.float64(0.31318720750260853)]\n",
      "Initial energy: -13.344007889667786\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30]...\n",
      "Starting point: [np.float64(-3.860676430737818e-08), np.float64(0.785398045776144), np.float64(-0.785398300216909), np.float64(0.785398052210152), np.float64(0.7853982709883812), np.float64(-0.17611281810263074), np.float64(-0.11934865086809408), np.float64(0.16550567403359656), np.float64(0.11644985105827142), np.float64(-0.13465578177064486), np.float64(0.16505643130463724), np.float64(-0.12969480506086192), np.float64(0.14060691374355389), np.float64(0.1145739261685191), np.float64(-0.02035089513104585), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.376729\n",
      "         Iterations: 25\n",
      "         Function evaluations: 80\n",
      "         Gradient evaluations: 67\n",
      "\n",
      "Current energy: -13.37672909807289\n",
      "(change of -0.0327212084051034)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.314911752067996e-07\n",
      "Operator 1: 2.836612970069652e-07\n",
      "Operator 2: -3.43461740826001e-07\n",
      "Operator 3: 3.371103673988074e-08\n",
      "Operator 4: 1.2189599107870208e-07\n",
      "Operator 5: 4.434039786727073e-07\n",
      "Operator 6: -2.417241135727055e-08\n",
      "Operator 7: -3.041672957948549e-07\n",
      "Operator 8: -7.93235366627898e-08\n",
      "Operator 9: 3.4181014685330524e-07\n",
      "Operator 10: 0.12773339237650913\n",
      "Operator 11: -0.12465780485790062\n",
      "Operator 12: -0.2764319780640849\n",
      "Operator 13: -0.14100207607261572\n",
      "Operator 14: 3.47728135755158e-07\n",
      "Operator 15: -0.02573419467967765\n",
      "Operator 16: 0.2389258004048601\n",
      "Operator 17: 0.2194386034074546\n",
      "Operator 18: -0.24711662754266256\n",
      "Operator 19: -0.2703531593057743\n",
      "Operator 20: -0.04547433546702699\n",
      "Operator 21: 0.0717919283577862\n",
      "Operator 22: -0.14510692502097894\n",
      "Operator 23: 0.18374691292329892\n",
      "Operator 24: -0.13625416096887447\n",
      "Operator 25: 5.103681475493449e-07\n",
      "Operator 26: -8.717791184949253e-08\n",
      "Operator 27: -3.7251317083764057e-07\n",
      "Operator 28: -3.570743178915592e-08\n",
      "Operator 29: 4.2481710682551466e-07\n",
      "Operator 30: -2.358002156739723e-07\n",
      "Operator 31: -4.910170791488921e-07\n",
      "Operator 32: 1.0623814106951526e-07\n",
      "Operator 33: 4.2622760099142454e-07\n",
      "Operator 34: 2.411432828419713e-08\n",
      "Operator 35: -4.371521513668597e-07\n",
      "Operator 36: -0.061060104452025504\n",
      "Operator 37: -4.174794520860664e-08\n",
      "Operator 38: -0.26834691242541225\n",
      "Operator 39: 0.221336057354031\n",
      "Operator 40: 0.03247961278291729\n",
      "Operator 41: -0.18474198168910616\n",
      "Operator 42: 0.006130726741987807\n",
      "Operator 43: 0.0709948820480788\n",
      "Operator 44: 0.26737998441534905\n",
      "Operator 45: 0.01783394852326986\n",
      "Operator 46: -5.476263246900759e-07\n",
      "Operator 47: -3.178905080836181e-08\n",
      "Operator 48: -0.10070114842187977\n",
      "Operator 49: -6.016332976255256e-08\n",
      "Operator 50: -1.3263640141758515e-07\n",
      "Operator 51: -0.1666623870168223\n",
      "Operator 52: 0.0020663424634975646\n",
      "Operator 53: -0.13397419631234667\n",
      "Operator 54: 0.25130622045286033\n",
      "Operator 55: 0.2639863069381916\n",
      "Operator 56: -0.08984397744456629\n",
      "Operator 57: 0.14879658096723916\n",
      "Operator 58: 0.08242585985077958\n",
      "Operator 59: -0.014290865988448715\n",
      "Operator 60: -0.20150253753799652\n",
      "Operator 61: 0.04263174786998161\n",
      "Operator 62: -0.19125718397832975\n",
      "Operator 63: 0.0798176659672675\n",
      "Operator 64: 0.05833399808213177\n",
      "Operator 65: 0.1851061711838789\n",
      "Operator 66: -0.00010049366814215594\n",
      "Operator 67: -0.06030795151348631\n",
      "Operator 68: 0.1780253126784675\n",
      "Operator 69: -0.0984885657224746\n",
      "Operator 70: 0.07856764746619718\n",
      "Operator 71: 0.16231353019285957\n",
      "Operator 72: 0.08187661580144284\n",
      "Operator 73: 0.11948484184058278\n",
      "Operator 74: 0.13318667120840594\n",
      "Operator 75: -0.16418099055093527\n",
      "Operator 76: 0.04425748216061211\n",
      "Operator 77: -0.02029985680991396\n",
      "Operator 78: -0.01114908574247669\n",
      "Operator 79: -0.2201888599940529\n",
      "Operator 80: 0.146236111641543\n",
      "Operator 81: -0.041619262186765836\n",
      "Operator 82: 0.1110893729353869\n",
      "Operator 83: 0.0011715190562718962\n",
      "Operator 84: -0.0887043370991919\n",
      "Operator 85: -0.024761464103035476\n",
      "Operator 86: -0.2021260603957291\n",
      "Operator 87: -0.00017052744095834096\n",
      "Total gradient norm: 1.1461137989419008\n",
      "Operators under consideration (1):\n",
      "[12]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.2764319780640849)]\n",
      "Operator(s) added to ansatz: [12]\n",
      "Gradients: [np.float64(-0.2764319780640849)]\n",
      "Initial energy: -13.37672909807289\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12]...\n",
      "Starting point: [np.float64(-1.798005627084892e-06), np.float64(0.7853980938715032), np.float64(-0.7853981658605631), np.float64(0.7853981577227672), np.float64(0.7853981123171935), np.float64(-0.17748359916162404), np.float64(-0.11925848665163877), np.float64(0.10296076254125706), np.float64(-0.06350295374483392), np.float64(-0.1365674082263404), np.float64(0.18691751741446772), np.float64(-0.17633792130569056), np.float64(0.14141111396029105), np.float64(0.11313164025280459), np.float64(-0.0004490979452716304), np.float64(-0.20285301796041982), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.384815\n",
      "         Iterations: 22\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 43\n",
      "\n",
      "Current energy: -13.384815448772038\n",
      "(change of -0.008086350699148426)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.3209912551465086e-06\n",
      "Operator 1: -6.59301552027175e-06\n",
      "Operator 2: -4.517466853370955e-07\n",
      "Operator 3: -3.2891509649246586e-07\n",
      "Operator 4: -1.0347369974184062e-06\n",
      "Operator 5: -3.7822265806439215e-06\n",
      "Operator 6: 1.355358976889206e-07\n",
      "Operator 7: -3.030892513029848e-06\n",
      "Operator 8: 1.8513152330347504e-06\n",
      "Operator 9: 4.995025733034453e-07\n",
      "Operator 10: 0.18484394921655486\n",
      "Operator 11: -0.06556126056482425\n",
      "Operator 12: 2.064187107292482e-07\n",
      "Operator 13: -0.1338251683354213\n",
      "Operator 14: -0.09547105612913281\n",
      "Operator 15: 0.06795076442087863\n",
      "Operator 16: 0.15304647640179808\n",
      "Operator 17: 0.14112562794891859\n",
      "Operator 18: -0.22621912635043406\n",
      "Operator 19: -0.3602387565468541\n",
      "Operator 20: -0.06376552220626726\n",
      "Operator 21: 0.07815927908277404\n",
      "Operator 22: -0.06596389719595608\n",
      "Operator 23: 0.20770481819556735\n",
      "Operator 24: -0.12668119973246214\n",
      "Operator 25: -4.532960508427802e-06\n",
      "Operator 26: 1.4575873926916255e-06\n",
      "Operator 27: -2.7991320992182267e-06\n",
      "Operator 28: 1.3374957834669243e-06\n",
      "Operator 29: -1.3434330875528033e-06\n",
      "Operator 30: 6.259436225959572e-07\n",
      "Operator 31: 4.51231217500625e-06\n",
      "Operator 32: -1.2367920461018277e-06\n",
      "Operator 33: 7.012046720211718e-07\n",
      "Operator 34: -8.344918890970665e-07\n",
      "Operator 35: 1.5239136380573726e-06\n",
      "Operator 36: -0.046309094794036464\n",
      "Operator 37: 1.302509542288349e-07\n",
      "Operator 38: -0.1963577140278888\n",
      "Operator 39: 0.24631235574162108\n",
      "Operator 40: 0.037473216596857095\n",
      "Operator 41: -0.06622778640494437\n",
      "Operator 42: -0.07995783316071728\n",
      "Operator 43: 0.03636130887123391\n",
      "Operator 44: 0.10943510867868178\n",
      "Operator 45: -0.07505451637572282\n",
      "Operator 46: 5.214769762222626e-06\n",
      "Operator 47: 3.668369503509461e-07\n",
      "Operator 48: -0.03900552320674824\n",
      "Operator 49: 8.965134412064479e-07\n",
      "Operator 50: 1.2458079954313916e-06\n",
      "Operator 51: -0.22961439979562198\n",
      "Operator 52: 0.006080062409257063\n",
      "Operator 53: -0.15378017733402774\n",
      "Operator 54: 0.23076821208158782\n",
      "Operator 55: 0.3801615612532443\n",
      "Operator 56: -0.1297894089678917\n",
      "Operator 57: 0.028064327825017915\n",
      "Operator 58: -0.11889101998675637\n",
      "Operator 59: 0.009994003656406063\n",
      "Operator 60: -0.12722834483694057\n",
      "Operator 61: 0.047830427046001156\n",
      "Operator 62: -0.09510157301263082\n",
      "Operator 63: 0.04810999903495076\n",
      "Operator 64: -0.11942908777996716\n",
      "Operator 65: 0.18427653447477566\n",
      "Operator 66: 0.007102674257350837\n",
      "Operator 67: -0.02686174844509221\n",
      "Operator 68: 0.1490689980773236\n",
      "Operator 69: -0.10124491857732672\n",
      "Operator 70: 0.07910638007979419\n",
      "Operator 71: 0.06723936129024809\n",
      "Operator 72: 0.10860024540521421\n",
      "Operator 73: 0.10253273079663812\n",
      "Operator 74: 0.10065255777380824\n",
      "Operator 75: -0.09746439582545141\n",
      "Operator 76: 0.03129106715184456\n",
      "Operator 77: -0.017566336371761738\n",
      "Operator 78: -0.03871771022878159\n",
      "Operator 79: -0.1722698205631879\n",
      "Operator 80: 0.2011762009727314\n",
      "Operator 81: -0.02493184257041862\n",
      "Operator 82: 0.005947933081765833\n",
      "Operator 83: -0.030693662932421387\n",
      "Operator 84: -0.16596455116896894\n",
      "Operator 85: 0.05294327082541482\n",
      "Operator 86: -0.22437187277974877\n",
      "Operator 87: 0.0004964670234200572\n",
      "Total gradient norm: 1.0617950642710507\n",
      "Operators under consideration (1):\n",
      "[55]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.3801615612532443)]\n",
      "Operator(s) added to ansatz: [55]\n",
      "Gradients: [np.float64(0.3801615612532443)]\n",
      "Initial energy: -13.384815448772038\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55]...\n",
      "Starting point: [np.float64(-7.30130421687019e-07), np.float64(0.7853989683997695), np.float64(-0.7853981636178531), np.float64(0.785399008484464), np.float64(0.7853985195936203), np.float64(-0.1743443216979231), np.float64(-0.11374746856530431), np.float64(0.10481361687051421), np.float64(-0.07053841236428163), np.float64(-0.13363224083737327), np.float64(0.18711518806524777), np.float64(-0.18759942491413006), np.float64(0.1357200061295993), np.float64(0.06318005464335762), np.float64(0.004965619431577702), np.float64(-0.2107206846790803), np.float64(0.05857900874951066), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.441296\n",
      "         Iterations: 26\n",
      "         Function evaluations: 82\n",
      "         Gradient evaluations: 70\n",
      "\n",
      "Current energy: -13.441295872877433\n",
      "(change of -0.05648042410539489)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 1: -2.3478733970905274e-08\n",
      "Operator 2: -2.0740505202176962e-08\n",
      "Operator 3: -1.7710649558022595e-08\n",
      "Operator 4: 2.4620554484044987e-08\n",
      "Operator 5: -1.814920341271231e-08\n",
      "Operator 9: 1.4841735174186965e-08\n",
      "Operator 10: 0.18523781666097439\n",
      "Operator 11: -0.07465341034127701\n",
      "Operator 12: -0.02524691138451427\n",
      "Operator 13: -0.34330986567872834\n",
      "Operator 14: -0.22232807085701586\n",
      "Operator 15: 0.15293719234097158\n",
      "Operator 16: 0.07804091246561437\n",
      "Operator 17: 0.2372654011631587\n",
      "Operator 18: -0.07340803968132352\n",
      "Operator 19: 0.10185090316469887\n",
      "Operator 20: -0.10039819186257602\n",
      "Operator 21: 0.05275590785335064\n",
      "Operator 22: -0.14018465869845237\n",
      "Operator 23: -0.1240407290659537\n",
      "Operator 24: -0.2079589945244542\n",
      "Operator 25: -2.2535697009884448e-08\n",
      "Operator 30: 8.288200296350513e-08\n",
      "Operator 31: 2.215512669850872e-08\n",
      "Operator 35: -1.4518234170424194e-08\n",
      "Operator 36: -0.04827524970113173\n",
      "Operator 37: 6.512193157516529e-07\n",
      "Operator 38: -0.31002806730994165\n",
      "Operator 39: 0.08784927648931165\n",
      "Operator 40: -0.1209948964696392\n",
      "Operator 41: 0.020857480374785117\n",
      "Operator 42: -0.14928516620037552\n",
      "Operator 43: 0.23040962521962033\n",
      "Operator 44: -0.051424212849948345\n",
      "Operator 45: 0.22607756260649514\n",
      "Operator 46: 2.652171778028123e-08\n",
      "Operator 48: 0.2345406415219975\n",
      "Operator 50: -3.150852023736661e-08\n",
      "Operator 51: -0.22967311213531125\n",
      "Operator 52: 0.0394980456134365\n",
      "Operator 53: -0.09797150706234153\n",
      "Operator 54: 0.20879837639008003\n",
      "Operator 55: -1.10747218342685e-07\n",
      "Operator 56: -0.13983622045226005\n",
      "Operator 57: -0.13922164853153496\n",
      "Operator 58: -0.04227606623123124\n",
      "Operator 59: 0.3144901371925899\n",
      "Operator 60: 0.10106508194464153\n",
      "Operator 61: 0.07600540277530697\n",
      "Operator 62: 0.08061854562488519\n",
      "Operator 63: 0.058106066317135406\n",
      "Operator 64: -0.07412506806042268\n",
      "Operator 65: 0.2474955654004241\n",
      "Operator 66: 0.030238434370751903\n",
      "Operator 67: -0.20938926222737947\n",
      "Operator 68: 0.1486278044029669\n",
      "Operator 69: -0.0007871045123329155\n",
      "Operator 70: 0.0043443052526343055\n",
      "Operator 71: -0.00418093479681308\n",
      "Operator 72: 0.06512555996764224\n",
      "Operator 73: 0.09724527670426988\n",
      "Operator 74: 0.339352293300354\n",
      "Operator 75: 0.09254676586569248\n",
      "Operator 76: -0.0103926275132951\n",
      "Operator 77: -0.006098989212528963\n",
      "Operator 78: -0.03974724592673677\n",
      "Operator 79: -0.23214984568182845\n",
      "Operator 80: -0.17357525382434202\n",
      "Operator 81: -0.10857229498534404\n",
      "Operator 82: -0.1090759599503949\n",
      "Operator 83: -0.05295749147507606\n",
      "Operator 84: 0.08227363086369863\n",
      "Operator 85: 0.2457910353035666\n",
      "Operator 86: -0.1504717761005121\n",
      "Operator 87: -0.004261547194854032\n",
      "Total gradient norm: 1.2049960964436248\n",
      "Operators under consideration (1):\n",
      "[13]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.34330986567872834)]\n",
      "Operator(s) added to ansatz: [13]\n",
      "Gradients: [np.float64(-0.34330986567872834)]\n",
      "Initial energy: -13.441295872877433\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13]...\n",
      "Starting point: [np.float64(5.340095904302803e-06), np.float64(0.7853981669799015), np.float64(-0.7853981669936184), np.float64(0.7853981664851449), np.float64(0.7853981639796961), np.float64(-0.18293456985832293), np.float64(0.06991025550077681), np.float64(0.10538955509476843), np.float64(-0.07835749619995981), np.float64(-0.15461283175967488), np.float64(0.19929793851205416), np.float64(-0.2015161511145909), np.float64(0.12269882338276125), np.float64(-0.00647911180021987), np.float64(0.013389664439316648), np.float64(-0.22055260956175277), np.float64(0.13685869397860798), np.float64(-0.2105059990391503), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.454807\n",
      "         Iterations: 20\n",
      "         Function evaluations: 71\n",
      "         Gradient evaluations: 60\n",
      "\n",
      "Current energy: -13.454807413368362\n",
      "(change of -0.013511540490929619)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.904277128928934e-07\n",
      "Operator 1: 6.291349963216675e-07\n",
      "Operator 2: 2.6397171269954e-06\n",
      "Operator 3: 2.692103418466818e-06\n",
      "Operator 4: -4.170417873261556e-06\n",
      "Operator 5: 1.0625954148055922e-06\n",
      "Operator 6: -1.4556630059756742e-08\n",
      "Operator 7: -1.1202553112932279e-07\n",
      "Operator 8: 3.1477099815546694e-08\n",
      "Operator 9: -1.776543714915313e-06\n",
      "Operator 10: 0.15994327216887383\n",
      "Operator 11: -0.04790672626548892\n",
      "Operator 12: -0.0287259258632597\n",
      "Operator 14: -0.20900399122454358\n",
      "Operator 15: 0.15246340513689785\n",
      "Operator 16: 0.035387545388521194\n",
      "Operator 17: -0.07173951332707278\n",
      "Operator 18: -0.13020540588835638\n",
      "Operator 19: 0.13803292132008405\n",
      "Operator 20: -0.10516320110720356\n",
      "Operator 21: 0.03329789654566006\n",
      "Operator 22: -0.17122367884411616\n",
      "Operator 23: 0.3297833875081074\n",
      "Operator 24: -0.11810254722512974\n",
      "Operator 25: 1.2973902448927543e-06\n",
      "Operator 26: -2.2227251659200087e-07\n",
      "Operator 27: -3.1383455244282654e-07\n",
      "Operator 28: -2.2398153198127657e-07\n",
      "Operator 29: -1.6403625798799837e-06\n",
      "Operator 30: 7.170064335646796e-07\n",
      "Operator 31: -1.2797660143004652e-06\n",
      "Operator 32: 1.3574915280401196e-07\n",
      "Operator 33: 4.203993150264651e-07\n",
      "Operator 34: -7.466941326535192e-08\n",
      "Operator 35: 2.4999408915205024e-06\n",
      "Operator 36: -0.04415453902530793\n",
      "Operator 37: 1.1546038931637801e-06\n",
      "Operator 38: -0.08993514580691997\n",
      "Operator 39: 0.1430101211249147\n",
      "Operator 40: -0.13961616970189983\n",
      "Operator 41: 0.02844326205905929\n",
      "Operator 42: -0.15967822672296475\n",
      "Operator 43: 0.1639686641438982\n",
      "Operator 44: -0.1442398238619823\n",
      "Operator 45: 0.10625983185906854\n",
      "Operator 46: -1.507261842399743e-06\n",
      "Operator 47: -3.015324203433506e-07\n",
      "Operator 48: 0.19223177721943108\n",
      "Operator 49: -1.6272924097116004e-07\n",
      "Operator 50: 5.225054151125619e-06\n",
      "Operator 51: -0.1996876591918563\n",
      "Operator 52: 0.05799172796158111\n",
      "Operator 53: -0.045924007852176256\n",
      "Operator 54: 0.14444660474887538\n",
      "Operator 55: -0.008696132959767455\n",
      "Operator 56: -0.12581351874915347\n",
      "Operator 57: -0.20126440862889705\n",
      "Operator 58: -0.010936966863180655\n",
      "Operator 59: -0.019370054982550716\n",
      "Operator 60: 0.05626660627934951\n",
      "Operator 61: 0.08127947259538308\n",
      "Operator 62: 0.14411318245176144\n",
      "Operator 63: 0.055357076398669505\n",
      "Operator 64: 0.026973075032942908\n",
      "Operator 65: -0.03586615630168005\n",
      "Operator 66: 0.039281424730850265\n",
      "Operator 67: -0.1362699153615568\n",
      "Operator 68: 0.1179207945491782\n",
      "Operator 69: 0.016497544931761657\n",
      "Operator 70: 0.004660871203754793\n",
      "Operator 71: -0.008981459370347106\n",
      "Operator 72: 0.03852696818517774\n",
      "Operator 73: 0.08857643182136216\n",
      "Operator 74: 0.12851192404654257\n",
      "Operator 75: 0.033098938533222225\n",
      "Operator 76: -0.08478720289009375\n",
      "Operator 77: -0.0012019290911813364\n",
      "Operator 78: -0.07174966609747528\n",
      "Operator 79: -0.1249067827684395\n",
      "Operator 80: -0.18765190555966285\n",
      "Operator 81: 0.05011469827750012\n",
      "Operator 82: -0.1295880755296258\n",
      "Operator 83: -0.06270915642822293\n",
      "Operator 84: 0.05202486063151883\n",
      "Operator 85: -0.07033937405603385\n",
      "Operator 86: -0.16102923049304851\n",
      "Operator 87: -0.007533781399882611\n",
      "Total gradient norm: 0.9025258741450854\n",
      "Operators under consideration (1):\n",
      "[23]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.3297833875081074)]\n",
      "Operator(s) added to ansatz: [23]\n",
      "Gradients: [np.float64(0.3297833875081074)]\n",
      "Initial energy: -13.454807413368362\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23]...\n",
      "Starting point: [np.float64(5.09151886438342e-06), np.float64(0.7853979807075266), np.float64(-0.785397631930072), np.float64(0.7853980816891895), np.float64(0.7853981530369701), np.float64(-0.25514404879119357), np.float64(0.08037808120382413), np.float64(0.10358353941514982), np.float64(-0.08236146488990524), np.float64(-0.15553962857321038), np.float64(0.20703695641472886), np.float64(-0.20491853245392871), np.float64(0.10256433627329982), np.float64(-0.024034676609569403), np.float64(0.01650633810120682), np.float64(-0.22543098686937252), np.float64(0.15764964574299137), np.float64(-0.2312408453907426), np.float64(0.07666765097474934), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.458705\n",
      "         Iterations: 17\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 48\n",
      "\n",
      "Current energy: -13.458705220781617\n",
      "(change of -0.003897807413254739)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23]\n",
      "On iteration 20.\n",
      "\n",
      "*** ADAPT-VQE Iteration 21 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.04639792144862e-06\n",
      "Operator 1: 3.371684151510303e-05\n",
      "Operator 2: -4.155818827313235e-06\n",
      "Operator 3: 1.2413961755998427e-05\n",
      "Operator 4: -9.686261986861962e-06\n",
      "Operator 5: 5.366005258583008e-07\n",
      "Operator 6: 9.522250956971768e-07\n",
      "Operator 7: 1.0080268027925319e-05\n",
      "Operator 8: -8.082193323137346e-06\n",
      "Operator 9: 7.870655061141551e-06\n",
      "Operator 10: 0.13755074412714305\n",
      "Operator 11: -0.002909663166902727\n",
      "Operator 12: 0.007061283235051356\n",
      "Operator 13: -9.257988799360796e-07\n",
      "Operator 14: -0.19615945809348975\n",
      "Operator 15: 0.14832029315057224\n",
      "Operator 16: 0.014176494649007635\n",
      "Operator 17: -0.02531412794765259\n",
      "Operator 18: -0.12880975532860245\n",
      "Operator 19: 0.15056696568609598\n",
      "Operator 20: -0.10754935917040598\n",
      "Operator 21: 0.01790141291597542\n",
      "Operator 22: -0.1386277199706175\n",
      "Operator 23: -1.928256304158675e-06\n",
      "Operator 24: -0.0636357697417689\n",
      "Operator 25: 2.2376984954028115e-06\n",
      "Operator 26: -2.8932145055292534e-06\n",
      "Operator 27: 1.0718934964788257e-05\n",
      "Operator 28: -8.326602166428298e-06\n",
      "Operator 29: 1.78582784790815e-05\n",
      "Operator 30: 4.450892406163175e-06\n",
      "Operator 31: -1.96844741666957e-06\n",
      "Operator 32: 8.178940099671771e-07\n",
      "Operator 33: -1.1717134337851183e-06\n",
      "Operator 34: 2.8332546355360413e-06\n",
      "Operator 35: -1.3586646469950544e-05\n",
      "Operator 36: -0.04352311580413583\n",
      "Operator 37: -1.7329608750499142e-06\n",
      "Operator 38: 0.03573824239056973\n",
      "Operator 39: 0.08981904751987618\n",
      "Operator 40: -0.13399057415144947\n",
      "Operator 41: 0.029088936178427813\n",
      "Operator 42: -0.158846592389326\n",
      "Operator 43: 0.17452989278783482\n",
      "Operator 44: -0.1635212505445171\n",
      "Operator 45: 0.08828243492724612\n",
      "Operator 46: -3.894438495074801e-06\n",
      "Operator 47: -8.838079048454403e-07\n",
      "Operator 48: 0.19980887614978743\n",
      "Operator 49: -1.4589750356163453e-06\n",
      "Operator 50: 1.4078065733532166e-05\n",
      "Operator 51: -0.17575557866480668\n",
      "Operator 52: 0.022616697054992592\n",
      "Operator 53: -0.03605119932303568\n",
      "Operator 54: 0.09517850878425269\n",
      "Operator 55: -0.023102082576594508\n",
      "Operator 56: -0.11191616990357595\n",
      "Operator 57: -0.24313056936697153\n",
      "Operator 58: -0.04874524671956395\n",
      "Operator 59: -0.13592388256385543\n",
      "Operator 60: 0.040720218573543576\n",
      "Operator 61: 0.08384157912126589\n",
      "Operator 62: 0.1820504024561247\n",
      "Operator 63: 0.037497547753384076\n",
      "Operator 64: 0.2026222261984107\n",
      "Operator 65: -0.02806033712950127\n",
      "Operator 66: 0.045329468336012306\n",
      "Operator 67: -0.14719983131220818\n",
      "Operator 68: 0.02127952610535038\n",
      "Operator 69: 0.025834944693312167\n",
      "Operator 70: -0.003276163793688097\n",
      "Operator 71: -0.00881863394285881\n",
      "Operator 72: 0.0216153620285342\n",
      "Operator 73: 0.0861107425510062\n",
      "Operator 74: 0.1870090833656452\n",
      "Operator 75: 0.060514647208117146\n",
      "Operator 76: -0.026135537822978106\n",
      "Operator 77: -0.0005199548537838984\n",
      "Operator 78: -0.07774441381692342\n",
      "Operator 79: -0.062071864497044285\n",
      "Operator 80: -0.2063679981794646\n",
      "Operator 81: -0.0885496718505401\n",
      "Operator 82: -0.13757416691444949\n",
      "Operator 83: -0.0713731688324999\n",
      "Operator 84: 0.041283962118344536\n",
      "Operator 85: -0.19967542088248563\n",
      "Operator 86: -0.1667390646793918\n",
      "Operator 87: -0.00934807944567895\n",
      "Total gradient norm: 0.8721850527793832\n",
      "Operators under consideration (1):\n",
      "[57]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.24313056936697153)]\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(-0.24313056936697153)]\n",
      "Initial energy: -13.458705220781617\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57]...\n",
      "Starting point: [np.float64(1.506792437330608e-06), np.float64(0.7853972241674045), np.float64(-0.7853959164590323), np.float64(0.7853951347772724), np.float64(0.7853954699402539), np.float64(-0.27857889916035894), np.float64(0.06634529994371315), np.float64(0.10286957111126027), np.float64(-0.08359353547605268), np.float64(-0.1640442567807343), np.float64(0.20852176842621697), np.float64(-0.20626737019194072), np.float64(0.09515283281571), np.float64(-0.034310581694570116), np.float64(0.01794507672299753), np.float64(-0.22692344640733053), np.float64(0.16937924515248334), np.float64(-0.22371108543944923), np.float64(0.10383960863454439), np.float64(-0.02400914026020959), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.460652\n",
      "         Iterations: 30\n",
      "         Function evaluations: 90\n",
      "         Gradient evaluations: 78\n",
      "\n",
      "Current energy: -13.460651807101687\n",
      "(change of -0.001946586320070054)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57]\n",
      "On iteration 21.\n",
      "\n",
      "*** ADAPT-VQE Iteration 22 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.8502383395402333e-06\n",
      "Operator 1: -1.6866386238767461e-06\n",
      "Operator 2: 1.2728152810614901e-06\n",
      "Operator 3: -1.5365571811032375e-06\n",
      "Operator 4: 1.5231911003610321e-06\n",
      "Operator 5: -1.8905878753153194e-07\n",
      "Operator 6: -1.638310302309225e-07\n",
      "Operator 7: 1.1445706897816876e-06\n",
      "Operator 8: 3.073728571702239e-07\n",
      "Operator 9: -1.713323550145418e-06\n",
      "Operator 10: 0.1213641553979107\n",
      "Operator 11: 0.021439118353882014\n",
      "Operator 12: 0.006498452974493947\n",
      "Operator 13: -0.00045339592571363465\n",
      "Operator 14: -0.21679092144539042\n",
      "Operator 15: 0.1305417250092567\n",
      "Operator 16: 0.04211856193652942\n",
      "Operator 17: -0.029916753101118427\n",
      "Operator 18: -0.11518598353046855\n",
      "Operator 19: 0.1685647845889898\n",
      "Operator 20: -0.10134140129401809\n",
      "Operator 21: 0.12568764934701904\n",
      "Operator 22: -0.05506611981821112\n",
      "Operator 23: -0.0026828349569833353\n",
      "Operator 24: -0.0676727433641574\n",
      "Operator 25: -2.632224989536358e-07\n",
      "Operator 26: 2.7778379062259884e-08\n",
      "Operator 27: 1.3357529501656362e-06\n",
      "Operator 28: 1.4284076289838854e-07\n",
      "Operator 29: -2.317387613803845e-06\n",
      "Operator 30: 0.001014567870576478\n",
      "Operator 31: 2.817950503861827e-07\n",
      "Operator 32: 4.9728059192082465e-08\n",
      "Operator 33: -1.6593090175764935e-06\n",
      "Operator 34: -3.1707044566287035e-08\n",
      "Operator 35: 1.8563359547307812e-06\n",
      "Operator 36: -0.06596248052201259\n",
      "Operator 37: 0.0009869635995129558\n",
      "Operator 38: 0.05700745171344884\n",
      "Operator 39: 0.07562062721420694\n",
      "Operator 40: -0.146595433739867\n",
      "Operator 41: 0.010501164088384536\n",
      "Operator 42: -0.08154603687691313\n",
      "Operator 43: 0.1761691993087734\n",
      "Operator 44: -0.17253415554135174\n",
      "Operator 45: 0.0978667191433086\n",
      "Operator 46: 3.568207300810866e-07\n",
      "Operator 47: 1.996901117726213e-07\n",
      "Operator 48: 0.22236979425111864\n",
      "Operator 49: 3.322142145822937e-07\n",
      "Operator 50: -2.0032459850938267e-06\n",
      "Operator 51: -0.1564843600986684\n",
      "Operator 52: -0.06297300193451592\n",
      "Operator 53: -0.02970992683274515\n",
      "Operator 54: 0.09381993854645174\n",
      "Operator 55: -0.02954296528906567\n",
      "Operator 56: -0.09746880713546947\n",
      "Operator 57: -1.3983790577654567e-06\n",
      "Operator 58: -0.02372294098880598\n",
      "Operator 59: -0.14096108818152014\n",
      "Operator 60: 0.05909416665621628\n",
      "Operator 61: 0.08262280677099422\n",
      "Operator 62: -0.06681196454813754\n",
      "Operator 63: 0.002178640623731807\n",
      "Operator 64: 0.20996401826587968\n",
      "Operator 65: -0.0346493642500558\n",
      "Operator 66: 0.03362737157475898\n",
      "Operator 67: -0.14557352970032317\n",
      "Operator 68: 0.007547576827534696\n",
      "Operator 69: 0.02258803250763544\n",
      "Operator 70: -0.0034557541432304554\n",
      "Operator 71: -0.01518132160449202\n",
      "Operator 72: -0.014083650078212299\n",
      "Operator 73: 0.10245167632761353\n",
      "Operator 74: 0.14010592487707502\n",
      "Operator 75: 0.05599248893647937\n",
      "Operator 76: -0.015712157971054776\n",
      "Operator 77: 0.0010180128849395446\n",
      "Operator 78: -0.08305265093250555\n",
      "Operator 79: 0.05736826944164286\n",
      "Operator 80: -0.20148944189933665\n",
      "Operator 81: -0.0985358728933863\n",
      "Operator 82: -0.052506504409683206\n",
      "Operator 83: -0.018014328212236683\n",
      "Operator 84: 0.0824821378805024\n",
      "Operator 85: -0.21023636112783084\n",
      "Operator 86: -0.15350884600495635\n",
      "Operator 87: -0.00021663954440632156\n",
      "Total gradient norm: 0.8016923985278993\n",
      "Operators under consideration (1):\n",
      "[48]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.22236979425111864)]\n",
      "Operator(s) added to ansatz: [48]\n",
      "Gradients: [np.float64(0.22236979425111864)]\n",
      "Initial energy: -13.460651807101687\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48]...\n",
      "Starting point: [np.float64(-1.0215236357896348e-06), np.float64(0.7853981864671945), np.float64(-0.7853984312573148), np.float64(0.7853981781425113), np.float64(0.7853984963148996), np.float64(-0.2840946185799167), np.float64(0.06506349181477382), np.float64(0.0902771511472124), np.float64(-0.10011216749819805), np.float64(-0.16419270015767837), np.float64(0.20683272132632832), np.float64(-0.2112700288707792), np.float64(0.09207858054078226), np.float64(-0.047752207895079224), np.float64(0.02967108749595574), np.float64(-0.24570365230489002), np.float64(0.18381065652750425), np.float64(-0.22424175957822326), np.float64(0.10959975609882298), np.float64(-0.026144706010777966), np.float64(0.01654026705429965), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.478333\n",
      "         Iterations: 28\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 46\n",
      "\n",
      "Current energy: -13.47833303006338\n",
      "(change of -0.017681222961693166)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48]\n",
      "On iteration 22.\n",
      "\n",
      "*** ADAPT-VQE Iteration 23 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.591933077453163e-07\n",
      "Operator 1: -1.347615909621277e-06\n",
      "Operator 2: -8.745666745229447e-08\n",
      "Operator 3: -3.5561992063040584e-07\n",
      "Operator 4: 2.611010832195837e-07\n",
      "Operator 5: -9.990194373946278e-08\n",
      "Operator 6: -1.0011100504623202e-08\n",
      "Operator 7: -7.021568998906978e-07\n",
      "Operator 8: 4.3372234853919824e-07\n",
      "Operator 9: -3.108300014858373e-08\n",
      "Operator 10: 0.045909779697120796\n",
      "Operator 11: 0.0093826599696443\n",
      "Operator 12: -0.0492524995367433\n",
      "Operator 13: -0.06912156111587155\n",
      "Operator 14: -0.009385093217853597\n",
      "Operator 15: 0.06487140961564869\n",
      "Operator 16: -0.029078715879594583\n",
      "Operator 17: 0.10394030133654458\n",
      "Operator 18: -0.21567246830692932\n",
      "Operator 19: -0.00406860869501027\n",
      "Operator 20: -0.10282064942471923\n",
      "Operator 21: 0.12999294126278113\n",
      "Operator 22: -0.21103084254736296\n",
      "Operator 23: -0.0045709077565986266\n",
      "Operator 24: -0.13750486648006935\n",
      "Operator 25: -2.582080587066124e-07\n",
      "Operator 26: 1.929142259750151e-07\n",
      "Operator 27: -6.71494371162274e-07\n",
      "Operator 28: 4.994121710284781e-07\n",
      "Operator 29: -3.7101937912752125e-07\n",
      "Operator 30: -0.001323431929696487\n",
      "Operator 31: 2.1166486778590994e-07\n",
      "Operator 32: -5.723503693917287e-08\n",
      "Operator 33: 2.978962055099374e-07\n",
      "Operator 34: -2.4098020232229336e-07\n",
      "Operator 35: 2.442774852664739e-07\n",
      "Operator 36: -0.056076654267639935\n",
      "Operator 37: 0.0008674396616988337\n",
      "Operator 38: -0.14668697073132814\n",
      "Operator 39: 0.0852429344039677\n",
      "Operator 40: -0.015270339590422562\n",
      "Operator 41: -0.017918281774950116\n",
      "Operator 42: -0.051258322275306155\n",
      "Operator 43: 0.1536278503419366\n",
      "Operator 44: -0.18658254090669746\n",
      "Operator 45: 0.14800490358257037\n",
      "Operator 46: 3.8328121166562043e-07\n",
      "Operator 47: 1.1762549581557774e-07\n",
      "Operator 48: -8.767355115156747e-07\n",
      "Operator 49: 2.7701866641186533e-08\n",
      "Operator 50: -3.9137496541524763e-07\n",
      "Operator 51: -0.041657874064032575\n",
      "Operator 52: -0.0016775342322002107\n",
      "Operator 53: -0.07367976410518139\n",
      "Operator 54: 0.20427729485928184\n",
      "Operator 55: 0.015565757825695664\n",
      "Operator 56: -0.06329404864881663\n",
      "Operator 57: 3.0492383009969533e-07\n",
      "Operator 58: 0.06492442591003869\n",
      "Operator 59: -0.19797037777258328\n",
      "Operator 60: -0.1495845821298587\n",
      "Operator 61: 0.10481209312251659\n",
      "Operator 62: -0.04482730290005524\n",
      "Operator 63: 0.07099623038378451\n",
      "Operator 64: 0.3111952629350479\n",
      "Operator 65: 0.05930299654983682\n",
      "Operator 66: 0.05189904902755977\n",
      "Operator 67: -0.13379321590537596\n",
      "Operator 68: 0.1938916334775611\n",
      "Operator 69: 0.027167839027238502\n",
      "Operator 70: -0.003565531311826787\n",
      "Operator 71: -0.005995335061269395\n",
      "Operator 72: -0.10419454869855399\n",
      "Operator 73: 0.09858654333256858\n",
      "Operator 74: 0.17365529417848616\n",
      "Operator 75: 0.10217777966476911\n",
      "Operator 76: -0.0045314964212820365\n",
      "Operator 77: 0.0084342436878582\n",
      "Operator 78: -0.11063299615237557\n",
      "Operator 79: -0.1153632660988341\n",
      "Operator 80: -0.3346066592245487\n",
      "Operator 81: -0.08932764737210408\n",
      "Operator 82: -0.030644381258633623\n",
      "Operator 83: -0.03838402407919788\n",
      "Operator 84: 0.04133682055519624\n",
      "Operator 85: -0.25503226682919405\n",
      "Operator 86: -0.15117536386265065\n",
      "Operator 87: 0.0003925024435933812\n",
      "Total gradient norm: 0.9345096691673347\n",
      "Operators under consideration (1):\n",
      "[80]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.3346066592245487)]\n",
      "Operator(s) added to ansatz: [80]\n",
      "Gradients: [np.float64(-0.3346066592245487)]\n",
      "Initial energy: -13.47833303006338\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80]...\n",
      "Starting point: [np.float64(1.2326435629417195e-06), np.float64(0.785398231064589), np.float64(-0.7853982200525749), np.float64(0.7853983250550584), np.float64(0.7853982134545542), np.float64(-0.17861271433894296), np.float64(0.036855124144481714), np.float64(0.08188423854935627), np.float64(-0.10933088635408796), np.float64(-0.1511029079415084), np.float64(0.20619366055110372), np.float64(-0.21602159127391773), np.float64(-0.026106592525741372), np.float64(-0.07889397225338161), np.float64(0.038465558380652916), np.float64(-0.25667984606979277), np.float64(0.22730010618428628), np.float64(-0.22470365506031006), np.float64(0.04706729009897973), np.float64(-0.009077161874474174), np.float64(0.023259861259385613), np.float64(-0.1482976732612015), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.481360\n",
      "         Iterations: 30\n",
      "         Function evaluations: 130\n",
      "         Gradient evaluations: 116\n",
      "\n",
      "Current energy: -13.481360062641082\n",
      "(change of -0.0030270325777017604)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80]\n",
      "On iteration 23.\n",
      "\n",
      "*** ADAPT-VQE Iteration 24 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 4: 1.3730655723520613e-08\n",
      "Operator 5: 2.4951583278698397e-08\n",
      "Operator 7: -1.02794039669174e-08\n",
      "Operator 10: 0.053160095873608766\n",
      "Operator 11: 0.00637629421269648\n",
      "Operator 12: -0.040611926343322266\n",
      "Operator 13: -0.033386586470363966\n",
      "Operator 14: 0.006518444138521469\n",
      "Operator 15: 0.07308007029475538\n",
      "Operator 16: -0.033137929289167645\n",
      "Operator 17: 0.08297639878839608\n",
      "Operator 18: -0.23826538953770243\n",
      "Operator 19: -0.010055957481644721\n",
      "Operator 20: -0.10341934181420762\n",
      "Operator 21: 0.13008146743285384\n",
      "Operator 22: -0.10680929674993588\n",
      "Operator 23: -0.004728787052813325\n",
      "Operator 24: -0.10792832071205874\n",
      "Operator 25: 2.932100185565334e-08\n",
      "Operator 27: -1.763017573597203e-08\n",
      "Operator 30: -0.000992609370917463\n",
      "Operator 31: -3.1115891661881445e-08\n",
      "Operator 33: 1.358570375642465e-08\n",
      "Operator 36: -0.05933188322885262\n",
      "Operator 37: -0.004058573414322517\n",
      "Operator 38: -0.022165468963059876\n",
      "Operator 39: 0.08557967447358107\n",
      "Operator 40: -0.02184637754890347\n",
      "Operator 41: -0.014718108328907603\n",
      "Operator 42: -0.05632035100991008\n",
      "Operator 43: 0.16664351850447987\n",
      "Operator 44: -0.18221287435666\n",
      "Operator 45: 0.020501329341339317\n",
      "Operator 46: -3.499669740114619e-08\n",
      "Operator 47: -1.0444994936673058e-08\n",
      "Operator 48: -0.00171585193392321\n",
      "Operator 50: -1.6987662831979833e-08\n",
      "Operator 51: -0.053980604509304315\n",
      "Operator 52: -0.002089242624719163\n",
      "Operator 53: -0.0949561331686116\n",
      "Operator 54: 0.22227093609377185\n",
      "Operator 55: 0.020062934561476307\n",
      "Operator 56: -0.0659799914479132\n",
      "Operator 57: 0.0015252092054133441\n",
      "Operator 58: 0.10705322776026163\n",
      "Operator 59: -0.24973522184104704\n",
      "Operator 60: -0.1819832555312115\n",
      "Operator 61: 0.1024732615636527\n",
      "Operator 62: -0.0465443043937253\n",
      "Operator 63: -0.04123621500512873\n",
      "Operator 64: 0.3343258181387999\n",
      "Operator 65: 0.07416907231588862\n",
      "Operator 66: 0.05339072298711163\n",
      "Operator 67: -0.13871108791810993\n",
      "Operator 68: 0.18239227379709388\n",
      "Operator 69: 0.030308147866670087\n",
      "Operator 70: -0.00040191590127950153\n",
      "Operator 71: -0.006831859064121919\n",
      "Operator 72: -0.09346127984383279\n",
      "Operator 73: 0.09466847794531065\n",
      "Operator 74: 0.1859715580715378\n",
      "Operator 75: -0.2345564604921796\n",
      "Operator 76: 0.004214074360373681\n",
      "Operator 77: 0.004614066932415723\n",
      "Operator 78: -0.10626868682241913\n",
      "Operator 79: -0.11199710165639201\n",
      "Operator 80: -6.602182058594521e-08\n",
      "Operator 81: -0.11031898333578036\n",
      "Operator 82: -0.033439792218365835\n",
      "Operator 83: -0.03455629790647198\n",
      "Operator 84: 0.06605697636349475\n",
      "Operator 85: -0.339509545133425\n",
      "Operator 86: -0.14077971029717315\n",
      "Operator 87: 0.00027041292263195124\n",
      "Total gradient norm: 0.9215715995683796\n",
      "Operators under consideration (1):\n",
      "[85]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.339509545133425)]\n",
      "Operator(s) added to ansatz: [85]\n",
      "Gradients: [np.float64(-0.339509545133425)]\n",
      "Initial energy: -13.481360062641082\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85]...\n",
      "Starting point: [np.float64(1.0900600104148921e-06), np.float64(0.7853981594311658), np.float64(-0.7853981652996775), np.float64(0.7853981633475248), np.float64(0.7853981644591713), np.float64(-0.19382219300166115), np.float64(0.04676359275955075), np.float64(0.08258916018620424), np.float64(-0.10816345298228182), np.float64(-0.15376048907083623), np.float64(0.2087880880251021), np.float64(-0.21588199771874192), np.float64(-0.029767995680728212), np.float64(-0.07688611036385536), np.float64(0.03807075133586692), np.float64(-0.25554758483779394), np.float64(0.225479945693166), np.float64(-0.23101976730261403), np.float64(0.06221244547887086), np.float64(-0.016435251796586794), np.float64(0.022788894732865446), np.float64(-0.14695586075673986), np.float64(0.018136353828324542), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.484508\n",
      "         Iterations: 26\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 77\n",
      "\n",
      "Current energy: -13.484507568543641\n",
      "(change of -0.003147505902559189)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85]\n",
      "On iteration 24.\n",
      "\n",
      "*** ADAPT-VQE Iteration 25 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.735520455432461e-07\n",
      "Operator 1: 2.2395324867641175e-06\n",
      "Operator 2: -4.6121663554998804e-07\n",
      "Operator 3: 8.745225250628152e-07\n",
      "Operator 4: -7.41333154863355e-07\n",
      "Operator 5: 2.398513272439118e-07\n",
      "Operator 6: 7.867444816456182e-08\n",
      "Operator 7: 3.71051573772041e-07\n",
      "Operator 8: -5.399836683697701e-07\n",
      "Operator 9: 7.699314277243019e-07\n",
      "Operator 10: 0.053862209136915716\n",
      "Operator 11: 0.004095832782708524\n",
      "Operator 12: -0.043421196306490065\n",
      "Operator 13: -0.03718476175962005\n",
      "Operator 14: 0.0067863348418568115\n",
      "Operator 15: 0.07706737642881598\n",
      "Operator 16: -0.03436976725094053\n",
      "Operator 17: 0.09678614853252199\n",
      "Operator 18: -0.10812523695137152\n",
      "Operator 19: -0.013366686541774558\n",
      "Operator 20: -0.10313301381255838\n",
      "Operator 21: 0.1248669505488369\n",
      "Operator 22: -0.018469355297841425\n",
      "Operator 23: -0.019538170033973814\n",
      "Operator 24: -0.11748805538581997\n",
      "Operator 25: 4.62636871922939e-07\n",
      "Operator 26: -2.4962273975637217e-07\n",
      "Operator 27: 2.0825511395927698e-07\n",
      "Operator 28: -5.080062323442602e-07\n",
      "Operator 29: 1.428022389671213e-06\n",
      "Operator 30: -0.0005962390500956483\n",
      "Operator 31: -4.272006726147026e-07\n",
      "Operator 32: 6.068508245915538e-08\n",
      "Operator 33: 3.7479787290406463e-07\n",
      "Operator 34: 1.823345759047884e-07\n",
      "Operator 35: -1.197379895234718e-06\n",
      "Operator 36: -0.06195487522983076\n",
      "Operator 37: -0.00573089726081262\n",
      "Operator 38: -0.022254250649491106\n",
      "Operator 39: 0.05720692152875532\n",
      "Operator 40: -0.012865283959104641\n",
      "Operator 41: -0.013842218602995907\n",
      "Operator 42: -0.058522146748595585\n",
      "Operator 43: 0.17754639984965415\n",
      "Operator 44: -0.15759917619785535\n",
      "Operator 45: 0.07337103105725212\n",
      "Operator 46: -6.72338023921526e-07\n",
      "Operator 47: -2.6307815392812017e-07\n",
      "Operator 48: -0.0010472831367726587\n",
      "Operator 49: -1.9861190403425155e-07\n",
      "Operator 50: 1.0035168394040284e-06\n",
      "Operator 51: -0.05723085708319431\n",
      "Operator 52: -0.004057260996547581\n",
      "Operator 53: -0.0994918322228836\n",
      "Operator 54: 0.18248748216806593\n",
      "Operator 55: 0.0004162860758880338\n",
      "Operator 56: -0.065034076607185\n",
      "Operator 57: 0.002098515081892755\n",
      "Operator 58: 0.13744964539650462\n",
      "Operator 59: -0.2775212932475259\n",
      "Operator 60: -0.02953631371676846\n",
      "Operator 61: 0.10065917538143179\n",
      "Operator 62: -0.04442637248311066\n",
      "Operator 63: -0.13146981351631754\n",
      "Operator 64: 0.3770524577290877\n",
      "Operator 65: 0.007415430001758927\n",
      "Operator 66: 0.05655993676502079\n",
      "Operator 67: -0.1331073364854948\n",
      "Operator 68: 0.05611529223914284\n",
      "Operator 69: 0.02512082452745857\n",
      "Operator 70: -0.00027739248084748813\n",
      "Operator 71: -0.006265055553880627\n",
      "Operator 72: -0.08800357988064353\n",
      "Operator 73: 0.08931539597067656\n",
      "Operator 74: 0.13954906366265143\n",
      "Operator 75: -0.11295492425453134\n",
      "Operator 76: 0.010021279688380024\n",
      "Operator 77: 0.0003631550745222603\n",
      "Operator 78: -0.1049069798237364\n",
      "Operator 79: 0.011245151479388892\n",
      "Operator 80: -0.006778763086115958\n",
      "Operator 81: -0.12120857363873502\n",
      "Operator 82: -0.03497376617225656\n",
      "Operator 83: -0.03358199583084144\n",
      "Operator 84: 0.057978709737206514\n",
      "Operator 85: 3.1911209247975636e-08\n",
      "Operator 86: -0.11980207111760871\n",
      "Operator 87: -0.00024670867556658704\n",
      "Total gradient norm: 0.7657407864100086\n",
      "Operators under consideration (1):\n",
      "[64]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.3770524577290877)]\n",
      "Operator(s) added to ansatz: [64]\n",
      "Gradients: [np.float64(0.3770524577290877)]\n",
      "Initial energy: -13.484507568543641\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64]...\n",
      "Starting point: [np.float64(8.592695600312002e-07), np.float64(0.7853980693512354), np.float64(-0.7853980170283208), np.float64(0.7853979609017304), np.float64(0.7853979709035575), np.float64(-0.2142183589758176), np.float64(0.047338808459140506), np.float64(0.08208639639236993), np.float64(-0.10958129500950771), np.float64(-0.15600700755002592), np.float64(0.20749110772891594), np.float64(-0.21639464342676024), np.float64(-0.0400833655490925), np.float64(-0.07691643299046787), np.float64(0.03814276911943539), np.float64(-0.2573023948035112), np.float64(0.22782302665959583), np.float64(-0.23425019345253842), np.float64(0.0869304762921519), np.float64(-0.024483722261543214), np.float64(0.022779998671259044), np.float64(-0.14915943409705607), np.float64(0.023119384966636063), np.float64(0.018565774063986126), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.488793\n",
      "         Iterations: 29\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 77\n",
      "\n",
      "Current energy: -13.488793267188692\n",
      "(change of -0.004285698645050573)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64]\n",
      "On iteration 25.\n",
      "\n",
      "*** ADAPT-VQE Iteration 26 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 3.192996801182346e-06\n",
      "Operator 1: 1.3832312167894045e-06\n",
      "Operator 2: -1.4100235057323829e-06\n",
      "Operator 3: 2.4724733780345787e-06\n",
      "Operator 4: -2.857381996790309e-06\n",
      "Operator 5: 5.455199136861211e-08\n",
      "Operator 6: 2.2924056303219587e-07\n",
      "Operator 7: -2.224086119273494e-06\n",
      "Operator 8: -2.300252548749493e-07\n",
      "Operator 9: 2.1468860366136155e-06\n",
      "Operator 10: 0.05408916075627562\n",
      "Operator 11: 0.005152370999528862\n",
      "Operator 12: -0.03537138175078264\n",
      "Operator 13: -0.03596771928761536\n",
      "Operator 14: -0.0027227337768143123\n",
      "Operator 15: 0.07903832066943492\n",
      "Operator 16: -0.035673394321360546\n",
      "Operator 17: 0.13388362567803314\n",
      "Operator 18: -0.043942978552141576\n",
      "Operator 19: 0.01001190767648899\n",
      "Operator 20: -0.10512176155304825\n",
      "Operator 21: 0.12080581837931559\n",
      "Operator 22: -0.011823323637313848\n",
      "Operator 23: -0.00964324816089477\n",
      "Operator 24: 0.03176591321507415\n",
      "Operator 26: 1.2345462846452548e-07\n",
      "Operator 27: -2.4378876171127217e-06\n",
      "Operator 28: 6.141315173208151e-07\n",
      "Operator 29: 2.8179351505563943e-06\n",
      "Operator 30: -0.00023602385821934468\n",
      "Operator 31: -8.584650501575197e-08\n",
      "Operator 32: -1.1448992379148315e-07\n",
      "Operator 33: 2.8129819034106163e-06\n",
      "Operator 34: -3.6751575986301854e-07\n",
      "Operator 35: -2.240708420364612e-06\n",
      "Operator 36: -0.06305736523002\n",
      "Operator 37: -0.005189274824363238\n",
      "Operator 38: -0.020826750600918216\n",
      "Operator 39: 0.049123559757630636\n",
      "Operator 40: -0.01613626798280863\n",
      "Operator 41: -0.013697109003413144\n",
      "Operator 42: -0.06136398178075974\n",
      "Operator 43: 0.17399194748907587\n",
      "Operator 44: -0.00011532758069551785\n",
      "Operator 45: 0.09104182690474945\n",
      "Operator 46: -4.036942957696965e-08\n",
      "Operator 47: -2.2170050675840436e-07\n",
      "Operator 48: -0.0010477478833251785\n",
      "Operator 49: -6.394910463392023e-07\n",
      "Operator 50: 3.584739991069341e-06\n",
      "Operator 51: -0.06009753184133526\n",
      "Operator 52: -0.0014343012961569868\n",
      "Operator 53: -0.10584804555775068\n",
      "Operator 54: 0.010440055212269525\n",
      "Operator 55: 0.0018744268841231482\n",
      "Operator 56: -0.06420383306049368\n",
      "Operator 57: 0.0009266452235975192\n",
      "Operator 58: 0.11864675332322656\n",
      "Operator 59: 0.08614313312789282\n",
      "Operator 60: -0.024348428709020986\n",
      "Operator 61: 0.10083738002021113\n",
      "Operator 62: -0.04129547480924741\n",
      "Operator 63: -0.11688881119831573\n",
      "Operator 64: -2.475688509612217e-07\n",
      "Operator 65: 0.004279370634797219\n",
      "Operator 66: 0.06240785618143359\n",
      "Operator 67: -0.150691335159871\n",
      "Operator 68: -0.0071151370025972285\n",
      "Operator 69: -0.023957569838114215\n",
      "Operator 70: 0.0027516790324364493\n",
      "Operator 71: -0.005022950482765354\n",
      "Operator 72: -0.08241742786595255\n",
      "Operator 73: 0.07736021826232967\n",
      "Operator 74: 0.1850154434530872\n",
      "Operator 75: -0.045672335335175014\n",
      "Operator 76: 0.014801205820013708\n",
      "Operator 77: -0.00628157442266946\n",
      "Operator 78: -0.09284574155131042\n",
      "Operator 79: -0.0009509100204626586\n",
      "Operator 80: -0.00044402975738625297\n",
      "Operator 81: -0.013526891221279494\n",
      "Operator 82: -0.03567001055918384\n",
      "Operator 83: -0.03694815940404318\n",
      "Operator 84: 0.164024414157093\n",
      "Operator 85: 5.995737410724455e-08\n",
      "Operator 86: -0.0570381409107215\n",
      "Operator 87: -0.0007599865383960958\n",
      "Total gradient norm: 0.5425899667526105\n",
      "Operators under consideration (1):\n",
      "[74]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.1850154434530872)]\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(0.1850154434530872)]\n",
      "Initial energy: -13.488793267188692\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74]...\n",
      "Starting point: [np.float64(-2.637287171181638e-06), np.float64(0.7853982181550293), np.float64(-0.7853977169025843), np.float64(0.7853982531601355), np.float64(0.7853977110558983), np.float64(-0.23160861776339875), np.float64(0.050839418198694364), np.float64(0.08239592741464337), np.float64(-0.1085456020626987), np.float64(-0.1604374821265918), np.float64(0.21036345052673766), np.float64(-0.21669594995097852), np.float64(-0.058969698250321737), np.float64(-0.07823529740062984), np.float64(0.03845851034391463), np.float64(-0.2563930130393603), np.float64(0.22703133370653855), np.float64(-0.2430192851489074), np.float64(0.1093237824714719), np.float64(-0.03872341425825343), np.float64(0.022710271961364662), np.float64(-0.1577951717608234), np.float64(0.02635039280395873), np.float64(0.021246020742717253), np.float64(-0.022729866766286834), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.490965\n",
      "         Iterations: 26\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 57\n",
      "\n",
      "Current energy: -13.490965317140525\n",
      "(change of -0.002172049951832733)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74]\n",
      "On iteration 26.\n",
      "\n",
      "*** ADAPT-VQE Iteration 27 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 7.163581536323571e-06\n",
      "Operator 1: -1.171791194047242e-05\n",
      "Operator 2: -3.6934925317089196e-06\n",
      "Operator 3: -6.308121835163494e-07\n",
      "Operator 4: 9.084923948687873e-07\n",
      "Operator 5: -1.4864046828169664e-06\n",
      "Operator 6: 2.3450871765025132e-07\n",
      "Operator 7: -9.26781454528569e-06\n",
      "Operator 8: 3.159091270151567e-06\n",
      "Operator 9: 3.4784487726591262e-06\n",
      "Operator 10: 0.00025752470592968425\n",
      "Operator 11: 0.004164537905832076\n",
      "Operator 12: -0.03979104931961705\n",
      "Operator 13: -0.02973270345042475\n",
      "Operator 14: -0.007642729305045698\n",
      "Operator 15: 0.07761731006244091\n",
      "Operator 16: -0.03040048680227299\n",
      "Operator 17: 0.0941249131398712\n",
      "Operator 18: -0.0579681846726306\n",
      "Operator 19: 0.007774241960116871\n",
      "Operator 20: -0.11170416905315934\n",
      "Operator 21: 0.09166647958040111\n",
      "Operator 22: 0.016119220485816026\n",
      "Operator 23: -0.01015371604476993\n",
      "Operator 24: 0.04239161947880985\n",
      "Operator 25: -3.112243397218606e-06\n",
      "Operator 26: 2.050266283237616e-06\n",
      "Operator 27: -9.059739741863748e-06\n",
      "Operator 28: 3.993227946280653e-06\n",
      "Operator 29: 2.730568092190744e-07\n",
      "Operator 30: -0.00020363787726222898\n",
      "Operator 31: 2.597556292104411e-06\n",
      "Operator 32: -7.793700698491279e-07\n",
      "Operator 33: 5.359840179075537e-06\n",
      "Operator 34: -2.0817902189841295e-06\n",
      "Operator 35: -3.3431214024881584e-07\n",
      "Operator 36: -0.0780168554636874\n",
      "Operator 37: -0.007403951933512896\n",
      "Operator 38: -0.012588261685611751\n",
      "Operator 39: -0.0026984667424524936\n",
      "Operator 40: -0.007737353554327277\n",
      "Operator 41: -0.0017447050959318398\n",
      "Operator 42: -0.06081090226195\n",
      "Operator 43: 0.1421020175447664\n",
      "Operator 44: -0.019459339298833145\n",
      "Operator 45: 0.0815142152881072\n",
      "Operator 46: 4.346692077014311e-06\n",
      "Operator 47: 9.838422254920126e-07\n",
      "Operator 48: 0.004404977947175861\n",
      "Operator 49: -1.340619182610414e-07\n",
      "Operator 50: -1.3348774703381139e-06\n",
      "Operator 51: -0.0430830142779218\n",
      "Operator 52: 0.0008212058473796068\n",
      "Operator 53: -0.028960682333076564\n",
      "Operator 54: 0.013694928222361543\n",
      "Operator 55: 0.002627241848352018\n",
      "Operator 56: -0.014744408623452498\n",
      "Operator 57: 0.0014084865585376842\n",
      "Operator 58: 0.09429470110855505\n",
      "Operator 59: 0.08082425496081887\n",
      "Operator 60: 0.029044595675493057\n",
      "Operator 61: 0.08581393670202495\n",
      "Operator 62: 0.01515420523398223\n",
      "Operator 63: -0.09875385367525583\n",
      "Operator 64: -4.262538999053273e-06\n",
      "Operator 65: -0.020967735612753807\n",
      "Operator 66: 0.049087521901598874\n",
      "Operator 67: -0.06090428638873288\n",
      "Operator 68: 0.006479266932478618\n",
      "Operator 69: -0.009619376096573696\n",
      "Operator 70: 0.0010191586195497147\n",
      "Operator 71: 0.004476334163382778\n",
      "Operator 72: -0.06192323113645887\n",
      "Operator 73: 0.09873974951271842\n",
      "Operator 74: -3.302238314092427e-07\n",
      "Operator 75: -0.06409748907470321\n",
      "Operator 76: 0.017567061264645478\n",
      "Operator 77: 0.004922543724980286\n",
      "Operator 78: -0.16400666368761968\n",
      "Operator 79: 0.04898479865555623\n",
      "Operator 80: -0.000578260473972332\n",
      "Operator 81: -0.009111727124101597\n",
      "Operator 82: 0.005427440782899299\n",
      "Operator 83: 0.001356833109321626\n",
      "Operator 84: 0.11887402249540258\n",
      "Operator 85: 0.0009147675148112282\n",
      "Operator 86: 0.0017069901442275778\n",
      "Operator 87: -0.00019125054457736266\n",
      "Total gradient norm: 0.4325806676248106\n",
      "Operators under consideration (1):\n",
      "[78]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.16400666368761968)]\n",
      "Operator(s) added to ansatz: [78]\n",
      "Gradients: [np.float64(-0.16400666368761968)]\n",
      "Initial energy: -13.490965317140525\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78]...\n",
      "Starting point: [np.float64(-1.231758881269792e-06), np.float64(0.7853989596061082), np.float64(-0.7853983784656406), np.float64(0.7853998657614423), np.float64(0.785398174102752), np.float64(-0.22095045690655638), np.float64(0.0518732380864149), np.float64(0.08784691085731013), np.float64(-0.10294961561383466), np.float64(-0.18381961650573309), np.float64(0.2157890775940744), np.float64(-0.2166644434539873), np.float64(-0.06345437268437466), np.float64(-0.0789014301706401), np.float64(0.0353584322326423), np.float64(-0.24963425166294387), np.float64(0.21831584692857783), np.float64(-0.24378285554110746), np.float64(0.10188144494015561), np.float64(-0.04663441686024485), np.float64(0.019050724742385887), np.float64(-0.16648983612479046), np.float64(0.027852047740440738), np.float64(0.018762919396714765), np.float64(-0.026010900383059393), np.float64(-0.023446309639033405), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.491559\n",
      "         Iterations: 28\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 63\n",
      "\n",
      "Current energy: -13.491559253310998\n",
      "(change of -0.0005939361704729862)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78]\n",
      "On iteration 27.\n",
      "\n",
      "*** ADAPT-VQE Iteration 28 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.7691786330098491e-06\n",
      "Operator 1: -5.585365350120608e-06\n",
      "Operator 2: 3.308732833717798e-06\n",
      "Operator 3: -4.776881403267819e-06\n",
      "Operator 4: 5.1090703978573515e-06\n",
      "Operator 5: 5.617937100306503e-06\n",
      "Operator 6: -3.2647053368160817e-07\n",
      "Operator 7: -3.2203845415357257e-07\n",
      "Operator 8: 1.3065824071281304e-06\n",
      "Operator 9: -4.76643941726973e-06\n",
      "Operator 10: 0.0170763234764407\n",
      "Operator 11: 0.0024954822770199897\n",
      "Operator 12: -0.04050730656713586\n",
      "Operator 13: -0.029587245183172795\n",
      "Operator 14: -0.009880422813928133\n",
      "Operator 15: 0.07433720974982973\n",
      "Operator 16: -0.030352224067781763\n",
      "Operator 17: 0.09248529903757578\n",
      "Operator 18: -0.05924390657710747\n",
      "Operator 19: 0.007528596201660118\n",
      "Operator 20: -0.04860412398261348\n",
      "Operator 21: 0.11121130467105962\n",
      "Operator 22: 0.01803333930563936\n",
      "Operator 23: -0.010200056560327221\n",
      "Operator 24: 0.043641350691771816\n",
      "Operator 25: 6.53662900351043e-06\n",
      "Operator 26: -9.220308585314307e-07\n",
      "Operator 27: -1.5140839861316202e-06\n",
      "Operator 28: -3.0549167336080263e-07\n",
      "Operator 29: -6.834913717002067e-06\n",
      "Operator 30: -8.418013509076903e-05\n",
      "Operator 31: -6.706599699628918e-06\n",
      "Operator 32: 5.851795734810802e-07\n",
      "Operator 33: -5.902464553092512e-07\n",
      "Operator 34: 3.040241821428494e-07\n",
      "Operator 35: 5.772837575857004e-06\n",
      "Operator 36: -0.03472797025668866\n",
      "Operator 37: -0.007231338951660324\n",
      "Operator 38: -0.016968876733707592\n",
      "Operator 39: -0.009129923076500956\n",
      "Operator 40: -0.006372690675026032\n",
      "Operator 41: 0.0017230534844492572\n",
      "Operator 42: -0.059252446309303444\n",
      "Operator 43: 0.10133143040364931\n",
      "Operator 44: -0.015822762510100404\n",
      "Operator 45: 0.08283124929760016\n",
      "Operator 46: -7.529539400370766e-06\n",
      "Operator 47: -1.67394292189052e-06\n",
      "Operator 48: 0.006717149778623997\n",
      "Operator 49: 1.2634702939995357e-06\n",
      "Operator 50: -6.498316143854765e-06\n",
      "Operator 51: -0.05835494616348345\n",
      "Operator 52: 0.013671586524323029\n",
      "Operator 53: -0.021400184659163365\n",
      "Operator 54: 0.014111626922322144\n",
      "Operator 55: 0.003077543289267211\n",
      "Operator 56: -0.0010543456429068154\n",
      "Operator 57: 0.0043809197827626364\n",
      "Operator 58: 0.09158021742645098\n",
      "Operator 59: 0.08153619630080185\n",
      "Operator 60: 0.03640496226214635\n",
      "Operator 61: 0.023948838816925153\n",
      "Operator 62: -0.008174812951120084\n",
      "Operator 63: -0.08115388623863434\n",
      "Operator 64: 1.2731266506137711e-06\n",
      "Operator 65: -0.024503684711638246\n",
      "Operator 66: 0.05079733744189015\n",
      "Operator 67: -0.07175863522981205\n",
      "Operator 68: 0.01158323847484146\n",
      "Operator 69: -0.00917204392742248\n",
      "Operator 70: -1.9480185101704726e-05\n",
      "Operator 71: 0.0026815755363344936\n",
      "Operator 72: -0.061104618848927664\n",
      "Operator 73: -0.048980777745557987\n",
      "Operator 74: -1.8689507133930026e-06\n",
      "Operator 75: -0.06689662710940308\n",
      "Operator 76: 0.019544135961775715\n",
      "Operator 77: 0.0019042135086098648\n",
      "Operator 78: 1.204388460786402e-06\n",
      "Operator 79: 0.046686441523481\n",
      "Operator 80: -0.0016485974954701058\n",
      "Operator 81: -0.009346460415848561\n",
      "Operator 82: 0.015333559205744607\n",
      "Operator 83: -0.005229811600453846\n",
      "Operator 84: 0.13289435845442582\n",
      "Operator 85: 0.0020201128037661605\n",
      "Operator 86: 0.009267555953696084\n",
      "Operator 87: -0.0019867491138352814\n",
      "Total gradient norm: 0.35896215947503435\n",
      "Operators under consideration (1):\n",
      "[84]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.13289435845442582)]\n",
      "Operator(s) added to ansatz: [84]\n",
      "Gradients: [np.float64(0.13289435845442582)]\n",
      "Initial energy: -13.491559253310998\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84]...\n",
      "Starting point: [np.float64(-2.1216857905674103e-06), np.float64(0.785397263323862), np.float64(-0.7853990270235119), np.float64(0.7853982937512481), np.float64(0.7853991644737626), np.float64(-0.21980334646248573), np.float64(0.05136063214842537), np.float64(0.08825872957486976), np.float64(-0.10239677435220021), np.float64(-0.18477022614074423), np.float64(0.2164045906014465), np.float64(-0.2166470693753072), np.float64(-0.06284752190372513), np.float64(-0.08212703914646874), np.float64(0.03483497711279116), np.float64(-0.249280220272268), np.float64(0.2197185007758299), np.float64(-0.2429651539650865), np.float64(0.10057129321364383), np.float64(-0.04690701899035551), np.float64(0.018781149866482605), np.float64(-0.16635139648979672), np.float64(0.027719246212600215), np.float64(0.01835086071816681), np.float64(-0.0261456181562706), np.float64(-0.026258834240745356), np.float64(0.007243745042963006), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.494007\n",
      "         Iterations: 32\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 53\n",
      "\n",
      "Current energy: -13.494007093029552\n",
      "(change of -0.0024478397185543344)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84]\n",
      "On iteration 28.\n",
      "\n",
      "*** ADAPT-VQE Iteration 29 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -9.5496321325661e-07\n",
      "Operator 1: -8.748125147658992e-06\n",
      "Operator 2: -1.9248487800327396e-06\n",
      "Operator 3: -1.9079860577031482e-06\n",
      "Operator 4: 2.717227265414124e-06\n",
      "Operator 5: -7.78925100861777e-06\n",
      "Operator 6: -4.907396701971795e-07\n",
      "Operator 7: -1.6471043975674338e-06\n",
      "Operator 8: 2.3254203345679763e-06\n",
      "Operator 9: 1.3562323898375084e-06\n",
      "Operator 10: 0.0006198487170023524\n",
      "Operator 11: -0.015037544685292241\n",
      "Operator 12: -0.003551688002478604\n",
      "Operator 13: -0.010307186145681543\n",
      "Operator 14: -0.006502553895630094\n",
      "Operator 15: 0.06293856956855265\n",
      "Operator 16: -0.029440653019124824\n",
      "Operator 17: 0.009354395430386943\n",
      "Operator 18: -0.028422422732977348\n",
      "Operator 19: 0.00017163284794921807\n",
      "Operator 20: -0.03224631274290285\n",
      "Operator 21: 0.08104626883293864\n",
      "Operator 22: 0.01967680746423094\n",
      "Operator 23: 0.0032158040972314387\n",
      "Operator 24: 0.0049984211471793645\n",
      "Operator 25: -1.037680335173874e-05\n",
      "Operator 26: 2.604159646871912e-06\n",
      "Operator 27: 1.3408392684211012e-06\n",
      "Operator 28: 3.159578520067763e-06\n",
      "Operator 29: -9.214587955996834e-07\n",
      "Operator 30: -0.0004031226396768638\n",
      "Operator 31: 1.0505103752618539e-05\n",
      "Operator 32: -1.0020706528533069e-06\n",
      "Operator 33: -2.585863269843891e-06\n",
      "Operator 34: -1.3807604308106768e-06\n",
      "Operator 35: 6.165043007211324e-07\n",
      "Operator 36: -0.03572348821267626\n",
      "Operator 37: -0.0015389699911494874\n",
      "Operator 38: -0.015844501385376557\n",
      "Operator 39: 0.004570995349837468\n",
      "Operator 40: 0.004291423421044249\n",
      "Operator 41: 0.0013622036846603104\n",
      "Operator 42: -0.03942202097031195\n",
      "Operator 43: 0.07163562278933702\n",
      "Operator 44: -0.019582916225071988\n",
      "Operator 45: 0.02777482524253081\n",
      "Operator 46: 1.313637974470483e-05\n",
      "Operator 47: 4.332471070497834e-06\n",
      "Operator 48: 0.004215285214545844\n",
      "Operator 49: 2.9521212263627916e-07\n",
      "Operator 50: -3.4715909764670982e-06\n",
      "Operator 51: -0.04638123749444983\n",
      "Operator 52: 0.02767245606342871\n",
      "Operator 53: 0.008411670150449184\n",
      "Operator 54: 0.010370400590455634\n",
      "Operator 55: 0.0039391293134658795\n",
      "Operator 56: 0.017836100695433458\n",
      "Operator 57: 0.006407124249437833\n",
      "Operator 58: 0.036688210239874605\n",
      "Operator 59: 0.0045849188579289\n",
      "Operator 60: 0.006788214144140969\n",
      "Operator 61: 0.005703993761755709\n",
      "Operator 62: 0.010609756982695576\n",
      "Operator 63: -0.049042684277401896\n",
      "Operator 64: 0.0008819659097590262\n",
      "Operator 65: -0.005087816859454064\n",
      "Operator 66: 0.03996625623405525\n",
      "Operator 67: -0.033393087498695975\n",
      "Operator 68: 0.020493890158963378\n",
      "Operator 69: -0.005422627940333163\n",
      "Operator 70: -0.005904384889625144\n",
      "Operator 71: 0.010941550349407195\n",
      "Operator 72: -0.05285665044478326\n",
      "Operator 73: -0.043817675420555646\n",
      "Operator 74: 1.133612400032378e-06\n",
      "Operator 75: -0.014066622009002536\n",
      "Operator 76: 0.004578586945819472\n",
      "Operator 77: 0.002295691037743699\n",
      "Operator 78: -0.0016405962338337907\n",
      "Operator 79: 0.010412414476144584\n",
      "Operator 80: -0.0032052329687871743\n",
      "Operator 81: 0.004381321536381316\n",
      "Operator 82: 0.017922375776918967\n",
      "Operator 83: 0.022667844454805762\n",
      "Operator 84: 1.2055027440193653e-07\n",
      "Operator 85: 0.008394650336994304\n",
      "Operator 86: -0.003341019795150412\n",
      "Operator 87: -0.0034513894051189445\n",
      "Total gradient norm: 0.2008849253776326\n",
      "Operators under consideration (1):\n",
      "[21]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.08104626883293864)]\n",
      "Operator(s) added to ansatz: [21]\n",
      "Gradients: [np.float64(0.08104626883293864)]\n",
      "Initial energy: -13.494007093029552\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84, 21]...\n",
      "Starting point: [np.float64(-1.335006462953015e-06), np.float64(0.7853998024620844), np.float64(-0.7853986221097746), np.float64(0.7853993638311358), np.float64(0.7853984239837173), np.float64(-0.22887356507820705), np.float64(0.06474425997217786), np.float64(0.07940452365164341), np.float64(-0.1137374375595216), np.float64(-0.18985406046067624), np.float64(0.2050268993875645), np.float64(-0.21915447780146488), np.float64(-0.06386214684679153), np.float64(-0.11980986952118988), np.float64(0.04386637277628894), np.float64(-0.2629218615006437), np.float64(0.22727846763158227), np.float64(-0.2524663073121355), np.float64(0.11184104881541661), np.float64(-0.05209427205978855), np.float64(0.02775971908792139), np.float64(-0.16412460314533678), np.float64(0.029496962338143255), np.float64(0.01959515271366093), np.float64(-0.03447789166338555), np.float64(-0.0181210309357953), np.float64(0.010482257179756448), np.float64(-0.03651187024225528), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -13.494743\n",
      "         Iterations: 32\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 57\n",
      "\n",
      "Current energy: -13.494742653721502\n",
      "(change of -0.0007355606919503543)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84, 21]\n",
      "On iteration 29.\n",
      "\n",
      "*** ADAPT-VQE Iteration 30 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -7.089115133043777e-07\n",
      "Operator 1: -9.108064871445748e-06\n",
      "Operator 2: -2.616660905729873e-07\n",
      "Operator 3: -6.382303574442583e-06\n",
      "Operator 4: 8.166552716293296e-06\n",
      "Operator 5: 1.928741491234731e-06\n",
      "Operator 6: -2.57268163281546e-07\n",
      "Operator 7: -2.396170629775929e-06\n",
      "Operator 8: 2.237316530589919e-06\n",
      "Operator 9: -1.7058096761224384e-06\n",
      "Operator 10: 0.007918132152946114\n",
      "Operator 11: -0.03380317275067819\n",
      "Operator 12: -0.009466562527852686\n",
      "Operator 13: -0.02918306064782556\n",
      "Operator 14: -0.006121757717916272\n",
      "Operator 15: 0.04632417672437754\n",
      "Operator 16: -0.017891080147236418\n",
      "Operator 17: 0.028103700443643832\n",
      "Operator 18: -0.039382845385302505\n",
      "Operator 19: 0.0038863302125560204\n",
      "Operator 20: -0.0078399507349524\n",
      "Operator 21: -5.731775921932038e-07\n",
      "Operator 22: -0.005949775740009797\n",
      "Operator 23: 0.0005210640741684343\n",
      "Operator 24: -0.0007661878298606096\n",
      "Operator 25: 1.4445399833216965e-06\n",
      "Operator 26: 3.39796384772606e-07\n",
      "Operator 27: -2.393323419384863e-06\n",
      "Operator 28: 1.3027570875331906e-06\n",
      "Operator 29: -4.542641686700044e-06\n",
      "Operator 30: 0.00446791177916675\n",
      "Operator 31: -1.9918542530812655e-06\n",
      "Operator 32: 9.346424784204693e-08\n",
      "Operator 33: -9.091204076355774e-08\n",
      "Operator 34: -4.0377196202506793e-07\n",
      "Operator 35: 3.068831882224732e-06\n",
      "Operator 36: -0.026267850579425354\n",
      "Operator 37: -0.001660745120822473\n",
      "Operator 38: -0.03486832969813526\n",
      "Operator 39: 0.007458035395428766\n",
      "Operator 40: -0.0018483095055692417\n",
      "Operator 41: -0.0005302449025299849\n",
      "Operator 42: -0.03586385476842924\n",
      "Operator 43: 0.044255104746024114\n",
      "Operator 44: -0.02105737022373787\n",
      "Operator 45: 0.044411410316050844\n",
      "Operator 46: -1.3809154817898128e-06\n",
      "Operator 47: -3.073932816378966e-07\n",
      "Operator 48: 0.004674062288895852\n",
      "Operator 49: 1.3054724641581306e-06\n",
      "Operator 50: -1.0274082314687405e-05\n",
      "Operator 51: -0.027212451601282918\n",
      "Operator 52: 0.03221568484440142\n",
      "Operator 53: -0.00209961645936624\n",
      "Operator 54: 0.025533423055791767\n",
      "Operator 55: 0.002084468841583014\n",
      "Operator 56: 0.004127154406101776\n",
      "Operator 57: 0.003839266624059934\n",
      "Operator 58: 0.03295384624186917\n",
      "Operator 59: 0.023977042729038996\n",
      "Operator 60: 0.0007361862794337752\n",
      "Operator 61: -0.0040825986223757766\n",
      "Operator 62: 0.02474265669033345\n",
      "Operator 63: -0.017499705568214143\n",
      "Operator 64: 0.0023981820472713695\n",
      "Operator 65: 0.008241018588638403\n",
      "Operator 66: 0.022200054573769248\n",
      "Operator 67: -0.012890938601333318\n",
      "Operator 68: 0.03726736750636085\n",
      "Operator 69: -0.0069261205310209035\n",
      "Operator 70: -0.0029626949260378697\n",
      "Operator 71: 0.007896760594097621\n",
      "Operator 72: -0.022049792583515714\n",
      "Operator 73: -0.023428587024438948\n",
      "Operator 74: -0.0022959308340140226\n",
      "Operator 75: -0.03482665070876943\n",
      "Operator 76: 0.007574792278399158\n",
      "Operator 77: 0.003395060464573965\n",
      "Operator 78: -0.0008475945834578772\n",
      "Operator 79: -0.027965157337094232\n",
      "Operator 80: -0.0015699260330894094\n",
      "Operator 81: -0.002798256771217889\n",
      "Operator 82: 0.0010908666727647148\n",
      "Operator 83: 0.039535009139720946\n",
      "Operator 84: -0.0014266081302289977\n",
      "Operator 85: 0.006666303328880124\n",
      "Operator 86: -0.008299858472577071\n",
      "Operator 87: -0.007171010839248763\n",
      "Total gradient norm: 0.16409811837499594\n",
      "Operators under consideration (1):\n",
      "[15]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.04632417672437754)]\n",
      "Operator(s) added to ansatz: [15]\n",
      "Gradients: [np.float64(0.04632417672437754)]\n",
      "Initial energy: -13.494742653721502\n",
      "Optimizing energy with indices [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84, 21, 15]...\n",
      "Starting point: [np.float64(-3.115251440700564e-06), np.float64(0.7853980542302658), np.float64(-0.7853994050085767), np.float64(0.7853988919743544), np.float64(0.7853990478248802), np.float64(-0.22309550258389585), np.float64(0.06354855052369679), np.float64(0.07200954017455802), np.float64(-0.12189086635294422), np.float64(-0.18546920379708115), np.float64(0.20466935875120346), np.float64(-0.22230574730882913), np.float64(-0.06379472210821847), np.float64(-0.10806107456232929), np.float64(0.06348434274814588), np.float64(-0.2719112500011216), np.float64(0.22412913127808837), np.float64(-0.2522133329275273), np.float64(0.10544926757039429), np.float64(-0.04815933202557822), np.float64(0.03603132692595852), np.float64(-0.16612900763175328), np.float64(0.028524157479126244), np.float64(0.01866145136372965), np.float64(-0.03181688843614849), np.float64(-0.01721505290413915), np.float64(0.01210499368829426), np.float64(-0.029944554348960935), np.float64(-0.018816933153063868), np.float64(0.0)]\n",
      "         Current function value: -13.495271\n",
      "         Iterations: 32\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 59\n",
      "\n",
      "Current energy: -13.495271316462574\n",
      "(change of -0.0005286627410718836)\n",
      "Current ansatz: [85, 71, 50, 84, 35, 40, 55, 41, 87, 53, 52, 71, 14, 37, 21, 30, 12, 55, 13, 23, 57, 48, 80, 85, 64, 74, 78, 84, 21, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 2 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(30):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "15\n",
      "20\n",
      "27\n",
      "34\n",
      "41\n",
      "48\n",
      "55\n",
      "62\n",
      "69\n",
      "76\n",
      "83\n",
      "90\n",
      "97\n",
      "104\n",
      "111\n",
      "118\n",
      "125\n",
      "132\n",
      "139\n",
      "146\n",
      "153\n",
      "160\n",
      "167\n",
      "174\n",
      "181\n",
      "188\n",
      "195\n",
      "202\n",
      "209\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 8, 'cx': 4, 'x': 3, 'rx': 2, 'barrier': 2, 'u2': 1, 'rz': 1, 'h': 1})\n",
      "On circuit 1/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 8, 'measure': 8, 'rx': 4, 'x': 3, 'h': 3, 'barrier': 3, 'rz': 2, 'u2': 1})\n",
      "On circuit 2/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 10, 'measure': 8, 'rx': 6, 'h': 5, 'barrier': 4, 'x': 3, 'rz': 3, 'u2': 1})\n",
      "On circuit 3/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 14, 'rx': 8, 'measure': 8, 'h': 7, 'barrier': 5, 'rz': 4, 'x': 3, 'u2': 1})\n",
      "On circuit 4/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 20, 'rx': 10, 'h': 9, 'measure': 8, 'barrier': 6, 'rz': 5, 'x': 3, 'u2': 1})\n",
      "On circuit 5/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 24, 'rx': 12, 'h': 11, 'measure': 8, 'barrier': 7, 'rz': 6, 'x': 3, 'u2': 1})\n",
      "On circuit 6/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 28, 'rx': 14, 'h': 13, 'barrier': 8, 'measure': 8, 'rz': 7, 'x': 3, 'u2': 1})\n",
      "On circuit 7/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 32, 'rx': 16, 'h': 15, 'barrier': 9, 'rz': 8, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 8/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'rx': 18, 'h': 17, 'barrier': 10, 'rz': 9, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 9/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 40, 'rx': 20, 'h': 19, 'barrier': 11, 'rz': 10, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 10/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 44, 'rx': 22, 'h': 21, 'barrier': 12, 'rz': 11, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 11/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'rx': 18, 'h': 16, 'barrier': 13, 'measure': 8, 'rz': 7, 'x': 4, 'unitary': 4})\n",
      "On circuit 12/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 46, 'h': 24, 'rx': 22, 'barrier': 14, 'rz': 11, 'measure': 8, 'x': 4, 'unitary': 1})\n",
      "On circuit 13/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 44, 'rx': 22, 'h': 20, 'barrier': 15, 'rz': 9, 'measure': 8, 'x': 4, 'unitary': 4})\n",
      "On circuit 14/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 56, 'h': 28, 'rx': 28, 'barrier': 16, 'rz': 14, 'measure': 8, 'x': 4})\n",
      "On circuit 15/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 60, 'rx': 30, 'h': 27, 'barrier': 17, 'rz': 14, 'measure': 8, 'x': 3, 'unitary': 2, 'u2': 1})\n",
      "On circuit 16/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 62, 'h': 30, 'rx': 30, 'barrier': 18, 'rz': 15, 'measure': 8, 'x': 4, 'unitary': 1})\n",
      "On circuit 17/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 64, 'rx': 32, 'h': 29, 'barrier': 19, 'rz': 14, 'measure': 8, 'unitary': 4, 'x': 3, 'u2': 1})\n",
      "On circuit 18/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 74, 'rx': 38, 'h': 35, 'barrier': 20, 'rz': 18, 'measure': 8, 'x': 3, 'u2': 1, 'unitary': 1})\n",
      "On circuit 19/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 80, 'rx': 40, 'h': 39, 'barrier': 21, 'rz': 20, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 20/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 80, 'h': 40, 'rx': 40, 'barrier': 22, 'rz': 20, 'measure': 8, 'x': 4})\n",
      "On circuit 21/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 88, 'rx': 44, 'h': 43, 'barrier': 23, 'rz': 22, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 22/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 80, 'rx': 40, 'h': 38, 'barrier': 24, 'rz': 18, 'measure': 8, 'x': 4, 'unitary': 4})\n",
      "On circuit 23/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 92, 'h': 46, 'rx': 46, 'barrier': 25, 'rz': 23, 'measure': 8, 'x': 4})\n",
      "On circuit 24/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 100, 'rx': 50, 'h': 49, 'barrier': 26, 'rz': 25, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 25/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 104, 'rx': 52, 'h': 51, 'barrier': 27, 'rz': 26, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 26/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 108, 'rx': 54, 'h': 53, 'barrier': 28, 'rz': 27, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 27/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 112, 'rx': 56, 'h': 55, 'barrier': 29, 'rz': 28, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 28/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 116, 'rx': 58, 'h': 57, 'barrier': 30, 'rz': 29, 'measure': 8, 'x': 3, 'u2': 1})\n",
      "On circuit 29/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 120, 'rx': 60, 'h': 59, 'barrier': 31, 'rz': 30, 'measure': 8, 'x': 3, 'u2': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sampler.run([to_run], shots=30_000)\n",
    "    job = sim.run(to_run)\n",
    "    # bit_array = job.result()[0].data.meas\n",
    "    # bit_array = job.result().data().meas\n",
    "    counts = job.result().data()['counts']\n",
    "    bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    all_counts = collections.Counter()\n",
    "    tuple_of_counts = tuple(counts_list[:i])\n",
    "    assert len(tuple_of_counts) == i\n",
    "    for counts in tuple_of_counts:\n",
    "        for bitstring, count in counts.items():\n",
    "            all_counts[bitstring] += count\n",
    "\n",
    "    bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAM6dJREFUeJzt3Ql8VOW9//HfTPYdEsIeSAISBApYBELxCiKW2l73VgFbccXeel3qVoSrgrTQq9alVerCX1m0uPwVLbUVC4J/qexYAYWwhCUQCFkg+z7n/3qeTEICgWSSMzkzcz7v+5rXnDMzmTw9M5d8fZbf4zAMwxAAAAAf5LS6AQAAAOdCUAEAAD6LoAIAAHwWQQUAAPgsggoAAPBZBBUAAOCzCCoAAMBnBYsfc7lckp2dLTExMeJwOKxuDgAAaAVVwq24uFh69uwpTqczcIOKCilJSUlWNwMAALRBVlaW9O7dO3CDiupJqf8fGhsba3VzAABAKxQVFemOhvq/4wEbVOqHe1RIIagAAOBfWjNtg8m0AADAZxFUAACAzyKoAAAAn0VQAQAAPougAgAAfBZBBQAA+CyCCgAA8FkEFQAA4LMIKgAAwGcRVAAAgM8iqAAAAJ9FUAEAAD6LoNIMwzDkeGGFHMov7fhPBAAANCCoNGPphkOSPn+1zPv7ruaeBgAAHYSg0ox+idH6fk9OSUd9DgAAoBkElWYM6Baj7w/ml0pFdW1zLwEAAB2AoNKMLtGhEh8VKoYhsu8EvSoAAFiFoNIMh8MhA7rVDf9kHC/u6M8EAAC4EVTOIc09/LMnh6ACAIBVCCrnMKB7XVDJIKgAAGAZgkpLPSoM/QAAYBmCyjlc4A4q2YUVUlRR3ZGfCQAAcCOonENcRIj0iAvXx3sZ/gEAwBIElVbUU8k4zhJlAACsQFA5jzT3hFpW/gAAYA2CSqt6VFiiDACAFQgq50EtFQAArEVQOY/+XaPF4RDJL62SvJLKjvtUAACARlA5j4jQIOkbH6mPqacCAEDHI6i0dp4KS5QBAOhwBJUWsPIHAADrEFRawMofAACsQ1BpdY9KiRiG0RGfCQAAcCOotCA5IUpCghxSUlmj9/0BAAAdh6DSgtBgp6R2idbHrPwBAKBjEVRaYYB7+IeVPwAAdCyCSiukdaNHBQAAKxBUWoFaKgAAWIOg4sHKn30nSqTWxcofAAA6CkGlFZI6R0p4iFMqa1xyuKDM+58KAADQCCqt4HQ6KPwGAIAFCCoezlPZw54/AADYL6i89NJL4nA4ZO3ateKL0ticEAAAewaV7OxseeaZZ8QfaqlQ9A0AAJsFlXvvvVdmzpwpvqy+R+VAXqlU1tRa3RwAAGzB8qCyYsUKCQkJkUmTJokv6xYbJrHhwVLjMnRYAQAA3hcsFiotLZVZs2bJypUrpbKyssXXq9c0fl1RUZF0FDV/RtVT2XzwpGQcL5aB3WM77HcDAGBXlvaoPP744/LLX/5SevTo0arXz58/X+Li4hpuSUlJ0pFY+QMAgE2CyrZt22Tjxo06qLTWY489JoWFhQ23rKwssaJCbcbxkg79vQAA2JVlQz+ffPKJlJeXy4QJE/R5RUWFvn/ggQekU6dOsnDhQunfv3+TnwkLC9M3q9CjAgBAx3IYhuETm9ccPHhQUlJSZM2aNTJ+/PhW/Yyao6KGgFTvSmys9+eMFJRWyffn/lMff/fUJIkMtXSKDwAAfsmTv9+Wr/rxJ/FRoZIYU9ejszeH4R8AALzNJ4KKGu6ZPHnyWce+iAq1AAB0HJ8Yu3jhhRfEX6h5Kuv25VGhFgAAu/So+JO07tH6PoPNCQEA8DqCiodY+QMAQMchqHjoAveePzlFlXKqrMobnwkAAHAjqHgoOixYeneO0Md7WPkDAIBXEVTagJU/AAB0DIJKGwxwl9Lfc7zY7M8DAAA0QlBpA3pUAADoGASVdq788ZEdCAAACEgElTZITYySIKdDTpVVS25xpfmfCgAA0AgqbRAeEiR9EyL1MYXfAADwHoJKe+epMKEWAACvIai0ERVqAQDwPoJKG6W5lyhnUPQNAACvIai0s0dlb06xuFys/AEAwBsIKm2UnBApoUFOKauqlaOnys39VAAAgEZQaaPgIKf06xqtj5lQCwCAdxBU2iGtmzuo5FBKHwAAbyComLHnD0EFAACvIKi0A7VUAADwLoKKCSt/MnNLpbrWZdZnAgAA3Agq7dCrU4REhQZJVa1LDuWXtuetAABAMwgq7eB0OuSChlL6Je15KwAA0AyCilnzVJhQCwCA6QgqZq38YXNCAABMR1AxqUeFJcoAAJiPoNJOA7rXFX07mF8qFdW1ZnwmAADAjaDSTonRYdI5MkTUvoT7TjChFgAAMxFU2snhcDTUU2H4BwAAcxFUTJDmnlDLyh8AAMxFUDFBQ48KK38AADAVQcXEHpU9OcxRAQDATAQVEwzoWhdUjp4ql+KKajPeEgAAEFTMERcZIt1jw/UxvSoAAJiHHhWzK9RSSh8AANMQVEyS1q2u8FsGE2oBADANQcUk1FIBAMB8BBXTV/4Um/WWAADYHkHFJP27RovDIZJXUiV5JZW2/2IBAGAGgopJIkODpU98pD6mVwUAAHMQVExEhVoAAMxFUDFRmruUfgYVagEAMAVBxUTUUgEAwFwEFS/0qKjNCQ3DMPOtAQCwJYKKiVK6REmw0yHFlTVyrLDCzLcGAMCWCComCg12SmpilD7OoJ4KAADtRlAxGSt/AAAwD0HFayt/qFALAEB7EVRMxsofAADMQ1DxUo/K3pwSqXWx8gcAgPYgqJgsKT5SwkOcUlnjkqyCMrPfHgAAWyGomCzI6ZALujJPBQAAMxBUvICVPwAAmIOg4gVp3aP1PSt/AABoH4KKF3tUvjtW5I23BwDANggqXjA8qZM4HCKZuaWSW1zpjV8BAIAtEFS8oFNkqAzsHquPNx0o8MavAADAFggqXpKeGq/vN2Tme+tXAAAQ8AgqXjI6JUHfE1QAAGg7goqXjE6p61HZe6JE8kqYpwIAQFsQVLykc5Sap1K3+od5KgAAtA1BxYvSUxn+AQCgPQgqHTChdmMmK38AAGgLgooXjXJPqFUVavOZpwIAgMcIKl4UHxUqae4qtcxTAQDAz4LKxx9/LFdeeaVcfvnlcskll8j3v/99WbZsmQQS6qkAAOCnQeXPf/6zTJkyRVavXi3r1q2TOXPmyM033yzbt2+XQJtQu5EKtQAA+FdQ+d3vfidTp05tOB8/frwYhiGZmZkSKEa566nsPl4sBaVVVjcHAAC/YmlQGTFihAQHB+vj6upqefbZZ2XQoEEyceJECRQJ0WEyoFu0Pt50gHL6AAD43WTae+65RxITE2XVqlWycuVKiY6u+8N+psrKSikqKmpy8696KixTBgDA74LKyy+/LHl5eXroZ+zYsXLs2LFmXzd//nyJi4truCUlJYk/YN8fAAD8OKgoagho7ty54nK55Lnnnmv2NY899pgUFhY23LKyssQfjE49PU/lJPNUAADwj6BSVdV0cqnT6ZQBAwbId9991+zrw8LCJDY2tsnNH3SJDpMLurrnqRxk+AcAAL8IKqpuypnUsE/Pnj0l0LDvDwAAfhZUVM/JJ5980nD+1ltvSUZGhkybNk0CTf3wDxNqAQBovbq1wRZ58cUXdS0VNUlWzU1xOBzy17/+VVepDTT1E2p3Hy+SU2VV0iky1OomAQDg8xyGqrDmp9TyZLX6R02s9Yf5KhOf+0L2nSiR134xQn44uLvVzQEAwOf/fvvMqh87GO2uUsvwDwAArUNQ6UBMqAUAwDMEFQsm1O46XiSFZdUd+asBAPBLBJUO1DUmXFITo0TNCqKeCgAALSOodDCGfwAAaD2CikVBZSM7KQMA0CKCSgdLd6/8+Ta7SArLmacCAICpQWX79u3y7bffevpjcOsaGy6pXermqWw+wL4/AACYGlSGDx8uzz//vKc/hkZGu4d/NmTmc10AADAzqKjy9gsXLvT0x9BIunuZ8kZ6VAAAMDeoDBkyRLKzs5t97uqrr/b07Wypft+fb7MLmacCAICZmxLGxMTID37wA7n88suld+/eEhQU1PDczp07PX07W+oeFy4pXaLkQF6pbDlYIJdf2M3qJgEAEBhB5bXXXtPzVDIzM/WtsVOnTpnZtoAf/lFBRQ3/EFQAADApqKg5KitWrGj2uSlTpnj6drYe/lm2KYsJtQAAmDlH5VwhRVm2bJmnbyd23/dn59FCKaqgngoAAKYVfDt06JDcd999ctlll+mbOlaPofV6xEVIckKkuAyRrQdPcukAADAjqKxdu1YGDhwoX375pXTp0kXf1q1bJxdeeKF88cUXnr6drdWv/qGeCgAAJs1RmTlzpvz1r3+VK664osnjq1atkhkzZsj69es9fUvbSu8XL+9uYZ4KAACm9agYhnFWSFEmTpyon4PnPSo7jhZKMfNUAABof1ApLS2VvLy8sx7Pzc2VsrIyT9/O1np2ipA+8XXzVLYcYp4KAADtHvqZNm2ajBgxQm677Tbp16+ffmzfvn2yePFiPakWntdTOVxQpuepXJbWlcsHAEB7gspDDz2kq9POmzdPDh8+rB/r06ePzJo1S+666y5P38720lMT5L0tR2RDJjspAwDQ7qBSVFSkC7tNnz5dSkpK9GPR0dGevg3O2ElZ1VMpqayR6DCPPxIAAAKWx3NUOnXqJDfccENDQCGktE+vThGSFB8htS5D7/sDAADaEVRGjhwpn332mac/hvNIb6inQlABAKBdQSUtLU2Ki4ubfU4NB6Htwz8bD+Rz+QAAaMTjCRFDhw6V8ePHy7XXXiu9e/eWoKCghudUhVp4bnRK3b4/248USmlljUQxTwUAgLYFlccff1y6d+8ub7zxxlnP5eTkePp2EJGk+Ejp3TlCjpws1/VUxg1I5LoAANCWoJKeni5r1qxp9jm1QSHaXqX2yEm1TDmfoAIAQFvnqNx5553y97//vdnnzhVg0LrCb8rGTOapAADQ5qCiKtJu3brV0x9DKwq/NZ6nAgAA2hBULr30Uj1PpTns9dO+eSqqpkqNy5Ct7PsDAEDb66js2LGj2ef+8z//09O3QyOj64d/WKYMAEDbJtNmZ2fr5cnDhw8/a3ny7t27PX07nDH88+G2oxR+AwCgrUFFVaW9+uqrG84Nw/D0LXAOY9zzVL7JOiVlVTUSGcq+PwAAe/P4L6Ea3nn99debfe7Xv/61GW2yLVVLpWdcuGQXVsi2Q6fkkgu6WN0kAAD8a47KuUKK8vzzz7e3PbbmcDgaVv+oeioAANidx0FFeffdd2XcuHEyduxYfT537lxZunSp2W2zJYIKAADtCCqvvvqqPPzwwzJs2DApLy/Xj11//fWyfPlyefHFFz19O5xj5c83R05JeVUt1wcAYGseBxXVc/LNN9/IH//4R4mLi9OPDR48WPeyfPDBB95oo630iY+UHnHhUl1ryLbDJ61uDgAA/hVUnE6nxMfHN8ypqBcSEiJVVVXmts6GmKcCAEA7gkplZaXs3LnzrMdXrVoltbUMVZhhdEpdEGRCLQDA7jxenjx79my9g/KECRNk7969eu+fjIwM2bZtm6xYscI7rbSZUe6g8s2RQql1GRLkPN1zBQCAnXjco3LllVfKxo0b9fBPt27ddDn9AQMGyNdffy1XXHGFd1ppM30ToiQ0yClVNS7JPlU3YRkAADtqU+lTNXl20aJF5rcGmupB6ZsQKXtPlEhmXqnesBAAADtqUx0VeF9Klyh9fyC3hMsNALAtgoqPSkl0B5W8UqubAgCAZQgqPqpfl2h9r4Z+AACwK4KKj/eoZOYSVAAA9uVxULn00ku90xI0O0clu7BcKqqpTwMAsCePg8p3330no0aNkjlz5sihQ4e80ypIQlSoxIYHi2GIHMynVwUAYE8eB5U77rhDvvrqKxk6dKjcf//9MmnSJHnrrbekoqLCOy20cSn9lMS6eSoHGP4BANiUx0Hlf//3fyU4OFiuu+46+eijj/QmhVu2bJEePXrI3XffLRs2bPBOS20o1T38w4RaAIBdeRxU3n//fX1fXV0t7733nkybNk1eeuklSUhIkF69esmbb74pl1xyiaxdu9Yb7bVnLRVW/gAAbMrjyrRqbsqXX34pb7/9tt4t+ac//al8/vnnTSbZnjp1Sn74wx/Kpk2bzG6vraQ2rPyh6BsAwJ6C2zKZVvWePPvss3LjjTdKVFTdH9PGdu3aJdnZ2Wa10bboUQEA2J3HQWXq1Kl68uz5qJ6WBQsWtKddaBRUTpZVy8nSKukcFcp1AQDYisdBJTU1tcXXjBs3rq3tQSORocHSIy5cjhVW6Am1IwgqAACb8TioqFU+ISEhYqgCH2dQjycnJ8uVV14pnTp1MquNYvdeFRVU1ITaEX07W90cAAB8O6j07dtXnnrqKb0cuU+fPrrex+HDhyU/P18uvvhiOXbsmK6vsnLlSrnooou802qbBZWv9ufLgTwm1AIA7Mfj5cljxoyRZcuW6XCybt06vQJIVahdvHix/OhHP5KMjAw9h+WRRx7xTottJrW+6BtLlAEANuRxUFFLjtWS5DPdcMMNepmyopYmqwm1MLHoG9VpAQA25HFQ2b9/v66TcqaCggLdmwLvLVF2uc6eFwQAQCDzeI7KVVddJSNGjNAVaVNSUvRjmZmZsmTJEl1WX1WsnT9/voSFhXmjvbbTu3OEhAQ5pLLGJceKKqRXpwirmwQAgO8GlRdeeEGXyv/Tn/6kJ84qamLtfffdJw8//LCUl5frgnAqrLSGKsO/cOFCqa2tlaKiIr1q6JlnntH3EAkOckqf+EjZn1uqK9QSVAAAduIwmltnfB4qTKiVPjExMfpYiY2NbXMDQkNDZcWKFXoXZpfLJbfeequeB/PNN9+02Cujfn9cXJwUFha2qw2+7s7FW2TVrhx56prBcssYAhwAwL958vfb4zkqqj6KmjirqDdvb0C45pprdEjRjXE6dc+Mmuuybdu2dr1vYO75U2p1UwAA6FAeB5WRI0fKZ599ZloD6ndjrhceHq7vKysrTfsdgbLyhyXKAAC78TiopKWlSXFxcbPPTZ8+vd0NWr9+vfTs2VPGjh171nMqvKjuosY3O638yaToGwDAZjyeTDt06FAZP368XHvttdK7d28JCgpqeE4VgGsPFUTURNqXXnpJl+M/k5qgO2fOHLGbFPfQz5GT5VJZUythwaevOQAAgczjybQRERHSvXv3Zp/LycmRsrKyNjdGTaRNSkqSuXPnnjPINB4SUj0q6vWBPplWfURDZ38mxZU18s9fXyoXdIuxukkAAHTIZFqPe1TS09NlzZo1zT532WWXSVvNmDFDIiMjzxlSFLUKyI71WdQqK9Wrsv1IoV6mTFABANiFx3NU/va3v53zuXMFmJb8/ve/l6ysLD3ko2zdulXf0HyFWgAA7MLjoBIVFaVDxZNPPikPPvigfmz58uWyd+/eNjXglVde0ZsY3nvvvXpJ8pYtW3RdlR07drTp/QI/qLCLMgDAPjwe+lETZtWmg/3795eamhp57rnndNl8VT7/xRdflMsvv7zV76VWD91zzz260JvalbmxN99809Om2WIXZWqpAADsxOOg8vjjj8vq1at1sKifk3LjjTfq45tuusmjoKKq26rS+WgZtVQAAHbkbMsKlPreDzXJs15iYiKhw4uS3UM/+aVVUlhW7c1fBQCA/wYVtZSouYJvat5KXl6eWe3CGaLDgqVbbN2KpwP5TKgFANiDx0Fl6tSpMnr0aD03JTc3V5YsWSIzZ87Uy5bvuusu77QSTSvU5jKhFgBgDx7PUXnkkUd0kZZ58+bJ4cOHdZG2Pn36yOzZswkqXpbSJVo2ZBawRBkAYBseB5X6PX3UraSk7r/so6PrVqTAu/rV76JMLRUAgE14PPTTmAoojUOK6m1BRwz9MEcFAGAPHveoqJopf/nLX+Tf//63rtXfeKugTz/9VG8qCO8GlYN5peJyGeJ0nl51BQBAIPI4qEybNk2+/PJLGTVqlK6D0niJMrwrKT5Sgp0OKa+ulZziCukRF8ElBwAENI+DiupJUeXyw8PDz3pOrf6B94QEOaVPfKSeo3Igt5SgAgAIeB7PURk4cGCzIUW55ZZbzGgTWjH8s58JtQAAG/C4R2Xy5Mny3//937qeSo8ePSQoKKjhudtvv12++uors9uI5jYnZEItAMAG2hRUlAULFjSZn6Im1TJfpeM2J2QXZQCAHXgcVFRV2nfeeeesx1VQmTJlilntQktLlBn6AQDYgMdB5dlnn5W+ffs2+9wrr7xiRptwHqnuom9ZBWVSVeOS0OB2lcIBAMCnefxXbuzYsed8btiwYe1tD1rQNSZMokKDxGWIHC4o43oBAAJaq4JKSkqKpKam6vopzXnvvff0ayIjI81uH86g5gGluHtVDjD8AwAIcK0a+klOTpY1a9bo4zlz5jSZNPvEE0/IjTfeqG9jxozxXkvRZHPCnUeL3Lsod+PKAADs3aPSOJio0KLmqKgJter4XK9DByxRpkcFABDg2lRCX1m0aBEF3izCLsoAALto85IRek+swy7KAAC7aFWPyrFjx2Tp0qVNdko+fvz4WY/l5uZ6p5VoNqjklVRKUUW1xIaHcIUAAPYNKhkZGQ1DPo2d+Ri9LB0jJjxEEmPCJLe4Ug7mlcrQ3p066DcDAOCDQz/jxo0Tl8vV4m3UqFHebzE0JtQCAOygVUHl6aefbtWbvfDCC+1tD1optX4XZTYnBADYPaiMHDmy1fsAoWPQowIAsAM2ivFT7KIMALADgoq/96jkljZZeQUAQCAhqPipPvGREuR0SGlVrZworrS6OQAAeAVBxU+FBjslqXOEPs5kQi0AIEARVPwYE2oBAIGOoOLnuygrdbsoAwAQeAgqfiw1kV2UAQCBjaASAEXfDuSVWt0UAAC8gqDix1LcPSqHC8qkutZldXMAADAdQcWPdYsJl4iQIKlxGZJVUGZ1cwAAMB1BxY85nQ5W/gAAAhpBJUCGf5inAgAIRAQVP8cuygCAQEZQCZglytRSAQAEHoJKgBR9Y+gHABCICCoBUkY/p6hSSiprrG4OAACmIqj4ubiIEOkSHaqPD1L4DQAQYAgqAdSrkklQAQAEGIJKIO2inEspfQBAYCGoBNIuyqz8AQAEGIJKAGAXZQBAoCKoBNIuyrmlYhiG1c0BAMA0BJUA0CchUpwOkeLKGsktqbS6OQAAmIagEgDCgoOkd+dIfcyEWgBAICGoBNrKH5YoAwACCEElQFBLBQAQiAgqAbbyJ5NaKgCAAEJQCRCpDZsTsosyACBwEFQCRIq7R+VwQZnU1Lqsbg4AAKYgqASIHrHhEh7ilOpaQ46cLLe6OQAAmIKgEiCcTockJ7DyBwAQWAgqgTihliXKAIAAQVAJxCXKuUyoBQAEBoJKQK78KbW6KQAAmIKgEoArfwgqAIBAQVAJwF2UjxVWSFlVjdXNAQCg3QgqAaRTZKh0jgzRx/SqAAACAUElwKQmMk8FABA4CCqBuosye/4AAAKA5UGlqqpKZsyYIcHBwXLw4EGrm+P32EUZABBILA0qKpiMGzdOjh07JrW1tVY2JWD0o+gbACCAWBpUSkpKZOnSpXLbbbdZ2YyAklJfSyW3RAzDsLo5AAC0S7BYaMiQIfr+yJEjrXp9ZWWlvtUrKiryWtv8Vd+ESHE4RIoqaqSgtEoSosOsbhIAAP47R8UT8+fPl7i4uIZbUlKS1U3yOeEhQdKrU4Q+Zs8fAIC/86ug8thjj0lhYWHDLSsry+om+SRW/gAAAoWlQz+eCgsL0ze0XKH2y7159KgAAPyeX/WooHXYRRkAECgIKgGI6rQAgEBBUAngHpVD+WWSU1RhdXMAAPDPoKKq0o4fP14eeOABfT558mT52c9+ZmWTAoJa9dO/a7RU1bpkymsbCCsAAL/lMPy4Kpiqo6KWKasVQLGxsVY3x6dkFZTJ5Nc2yNFT5Xpy7TvT06VrbLjVzQIAQDz5+83QT4BKio/U4UT1rqh6KpNf3yAnGAYCAPgZgopdwkpuqUxRYaWYOSsAAP9BULFBWFl2V7r0jAuX/SqsvEZYAQD4D4KKDfRJUD0rYxrCytTXN0pu8ek9kwAA8FUEFRuFlWXT06VHXLjsO1Gih4EIKwAAX0dQsZG+CXWrf+rDylTCCgDAxxFUbBhW1JyV7rHhstcdVvJKGAYCAPgmgooNJbvrqtSHFTXBlrACAPBFBBUbhxU1Z6VbbBg9KwAAn0VQsfmeQGo1kAore3JK5ObXN0o+w0AAAB9CULE5FVbUnJWuMWGSkVOsly4TVgAAvoKgAklNjK7bC8gdVm5eSFgBAPgGggoawsoyd1jZfbwurBSUVnF1AACWIqigQT93WEl0h5XJr62XHUcKuUIAAMsQVHB2WLmrLqyoCbZXv7xOZi3fIafK6F0BAHQ8ggrO0r9rtHxy7yVy7fCeYhgib288LJc9u1aWbTosLpfBFQMAdBiHYag/Rf6pqKhI4uLipLCwUGJjY61uTkDakJkvT378rZ5kqwxL6iRzrxksQ3t3srppAAAb/P0mqKBF1bUuWbL+kDz/zz1SUlkjDofI5JF95NFJadI5KpQrCADwWlBh6ActCglyyh2XpMjnD42T6y7qpYeD1DDQZX9YK3/ZeFhqGQ4CAHgJPSrw2KYDBfLExzv1yiBlaO84eeqaITI8ieEgAEDLGPqB19U0Gg4qdg8H3XRxkjz6o4ESz3AQAOA8GPqB1wUHOeX2S1Jk9cPj5Prv1w0HvbM5S68OemvDIYaDAACmYOgHpth8sEAe/+j0cND3eqnhoMFyUZ/OXGEAQBP0qKDDjUyOl7/de4nMvmqQxIQHy46jhXLDn7+S9zZn8WkAANqMVT8wdTjo1rFqddB4uWpYT1GLgR79YLu8+a8DXGUAQJsQVGA6VX7/j5OHy13/kaLP56z4Tl5es48rDQDwGEEFXuFwOGTmjy+UByZeoM+fWZkhT3+6W/y4EDIAwAIEFXg1rDwwcYDM+vGF+nzB2v26d4X9ggAArUVQgdfddWmq/PbaIfp40VcH5TcfbGf5MgCgVQgq6BA/T+8rf/jZMHE6RN7fekTuf+drvYcQAADnQ1BBh7lhRG95eer3JSTIIX/bfkz+662tUlFdyycAADgnggo61JXf6yGv3XKxhAU7ZdWuE3LH4s1SWlnDpwAAaBZBBR3usrSusui2URIVGiT/2pcvt7yxSQrLq/kkAABnIajAEmP6Jchbd46W2PBg2XropEx9fYMUlFbxaQAAmiCowDJqH6B3po+RhKhQ+Ta7SG56db3kFFXwiQAAGhBUYKlBPWPl3bvHSPfYcNl7okRufHW9HDlZxqcCANAIKrBc/67R8v4vx0hSfIQcyi+Tn72yXjJzS6xuFgDABxBU4BOS4iPl/bt/IP0So+RYYYXc+OoG2X28yOpmAQAsRlCBz+geF66HgQb1iJW8kkq56dUNsvNoodXNAgBYiKACn9IlOkyW3ZUuw5M66SXL097YJPsZBgIA2yKowOfERYbIkjtGyZBesZJfWiU/X7iRCbYAYFMEFfik2PAQWXzbqIY5Kyqs5BZXWt0sAEAHI6jAZyVEh+micL06RcjB/DL5xf/ZKIVlVLAFADshqMCn9YiLkLfvHC2JMWGy+3ix3LpoE3sDAYCNEFTg85K7RMnSO0ZJXESIfH34lExfuoVdlwHAJggq8AsDu8fK4ttPb2R477KvpabWZXWzAABeRlCB31BLll+fdrGEBjvln9/lyKP/d7u4XIbVzQIAeBFBBX7lB/26yIKp35cgp0M+/PqozF7xrRgGYQUAAhVBBX5n4qBu8tyNw8ThEFmy/pD84bM9VjcJAOAlBBX4pWuG95K51wzRxy+t2SevfrHf6iYBALyAoAK/9fP0vvKbHw3Ux/P/sVv+svGw1U0CAJiMoAK/9l/j+8mvxvfTx7M+2iEf//uo1U0CAJiIoAK/98ikNPlFel9Rc2ofeu8bWb0rx+omAQBMQlCB33M4HDLn6sFy3UW9pMZlyK/e3ibr9+db3SwAgAkIKggITqdDnv7pULliUDeprHHJnYs3yzdZp6xuFgCgnQgqCBghQU7505SL5Af9EqS0qlamvblJ/vBZhvy/PblSUlljdfMAAG3gMPy4WlZRUZHExcVJYWGhxMbGWt0c+IjSyhq5eeFG+XejHhWnQ2RQz1gZmRyvbxcnd5auMeGWthMA7KrIg7/fBBUEpLKqGvn439my+UCBbDpYIEdOlp/1muSEyIbgMjIlXp+r+S4AAO8iqABnOFZYLpsPnpQtBwtk04ECycgp1quEGusSHSYjkzvLxcnxMio5Xi7sESPBQYyOAoDZCCpACwrLq2XboZOy+WCBvn2TVShVZ+zGHBbslIjQoHZdy9jwEOkcFSoJUaHSOTJUEqLr7uOjQiQ+Kkzf68ejwiQmPFhPCgaAQFfE0A/gmYrqWtlxtLAuuBwokC2HTkpxRcdOwFUbLdaHmLr7UL1TdHtEhgZJXIQKRyHSKTKk0XHdfVxkiIQFty+MAYCnCCpAO7lchmSdLJPq2rbPNVfz1IsqqiW/pEpOllVJQWm1FJRW6nt1nl9aJSdL1eNVlq5KiggJcoeWswNNj7hw6dc1Wvp3jZbE6DDm8ADo8KASbM6vBAKLGoLpmxDVYb+vsqZWTpU1DjV19+0NSmVVde97qqxKTpXXBaRCdV5e95jLECmvrpXywlrJLqw47/vFhgfXhZbEuuBSf+vdOVL3BgGANxBUAB+ghl+6xapbeIf2GhVX1tSFmDJ3iFFhptQdakqrJOtkuezPLZHDBWVSVFEjXx8+pW9N2+6UlC5RDcGlnzvIqMfCQxhWAhAAQWX58uUyb948CQ8PF6fTKQsWLJDBgwdb3Swg4HuN4iLUME+I9E1oeQ7PgbxS2XeipO6WWyL7T5RIZl6prgS8+3ixvjV5f4fouTBW9rWo5eZRYUESFRos0WHBEqmOw4IlKrT+Prju3v0afa9e5369Oo8JD5GYMCY6A7YNKps2bZJp06bJ1q1b5YILLpAlS5bIpEmTZNeuXRITE2N18wCI6J6RC3vE6ltjtS5DjpwsOx1g3CFG3avJyGoIy2p5Je1/D1VeRwUXtYorNiJEr9CqO3bfq/OIkIbHVLipP1bBR/2s6nmiTg/gOcsLvl1//fUSFhYmy5Yt0+cul0t69uwps2bNknvvvfe8P0tlWsA3qX9Wcksq9ZCSlWpq1TydGr2lgqpY3HBrfF5/3OQxdV+rJzlX1TRdtt5Wah6PCiz1PTX1AabuvPnHVc+O+jnVO6XCkkP9nz5WR3WPOd3H0uhYPV//M/oJ+D3nGZ/76e/C2ceNP/u6x+rUB+XT56ff3/0tavJY/bn6HqpViLadTLt69Wp54oknGs7V0M+IESNk1apVZwWVyspKfWv8PxSA71H/IKotCgJhmwI17KV6h9QKrqLy6kbHNVKs7t3H6l4/V17d5FgFoPreJzUHSN0Af3L1sJ7yxykXWfb7LQ0q+fn5Omx069atyePdu3eXzZs3n/X6+fPny5w5czqwhQDsTg17qVtiTFibJy037qFRPTbqvrlj9RoVcE736tSIXvhlGHqFlqH+z5C6Y3dnuMuoe6zuZU2P1evg/wz35153U2fu40bfB312xmdf/1r3mzR6P/d9owGV0481/b31G75aydKgUlZWpu/V0E9j6rz+ucYee+wxefDBBxvOVchJSkrqgJYCQNsnLesJueEhXELA34JKZGSkvm88nFN/Xv/cmQHmzFADAAACl6X9OQkJCXoyTU5OTpPHjx8/LqmpqZa1CwAA+AbLt4adMGGCXposjcbMtm3bJhMnTrS0XQAAwHqWB5UZM2bIJ598Ivv27dPnb7/9tgQFBenaKgAAwN4sX548atQoWbRokUyePFkiIiL08uSVK1dS7A0AAFhf8K09KPgGAEBg//22fOgHAADgXAgqAADAZxFUAACAzyKoAAAAn0VQAQAAPougAgAAfBZBBQAA+CyCCgAA8FmWV6Ztj/padapwDAAA8A/1f7dbU3PWr4NKcXGxvk9KSrK6KQAAoA1/x1WF2oAtoe9yuSQ7O1vvC+RwOExPeyoAZWVltVjeF1w/b+A7yPWzGt9Brp+3qOihQkrPnj31Hn8B26Oi/sf17t3bq79DhRSCCtfPSnwHuX5W4zvI9fOGlnpS6jGZFgAA+CyCCgAA8FkElXMICwuTJ598Ut/Dc1y/9uMacv2sxneQ6+cL/HoyLQAACGz0qAAAAJ9FUAEAAD6LoAIAAHyWX9dR8Zbly5fLvHnzJDw8XNdqWbBggQwePNjqZvmF2bNny0cffSSdOnVqeCw+Pl4+/PBDS9vl66qqquSJJ56QZ599Vvbt2yfJyclNnn/11Vfltdde099JdW3Vca9evSxrrz9dv1tvvVV2796tr129QYMG6f+/Rp333ntPFi5cKLW1tbrIm7p+zzzzTMN1VFMZ586dq/9/Ozg4WAYMGCAvv/xyq+tg2P36jR8//qyfmTBhgv7OohXUZFqctnHjRiMmJsbYs2ePPl+8eLHRq1cvo6ioiMvUCk8++aSxZs0arpUHDhw4YKSnpxu33HKLmtiuzxv74IMPjB49ehi5ubn6fM6cOcbw4cON2tparnMrrt+0adPOegxNhYSEGJ9++qk+Vt+rX/ziF0ZaWppRUVGhH/vDH/5gDB061CgrK9Pnt912m3HVVVdxGVt5/caNG8e1ageGfs7w+9//Xn7yk5/IBRdcoM9//vOfS01NjSxatKg1uQ/wWElJiSxdulRuu+22Zp//7W9/K9OmTZMuXbro8/vvv1927twpn3zyCVe7FdcPLbvmmmtk0qRJ+lj1It93332SkZEh27Zt070E6t/FX/3qVxIREaFf8/DDD8uKFStkx44dXN4Wrh/aj6ByhtWrV8vFF198+gI5nTJixAhZtWqVCZcbONuQIUOkf//+zV6agoIC+frrr5t8J1V3u+p65zvZ8vVD67z//vtNzuuHySorK2X79u2Sm5vb5Dt44YUXSlRUFN/BVlw/tB9BpZH8/Hw9vtitW7cmF6l79+5y4MABEy63Pbzxxht6THbs2LG6J2D//v1WN8lv1X/v+E62z/z58/V38pJLLpF77rlHcnJyTPl8AtX69ev1ZnHq/4czMzPP+g6qTWDVOf8utnz96qme0HHjxsmll14qM2bM0BvyoXUIKo2UlZXp+zOr0arz+udwfn369JGLLrpI/5fWl19+KSkpKbpH6ujRo1y6NuA72X6q90n9cfj8889lzZo1+r9y09PT9ZARzqauj5oI+tJLL0lISAjfwXZeP2X48OF6SsEXX3whf//73/WQ2RVXXKGH1dAygkojkZGRzXbXqfP653B+t99+u/z617/WKwPUsNnjjz+uu0FZYdE2fCfbb+bMmXLzzTfr76P6w/Hcc8/J4cOHZdmyZSa8e+C5++675aabbpLrrrtOn/MdbN/1U1544QX54Q9/qI+jo6Pl6aeflo0bN+rwjJYRVBpJSEjQ4/9ndgsfP35cUlNTW3E5caagoCC9RI/hn7ap/97xnTRPbGysJCYm8p1shhqSUMFELUVu6Tuozvl3seXr15x+/frpe/5dbB2CSjNr27du3dpwruoHqJnbEydObOUltTc1Dnum7OxsPSQEz3Xu3FkPpTX+Tqp5VHv27OE72cbvpOohVfPR+E42pVb2ZGVl6SELRX3n1G3o0KE62DX+Du7atUtKS0v5Drbi+p04cUJ+97vfNbnW9UPhfAdbqT1rmwO1jkpsbKyxd+9efb506VLqqHggOTnZ+PjjjxvOX3/9dSM8PNzYtWuX+R9WgFH1Z85VR6Vnz55GXl6ePp87dy51VDy4fqGhocbmzZsbzv/nf/7HSExMNE6cOOGdD9IP/fnPfzYGDx5srF+/Xl8rdVM1kd58882GOirDhg1rqKNyxx13UEellddPfR/j4+Mbvpc1NTW6ts/AgQON8vJyKz5uv0Nl2jOMGjVK10yZPHmyrhmgxrVXrlwpMTExrc1+tqb+y0GNx6p5AKpaqJqIrCbWDhw40Oqm+Sx1ndT49alTp/S5+u4lJSU1LHm8/vrr9X+Vqcl3ar6P6mVRNSzUdxMtXz9VrbZ+3pSanKx6B9SkWnUP0atP1Eool8slY8aMaXJJ3nzzTX2vrp+afKxWsajrqOpMLVmyhMvXiuunVo0+9NBDMmXKFP3voeqJUtdP/V1pXC0Z5+ZQaeU8zwMAAFiG/yQDAAA+i6ACAAB8FkEFAAD4LIIKAADwWQQVAADgswgqAADAZxFUAACAzyKoAAAAn0VQAdCiTZs2yfjx48XhcOgqw0899ZSuBDt79uyGirAd4eDBg/p3nunaa6+V559/vsPaAaDjUJkWQOv/wXA4dFnwW2+9VYeGlJQUOXDggN4huyOsXbtWLrvsMr1ZaGOqxLva/kKVKQcQWNjrB4DfozcFCFwM/QDw2Hfffac3/1PUvRoWWr58uT5Xm9fdddddctFFF8m4ceP0sMzhw4f1c+vWrZP09HTdM6M2Dbzmmmukf//+Mnz4cP38ggULZPTo0brXZOTIkXqTy/rek88//1weeOABfax+n7qtX79eHn30Ud2jo84bW7p0qX5f9X6qLfWbFCp33nmn3izulltukd/85je6nWlpaXqjOAA+xurtmwH4D/VPhtq6XlHb1qvz+u3r602ZMkXfamtr9fm8efOMQYMG6e3tG//c7bffrl9TXFxsjB8/Xj83cuRIY8eOHfq4pKTEGDp0qLF48eKG916zZo3+2TM9+eSTxrhx4xrOV65caURHRxu7d+/W59u3bzfCw8ONf/3rXw2vmTZtmtG5c2dj165d+vzFF180+vTpY+LVAmAGelQAmCYzM1PeeecdefDBB8XprPvnZfr06boHRs0vaUz1ZqjXREdHy5o1a/RjqtdjyJAh+jgqKkp+/OMfyz/+8Q+P26F6YlRPjuolUb73ve/JpEmTZN68eU1ep3pa1ORgRfXIqJ6fkydPtvF/PQBvYI4KANN8++23eqjm/vvvl5CQkIbH+/btK7m5uU1e27t377N+/siRI3LfffdJXl6e/vn6Cbue2rlzp0yYMKHJY2qIqfHwj9KzZ8+G45iYGH1fVFQknTt39vh3AvAOggoA07311lstBoygoKAm54cOHZIrrrhCL31++OGH9WNqKfKZPTFmatwGNW9GOXNFEQBrMfQDoG3/eLiHdhSXyyWlpaUyePBgfZ6RkdHktU888YTs3r37vO+3ZcsWKS8vl5tuuqnhsaqqqnP+zpqaGv365qjho3379jV5bP/+/XoICIB/IagAaJOEhAQdHNScDhUyVG2V1NRUXcvk6aefloqKCv26r776Sj744AM99HI+aq6I6tVYvXq1Plch5Mz5KYmJifpe/c4PP/xQB6DmzJo1Sz7++GPZu3dvw5DUp59+KjNnzuTTBvyNKVNyAQS0jRs36lU16p+MtLQ0Y86cOfrxRx991Bg8eLAxevRoY926dfoxtYpn+vTp+nVqNc9VV11l7N27Vz/39ddf69eq91H3f/rTn5r8nldeecVITk42/uM//sP46U9/atxwww1GXFycMXXq1IbXqOPhw4cbY8aM0at6HnnkEaNv3776dT/5yU8aXqdWCw0bNswYNWqUfv27777b8Nz9999vdOvWTd/Uz6v3adwutUoIgG+gMi0AAPBZDP0AAACfRVABAAA+i6ACAAB8FkEFAAD4LIIKAADwWQQVAADgswgqAADAZxFUAACAzyKoAAAAn0VQAQAAPougAgAAxFf9f1T98k1kn/R8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3227c7c50>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"Separate\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"Stacked\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
