{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "268707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474280784)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 198]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 26, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926735078)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [244, 26, 228, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981509462123), np.float64(0.7853981644537731), np.float64(0.1635701974083778), np.float64(-0.1635696366828293), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614646871\n",
      "(change of -0.13682545982715677)\n",
      "Current ansatz: [244, 26, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002921624091770451\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053169 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710484797025)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 198]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.3272761548199705\n",
      "(change of -0.20417052920232504)\n",
      "Current ansatz: [244, 31, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041381052\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.089491643868054)]\n",
      "Initial energy: -6.3272761548199705\n",
      "Optimizing energy with indices [244, 31, 225, 198, 120]...\n",
      "Starting point: [np.float64(0.7853981333456251), np.float64(-0.7853981977509684), np.float64(-0.1635702864850734), np.float64(-0.16356997194309295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614695103\n",
      "(change of -0.13682545987513262)\n",
      "Current ansatz: [244, 31, 225, 198, 120]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002977212948081906\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041381052 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819709\n",
      "(change of -0.20417052920205858)\n",
      "Current ansatz: [244, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580530028\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089492926737891)]\n",
      "Initial energy: -6.327276154819709\n",
      "Optimizing energy with indices [244, 79, 225, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981700346647), np.float64(0.7853981900506334), np.float64(-0.1635701974085294), np.float64(0.16356963668219085), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614835283\n",
      "(change of -0.13682546001557316)\n",
      "Current ansatz: [244, 79, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00027271978716130764\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580530028 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072685\n",
      "(change of -0.13682546025269726)\n",
      "Current ansatz: [244, 79, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013508605784767053\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048479704)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481971\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [244, 31, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531929\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267349482)]\n",
      "Initial energy: -6.32727615481971\n",
      "Optimizing energy with indices [244, 31, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981631072092), np.float64(-0.7853981620731963), np.float64(-0.1635701974084101), np.float64(0.16356963668286922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135289\n",
      "(change of -0.13682546031557852)\n",
      "Current ansatz: [244, 31, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.615543193896931e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531929 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998192\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [225, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140577\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.9999999999993738)]\n",
      "Initial energy: -5.999999999998192\n",
      "Optimizing energy with indices [225, 26, 216]...\n",
      "Starting point: [np.float64(0.7853985607314246), np.float64(0.7853989420959444), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625475228\n",
      "(change of -0.12310562547703618)\n",
      "Current ansatz: [225, 26, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526155313158\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.4850710218598953)]\n",
      "Initial energy: -6.123105625475228\n",
      "Optimizing energy with indices [225, 26, 216, 225]...\n",
      "Starting point: [np.float64(0.7854038548068972), np.float64(0.7854037257861561), np.float64(-0.1224892729163162), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154553904\n",
      "(change of -0.2041705290786764)\n",
      "Current ansatz: [225, 26, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 6.240988002119852\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894919050022094)]\n",
      "Initial energy: -6.327276154553904\n",
      "Optimizing energy with indices [225, 26, 216, 225, 201]...\n",
      "Starting point: [np.float64(0.7853981579292555), np.float64(0.7853980639603538), np.float64(-0.16357577683842833), np.float64(-0.16357143998142698), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615031853\n",
      "(change of -0.13682546047794908)\n",
      "Current ansatz: [225, 26, 216, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016117601612950237\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240988002119852 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 147]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154820074\n",
      "(change of -0.20417052920242362)\n",
      "Current ansatz: [244, 79, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963655680355\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.0894919825745353)]\n",
      "Initial energy: -6.327276154820074\n",
      "Optimizing energy with indices [244, 79, 225, 147, 45]...\n",
      "Starting point: [np.float64(0.7853981479801436), np.float64(0.7853981834326691), np.float64(-0.16357026296685157), np.float64(-0.16356988342672032), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615066574\n",
      "(change of -0.13682546024649955)\n",
      "Current ansatz: [244, 79, 225, 147, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013897879088857086\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963655680355 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916438682456)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 57]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614608976\n",
      "(change of -0.13682545978899352)\n",
      "Current ansatz: [241, 79, 228, 210, 57]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0003254105179039936\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0000000000000036)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 216]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.12310562561765\n",
      "(change of -0.12310562561764549)\n",
      "Current ansatz: [241, 74, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327147\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071048479691)]\n",
      "Initial energy: -6.12310562561765\n",
      "Optimizing energy with indices [241, 74, 216, 225]...\n",
      "Starting point: [np.float64(-0.7853981642365402), np.float64(-0.7853981626303626), np.float64(-0.12248927961411066), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920233747)\n",
      "Current ansatz: [241, 74, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041887953\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.089491643422401)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [241, 74, 216, 225, 201]...\n",
      "Starting point: [np.float64(-0.7853981605252512), np.float64(-0.7853981644915342), np.float64(-0.16357028651591438), np.float64(-0.163569972059532), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615069895\n",
      "(change of -0.1368254602499075)\n",
      "Current ansatz: [241, 74, 216, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001370486574862192\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041887953 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.000000000000007)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 198]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617653\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [244, 26, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199635551\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.4850710475368856)]\n",
      "Initial energy: -6.123105625617653\n",
      "Optimizing energy with indices [244, 26, 198, 216]...\n",
      "Starting point: [np.float64(0.785398164754688), np.float64(0.785398164778003), np.float64(-0.12248927937115472), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819981\n",
      "(change of -0.2041705292023286)\n",
      "Current ansatz: [244, 26, 198, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041001976\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.0894916442003684)]\n",
      "Initial energy: -6.327276154819981\n",
      "Optimizing energy with indices [244, 26, 198, 216, 177]...\n",
      "Starting point: [np.float64(0.7853981635680788), np.float64(0.785398162840564), np.float64(-0.1635702864618822), np.float64(-0.16356997185621627), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072803\n",
      "(change of -0.1368254602528216)\n",
      "Current ansatz: [244, 26, 198, 216, 177]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350417378645441\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041001976 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 198]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819706\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531788\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929267348807)]\n",
      "Initial energy: -6.327276154819706\n",
      "Optimizing energy with indices [244, 79, 228, 198, 120]...\n",
      "Starting point: [np.float64(0.7853981646905418), np.float64(0.7853981643651434), np.float64(0.16357019740836648), np.float64(-0.1635696366828733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615131534\n",
      "(change of -0.13682546031182774)\n",
      "Current ansatz: [244, 79, 228, 198, 120]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.672109650026482e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531788 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929513\n",
      "(change of -1.7639320224297146)\n",
      "Current ansatz: [225, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647981\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.000000000019366)]\n",
      "Initial energy: -5.999999999929513\n",
      "Optimizing energy with indices [225, 74, 225]...\n",
      "Starting point: [np.float64(0.7853947065773572), np.float64(-0.78539937772625), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562494\n",
      "(change of -0.12310562563298078)\n",
      "Current ansatz: [225, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620177583\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484682854)]\n",
      "Initial energy: -6.123105625562494\n",
      "Optimizing energy with indices [225, 74, 225, 210]...\n",
      "Starting point: [np.float64(0.785394706577262), np.float64(-0.7853985308794446), np.float64(-0.12248927961669949), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154764749\n",
      "(change of -0.20417052920225487)\n",
      "Current ansatz: [225, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962581220583\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267302125)]\n",
      "Initial energy: -6.327276154764749\n",
      "Optimizing energy with indices [225, 74, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853947065771462), np.float64(-0.7853982468607471), np.float64(-0.16357019741060325), np.float64(0.163569636683335), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615080617\n",
      "(change of -0.13682546031586806)\n",
      "Current ansatz: [225, 74, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 9.691226068882237e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962581220583 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000004)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 216]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617655\n",
      "(change of -0.12310562561764904)\n",
      "Current ansatz: [244, 31, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620132718\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071048479707)]\n",
      "Initial energy: -6.123105625617655\n",
      "Optimizing energy with indices [244, 31, 216, 225]...\n",
      "Starting point: [np.float64(0.7853981646238432), np.float64(-0.7853981621539974), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819991\n",
      "(change of -0.2041705292023357)\n",
      "Current ansatz: [244, 31, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042638306\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894916427634773)]\n",
      "Initial energy: -6.327276154819991\n",
      "Optimizing energy with indices [244, 31, 216, 225, 201]...\n",
      "Starting point: [np.float64(0.7853981628939728), np.float64(-0.7853981669250067), np.float64(-0.16357028656166703), np.float64(-0.16356997223173272), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072744\n",
      "(change of -0.13682546025275322)\n",
      "Current ansatz: [244, 31, 216, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013507009758898745\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042638306 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [225, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999999944214)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [225, 79, 228]...\n",
      "Starting point: [np.float64(0.7853985607314237), np.float64(0.7853989420959421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610647\n",
      "(change of -0.12310562561245852)\n",
      "Current ansatz: [225, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955765\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850687898689605)]\n",
      "Initial energy: -6.123105625610647\n",
      "Optimizing energy with indices [225, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.7853983869831845), np.float64(0.7853991695302365), np.float64(0.12248869758311001), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816644\n",
      "(change of -0.20417052920599765)\n",
      "Current ansatz: [225, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964056416012\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916407310458)]\n",
      "Initial energy: -6.327276154816644\n",
      "Optimizing energy with indices [225, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853984379184568), np.float64(0.7853991591701261), np.float64(0.16357028929940762), np.float64(0.16356997348722394), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615069479\n",
      "(change of -0.13682546025283493)\n",
      "Current ansatz: [225, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001362921087785538\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964056416012 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477671423)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197005\n",
      "(change of -0.20417052920205325)\n",
      "Current ansatz: [244, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053184\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267348767)]\n",
      "Initial energy: -6.3272761548197005\n",
      "Optimizing energy with indices [244, 74, 225, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981634001291), np.float64(-0.785398163399394), np.float64(-0.16357019740837944), np.float64(0.16356963668287733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135366\n",
      "(change of -0.13682546031566556)\n",
      "Current ansatz: [244, 74, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887660231807212e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053184 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441852\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047428929)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 31, 228, 147]...\n",
      "Starting point: [np.float64(-0.7853981609748782), np.float64(-0.785398165605737), np.float64(0.12248927934333585), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [241, 31, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531371\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894929267356046)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [241, 31, 228, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981663792285), np.float64(-0.7853981742082982), np.float64(0.16357019740840556), np.float64(-0.1635696366827097), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150404464\n",
      "(change of -0.1368254602207326)\n",
      "Current ansatz: [241, 31, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00012733911260365673\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531371 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0000000000000044)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 210]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.12310562561764\n",
      "(change of -0.12310562561763483)\n",
      "Current ansatz: [241, 79, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199634684\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.4850710475364073)]\n",
      "Initial energy: -6.12310562561764\n",
      "Optimizing energy with indices [241, 79, 210, 225]...\n",
      "Starting point: [np.float64(-0.7853981634187125), np.float64(0.7853981633741678), np.float64(0.12248927937103306), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [241, 79, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531805\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.089492926734879)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 79, 210, 225, 201]...\n",
      "Starting point: [np.float64(-0.7853981633782906), np.float64(0.7853981634050643), np.float64(0.16357019740837012), np.float64(-0.16356963668287453), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151353945\n",
      "(change of -0.13682546031568954)\n",
      "Current ansatz: [241, 79, 210, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5897570308605452e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531805 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 198]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819706\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531788\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734881)]\n",
      "Initial energy: -6.327276154819706\n",
      "Optimizing energy with indices [244, 79, 228, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981646905418), np.float64(0.7853981643651434), np.float64(0.16357019740836648), np.float64(-0.1635696366828733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132637\n",
      "(change of -0.13682546031293086)\n",
      "Current ansatz: [244, 79, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.3721444024673296e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531788 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.99999999999442)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610642\n",
      "(change of -0.12310562561245764)\n",
      "Current ansatz: [228, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955728\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868942)]\n",
      "Initial energy: -6.123105625610642\n",
      "Optimizing energy with indices [228, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853983869831797), np.float64(-0.7853991695302367), np.float64(-0.12248869758310571), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816355\n",
      "(change of -0.20417052920571255)\n",
      "Current ansatz: [228, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096256714494\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929485632674)]\n",
      "Initial energy: -6.327276154816355\n",
      "Optimizing energy with indices [228, 74, 225, 210, 198]...\n",
      "Starting point: [np.float64(-0.785398438893094), np.float64(-0.7853991591701955), np.float64(-0.1635701984907769), np.float64(0.1635696317030657), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132105\n",
      "(change of -0.13682546031574994)\n",
      "Current ansatz: [228, 74, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.185632046620665e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096256714494 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929513\n",
      "(change of -1.7639320224297146)\n",
      "Current ansatz: [225, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647981\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-1.9999999999664206)]\n",
      "Initial energy: -5.999999999929513\n",
      "Optimizing energy with indices [225, 74, 210]...\n",
      "Starting point: [np.float64(0.7853947065773572), np.float64(-0.78539937772625), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625381212\n",
      "(change of -0.12310562545169912)\n",
      "Current ansatz: [225, 74, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917558862857067\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.485089250640418)]\n",
      "Initial energy: -6.123105625381212\n",
      "Optimizing energy with indices [225, 74, 210, 201]...\n",
      "Starting point: [np.float64(0.7853968399243979), np.float64(-0.7853935974090278), np.float64(0.12249397021421135), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154159117\n",
      "(change of -0.20417052877790454)\n",
      "Current ansatz: [225, 74, 210, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962595511027\n",
      "Operator(s) added to ansatz: [180]\n",
      "Gradients: [np.float64(2.089493016838319)]\n",
      "Initial energy: -6.327276154159117\n",
      "Optimizing energy with indices [225, 74, 210, 201, 180]...\n",
      "Starting point: [np.float64(0.7853890692890542), np.float64(-0.7853824785341463), np.float64(0.16357021815749057), np.float64(0.1635696199599279), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614323176\n",
      "(change of -0.13682546016405883)\n",
      "Current ansatz: [225, 74, 210, 201, 180]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00034690067784600754\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962595511027 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 228]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [244, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199440323\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428078)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 26, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981611549227), np.float64(0.7853981650265573), np.float64(0.1224892793431165), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481998\n",
      "(change of -0.20417052920233303)\n",
      "Current ansatz: [244, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964040371926\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916447538887)]\n",
      "Initial energy: -6.32727615481998\n",
      "Optimizing energy with indices [244, 26, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.785398156811469), np.float64(0.785398154431449), np.float64(0.16357028642350993), np.float64(0.16356997171157828), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.46410161505858\n",
      "(change of -0.1368254602386001)\n",
      "Current ansatz: [244, 26, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0001435936303541698\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964040371926 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 198]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819701\n",
      "(change of -0.20417052920205414)\n",
      "Current ansatz: [244, 74, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531868\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929267348714)]\n",
      "Initial energy: -6.327276154819701\n",
      "Optimizing energy with indices [244, 74, 228, 198, 120]...\n",
      "Starting point: [np.float64(0.7853981767050704), np.float64(-0.7853981525341414), np.float64(0.16357019740837964), np.float64(-0.16356963668287866), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614749523\n",
      "(change of -0.1368254599298213)\n",
      "Current ansatz: [244, 74, 228, 198, 120]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00031207846756742036\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531868 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 228]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [244, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981639978813), np.float64(-0.7853981625399111), np.float64(0.12248927961411445), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233392)\n",
      "Current ansatz: [244, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964044667907\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089491640981468)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981417276958), np.float64(-0.7853981819240415), np.float64(0.1635702866854681), np.float64(0.16356997269745452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071121\n",
      "(change of -0.13682546025114384)\n",
      "Current ansatz: [244, 31, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.000136128583548991\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044667907 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 147]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819713\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [244, 74, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531879\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734878)]\n",
      "Initial energy: -6.327276154819713\n",
      "Optimizing energy with indices [244, 74, 228, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981767049507), np.float64(-0.7853981525341226), np.float64(0.16357019740838014), np.float64(-0.1635696366828784), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614750672\n",
      "(change of -0.13682545993095907)\n",
      "Current ansatz: [244, 74, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00031096863104971493\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531879 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113274\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 26, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257758), np.float64(0.7853981815917098), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617638\n",
      "(change of -0.12310562561762772)\n",
      "Current ansatz: [241, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441738\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710474288693)]\n",
      "Initial energy: -6.123105625617638\n",
      "Optimizing energy with indices [241, 26, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853981634447681), np.float64(0.7853981634524064), np.float64(0.12248927934332132), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819988\n",
      "(change of -0.2041705292023499)\n",
      "Current ansatz: [241, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138126\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643867201)]\n",
      "Initial energy: -6.327276154819988\n",
      "Optimizing energy with indices [241, 26, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853981604483139), np.float64(0.7853981623217249), np.float64(0.16357028648498867), np.float64(0.16356997194327816), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071929\n",
      "(change of -0.13682546025194053)\n",
      "Current ansatz: [241, 26, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013555110317705658\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138126 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 120]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819724\n",
      "(change of -0.2041705292020728)\n",
      "Current ansatz: [244, 79, 225, 120]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 6.240962580531949\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267348585)]\n",
      "Initial energy: -6.327276154819724\n",
      "Optimizing energy with indices [244, 79, 225, 120, 147]...\n",
      "Starting point: [np.float64(0.7853981434097175), np.float64(0.7853981968242422), np.float64(-0.16357019740836898), np.float64(0.1635696366828803), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614731037\n",
      "(change of -0.13682545991131345)\n",
      "Current ansatz: [244, 79, 225, 120, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00031663975990918946\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531949 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477671423)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197005\n",
      "(change of -0.20417052920205325)\n",
      "Current ansatz: [244, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053184\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267348767)]\n",
      "Initial energy: -6.3272761548197005\n",
      "Optimizing energy with indices [244, 74, 225, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981634001291), np.float64(-0.785398163399394), np.float64(-0.16357019740837944), np.float64(0.16356963668287733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135366\n",
      "(change of -0.13682546031566556)\n",
      "Current ansatz: [244, 74, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887660231807212e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053184 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.99999999999442)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.12310562561065\n",
      "(change of -0.12310562561246563)\n",
      "Current ansatz: [228, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752214895575\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850687898689516)]\n",
      "Initial energy: -6.12310562561065\n",
      "Optimizing energy with indices [228, 74, 228, 147]...\n",
      "Starting point: [np.float64(-0.7853983869831829), np.float64(-0.7853991695302484), np.float64(0.12248869758310722), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481634\n",
      "(change of -0.20417052920568945)\n",
      "Current ansatz: [228, 74, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962567146117\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929485572242)]\n",
      "Initial energy: -6.32727615481634\n",
      "Optimizing energy with indices [228, 74, 228, 147, 120]...\n",
      "Starting point: [np.float64(-0.785398438883656), np.float64(-0.7853991591702224), np.float64(0.16357019849085808), np.float64(-0.16356963170333133), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614774126\n",
      "(change of -0.1368254599577865)\n",
      "Current ansatz: [228, 74, 228, 147, 120]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00025827303978379404\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962567146117 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047767142)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 198]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819957\n",
      "(change of -0.20417052920230994)\n",
      "Current ansatz: [244, 74, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640413812895\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.0894916438671864)]\n",
      "Initial energy: -6.327276154819957\n",
      "Optimizing energy with indices [244, 74, 225, 198, 135]...\n",
      "Starting point: [np.float64(0.7853981633827103), np.float64(-0.7853981633905597), np.float64(-0.1635702864850013), np.float64(-0.16356997194328274), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150727975\n",
      "(change of -0.13682546025284026)\n",
      "Current ansatz: [244, 74, 225, 198, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350387836355389\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640413812895 > 1e-05)\n",
      "Pool will be tiled from 19 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 16\n",
      "Got DMRG energy -2.76469e+01\n",
      "Tiled pool has 201 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -14.999999999999936\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.9999999999999853\n",
      "Operator 1: 3.9999999999999782\n",
      "Operator 2: -3.9999999999999756\n",
      "Operator 3: 3.9999999999999787\n",
      "Operator 4: -3.9999999999999822\n",
      "Operator 5: 4.000000000000005\n",
      "Operator 6: -3.999999999999995\n",
      "Operator 7: 3.9999999999999885\n",
      "Operator 8: -3.999999999999975\n",
      "Operator 9: 3.999999999999987\n",
      "Operator 10: -3.999999999999984\n",
      "Operator 11: 3.999999999999991\n",
      "Operator 12: -3.9999999999999916\n",
      "Operator 13: -3.9999999999999702\n",
      "Operator 14: 4.000000000000002\n",
      "Operator 15: -3.9999999999999853\n",
      "Operator 16: 3.9999999999999782\n",
      "Operator 17: -3.9999999999999756\n",
      "Operator 18: 3.9999999999999787\n",
      "Operator 19: -3.9999999999999822\n",
      "Operator 20: 4.000000000000005\n",
      "Operator 21: -3.999999999999995\n",
      "Operator 22: 3.9999999999999885\n",
      "Operator 23: -3.999999999999975\n",
      "Operator 24: 3.999999999999987\n",
      "Operator 25: -3.999999999999984\n",
      "Operator 26: 3.9999999999999853\n",
      "Operator 27: 3.9999999999999782\n",
      "Operator 28: 3.9999999999999756\n",
      "Operator 29: 3.9999999999999787\n",
      "Operator 30: 3.9999999999999822\n",
      "Operator 31: 4.000000000000005\n",
      "Operator 32: 3.999999999999995\n",
      "Operator 33: 3.9999999999999885\n",
      "Operator 34: 3.999999999999975\n",
      "Operator 35: 3.999999999999987\n",
      "Operator 36: 3.999999999999984\n",
      "Operator 37: 3.999999999999991\n",
      "Operator 38: 3.9999999999999916\n",
      "Operator 39: 4.000000000000002\n",
      "Operator 40: 3.9999999999999853\n",
      "Operator 41: 3.9999999999999782\n",
      "Operator 42: 3.9999999999999756\n",
      "Operator 43: 3.9999999999999787\n",
      "Operator 44: 3.9999999999999822\n",
      "Operator 45: 4.000000000000005\n",
      "Operator 46: 3.999999999999995\n",
      "Operator 47: 3.9999999999999885\n",
      "Operator 48: 3.999999999999975\n",
      "Operator 49: 3.999999999999987\n",
      "Operator 50: 3.999999999999984\n",
      "Operator 51: 3.999999999999991\n",
      "Operator 52: -4.000000000000002\n",
      "Operator 53: -3.9999999999999853\n",
      "Operator 54: -3.9999999999999782\n",
      "Operator 55: -3.9999999999999756\n",
      "Operator 56: -3.9999999999999787\n",
      "Operator 57: -3.9999999999999822\n",
      "Operator 58: -4.000000000000005\n",
      "Operator 59: -3.999999999999995\n",
      "Operator 60: -3.9999999999999885\n",
      "Operator 61: -3.999999999999975\n",
      "Operator 62: -3.999999999999987\n",
      "Operator 63: -3.999999999999984\n",
      "Operator 64: -3.999999999999991\n",
      "Operator 65: 3.9999999999999702\n",
      "Operator 66: -4.000000000000002\n",
      "Operator 67: 3.9999999999999853\n",
      "Operator 68: -3.9999999999999782\n",
      "Operator 69: 3.9999999999999756\n",
      "Operator 70: -3.9999999999999787\n",
      "Operator 71: 3.9999999999999822\n",
      "Operator 72: -4.000000000000005\n",
      "Operator 73: 3.999999999999995\n",
      "Operator 74: -3.9999999999999885\n",
      "Operator 75: 3.999999999999975\n",
      "Operator 76: -3.999999999999987\n",
      "Operator 77: 3.999999999999984\n",
      "Operator 78: -3.9999999999999853\n",
      "Operator 79: -3.9999999999999782\n",
      "Operator 80: -3.9999999999999756\n",
      "Operator 81: -3.9999999999999787\n",
      "Operator 82: -3.9999999999999822\n",
      "Operator 83: -4.000000000000005\n",
      "Operator 84: -3.999999999999995\n",
      "Operator 85: -3.9999999999999885\n",
      "Operator 86: -3.999999999999975\n",
      "Operator 87: -3.999999999999987\n",
      "Operator 88: -3.999999999999984\n",
      "Operator 89: -3.999999999999991\n",
      "Operator 90: -3.9999999999999916\n",
      "Operator 104: -3.9999999999999702\n",
      "Operator 105: 4.000000000000002\n",
      "Operator 106: 4.000000000000002\n",
      "Operator 107: -3.999999999999991\n",
      "Operator 108: 3.9999999999999916\n",
      "Operator 161: 3.9999999999999702\n",
      "Operator 162: -4.000000000000002\n",
      "Operator 163: 3.9999999999999853\n",
      "Operator 164: -3.9999999999999782\n",
      "Operator 165: 3.9999999999999756\n",
      "Operator 166: -3.9999999999999787\n",
      "Operator 167: 3.9999999999999822\n",
      "Operator 168: -4.000000000000005\n",
      "Operator 169: 3.999999999999995\n",
      "Operator 170: -3.9999999999999885\n",
      "Operator 171: 3.999999999999975\n",
      "Operator 172: -3.999999999999987\n",
      "Operator 173: 3.999999999999984\n",
      "Operator 174: 3.9999999999999853\n",
      "Operator 175: 3.9999999999999782\n",
      "Operator 176: 3.9999999999999756\n",
      "Operator 177: 3.9999999999999787\n",
      "Operator 178: 3.9999999999999822\n",
      "Operator 179: 4.000000000000005\n",
      "Operator 180: 3.999999999999995\n",
      "Operator 181: 3.9999999999999885\n",
      "Operator 182: 3.999999999999975\n",
      "Operator 183: 3.999999999999987\n",
      "Operator 184: 3.999999999999984\n",
      "Operator 185: 3.999999999999991\n",
      "Operator 186: 3.9999999999999916\n",
      "Operator 187: -3.9999999999999853\n",
      "Operator 188: -3.9999999999999782\n",
      "Operator 189: -3.9999999999999756\n",
      "Operator 190: -3.9999999999999787\n",
      "Operator 191: -3.9999999999999822\n",
      "Operator 192: -4.000000000000005\n",
      "Operator 193: -3.999999999999995\n",
      "Operator 194: -3.9999999999999885\n",
      "Operator 195: -3.999999999999975\n",
      "Operator 196: -3.999999999999987\n",
      "Operator 197: -3.999999999999984\n",
      "Operator 198: -3.999999999999991\n",
      "Operator 199: -3.9999999999999916\n",
      "Operator 200: -4.000000000000002\n",
      "Total gradient norm: 46.647615158762235\n",
      "Operators under consideration (1):\n",
      "[197]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.999999999999984)]\n",
      "Operator(s) added to ansatz: [197]\n",
      "Gradients: [np.float64(-3.999999999999984)]\n",
      "Initial energy: -14.999999999999936\n",
      "Optimizing energy with indices [197]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -15.828427\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -15.82842712474611\n",
      "(change of -0.8284271247461739)\n",
      "Current ansatz: [197]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.999999999999983\n",
      "Operator 1: 3.999999999999974\n",
      "Operator 2: -3.9999999999999725\n",
      "Operator 3: 3.999999999999975\n",
      "Operator 4: -3.99999999999998\n",
      "Operator 5: 4.000000000000003\n",
      "Operator 6: -3.9999999999999916\n",
      "Operator 7: 3.999999999999985\n",
      "Operator 8: -3.999999999999971\n",
      "Operator 9: 3.4142135623691545\n",
      "Operator 11: 3.4142135623691585\n",
      "Operator 12: -3.999999999999985\n",
      "Operator 13: -3.999999999999967\n",
      "Operator 14: 3.9999999999999987\n",
      "Operator 15: -3.999999999999983\n",
      "Operator 16: 3.999999999999974\n",
      "Operator 17: -3.999999999999972\n",
      "Operator 18: 3.999999999999975\n",
      "Operator 19: -3.99999999999998\n",
      "Operator 20: 4.000000000000003\n",
      "Operator 21: -3.9999999999999916\n",
      "Operator 22: 2.8284271247383286\n",
      "Operator 23: -3.999999999999971\n",
      "Operator 24: 3.4142135623691536\n",
      "Operator 26: 3.999999999999983\n",
      "Operator 27: 3.999999999999974\n",
      "Operator 28: 3.9999999999999725\n",
      "Operator 29: 3.999999999999975\n",
      "Operator 30: 3.99999999999998\n",
      "Operator 31: 4.000000000000003\n",
      "Operator 32: 3.9999999999999916\n",
      "Operator 33: 3.999999999999985\n",
      "Operator 34: 3.999999999999971\n",
      "Operator 35: 3.4142135623691545\n",
      "Operator 37: 3.4142135623691567\n",
      "Operator 38: 2.828427124738332\n",
      "Operator 39: 3.9999999999999987\n",
      "Operator 40: 3.999999999999983\n",
      "Operator 41: 3.999999999999974\n",
      "Operator 42: 3.999999999999972\n",
      "Operator 43: 3.999999999999975\n",
      "Operator 44: 3.99999999999998\n",
      "Operator 45: 4.000000000000003\n",
      "Operator 46: 3.9999999999999916\n",
      "Operator 47: 3.999999999999985\n",
      "Operator 48: 2.8284271247383184\n",
      "Operator 49: 3.4142135623691536\n",
      "Operator 51: 3.4142135623691567\n",
      "Operator 52: -3.9999999999999987\n",
      "Operator 53: -3.999999999999983\n",
      "Operator 54: -3.999999999999974\n",
      "Operator 55: -3.9999999999999725\n",
      "Operator 56: -3.999999999999975\n",
      "Operator 57: -3.99999999999998\n",
      "Operator 58: -4.000000000000003\n",
      "Operator 59: -3.9999999999999916\n",
      "Operator 60: -3.999999999999985\n",
      "Operator 61: -2.8284271247383184\n",
      "Operator 62: -3.414213562369156\n",
      "Operator 64: -3.4142135623691585\n",
      "Operator 65: 3.999999999999967\n",
      "Operator 66: -3.9999999999999987\n",
      "Operator 67: 3.999999999999983\n",
      "Operator 68: -3.999999999999974\n",
      "Operator 69: 3.999999999999972\n",
      "Operator 70: -3.999999999999975\n",
      "Operator 71: 3.99999999999998\n",
      "Operator 72: -4.000000000000003\n",
      "Operator 73: 3.9999999999999916\n",
      "Operator 74: -3.999999999999985\n",
      "Operator 75: 3.999999999999971\n",
      "Operator 76: -3.4142135623691554\n",
      "Operator 78: -3.999999999999983\n",
      "Operator 79: -3.999999999999974\n",
      "Operator 80: -3.999999999999972\n",
      "Operator 81: -3.999999999999975\n",
      "Operator 82: -3.99999999999998\n",
      "Operator 83: -4.000000000000003\n",
      "Operator 84: -3.9999999999999916\n",
      "Operator 85: -3.999999999999985\n",
      "Operator 86: -3.999999999999971\n",
      "Operator 87: -3.4142135623691554\n",
      "Operator 89: -3.4142135623691585\n",
      "Operator 90: -2.828427124738332\n",
      "Operator 104: -3.999999999999967\n",
      "Operator 105: 3.9999999999999987\n",
      "Operator 106: 3.9999999999999987\n",
      "Operator 107: -3.4142135623691567\n",
      "Operator 108: 3.999999999999985\n",
      "Operator 119: 1.4142135623770127\n",
      "Operator 120: -1.414213562377011\n",
      "Operator 132: -1.4142135623770113\n",
      "Operator 133: 1.4142135623770142\n",
      "Operator 161: 3.999999999999967\n",
      "Operator 162: -3.9999999999999987\n",
      "Operator 163: 3.999999999999983\n",
      "Operator 164: -3.999999999999974\n",
      "Operator 165: 3.9999999999999725\n",
      "Operator 166: -3.999999999999975\n",
      "Operator 167: 3.99999999999998\n",
      "Operator 168: -4.000000000000003\n",
      "Operator 169: 3.9999999999999916\n",
      "Operator 170: -2.8284271247383286\n",
      "Operator 171: 3.999999999999971\n",
      "Operator 172: -3.414213562369156\n",
      "Operator 174: 3.999999999999983\n",
      "Operator 175: 3.999999999999974\n",
      "Operator 176: 3.999999999999972\n",
      "Operator 177: 3.999999999999975\n",
      "Operator 178: 3.99999999999998\n",
      "Operator 179: 4.000000000000003\n",
      "Operator 180: 3.9999999999999916\n",
      "Operator 181: 3.999999999999985\n",
      "Operator 182: 3.999999999999971\n",
      "Operator 183: 3.4142135623691554\n",
      "Operator 185: 3.4142135623691567\n",
      "Operator 186: 2.828427124738332\n",
      "Operator 187: -3.999999999999983\n",
      "Operator 188: -3.999999999999974\n",
      "Operator 189: -3.9999999999999725\n",
      "Operator 190: -3.999999999999975\n",
      "Operator 191: -3.99999999999998\n",
      "Operator 192: -4.000000000000003\n",
      "Operator 193: -3.9999999999999916\n",
      "Operator 194: -3.999999999999985\n",
      "Operator 195: -3.999999999999971\n",
      "Operator 196: -3.4142135623691545\n",
      "Operator 198: -3.4142135623691585\n",
      "Operator 199: -2.828427124738332\n",
      "Operator 200: -3.9999999999999987\n",
      "Total gradient norm: 43.37998820297272\n",
      "Operators under consideration (1):\n",
      "[200]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999987)]\n",
      "Operator(s) added to ansatz: [200]\n",
      "Gradients: [np.float64(-3.9999999999999987)]\n",
      "Initial energy: -15.82842712474611\n",
      "Optimizing energy with indices [197, 200]...\n",
      "Starting point: [np.float64(0.3926990817001114), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -16.656854\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -16.656854249492298\n",
      "(change of -0.8284271247461881)\n",
      "Current ansatz: [197, 200]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135623691514\n",
      "Operator 1: 3.999999999999971\n",
      "Operator 2: -3.999999999999971\n",
      "Operator 3: 3.999999999999975\n",
      "Operator 4: -3.999999999999978\n",
      "Operator 5: 4.000000000000003\n",
      "Operator 6: -3.9999999999999893\n",
      "Operator 7: 3.999999999999985\n",
      "Operator 8: -3.9999999999999707\n",
      "Operator 9: 3.414213561940275\n",
      "Operator 11: 3.414213561940279\n",
      "Operator 12: -3.999999999999985\n",
      "Operator 13: -3.4142135623691354\n",
      "Operator 15: -3.4142135623691496\n",
      "Operator 16: 3.999999999999971\n",
      "Operator 17: -3.999999999999971\n",
      "Operator 18: 3.999999999999975\n",
      "Operator 19: -3.999999999999978\n",
      "Operator 20: 4.000000000000003\n",
      "Operator 21: -3.9999999999999893\n",
      "Operator 22: 2.8284271238805703\n",
      "Operator 23: -3.9999999999999707\n",
      "Operator 24: 3.414213561940275\n",
      "Operator 26: 3.4142135623691496\n",
      "Operator 27: 2.828427124738316\n",
      "Operator 28: 3.999999999999971\n",
      "Operator 29: 3.999999999999975\n",
      "Operator 30: 3.999999999999978\n",
      "Operator 31: 4.000000000000003\n",
      "Operator 32: 3.9999999999999893\n",
      "Operator 33: 3.999999999999985\n",
      "Operator 34: 3.9999999999999707\n",
      "Operator 35: 3.414213561940275\n",
      "Operator 37: 3.414213561940277\n",
      "Operator 38: 2.828427123880574\n",
      "Operator 40: 3.4142135623691496\n",
      "Operator 41: 3.999999999999971\n",
      "Operator 42: 3.999999999999971\n",
      "Operator 43: 3.999999999999975\n",
      "Operator 44: 3.999999999999978\n",
      "Operator 45: 4.000000000000003\n",
      "Operator 46: 3.9999999999999893\n",
      "Operator 47: 3.999999999999985\n",
      "Operator 48: 2.8284271238805596\n",
      "Operator 49: 3.414213561940275\n",
      "Operator 51: 3.414213561940277\n",
      "Operator 53: -3.4142135623691514\n",
      "Operator 54: -3.999999999999971\n",
      "Operator 55: -3.999999999999971\n",
      "Operator 56: -3.999999999999975\n",
      "Operator 57: -3.999999999999978\n",
      "Operator 58: -4.000000000000003\n",
      "Operator 59: -3.9999999999999893\n",
      "Operator 60: -3.999999999999985\n",
      "Operator 61: -2.8284271238805596\n",
      "Operator 62: -3.414213561940276\n",
      "Operator 64: -3.414213561940279\n",
      "Operator 65: 3.414213562369139\n",
      "Operator 67: 3.4142135623691496\n",
      "Operator 68: -3.999999999999971\n",
      "Operator 69: 3.999999999999971\n",
      "Operator 70: -3.999999999999975\n",
      "Operator 71: 3.999999999999978\n",
      "Operator 72: -4.000000000000003\n",
      "Operator 73: 3.9999999999999893\n",
      "Operator 74: -3.999999999999985\n",
      "Operator 75: 3.9999999999999707\n",
      "Operator 76: -3.414213561940276\n",
      "Operator 78: -3.4142135623691514\n",
      "Operator 79: -2.828427124738316\n",
      "Operator 80: -3.999999999999971\n",
      "Operator 81: -3.999999999999975\n",
      "Operator 82: -3.999999999999978\n",
      "Operator 83: -4.000000000000003\n",
      "Operator 84: -3.9999999999999893\n",
      "Operator 85: -3.999999999999985\n",
      "Operator 86: -3.9999999999999707\n",
      "Operator 87: -3.414213561940276\n",
      "Operator 89: -3.414213561940279\n",
      "Operator 90: -2.828427123880574\n",
      "Operator 104: -3.4142135623691354\n",
      "Operator 107: -3.414213561940277\n",
      "Operator 108: 3.999999999999985\n",
      "Operator 109: -1.4142135623770131\n",
      "Operator 119: 1.4142135628058923\n",
      "Operator 120: -1.4142135628058903\n",
      "Operator 122: 1.4142135623770173\n",
      "Operator 132: -1.4142135628058903\n",
      "Operator 133: 1.4142135628058943\n",
      "Operator 161: 3.414213562369139\n",
      "Operator 163: 3.4142135623691514\n",
      "Operator 164: -3.999999999999971\n",
      "Operator 165: 3.999999999999971\n",
      "Operator 166: -3.999999999999975\n",
      "Operator 167: 3.999999999999978\n",
      "Operator 168: -4.000000000000003\n",
      "Operator 169: 3.9999999999999893\n",
      "Operator 170: -2.8284271238805703\n",
      "Operator 171: 3.9999999999999707\n",
      "Operator 172: -3.414213561940276\n",
      "Operator 174: 3.4142135623691496\n",
      "Operator 175: 2.828427124738316\n",
      "Operator 176: 2.828427124738316\n",
      "Operator 177: 3.999999999999975\n",
      "Operator 178: 3.999999999999978\n",
      "Operator 179: 4.000000000000003\n",
      "Operator 180: 3.9999999999999893\n",
      "Operator 181: 3.999999999999985\n",
      "Operator 182: 3.9999999999999707\n",
      "Operator 183: 3.414213561940276\n",
      "Operator 185: 3.414213561940277\n",
      "Operator 186: 2.828427123880574\n",
      "Operator 187: -3.4142135623691514\n",
      "Operator 188: -2.828427124738316\n",
      "Operator 189: -2.828427124738316\n",
      "Operator 190: -3.999999999999975\n",
      "Operator 191: -3.999999999999978\n",
      "Operator 192: -4.000000000000003\n",
      "Operator 193: -3.9999999999999893\n",
      "Operator 194: -3.999999999999985\n",
      "Operator 195: -3.9999999999999707\n",
      "Operator 196: -3.414213561940275\n",
      "Operator 198: -3.414213561940279\n",
      "Operator 199: -2.828427123880574\n",
      "Total gradient norm: 40.608119088375005\n",
      "Operators under consideration (1):\n",
      "[195]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999707)]\n",
      "Operator(s) added to ansatz: [195]\n",
      "Gradients: [np.float64(-3.9999999999999707)]\n",
      "Initial energy: -16.656854249492298\n",
      "Optimizing energy with indices [197, 200, 195]...\n",
      "Starting point: [np.float64(0.39269908185174307), np.float64(0.39269908170011286), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -17.591725\n",
      "         Iterations: 6\n",
      "         Function evaluations: 9\n",
      "         Gradient evaluations: 9\n",
      "\n",
      "Current energy: -17.591724953300698\n",
      "(change of -0.9348707038084001)\n",
      "Current ansatz: [197, 200, 195]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.414213562526488\n",
      "Operator 1: 3.999999999999977\n",
      "Operator 2: -3.9999999999999765\n",
      "Operator 3: 3.9999999999999796\n",
      "Operator 4: -3.9999999999999822\n",
      "Operator 5: 4.000000000000005\n",
      "Operator 6: -3.9999999999999947\n",
      "Operator 7: 3.2645854456282555\n",
      "Operator 9: 2.5291708912555495\n",
      "Operator 11: 3.2645854456272847\n",
      "Operator 12: -3.999999999999985\n",
      "Operator 13: -3.4142135625264736\n",
      "Operator 15: -3.4142135625264878\n",
      "Operator 16: 3.999999999999977\n",
      "Operator 17: -3.9999999999999765\n",
      "Operator 18: 3.9999999999999796\n",
      "Operator 19: -3.9999999999999822\n",
      "Operator 20: 2.5291708912565305\n",
      "Operator 21: -3.9999999999999942\n",
      "Operator 22: 2.0641736202740937\n",
      "Operator 24: 2.7995881746467943\n",
      "Operator 26: 3.4142135625264878\n",
      "Operator 27: 2.828427125052986\n",
      "Operator 28: 3.9999999999999765\n",
      "Operator 29: 3.9999999999999796\n",
      "Operator 30: 3.9999999999999822\n",
      "Operator 31: 4.000000000000005\n",
      "Operator 32: 3.9999999999999947\n",
      "Operator 33: 3.2645854456282555\n",
      "Operator 35: 2.7995881746467943\n",
      "Operator 36: -0.9299945419615234\n",
      "Operator 37: 3.264585445627283\n",
      "Operator 38: 2.5291708912545836\n",
      "Operator 40: 3.4142135625264878\n",
      "Operator 41: 3.999999999999977\n",
      "Operator 42: 3.9999999999999765\n",
      "Operator 43: 3.9999999999999796\n",
      "Operator 44: 3.9999999999999822\n",
      "Operator 45: 4.000000000000005\n",
      "Operator 46: 2.5291708912565234\n",
      "Operator 47: 3.2645854456282555\n",
      "Operator 48: -0.9299945419594859\n",
      "Operator 49: 2.7995881746467943\n",
      "Operator 51: 3.264585445627283\n",
      "Operator 53: -3.414213562526488\n",
      "Operator 54: -3.999999999999977\n",
      "Operator 55: -3.9999999999999765\n",
      "Operator 56: -3.9999999999999796\n",
      "Operator 57: -3.9999999999999822\n",
      "Operator 58: -4.000000000000005\n",
      "Operator 59: -2.5291708912565234\n",
      "Operator 60: -3.264585445628255\n",
      "Operator 61: 0.9299945419594859\n",
      "Operator 62: -2.7995881746467934\n",
      "Operator 64: -3.2645854456272847\n",
      "Operator 65: 3.4142135625264776\n",
      "Operator 67: 3.4142135625264878\n",
      "Operator 68: -3.999999999999977\n",
      "Operator 69: 3.9999999999999765\n",
      "Operator 70: -3.9999999999999796\n",
      "Operator 71: 3.9999999999999822\n",
      "Operator 72: -4.000000000000005\n",
      "Operator 73: 3.9999999999999942\n",
      "Operator 74: -3.264585445628255\n",
      "Operator 76: -2.5291708912555517\n",
      "Operator 78: -3.414213562526489\n",
      "Operator 79: -2.828427125052986\n",
      "Operator 80: -3.9999999999999765\n",
      "Operator 81: -3.9999999999999796\n",
      "Operator 82: -3.9999999999999822\n",
      "Operator 83: -4.000000000000005\n",
      "Operator 84: -3.9999999999999942\n",
      "Operator 85: -3.264585445628255\n",
      "Operator 87: -2.7995881746467934\n",
      "Operator 88: 0.9299945419615234\n",
      "Operator 89: -3.2645854456272847\n",
      "Operator 90: -2.5291708912545836\n",
      "Operator 101: -1.2004118253531875\n",
      "Operator 104: -3.4142135625264736\n",
      "Operator 107: -3.264585445627283\n",
      "Operator 108: 3.999999999999985\n",
      "Operator 109: -1.414213562219682\n",
      "Operator 117: 1.5494591478013036\n",
      "Operator 118: -1.5494591478013031\n",
      "Operator 119: 1.5494591478020934\n",
      "Operator 120: -1.5494591478020918\n",
      "Operator 122: 1.4142135622196856\n",
      "Operator 130: -1.5494591478013051\n",
      "Operator 131: 1.5494591478013011\n",
      "Operator 132: -1.549459147802093\n",
      "Operator 133: 1.5494591478020963\n",
      "Operator 145: -1.2004118253531875\n",
      "Operator 158: 1.2004118253531881\n",
      "Operator 161: 3.4142135625264776\n",
      "Operator 163: 3.414213562526488\n",
      "Operator 164: -3.999999999999977\n",
      "Operator 165: 3.9999999999999765\n",
      "Operator 166: -3.9999999999999796\n",
      "Operator 167: 3.9999999999999822\n",
      "Operator 168: -2.5291708912565305\n",
      "Operator 169: 3.9999999999999947\n",
      "Operator 170: -2.0641736202740937\n",
      "Operator 172: -2.7995881746467934\n",
      "Operator 174: 3.4142135625264878\n",
      "Operator 175: 2.828427125052986\n",
      "Operator 176: 2.8284271250529853\n",
      "Operator 177: 3.9999999999999796\n",
      "Operator 178: 3.9999999999999822\n",
      "Operator 179: 4.000000000000005\n",
      "Operator 180: 3.9999999999999942\n",
      "Operator 181: 3.264585445628255\n",
      "Operator 183: 2.5291708912555517\n",
      "Operator 184: -0.9299945419615234\n",
      "Operator 185: 2.0641736202750645\n",
      "Operator 186: 2.5291708912545836\n",
      "Operator 187: -3.414213562526488\n",
      "Operator 188: -2.828427125052986\n",
      "Operator 189: -2.8284271250529853\n",
      "Operator 190: -3.9999999999999796\n",
      "Operator 191: -3.9999999999999822\n",
      "Operator 192: -4.000000000000005\n",
      "Operator 193: -3.9999999999999947\n",
      "Operator 194: -3.2645854456282555\n",
      "Operator 196: -2.5291708912555495\n",
      "Operator 197: 0.9299945419615234\n",
      "Operator 198: -2.064173620275066\n",
      "Operator 199: -2.5291708912545836\n",
      "Total gradient norm: 36.958760621782204\n",
      "Operators under consideration (1):\n",
      "[179]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000005)]\n",
      "Operator(s) added to ansatz: [179]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -17.591724953300698\n",
      "Optimizing energy with indices [197, 200, 195, 179]...\n",
      "Starting point: [np.float64(0.4431436457804466), np.float64(0.3926990816444871), np.float64(0.4431436457801333), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18.420152\n",
      "         Iterations: 5\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "\n",
      "Current energy: -18.420152078046925\n",
      "(change of -0.8284271247462272)\n",
      "Current ansatz: [197, 200, 195, 179]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.414213562376883\n",
      "Operator 1: 3.9999999999999853\n",
      "Operator 2: -3.999999999999986\n",
      "Operator 3: 3.9999999999999876\n",
      "Operator 4: -3.4142135621246132\n",
      "Operator 6: -3.4142135621246235\n",
      "Operator 7: 3.264585445600023\n",
      "Operator 9: 2.529170891236667\n",
      "Operator 11: 3.2645854456366408\n",
      "Operator 12: -3.9999999999999916\n",
      "Operator 13: -3.4142135623768666\n",
      "Operator 15: -3.4142135623768817\n",
      "Operator 16: 3.9999999999999853\n",
      "Operator 17: -2.8284271242492283\n",
      "Operator 18: 3.9999999999999876\n",
      "Operator 19: -3.4142135621246132\n",
      "Operator 21: -3.41421356212462\n",
      "Operator 22: 2.0641736202715055\n",
      "Operator 24: 2.7995881746348577\n",
      "Operator 26: 3.4142135623768817\n",
      "Operator 27: 2.828427124753766\n",
      "Operator 28: 3.999999999999986\n",
      "Operator 29: 3.9999999999999876\n",
      "Operator 30: 3.4142135621246132\n",
      "Operator 32: 3.41421356212462\n",
      "Operator 33: 2.3084105059411\n",
      "Operator 35: 2.7995881746348577\n",
      "Operator 36: -0.9299945419510713\n",
      "Operator 37: 3.264585445636639\n",
      "Operator 38: 2.529170891273289\n",
      "Operator 40: 3.4142135623768817\n",
      "Operator 41: 3.9999999999999853\n",
      "Operator 42: 3.999999999999986\n",
      "Operator 43: 2.828427124249231\n",
      "Operator 44: 3.4142135621246132\n",
      "Operator 46: 2.158782389416508\n",
      "Operator 47: 3.264585445600023\n",
      "Operator 48: -0.9299945420282321\n",
      "Operator 49: 2.7995881746348577\n",
      "Operator 51: 3.264585445636639\n",
      "Operator 53: -3.414213562376883\n",
      "Operator 54: -3.9999999999999853\n",
      "Operator 55: -3.999999999999986\n",
      "Operator 56: -2.828427124249231\n",
      "Operator 57: -3.414213562124611\n",
      "Operator 59: -2.15878238941651\n",
      "Operator 60: -3.264585445600023\n",
      "Operator 61: 0.9299945420282321\n",
      "Operator 62: -2.7995881746348568\n",
      "Operator 64: -3.2645854456366408\n",
      "Operator 65: 3.41421356237687\n",
      "Operator 67: 3.4142135623768817\n",
      "Operator 68: -3.9999999999999853\n",
      "Operator 69: 3.999999999999986\n",
      "Operator 70: -3.9999999999999876\n",
      "Operator 71: 3.414213562124611\n",
      "Operator 73: 3.41421356212462\n",
      "Operator 74: -3.264585445600023\n",
      "Operator 76: -2.52917089123667\n",
      "Operator 78: -3.414213562376883\n",
      "Operator 79: -2.828427124753766\n",
      "Operator 80: -3.999999999999986\n",
      "Operator 81: -3.9999999999999876\n",
      "Operator 82: -3.414213562124611\n",
      "Operator 84: -3.4142135621246235\n",
      "Operator 85: -2.3084105059411\n",
      "Operator 87: -2.7995881746348568\n",
      "Operator 88: 0.9299945419510713\n",
      "Operator 89: -3.2645854456366408\n",
      "Operator 90: -2.529170891273289\n",
      "Operator 101: -1.2004118253651312\n",
      "Operator 104: -3.4142135623768666\n",
      "Operator 107: -3.264585445636639\n",
      "Operator 108: 3.9999999999999916\n",
      "Operator 109: -1.4142135623692986\n",
      "Operator 114: 1.4142135626215622\n",
      "Operator 115: -1.4142135626215644\n",
      "Operator 117: 1.5494591478243525\n",
      "Operator 118: -1.5494591478243511\n",
      "Operator 119: 1.5494591477944648\n",
      "Operator 120: -1.5494591477944628\n",
      "Operator 122: 1.4142135623693017\n",
      "Operator 127: -1.4142135626215682\n",
      "Operator 128: 1.414213562621574\n",
      "Operator 130: -1.5494591478243536\n",
      "Operator 131: 1.5494591478243493\n",
      "Operator 132: -1.549459147794464\n",
      "Operator 133: 1.549459147794467\n",
      "Operator 145: -1.200411825365131\n",
      "Operator 158: 1.2004118253651317\n",
      "Operator 161: 3.41421356237687\n",
      "Operator 163: 3.414213562376883\n",
      "Operator 164: -3.9999999999999853\n",
      "Operator 165: 2.8284271242492283\n",
      "Operator 166: -3.9999999999999876\n",
      "Operator 167: 3.414213562124611\n",
      "Operator 169: 3.4142135621246235\n",
      "Operator 170: -2.0641736202715055\n",
      "Operator 172: -2.7995881746348568\n",
      "Operator 174: 3.4142135623768817\n",
      "Operator 175: 2.828427124753766\n",
      "Operator 176: 2.828427124753764\n",
      "Operator 177: 3.9999999999999876\n",
      "Operator 178: 3.414213562124611\n",
      "Operator 180: 3.41421356212462\n",
      "Operator 181: 2.3084105059411\n",
      "Operator 183: 2.52917089123667\n",
      "Operator 184: -0.9299945419510713\n",
      "Operator 185: 2.0641736202348886\n",
      "Operator 186: 2.529170891273289\n",
      "Operator 187: -3.414213562376883\n",
      "Operator 188: -2.828427124753766\n",
      "Operator 189: -2.828427124753764\n",
      "Operator 190: -3.9999999999999876\n",
      "Operator 191: -3.4142135621246132\n",
      "Operator 193: -3.4142135621246235\n",
      "Operator 194: -2.3084105059411\n",
      "Operator 196: -2.529170891236667\n",
      "Operator 197: 0.9299945419510713\n",
      "Operator 198: -2.0641736202348895\n",
      "Operator 199: -2.529170891273289\n",
      "Total gradient norm: 33.138982906470204\n",
      "Operators under consideration (1):\n",
      "[190]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999876)]\n",
      "Operator(s) added to ansatz: [190]\n",
      "Gradients: [np.float64(-3.9999999999999876)]\n",
      "Initial energy: -18.420152078046925\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190]...\n",
      "Starting point: [np.float64(0.44314364577742865), np.float64(0.3926990816973836), np.float64(0.4431436457892455), np.float64(-0.392699081786573), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.355023\n",
      "         Iterations: 7\n",
      "         Function evaluations: 109\n",
      "         Gradient evaluations: 95\n",
      "\n",
      "Current energy: -19.35502278185536\n",
      "(change of -0.9348707038084356)\n",
      "Current ansatz: [197, 200, 195, 179, 190]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135623646674\n",
      "Operator 1: 3.999999999999992\n",
      "Operator 2: -3.264585447314688\n",
      "Operator 4: -2.5291708989328825\n",
      "Operator 5: 1.865574558147667e-08\n",
      "Operator 6: -3.2645854516181987\n",
      "Operator 7: 3.264585437641306\n",
      "Operator 8: 3.2603299782163475e-08\n",
      "Operator 9: 2.5291708910037785\n",
      "Operator 10: -3.19622994737756e-08\n",
      "Operator 11: 3.264585453362475\n",
      "Operator 12: -3.9999999999999964\n",
      "Operator 13: -3.414213562364653\n",
      "Operator 15: -2.1587823924954095\n",
      "Operator 16: 3.999999999999992\n",
      "Operator 17: -2.0641736311193077\n",
      "Operator 19: -2.7995881795011197\n",
      "Operator 20: 1.1795893192194169e-08\n",
      "Operator 21: -3.264585451618194\n",
      "Operator 22: 2.064173627850083\n",
      "Operator 23: 3.2603299782163475e-08\n",
      "Operator 24: 2.7995881744876048\n",
      "Operator 25: -3.19622994737756e-08\n",
      "Operator 26: 3.414213562364667\n",
      "Operator 27: 2.82842712472933\n",
      "Operator 28: 3.264585447314688\n",
      "Operator 30: 2.799588179501119\n",
      "Operator 31: -0.929994525578403\n",
      "Operator 32: 3.264585451618194\n",
      "Operator 33: 2.0641736250029012\n",
      "Operator 34: -3.2603299782163475e-08\n",
      "Operator 35: 2.7995881744876048\n",
      "Operator 36: -0.9299945257874551\n",
      "Operator 37: 3.264585453362473\n",
      "Operator 38: 2.5291709067249513\n",
      "Operator 40: 3.414213562364667\n",
      "Operator 41: 2.529170894629382\n",
      "Operator 42: 3.264585447314688\n",
      "Operator 43: -0.9299945346455424\n",
      "Operator 44: 2.7995881795011197\n",
      "Operator 45: 1.8655745925525717e-08\n",
      "Operator 46: 2.0641736110260145\n",
      "Operator 47: 3.264585437641306\n",
      "Operator 48: -0.9299945589107034\n",
      "Operator 49: 2.7995881744876048\n",
      "Operator 50: 3.19622994737756e-08\n",
      "Operator 51: 3.264585453362473\n",
      "Operator 53: -3.4142135623646674\n",
      "Operator 54: -2.529170894629382\n",
      "Operator 55: -3.2645854473146856\n",
      "Operator 56: 0.9299945346455429\n",
      "Operator 57: -2.799588179501118\n",
      "Operator 58: -1.865574558147667e-08\n",
      "Operator 59: -2.0641736110260176\n",
      "Operator 60: -3.2645854376413053\n",
      "Operator 61: 0.929994558910705\n",
      "Operator 62: -2.7995881744876043\n",
      "Operator 63: -3.19622994737756e-08\n",
      "Operator 64: -3.264585453362475\n",
      "Operator 65: 3.4142135623646555\n",
      "Operator 67: 3.414213562364667\n",
      "Operator 68: -3.999999999999992\n",
      "Operator 69: 3.2645854473146856\n",
      "Operator 71: 2.529170898932878\n",
      "Operator 72: -1.8655745925525717e-08\n",
      "Operator 73: 3.264585451618194\n",
      "Operator 74: -3.2645854376413053\n",
      "Operator 75: -3.260330050380844e-08\n",
      "Operator 76: -2.5291708910037802\n",
      "Operator 77: 3.19622994737756e-08\n",
      "Operator 78: -3.4142135623646674\n",
      "Operator 79: -2.82842712472933\n",
      "Operator 80: -3.2645854473146856\n",
      "Operator 82: -2.799588179501118\n",
      "Operator 83: 0.929994525578403\n",
      "Operator 84: -3.2645854516181987\n",
      "Operator 85: -2.0641736250029012\n",
      "Operator 86: 3.260330050380844e-08\n",
      "Operator 87: -2.7995881744876043\n",
      "Operator 88: 0.9299945257874551\n",
      "Operator 89: -3.264585453362475\n",
      "Operator 90: -2.5291709067249513\n",
      "Operator 96: -1.2004118204988796\n",
      "Operator 101: -1.2004118255123888\n",
      "Operator 104: -3.414213562364652\n",
      "Operator 107: -3.264585453362473\n",
      "Operator 108: 3.9999999999999964\n",
      "Operator 109: -1.4142135623815206\n",
      "Operator 112: 1.5494591464249312\n",
      "Operator 113: -1.549459146424934\n",
      "Operator 114: 1.5494591429126503\n",
      "Operator 115: -1.5494591429126494\n",
      "Operator 117: 1.5494591543198364\n",
      "Operator 118: -1.5494591543198353\n",
      "Operator 119: 1.549459141489059\n",
      "Operator 120: -1.5494591414890573\n",
      "Operator 122: 1.414213562381524\n",
      "Operator 125: -1.549459146424935\n",
      "Operator 126: 1.5494591464249388\n",
      "Operator 127: -1.5494591429126543\n",
      "Operator 128: 1.5494591429126603\n",
      "Operator 130: -1.5494591543198375\n",
      "Operator 131: 1.5494591543198328\n",
      "Operator 132: -1.5494591414890584\n",
      "Operator 133: 1.549459141489062\n",
      "Operator 140: -1.2004118204988796\n",
      "Operator 145: -1.2004118255123888\n",
      "Operator 153: 1.2004118204988798\n",
      "Operator 158: 1.2004118255123895\n",
      "Operator 161: 3.4142135623646555\n",
      "Operator 163: 2.15878239249541\n",
      "Operator 164: -3.999999999999992\n",
      "Operator 165: 2.0641736311193073\n",
      "Operator 167: 2.799588179501118\n",
      "Operator 168: -1.1795893060993356e-08\n",
      "Operator 169: 3.2645854516181987\n",
      "Operator 170: -2.064173627850082\n",
      "Operator 171: -3.2603299782163475e-08\n",
      "Operator 172: -2.7995881744876043\n",
      "Operator 173: 3.19622994737756e-08\n",
      "Operator 174: 3.414213562364667\n",
      "Operator 175: 2.82842712472933\n",
      "Operator 176: 2.3084105075453767\n",
      "Operator 178: 2.529170898932878\n",
      "Operator 179: -0.9299945255784028\n",
      "Operator 180: 2.0641736268158177\n",
      "Operator 181: 2.0641736250029012\n",
      "Operator 182: -2.0614828843967814e-08\n",
      "Operator 183: 2.5291708910037802\n",
      "Operator 184: -0.9299945257874551\n",
      "Operator 185: 2.0641736121289136\n",
      "Operator 186: 2.5291709067249513\n",
      "Operator 187: -3.4142135623646674\n",
      "Operator 188: -2.82842712472933\n",
      "Operator 189: -2.308410507545378\n",
      "Operator 191: -2.5291708989328825\n",
      "Operator 192: 0.9299945255784028\n",
      "Operator 193: -2.0641736268158204\n",
      "Operator 194: -2.0641736250029012\n",
      "Operator 195: 2.0614828843967814e-08\n",
      "Operator 196: -2.5291708910037785\n",
      "Operator 197: 0.9299945257874551\n",
      "Operator 198: -2.0641736121289145\n",
      "Operator 199: -2.5291709067249513\n",
      "Total gradient norm: 28.757976647442757\n",
      "Operators under consideration (1):\n",
      "[164]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.999999999999992)]\n",
      "Operator(s) added to ansatz: [164]\n",
      "Gradients: [np.float64(-3.999999999999992)]\n",
      "Initial energy: -19.35502278185536\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164]...\n",
      "Starting point: [np.float64(0.443143643284356), np.float64(0.3926990817017036), np.float64(0.4431436483574713), np.float64(-0.443143643847225), np.float64(0.44314364523593364), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -20.479100\n",
      "         Iterations: 11\n",
      "         Function evaluations: 19\n",
      "         Gradient evaluations: 19\n",
      "\n",
      "Current energy: -20.479100049240195\n",
      "(change of -1.124077267384834)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.1202961806595826\n",
      "Operator 2: -1.8733788485386456\n",
      "Operator 4: -2.120296188417795\n",
      "Operator 6: -3.183606762164269\n",
      "Operator 7: 3.264585446353268\n",
      "Operator 9: 2.529170893003002\n",
      "Operator 11: 3.264585446649737\n",
      "Operator 12: -3.999999999999998\n",
      "Operator 13: -1.4910253876423791\n",
      "Operator 15: -1.1963097449246343\n",
      "Operator 17: -1.4432270816666364\n",
      "Operator 19: -2.5543359694807206\n",
      "Operator 21: -3.1836067621642634\n",
      "Operator 22: 2.0641736224014386\n",
      "Operator 24: 2.7995881757516994\n",
      "Operator 26: 2.55433596535727\n",
      "Operator 27: -1.1482009588405724\n",
      "Operator 28: 2.4386935387689985\n",
      "Operator 29: -1.3794858088236994\n",
      "Operator 30: 2.5543359694807206\n",
      "Operator 31: -1.2585415807261238\n",
      "Operator 32: 3.1836067621642634\n",
      "Operator 33: 1.9319927049833798\n",
      "Operator 35: 2.7995881757516994\n",
      "Operator 36: -0.9299945389513207\n",
      "Operator 37: 3.264585446649735\n",
      "Operator 38: 2.529170893299476\n",
      "Operator 39: -1.258541589859642\n",
      "Operator 40: 2.55433596535727\n",
      "Operator 41: -1.3794858120171059\n",
      "Operator 42: 2.4386935387689985\n",
      "Operator 43: -1.14820094740025\n",
      "Operator 44: 2.5543359694807206\n",
      "Operator 46: 2.0129713891723844\n",
      "Operator 47: 3.264585446353268\n",
      "Operator 48: -0.9299945395759667\n",
      "Operator 49: 2.7995881757516994\n",
      "Operator 51: 3.264585446649735\n",
      "Operator 52: 1.2585415898596417\n",
      "Operator 53: -2.5543359653572684\n",
      "Operator 54: 1.3794858120171059\n",
      "Operator 55: -2.4386935387689963\n",
      "Operator 56: 1.14820094740025\n",
      "Operator 57: -2.5543359694807215\n",
      "Operator 59: -2.0129713891723884\n",
      "Operator 60: -3.264585446353268\n",
      "Operator 61: 0.9299945395759667\n",
      "Operator 62: -2.7995881757516994\n",
      "Operator 64: -3.264585446649737\n",
      "Operator 65: 3.1836067583744505\n",
      "Operator 67: 2.1202961806595777\n",
      "Operator 69: 1.8733788485386431\n",
      "Operator 71: 2.1202961884177873\n",
      "Operator 73: 3.1836067621642634\n",
      "Operator 74: -3.264585446353268\n",
      "Operator 76: -2.5291708930030037\n",
      "Operator 78: -2.5543359653572684\n",
      "Operator 79: 1.1482009588405724\n",
      "Operator 80: -2.4386935387689963\n",
      "Operator 81: 1.3794858088236994\n",
      "Operator 82: -2.5543359694807215\n",
      "Operator 83: 1.2585415807261238\n",
      "Operator 84: -3.183606762164269\n",
      "Operator 85: -1.931992704983379\n",
      "Operator 87: -2.7995881757516994\n",
      "Operator 88: 0.9299945389513207\n",
      "Operator 89: -3.264585446649737\n",
      "Operator 90: -2.529170893299476\n",
      "Operator 92: -1.4244214019025585\n",
      "Operator 94: -1.5613064612310035\n",
      "Operator 96: -1.4244213977485745\n",
      "Operator 101: -1.2004118242482953\n",
      "Operator 104: -3.183606758374445\n",
      "Operator 107: -3.264585446649735\n",
      "Operator 108: 3.999999999999998\n",
      "Operator 109: -1.6121647067003726\n",
      "Operator 110: 1.7670916575489741\n",
      "Operator 111: -1.7670916575489741\n",
      "Operator 112: 1.7670916554454161\n",
      "Operator 113: -1.7670916554454141\n",
      "Operator 114: 1.6121647039180071\n",
      "Operator 115: -1.6121647039180045\n",
      "Operator 117: 1.5494591472096015\n",
      "Operator 118: -1.5494591472096004\n",
      "Operator 119: 1.5494591469676364\n",
      "Operator 120: -1.5494591469676346\n",
      "Operator 122: 1.6121647067003733\n",
      "Operator 123: -1.7670916575489777\n",
      "Operator 124: 1.7670916575489743\n",
      "Operator 125: -1.7670916554454188\n",
      "Operator 126: 1.767091655445419\n",
      "Operator 127: -1.6121647039180096\n",
      "Operator 128: 1.6121647039180154\n",
      "Operator 130: -1.5494591472096024\n",
      "Operator 131: 1.5494591472095989\n",
      "Operator 132: -1.5494591469676355\n",
      "Operator 133: 1.5494591469676389\n",
      "Operator 136: -1.4244214019025585\n",
      "Operator 138: -1.561306461231003\n",
      "Operator 140: -1.4244213977485745\n",
      "Operator 145: -1.2004118242482953\n",
      "Operator 149: 1.4244214019025596\n",
      "Operator 151: 1.5613064612310041\n",
      "Operator 153: 1.4244213977485738\n",
      "Operator 158: 1.2004118242482957\n",
      "Operator 161: 1.4910253876423814\n",
      "Operator 163: 1.1963097449246338\n",
      "Operator 165: 1.4432270816666353\n",
      "Operator 167: 2.5543359694807215\n",
      "Operator 169: 3.183606762164269\n",
      "Operator 170: -2.0641736224014386\n",
      "Operator 172: -2.7995881757516994\n",
      "Operator 174: 2.1202961806595777\n",
      "Operator 175: -1.148200958840572\n",
      "Operator 176: 1.1086719330630546\n",
      "Operator 177: -1.3794858088236999\n",
      "Operator 178: 0.9930295059011938\n",
      "Operator 179: -1.2585415807261238\n",
      "Operator 180: 1.4910253957342579\n",
      "Operator 181: 1.931992704983379\n",
      "Operator 183: 2.5291708930030037\n",
      "Operator 184: -0.9299945389513207\n",
      "Operator 185: 2.064173622104969\n",
      "Operator 186: 2.529170893299476\n",
      "Operator 187: -2.1202961806595826\n",
      "Operator 188: 1.148200958840572\n",
      "Operator 189: -1.1086719330630557\n",
      "Operator 190: 1.3794858088236999\n",
      "Operator 191: -0.993029505901197\n",
      "Operator 192: 1.2585415807261238\n",
      "Operator 193: -1.4910253957342607\n",
      "Operator 194: -1.9319927049833798\n",
      "Operator 196: -2.529170893003002\n",
      "Operator 197: 0.9299945389513207\n",
      "Operator 198: -2.0641736221049705\n",
      "Operator 199: -2.529170893299476\n",
      "Total gradient norm: 24.878260729882527\n",
      "Operators under consideration (1):\n",
      "[108]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.999999999999998)]\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(3.999999999999998)]\n",
      "Initial energy: -20.479100049240195\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108]...\n",
      "Starting point: [np.float64(0.44314364545051105), np.float64(0.46875105194373023), np.float64(0.44314364554617985), np.float64(-0.4687510507683549), np.float64(0.5416899700165159), np.float64(0.5416899711393839), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -22.032753\n",
      "         Iterations: 14\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 61\n",
      "\n",
      "Current energy: -22.032752887823747\n",
      "(change of -1.5536528385835524)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.1202961904742152\n",
      "Operator 2: -1.8733788573191907\n",
      "Operator 4: -2.120296190928282\n",
      "Operator 6: -3.183606761501965\n",
      "Operator 7: 3.1136282852056802\n",
      "Operator 9: 1.795011694145141\n",
      "Operator 10: 1.0015002938068562e-08\n",
      "Operator 11: 1.017237109782073\n",
      "Operator 13: -1.4910253985390762\n",
      "Operator 15: -1.196309751453888\n",
      "Operator 17: -1.4432270832927265\n",
      "Operator 19: -2.5543359710482108\n",
      "Operator 21: -3.1836067615019603\n",
      "Operator 22: 1.0607873275719009\n",
      "Operator 24: 0.3995658059360361\n",
      "Operator 25: 1.0015002938068562e-08\n",
      "Operator 26: 2.554335970646235\n",
      "Operator 27: -1.1482009478661146\n",
      "Operator 28: 2.438693542881329\n",
      "Operator 29: -1.3794858011092408\n",
      "Operator 30: 2.5543359710482103\n",
      "Operator 31: -1.2585415808627083\n",
      "Operator 32: 3.1836067615019603\n",
      "Operator 33: 1.8426557455865633\n",
      "Operator 35: 2.3794039186324354\n",
      "Operator 36: -1.2973626475854514\n",
      "Operator 37: 2.1144225697925396\n",
      "Operator 38: -1.7427537779512123\n",
      "Operator 39: -1.2585415792669472\n",
      "Operator 40: 2.554335970646235\n",
      "Operator 41: -1.379485803395914\n",
      "Operator 42: 2.4386935428813294\n",
      "Operator 43: -1.1482009447754709\n",
      "Operator 44: 2.5543359710482108\n",
      "Operator 46: 1.772677269290287\n",
      "Operator 47: 3.1136282852056802\n",
      "Operator 48: -1.4684487317092365\n",
      "Operator 49: 2.3794039186324354\n",
      "Operator 50: -1.8273253452652611\n",
      "Operator 51: 2.1144225697925396\n",
      "Operator 52: 1.258541579266947\n",
      "Operator 53: -2.5543359706462336\n",
      "Operator 54: 1.379485803395914\n",
      "Operator 55: -2.438693542881328\n",
      "Operator 56: 1.1482009447754713\n",
      "Operator 57: -2.5543359710482134\n",
      "Operator 59: -1.77267726929029\n",
      "Operator 60: -3.11362828520568\n",
      "Operator 61: 1.4684487317092365\n",
      "Operator 62: -2.379403918632436\n",
      "Operator 63: 1.8273253452652602\n",
      "Operator 64: -2.1144225697925396\n",
      "Operator 65: 3.183606762581352\n",
      "Operator 67: 2.120296190474211\n",
      "Operator 69: 1.8733788573191883\n",
      "Operator 71: 2.1202961909282734\n",
      "Operator 73: 3.1836067615019603\n",
      "Operator 74: -3.11362828520568\n",
      "Operator 76: -1.7950116941451428\n",
      "Operator 77: -1.0015002938068562e-08\n",
      "Operator 78: -2.5543359706462336\n",
      "Operator 79: 1.1482009478661146\n",
      "Operator 80: -2.438693542881328\n",
      "Operator 81: 1.3794858011092406\n",
      "Operator 82: -2.5543359710482125\n",
      "Operator 83: 1.2585415808627076\n",
      "Operator 84: -3.183606761501965\n",
      "Operator 85: -1.8426557455865633\n",
      "Operator 87: -2.379403918632436\n",
      "Operator 88: 1.2973626475854534\n",
      "Operator 89: -2.1144225697925396\n",
      "Operator 90: 1.7427537779512123\n",
      "Operator 92: -1.4244213967775754\n",
      "Operator 94: -1.561306457118695\n",
      "Operator 96: -1.4244213968225305\n",
      "Operator 101: -1.561888608667589\n",
      "Operator 103: -1.8536481505750517\n",
      "Operator 104: -3.1836067625813484\n",
      "Operator 107: -1.0172371097820692\n",
      "Operator 109: -1.6121647036118052\n",
      "Operator 110: 1.7670916545764692\n",
      "Operator 111: -1.7670916545764683\n",
      "Operator 112: 1.767091653763609\n",
      "Operator 113: -1.7670916537636068\n",
      "Operator 114: 1.6121647044042744\n",
      "Operator 115: -1.6121647044042722\n",
      "Operator 117: 1.6612742225141288\n",
      "Operator 118: -1.6612742225141268\n",
      "Operator 119: 1.8803501402722982\n",
      "Operator 120: -1.8803501402722962\n",
      "Operator 121: 1.9715989175363116\n",
      "Operator 122: 1.612164703611806\n",
      "Operator 123: -1.7670916545764737\n",
      "Operator 124: 1.7670916545764683\n",
      "Operator 125: -1.7670916537636117\n",
      "Operator 126: 1.7670916537636119\n",
      "Operator 127: -1.6121647044042773\n",
      "Operator 128: 1.612164704404283\n",
      "Operator 130: -1.66127422251413\n",
      "Operator 131: 1.6612742225141255\n",
      "Operator 132: -1.8803501402722973\n",
      "Operator 133: 1.8803501402722982\n",
      "Operator 134: -1.971598917536311\n",
      "Operator 136: -1.4244213967775754\n",
      "Operator 138: -1.5613064571186952\n",
      "Operator 140: -1.4244213968225308\n",
      "Operator 145: -1.561888608667589\n",
      "Operator 147: -1.8536481505750517\n",
      "Operator 149: 1.4244213967775758\n",
      "Operator 151: 1.5613064571186965\n",
      "Operator 153: 1.4244213968225303\n",
      "Operator 158: 1.5618886086675898\n",
      "Operator 160: 1.8536481505750502\n",
      "Operator 161: 1.4910253985390773\n",
      "Operator 163: 1.1963097514538878\n",
      "Operator 165: 1.4432270832927259\n",
      "Operator 167: 2.5543359710482134\n",
      "Operator 169: 3.183606761501965\n",
      "Operator 170: -1.0607873275719009\n",
      "Operator 172: -0.39956580593603586\n",
      "Operator 173: -1.0015002938068562e-08\n",
      "Operator 174: 2.120296190474211\n",
      "Operator 175: -1.1482009478661146\n",
      "Operator 176: 1.1086719421999536\n",
      "Operator 177: -1.37948580110924\n",
      "Operator 178: 0.9930295130219988\n",
      "Operator 179: -1.2585415808627076\n",
      "Operator 180: 1.4910254004745407\n",
      "Operator 181: 1.8426557455865633\n",
      "Operator 183: 1.7950116941451428\n",
      "Operator 184: -1.2973626475854534\n",
      "Operator 185: 0.5664120091070853\n",
      "Operator 186: -1.7427537779512159\n",
      "Operator 187: -2.1202961904742152\n",
      "Operator 188: 1.148200947866115\n",
      "Operator 189: -1.1086719421999556\n",
      "Operator 190: 1.3794858011092408\n",
      "Operator 191: -0.9930295130220019\n",
      "Operator 192: 1.2585415808627076\n",
      "Operator 193: -1.491025400474543\n",
      "Operator 194: -1.8426557455865633\n",
      "Operator 196: -1.795011694145141\n",
      "Operator 197: 1.2973626475854514\n",
      "Operator 198: -0.5664120091070872\n",
      "Operator 199: 1.7427537779512159\n",
      "Total gradient norm: 22.60874466093471\n",
      "Operators under consideration (1):\n",
      "[104]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.1836067625813484)]\n",
      "Operator(s) added to ansatz: [104]\n",
      "Gradients: [np.float64(-3.1836067625813484)]\n",
      "Initial energy: -22.032752887823747\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104]...\n",
      "Starting point: [np.float64(0.6115719042262776), np.float64(0.4687510506389988), np.float64(0.4901254589345913), np.float64(-0.4687510509737699), np.float64(0.5416899691187712), np.float64(0.5416899695526703), np.float64(-0.701035024776171), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -22.743117\n",
      "         Iterations: 9\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 50\n",
      "\n",
      "Current energy: -22.743117347290898\n",
      "(change of -0.7103644594671508)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.7172349966435467\n",
      "Operator 2: -2.089892526749742\n",
      "Operator 4: -2.193576273274234\n",
      "Operator 6: -3.1986897467936286\n",
      "Operator 7: 3.113628372472058\n",
      "Operator 8: -3.7425237183023647e-07\n",
      "Operator 9: 1.7950116371202407\n",
      "Operator 10: 5.243766065810007e-07\n",
      "Operator 11: 1.0172370780644167\n",
      "Operator 12: -3.798619196970776e-07\n",
      "Operator 15: -1.43670331739942\n",
      "Operator 17: -1.5251549119578551\n",
      "Operator 19: -2.5962801392577233\n",
      "Operator 21: -3.1986897467936237\n",
      "Operator 22: 1.0607871326681237\n",
      "Operator 23: -3.7425237225443883e-07\n",
      "Operator 24: 0.3995659313658937\n",
      "Operator 25: 5.24376607802246e-07\n",
      "Operator 26: 1.9019869596894128\n",
      "Operator 27: -0.5725298453991955\n",
      "Operator 28: 2.5447033580416\n",
      "Operator 29: -1.2149123279052412\n",
      "Operator 30: 2.596280139257723\n",
      "Operator 31: -1.2048192105651072\n",
      "Operator 32: 3.1986897467936237\n",
      "Operator 33: 1.8661372027039538\n",
      "Operator 34: 3.7425237183023647e-07\n",
      "Operator 35: 2.379403868019935\n",
      "Operator 36: -1.297362878696666\n",
      "Operator 37: 2.11442258391505\n",
      "Operator 38: -1.742753730529742\n",
      "Operator 39: -0.819123648304444\n",
      "Operator 40: 2.8881752424202842\n",
      "Operator 41: -1.2594736141565366\n",
      "Operator 42: 2.544703358041599\n",
      "Operator 43: -1.111758765472995\n",
      "Operator 44: 2.5962801392577233\n",
      "Operator 46: 1.781075828382389\n",
      "Operator 47: 3.113628372472058\n",
      "Operator 48: -1.4684486346518848\n",
      "Operator 49: 2.3794038680199354\n",
      "Operator 50: -1.8273254469064515\n",
      "Operator 51: 2.11442258391505\n",
      "Operator 52: -0.28096272851805504\n",
      "Operator 53: -2.8881752424202825\n",
      "Operator 54: 1.2594736141565366\n",
      "Operator 55: -2.544703358041599\n",
      "Operator 56: 1.111758765472995\n",
      "Operator 57: -2.5962801392577246\n",
      "Operator 59: -1.781075828382392\n",
      "Operator 60: -3.113628372472058\n",
      "Operator 61: 1.4684486346518848\n",
      "Operator 62: -2.3794038680199354\n",
      "Operator 63: 1.8273254469064502\n",
      "Operator 64: -2.114422583915051\n",
      "Operator 65: 1.1645952143441936\n",
      "Operator 67: 2.7172349966435423\n",
      "Operator 69: 2.0898925267497392\n",
      "Operator 71: 2.1935762732742288\n",
      "Operator 73: 3.1986897467936237\n",
      "Operator 74: -3.113628372472058\n",
      "Operator 75: 3.7425237225443883e-07\n",
      "Operator 76: -1.7950116371202418\n",
      "Operator 77: -5.24376607802246e-07\n",
      "Operator 78: -1.9019869596894117\n",
      "Operator 79: 0.5725298453991965\n",
      "Operator 80: -2.5447033580416\n",
      "Operator 81: 1.2149123279052412\n",
      "Operator 82: -2.5962801392577255\n",
      "Operator 83: 1.2048192105651072\n",
      "Operator 84: -3.1986897467936286\n",
      "Operator 85: -1.8661372027039533\n",
      "Operator 86: -3.7425237225443883e-07\n",
      "Operator 87: -2.3794038680199354\n",
      "Operator 88: 1.297362878696666\n",
      "Operator 89: -2.114422583915051\n",
      "Operator 90: 1.742753730529742\n",
      "Operator 92: -0.6446290269328367\n",
      "Operator 94: -1.4518486171305112\n",
      "Operator 96: -1.3888461739272784\n",
      "Operator 101: -1.5618885971000434\n",
      "Operator 103: -1.853648184090221\n",
      "Operator 105: 1.4984222839020087\n",
      "Operator 107: -1.017237078064413\n",
      "Operator 108: 3.798619196970776e-07\n",
      "Operator 109: -0.7703467515365708\n",
      "Operator 110: 1.6736074388501856\n",
      "Operator 111: -1.6736074388501838\n",
      "Operator 112: 1.7349930257575537\n",
      "Operator 113: -1.7349930257575537\n",
      "Operator 114: 1.6009818521557055\n",
      "Operator 115: -1.6009818521557038\n",
      "Operator 117: 1.6612741640154707\n",
      "Operator 118: -1.6612741640154687\n",
      "Operator 119: 1.8803501925591928\n",
      "Operator 120: -1.880350192559191\n",
      "Operator 121: 1.9715988983598582\n",
      "Operator 122: 1.1697748002594768\n",
      "Operator 123: -1.67360743885019\n",
      "Operator 124: 1.6736074388501845\n",
      "Operator 125: -1.7349930257575565\n",
      "Operator 126: 1.7349930257575588\n",
      "Operator 127: -1.6009818521557084\n",
      "Operator 128: 1.6009818521557144\n",
      "Operator 130: -1.6612741640154722\n",
      "Operator 131: 1.6612741640154673\n",
      "Operator 132: -1.8803501925591926\n",
      "Operator 133: 1.8803501925591934\n",
      "Operator 134: -1.971598898359858\n",
      "Operator 136: -0.9788719037468676\n",
      "Operator 138: -1.4518486171305107\n",
      "Operator 140: -1.3888461739272784\n",
      "Operator 145: -1.5618885971000434\n",
      "Operator 147: -1.8536481840902206\n",
      "Operator 148: -0.8803061772583167\n",
      "Operator 149: 0.9788719037468683\n",
      "Operator 151: 1.4518486171305114\n",
      "Operator 153: 1.3888461739272777\n",
      "Operator 158: 1.5618885971000438\n",
      "Operator 160: 1.8536481840902195\n",
      "Operator 161: 0.6376193737957971\n",
      "Operator 162: -1.4984222839020087\n",
      "Operator 163: 1.4367033173994188\n",
      "Operator 165: 1.5251549119578551\n",
      "Operator 167: 2.5962801392577246\n",
      "Operator 169: 3.1986897467936286\n",
      "Operator 170: -1.0607871326681235\n",
      "Operator 171: 3.742523706424699e-07\n",
      "Operator 172: -0.3995659313658939\n",
      "Operator 173: -5.243766065810007e-07\n",
      "Operator 174: 1.7894154946418104\n",
      "Operator 175: -0.3770353972945667\n",
      "Operator 176: 1.6951421280998038\n",
      "Operator 177: -1.2149123279052407\n",
      "Operator 178: 1.2009895906416204\n",
      "Operator 179: -1.2048192105651072\n",
      "Operator 180: 1.5911666657383416\n",
      "Operator 181: 1.8661372027039533\n",
      "Operator 182: 2.243062406373407e-07\n",
      "Operator 183: 1.7950116371202418\n",
      "Operator 184: -1.2973628786966642\n",
      "Operator 185: 0.5664120358315434\n",
      "Operator 186: -1.7427537305297403\n",
      "Operator 187: -1.7894154946418133\n",
      "Operator 188: 0.37703539729456637\n",
      "Operator 189: -1.695142128099806\n",
      "Operator 190: 1.2149123279052412\n",
      "Operator 191: -1.2009895906416233\n",
      "Operator 192: 1.2048192105651059\n",
      "Operator 193: -1.5911666657383443\n",
      "Operator 194: -1.8661372027039538\n",
      "Operator 195: -2.243062406373407e-07\n",
      "Operator 196: -1.7950116371202407\n",
      "Operator 197: 1.2973628786966653\n",
      "Operator 198: -0.5664120358315453\n",
      "Operator 199: 1.7427537305297403\n",
      "Operator 200: -1.4984222839020074\n",
      "Total gradient norm: 22.13788949355754\n",
      "Operators under consideration (1):\n",
      "[73]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.1986897467936237)]\n",
      "Operator(s) added to ansatz: [73]\n",
      "Gradients: [np.float64(3.1986897467936237)]\n",
      "Initial energy: -22.743117347290898\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73]...\n",
      "Starting point: [np.float64(0.6115719425944712), np.float64(0.3123706148169066), np.float64(0.4901254326696982), np.float64(-0.4640569375111622), np.float64(0.5250736545030593), np.float64(0.49570949257374153), np.float64(-0.7010349962273561), np.float64(0.42595780058947463), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -23.313430\n",
      "         Iterations: 17\n",
      "         Function evaluations: 46\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "Current energy: -23.3134299322604\n",
      "(change of -0.5703125849695034)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.758988142355851\n",
      "Operator 2: -2.249210843298016\n",
      "Operator 4: -2.681124119660912\n",
      "Operator 6: -1.2243499263927333\n",
      "Operator 7: 2.453296488013554\n",
      "Operator 9: 1.515099177008913\n",
      "Operator 11: 0.8723077522058332\n",
      "Operator 15: -1.6320971027816813\n",
      "Operator 17: -2.0579006331374767\n",
      "Operator 19: -2.8736915183396503\n",
      "Operator 22: 0.7903908814568703\n",
      "Operator 24: 0.3279711463144068\n",
      "Operator 26: 1.9194651887573948\n",
      "Operator 27: -0.5572697684446615\n",
      "Operator 28: 2.632343957942373\n",
      "Operator 29: -1.127613731334789\n",
      "Operator 30: 2.873691518339651\n",
      "Operator 31: -0.8577164007426306\n",
      "Operator 33: 2.11765913412803\n",
      "Operator 34: -0.7451852614766281\n",
      "Operator 35: 2.271807332180718\n",
      "Operator 36: -1.497979112027842\n",
      "Operator 37: 2.084248420340189\n",
      "Operator 38: -1.8105519542275266\n",
      "Operator 39: -0.790419013687063\n",
      "Operator 40: 2.9207853016804997\n",
      "Operator 41: -1.13415245592089\n",
      "Operator 42: 2.632343957942373\n",
      "Operator 43: -0.6449186105402251\n",
      "Operator 44: 2.186553919874199\n",
      "Operator 46: -1.0162393659098043\n",
      "Operator 47: 2.7087834742558234\n",
      "Operator 48: -1.6191375456268418\n",
      "Operator 49: 2.271807332180718\n",
      "Operator 50: -1.8730969357089164\n",
      "Operator 51: 2.084248420340189\n",
      "Operator 52: -0.3295290508176997\n",
      "Operator 53: -2.920785301680498\n",
      "Operator 54: 1.13415245592089\n",
      "Operator 55: -2.632343957942372\n",
      "Operator 56: 0.6449186105402243\n",
      "Operator 57: -2.1865539198742017\n",
      "Operator 58: -1.2740731329815675\n",
      "Operator 59: 0.22421550556656372\n",
      "Operator 60: -2.7087834742558234\n",
      "Operator 61: 1.619137545626844\n",
      "Operator 62: -2.271807332180718\n",
      "Operator 63: 1.873096935708919\n",
      "Operator 64: -2.0842484203401894\n",
      "Operator 65: 1.1572047411920479\n",
      "Operator 67: 2.7589881423558467\n",
      "Operator 69: 2.2492108432980134\n",
      "Operator 71: 2.6811241196609066\n",
      "Operator 72: -1.274073132981568\n",
      "Operator 74: -2.453296488013553\n",
      "Operator 76: -1.515099177008915\n",
      "Operator 78: -1.9194651887573921\n",
      "Operator 79: 0.5572697684446615\n",
      "Operator 80: -2.632343957942372\n",
      "Operator 81: 1.1276137313347891\n",
      "Operator 82: -2.873691518339654\n",
      "Operator 83: -0.05931053085228643\n",
      "Operator 84: -1.2243499263927333\n",
      "Operator 85: -2.1176591341280298\n",
      "Operator 86: 0.7451852614766292\n",
      "Operator 87: -2.2718073321807184\n",
      "Operator 88: 1.4979791120278425\n",
      "Operator 89: -2.08424842034019\n",
      "Operator 90: 1.8105519542275266\n",
      "Operator 92: -0.6299869574405733\n",
      "Operator 94: -1.3675837434345093\n",
      "Operator 96: -1.0342516123022345\n",
      "Operator 99: -0.897797765327413\n",
      "Operator 101: -1.6928033076459248\n",
      "Operator 103: -1.892926481174635\n",
      "Operator 105: 1.500432379473375\n",
      "Operator 107: -0.8723077522058297\n",
      "Operator 109: -0.7640558975387004\n",
      "Operator 110: 1.6490598645203645\n",
      "Operator 111: -1.6490598645203614\n",
      "Operator 112: 1.6586223130623396\n",
      "Operator 113: -1.6586223130623376\n",
      "Operator 114: 1.2471213056246457\n",
      "Operator 115: 1.5350008525234715\n",
      "Operator 116: -1.0145635640311663\n",
      "Operator 117: 1.76982063452032\n",
      "Operator 118: -1.7698206345203176\n",
      "Operator 119: 1.912965952173725\n",
      "Operator 120: -1.9129659521737237\n",
      "Operator 121: 1.9790487949079152\n",
      "Operator 122: 1.1626380349403673\n",
      "Operator 123: -1.6490598645203691\n",
      "Operator 124: 1.6490598645203618\n",
      "Operator 125: -1.658622313062342\n",
      "Operator 126: 1.6586223130623439\n",
      "Operator 127: -0.9489181291622204\n",
      "Operator 128: 1.4939872861976433\n",
      "Operator 129: 1.2977693017442733\n",
      "Operator 130: -1.769820634520321\n",
      "Operator 131: 1.769820634520316\n",
      "Operator 132: -1.912965952173725\n",
      "Operator 133: 1.9129659521737261\n",
      "Operator 134: -1.9790487949079152\n",
      "Operator 136: -0.9586298601924838\n",
      "Operator 138: -1.3675837434345093\n",
      "Operator 140: -0.7869483911489121\n",
      "Operator 143: -1.1484094445370145\n",
      "Operator 145: -1.6928033076459243\n",
      "Operator 147: -1.8929264811746354\n",
      "Operator 148: -0.8763251598158802\n",
      "Operator 149: 0.9586298601924843\n",
      "Operator 151: 1.3675837434345097\n",
      "Operator 153: 0.7869483911489115\n",
      "Operator 154: -0.4521916716087153\n",
      "Operator 155: -0.809237872995447\n",
      "Operator 156: 1.1484094445370143\n",
      "Operator 158: 1.6928033076459255\n",
      "Operator 160: 1.8929264811746336\n",
      "Operator 161: 0.6547676625586556\n",
      "Operator 162: -1.500432379473375\n",
      "Operator 163: 1.63209710278168\n",
      "Operator 165: 2.057900633137476\n",
      "Operator 167: 2.873691518339654\n",
      "Operator 168: -0.5934141526963608\n",
      "Operator 169: 1.2243499263927333\n",
      "Operator 170: -0.7903908814568703\n",
      "Operator 172: -0.32797114631440694\n",
      "Operator 174: 1.8131362453787696\n",
      "Operator 175: -0.36622339911837376\n",
      "Operator 176: 1.8301282836022559\n",
      "Operator 177: -1.1276137313347891\n",
      "Operator 178: 1.5170291914390381\n",
      "Operator 179: 0.05931053085228643\n",
      "Operator 181: 1.917925801726693\n",
      "Operator 182: -0.5825671895082314\n",
      "Operator 183: 1.1528189519803094\n",
      "Operator 184: -1.4979791120278425\n",
      "Operator 185: 0.4062873254805382\n",
      "Operator 186: -1.8105519542275248\n",
      "Operator 187: -1.8131362453787718\n",
      "Operator 188: 0.36622339911837376\n",
      "Operator 189: -1.8301282836022577\n",
      "Operator 190: 1.127613731334789\n",
      "Operator 191: -1.5170291914390406\n",
      "Operator 192: 0.8577164007426306\n",
      "Operator 193: -0.6841509256112823\n",
      "Operator 194: -1.9179258017266934\n",
      "Operator 195: 0.5825671895082314\n",
      "Operator 196: -1.1528189519803076\n",
      "Operator 197: 1.497979112027842\n",
      "Operator 198: -0.40628732548053964\n",
      "Operator 199: 1.8105519542275248\n",
      "Operator 200: -1.5004323794733736\n",
      "Total gradient norm: 20.65921412279981\n",
      "Operators under consideration (1):\n",
      "[53]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.920785301680498)]\n",
      "Operator(s) added to ansatz: [53]\n",
      "Gradients: [np.float64(-2.920785301680498)]\n",
      "Initial energy: -23.3134299322604\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53]...\n",
      "Starting point: [np.float64(0.6373502649107075), np.float64(0.3101744070530129), np.float64(0.5431507125075874), np.float64(-0.33664452540758794), np.float64(0.4889369033203926), np.float64(0.4846854606611143), np.float64(-0.7129621795253697), np.float64(0.4268662823195797), np.float64(-0.35305883229796736), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -23.707378\n",
      "         Iterations: 17\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 68\n",
      "\n",
      "Current energy: -23.707378193222542\n",
      "(change of -0.39394826096214075)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.2392320703274882\n",
      "Operator 2: -2.673319127336862\n",
      "Operator 4: -2.7883578189125773\n",
      "Operator 6: -1.2046817427455698\n",
      "Operator 7: 2.4450034654890143\n",
      "Operator 9: 1.5112226850134796\n",
      "Operator 11: 0.8702527926789582\n",
      "Operator 14: 0.7854650834227612\n",
      "Operator 15: -1.0494671633022847\n",
      "Operator 16: 0.9592589926575292\n",
      "Operator 17: -2.2764980322373445\n",
      "Operator 19: -2.954978579564979\n",
      "Operator 22: 0.7871796316745745\n",
      "Operator 24: 0.3270119769040938\n",
      "Operator 26: 0.48869939673175466\n",
      "Operator 27: 0.6504093872218278\n",
      "Operator 28: 2.4506842625915866\n",
      "Operator 29: -0.7402890461848434\n",
      "Operator 30: 2.954978579564979\n",
      "Operator 31: -0.7787561963067788\n",
      "Operator 33: 2.13356192455015\n",
      "Operator 34: -0.7535390855355664\n",
      "Operator 35: 2.270437113288503\n",
      "Operator 36: -1.500527404772785\n",
      "Operator 37: 2.083853347046545\n",
      "Operator 38: -1.8114398315000448\n",
      "Operator 39: 0.23411630391657937\n",
      "Operator 40: 1.7340948517987742\n",
      "Operator 41: -0.15352721295252328\n",
      "Operator 42: 2.885357630519005\n",
      "Operator 43: -0.6010471480928938\n",
      "Operator 44: 2.2398335212276272\n",
      "Operator 46: -1.0230541107792126\n",
      "Operator 47: 2.7041926204238242\n",
      "Operator 48: -1.6210500998062165\n",
      "Operator 49: 2.270437113288503\n",
      "Operator 50: -1.8736949372567158\n",
      "Operator 51: 2.083853347046545\n",
      "Operator 52: -0.8586802830177678\n",
      "Operator 54: 0.8642669474172371\n",
      "Operator 55: -2.8853576305190023\n",
      "Operator 56: 0.601047148092894\n",
      "Operator 57: -2.2398335212276295\n",
      "Operator 58: -1.2789045358616806\n",
      "Operator 59: 0.24757910789311655\n",
      "Operator 60: -2.7041926204238242\n",
      "Operator 61: 1.621050099806216\n",
      "Operator 62: -2.2704371132885024\n",
      "Operator 63: 1.873694937256717\n",
      "Operator 64: -2.0838533470465457\n",
      "Operator 65: 0.8527433960235675\n",
      "Operator 67: 0.6413865983965694\n",
      "Operator 68: -0.9592589926575292\n",
      "Operator 69: 2.6733191273368595\n",
      "Operator 71: 2.7883578189125724\n",
      "Operator 72: -1.2789045358616806\n",
      "Operator 74: -2.4450034654890143\n",
      "Operator 76: -1.5112226850134811\n",
      "Operator 78: 0.7265415557269231\n",
      "Operator 79: 0.3870627086144855\n",
      "Operator 80: -2.4506842625915857\n",
      "Operator 81: 0.7402890461848435\n",
      "Operator 82: -2.954978579564981\n",
      "Operator 83: -0.18370124104164465\n",
      "Operator 84: -1.2046817427455698\n",
      "Operator 85: -2.1335619245501496\n",
      "Operator 86: 0.7535390855355668\n",
      "Operator 87: -2.2704371132885024\n",
      "Operator 88: 1.5005274047727843\n",
      "Operator 89: -2.0838533470465457\n",
      "Operator 90: 1.8114398315000448\n",
      "Operator 91: -0.4547677655104802\n",
      "Operator 92: 0.8606983397601626\n",
      "Operator 93: 0.7198296387743435\n",
      "Operator 94: -1.0856580950783388\n",
      "Operator 96: -0.9782428529751568\n",
      "Operator 99: -0.9114770900322979\n",
      "Operator 101: -1.6944293085574493\n",
      "Operator 103: -1.89343731051006\n",
      "Operator 105: 1.8326600668995359\n",
      "Operator 106: 0.7854650834227617\n",
      "Operator 107: -0.8702527926789544\n",
      "Operator 109: -0.22441866480338102\n",
      "Operator 110: -1.3293432294962744\n",
      "Operator 111: -1.3637609273762226\n",
      "Operator 112: 1.5921531014487522\n",
      "Operator 113: -1.5921531014487518\n",
      "Operator 114: 1.2288301320834296\n",
      "Operator 115: 1.546956497985141\n",
      "Operator 116: -1.0292604390287599\n",
      "Operator 117: 1.771130134744899\n",
      "Operator 118: -1.7711301347448964\n",
      "Operator 119: 1.9133877012391411\n",
      "Operator 120: -1.9133877012391396\n",
      "Operator 121: 1.9791465255931582\n",
      "Operator 122: -1.3427949717078844\n",
      "Operator 123: -1.7799193318989128\n",
      "Operator 124: 1.1583130657037648\n",
      "Operator 125: -1.5921531014487558\n",
      "Operator 126: 1.5921531014487582\n",
      "Operator 127: -0.9314365054179984\n",
      "Operator 128: 1.4861815824842428\n",
      "Operator 129: 1.3045407549174413\n",
      "Operator 130: -1.7711301347448998\n",
      "Operator 131: 1.7711301347448951\n",
      "Operator 132: -1.9133877012391405\n",
      "Operator 133: 1.9133877012391416\n",
      "Operator 134: -1.979146525593157\n",
      "Operator 135: -0.6217230093020154\n",
      "Operator 136: 1.1590653676836578\n",
      "Operator 137: 0.3930972774027464\n",
      "Operator 138: -0.9221058700044297\n",
      "Operator 140: -0.7414947604519264\n",
      "Operator 143: -1.1552557215185657\n",
      "Operator 145: -1.6944293085574493\n",
      "Operator 147: -1.89343731051006\n",
      "Operator 148: 0.903225368432777\n",
      "Operator 149: 0.49924121698820373\n",
      "Operator 151: 0.9221058700044302\n",
      "Operator 153: 0.7414947604519257\n",
      "Operator 154: -0.4850823953428679\n",
      "Operator 155: -0.8015294940867052\n",
      "Operator 156: 1.1552557215185655\n",
      "Operator 158: 1.69442930855745\n",
      "Operator 160: 1.8934373105100586\n",
      "Operator 161: 0.6237507748005481\n",
      "Operator 162: -1.6413582372608002\n",
      "Operator 165: 2.2764980322373436\n",
      "Operator 167: 2.9549785795649806\n",
      "Operator 168: -0.5940707895570954\n",
      "Operator 169: 1.2046817427455698\n",
      "Operator 170: -0.7871796316745744\n",
      "Operator 172: -0.3270119769040939\n",
      "Operator 174: -0.29747161086256835\n",
      "Operator 175: 0.36316000991003095\n",
      "Operator 176: 2.0488625443133195\n",
      "Operator 177: -0.6287659789777251\n",
      "Operator 178: 2.0395823152405845\n",
      "Operator 179: 0.18370124104164498\n",
      "Operator 181: 1.9290660953519434\n",
      "Operator 182: -0.5945295055597946\n",
      "Operator 183: 1.1454862148040101\n",
      "Operator 184: -1.5005274047727843\n",
      "Operator 185: 0.40424578157643654\n",
      "Operator 186: -1.8114398315000448\n",
      "Operator 187: -0.16049399084989896\n",
      "Operator 188: 0.2161188012624644\n",
      "Operator 189: -2.048862544313322\n",
      "Operator 190: 0.628765978977725\n",
      "Operator 191: -2.0395823152405876\n",
      "Operator 192: 0.7787561963067793\n",
      "Operator 193: -0.7290684992979557\n",
      "Operator 194: -1.9290660953519436\n",
      "Operator 195: 0.5945295055597961\n",
      "Operator 196: -1.1454862148040093\n",
      "Operator 197: 1.500527404772785\n",
      "Operator 198: -0.4042457815764381\n",
      "Operator 199: 1.8114398315000448\n",
      "Operator 200: -1.6413582372608002\n",
      "Total gradient norm: 19.801294751848765\n",
      "Operators under consideration (1):\n",
      "[167]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.9549785795649806)]\n",
      "Operator(s) added to ansatz: [167]\n",
      "Gradients: [np.float64(2.9549785795649806)]\n",
      "Initial energy: -23.707378193222542\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167]...\n",
      "Starting point: [np.float64(0.637712042686977), np.float64(0.2228043132005531), np.float64(0.5438545352856957), np.float64(-0.3308221875648782), np.float64(0.4603921753518454), np.float64(0.37516519376056295), np.float64(-0.7131316177894776), np.float64(0.4891967387285678), np.float64(-0.3552879328168382), np.float64(0.2780199656443427), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.089052\n",
      "         Iterations: 15\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 52\n",
      "\n",
      "Current energy: -24.089051963259337\n",
      "(change of -0.3816737700367945)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.182889522901137\n",
      "Operator 1: -1.5062131836406887e-07\n",
      "Operator 2: -3.006449620730936\n",
      "Operator 3: 0.8892921668176529\n",
      "Operator 4: -0.717703968964793\n",
      "Operator 5: 2.85679237502192e-07\n",
      "Operator 6: -0.9246307131197642\n",
      "Operator 7: 2.238892034976169\n",
      "Operator 8: -6.021071367950559e-08\n",
      "Operator 9: 1.4118892241795586\n",
      "Operator 10: 3.411711657630434e-08\n",
      "Operator 11: 0.8171714208152326\n",
      "Operator 12: -1.3282502209222002e-08\n",
      "Operator 14: 0.8214310789826439\n",
      "Operator 15: -1.2542579773322449\n",
      "Operator 16: 0.8479429430058142\n",
      "Operator 17: -2.7487216613350847\n",
      "Operator 18: 0.6107036645687679\n",
      "Operator 19: -1.7439136077284216\n",
      "Operator 20: 0.32752438266781325\n",
      "Operator 21: -5.5782752180998994e-08\n",
      "Operator 22: 0.708842051170573\n",
      "Operator 23: -6.021071136330023e-08\n",
      "Operator 24: 0.3027207453314965\n",
      "Operator 25: 3.411711657630434e-08\n",
      "Operator 26: 0.4114532253707662\n",
      "Operator 27: 0.6967162961196727\n",
      "Operator 28: 2.632197608083904\n",
      "Operator 29: 0.18501454160173036\n",
      "Operator 30: 1.7439136077284227\n",
      "Operator 31: 0.19590165626196382\n",
      "Operator 32: -0.3697460386269428\n",
      "Operator 33: 2.2784767832257473\n",
      "Operator 34: -0.9527404771534075\n",
      "Operator 35: 2.236460798313668\n",
      "Operator 36: -1.5636615804474123\n",
      "Operator 37: 2.0739668520346033\n",
      "Operator 38: -1.8336600665083003\n",
      "Operator 39: 0.32244536208280794\n",
      "Operator 40: 1.6841435269280607\n",
      "Operator 41: 0.2431156244147291\n",
      "Operator 42: 2.702612427489834\n",
      "Operator 43: 0.5174797442837387\n",
      "Operator 44: 0.8490044892363257\n",
      "Operator 45: 0.7569038725529191\n",
      "Operator 46: -1.1816063227356328\n",
      "Operator 47: 2.5943181100448505\n",
      "Operator 48: -1.6684551006157795\n",
      "Operator 49: 2.2364607983136686\n",
      "Operator 50: -1.8886494730055543\n",
      "Operator 51: 2.0739668520346033\n",
      "Operator 52: -0.9469850425680941\n",
      "Operator 53: -4.9393192912553784e-08\n",
      "Operator 54: 0.5804723133322108\n",
      "Operator 55: -2.702612427489832\n",
      "Operator 56: 0.45911316094123217\n",
      "Operator 57: 0.5329541490195722\n",
      "Operator 58: -1.4055625915303196\n",
      "Operator 59: 0.6376510100079078\n",
      "Operator 60: -2.59431811004485\n",
      "Operator 61: 1.6684551006157773\n",
      "Operator 62: -2.236460798313668\n",
      "Operator 63: 1.8886494730055556\n",
      "Operator 64: -2.073966852034604\n",
      "Operator 65: 0.8320737138983365\n",
      "Operator 67: 0.6388902387986954\n",
      "Operator 68: -0.981956490925328\n",
      "Operator 69: 3.0064496207309355\n",
      "Operator 70: -5.96672436551769e-07\n",
      "Operator 71: 1.1767967862741844\n",
      "Operator 72: -1.733525427861084\n",
      "Operator 73: 5.5782752180998994e-08\n",
      "Operator 74: -2.2388920349761685\n",
      "Operator 75: 6.021071273052575e-08\n",
      "Operator 76: -1.4118892241795604\n",
      "Operator 77: -3.411711657630434e-08\n",
      "Operator 78: 0.7525704726253517\n",
      "Operator 79: 0.3556108212022984\n",
      "Operator 80: -2.6321976080839034\n",
      "Operator 81: 0.5668574183894666\n",
      "Operator 82: -3.101524041718485e-07\n",
      "Operator 83: -0.6067798568970586\n",
      "Operator 84: -0.4737088514153716\n",
      "Operator 85: -2.2784767832257473\n",
      "Operator 86: 0.9527404771534075\n",
      "Operator 87: -2.236460798313668\n",
      "Operator 88: 1.5636615804474128\n",
      "Operator 89: -2.073966852034604\n",
      "Operator 90: 1.8336600665083003\n",
      "Operator 91: -0.4938931154879796\n",
      "Operator 92: 0.8461918689626906\n",
      "Operator 93: 0.7051303686723582\n",
      "Operator 94: -0.7511089194070785\n",
      "Operator 95: 0.44837752901490924\n",
      "Operator 96: 1.147179556620038\n",
      "Operator 97: -0.5536676869888781\n",
      "Operator 99: -1.151102142862337\n",
      "Operator 101: -1.734452570997389\n",
      "Operator 103: -1.9061923754808023\n",
      "Operator 105: 1.8402700771575287\n",
      "Operator 106: 0.821431078982644\n",
      "Operator 107: -0.817171420815229\n",
      "Operator 108: 1.3282502209222002e-08\n",
      "Operator 109: -0.16161189915498664\n",
      "Operator 110: -1.3767096835442245\n",
      "Operator 111: -1.3033937752974427\n",
      "Operator 112: 1.1525433581814866\n",
      "Operator 113: -1.7964608950376744\n",
      "Operator 114: -1.318154141524102\n",
      "Operator 115: 1.6982020060815852\n",
      "Operator 116: -1.276831326537296\n",
      "Operator 117: 1.803060621928935\n",
      "Operator 118: -1.8030606219289322\n",
      "Operator 119: 1.9238982315989577\n",
      "Operator 120: -1.9238982315989552\n",
      "Operator 121: 1.981593770577527\n",
      "Operator 122: -1.377412035133339\n",
      "Operator 123: -1.7404334134945578\n",
      "Operator 124: 1.096187254377693\n",
      "Operator 125: -1.3346976243794604\n",
      "Operator 126: -1.3132765807918039\n",
      "Operator 127: -0.5749505003360409\n",
      "Operator 128: 0.5964406103117628\n",
      "Operator 129: 1.4538250546571847\n",
      "Operator 130: -1.8030606219289353\n",
      "Operator 131: 1.803060621928931\n",
      "Operator 132: -1.9238982315989577\n",
      "Operator 133: 1.9238982315989577\n",
      "Operator 134: -1.9815937705775264\n",
      "Operator 135: -0.6511621067862583\n",
      "Operator 136: 1.1595701236146754\n",
      "Operator 137: 0.4763911890973268\n",
      "Operator 138: -0.7315392621464718\n",
      "Operator 139: 0.6730984742486873\n",
      "Operator 140: 0.9578787643422335\n",
      "Operator 141: -0.41234140801739083\n",
      "Operator 143: -1.3106673536130196\n",
      "Operator 145: -1.7344525709973886\n",
      "Operator 147: -1.9061923754808028\n",
      "Operator 148: 0.9392434211741436\n",
      "Operator 149: 0.46124706999550624\n",
      "Operator 151: 0.7315392621464727\n",
      "Operator 153: 0.3784806208926518\n",
      "Operator 154: 0.36004471783841085\n",
      "Operator 155: -0.600325581172769\n",
      "Operator 156: 1.3106673536130198\n",
      "Operator 158: 1.7344525709973893\n",
      "Operator 160: 1.9061923754808006\n",
      "Operator 161: 0.6311108625487021\n",
      "Operator 162: -1.6492049931472619\n",
      "Operator 163: 3.678534980909533e-08\n",
      "Operator 164: 1.3006511510146884e-07\n",
      "Operator 165: 2.7487216613350847\n",
      "Operator 166: -4.0975289784463246e-07\n",
      "Operator 167: 3.1015240437192734e-07\n",
      "Operator 168: -0.6082093602444261\n",
      "Operator 169: 0.9246307131197637\n",
      "Operator 170: -0.7088420511705734\n",
      "Operator 171: 6.021071296926611e-08\n",
      "Operator 172: -0.3027207453314964\n",
      "Operator 173: -3.411711657630434e-08\n",
      "Operator 174: -0.33018103279636335\n",
      "Operator 175: 0.38443205879185927\n",
      "Operator 176: 2.2937684031573267\n",
      "Operator 177: -0.4767414797160343\n",
      "Operator 178: 0.8925762494652966\n",
      "Operator 179: 1.0384563232006867\n",
      "Operator 180: -0.27536662470849294\n",
      "Operator 181: 1.6979657138251851\n",
      "Operator 182: -0.8367505315668218\n",
      "Operator 183: 0.9695867740038424\n",
      "Operator 184: -1.5636615804474117\n",
      "Operator 185: 0.3536031124184478\n",
      "Operator 186: -1.833660066508302\n",
      "Operator 187: -0.0818819812646565\n",
      "Operator 188: 0.196217887947863\n",
      "Operator 189: -2.2937684031573267\n",
      "Operator 190: -0.15560192646467771\n",
      "Operator 191: -0.5443637544874257\n",
      "Operator 192: 0.6407264752016226\n",
      "Operator 193: -0.352792440977038\n",
      "Operator 194: -1.6979657138251858\n",
      "Operator 195: 0.8367505315668218\n",
      "Operator 196: -0.9695867740038406\n",
      "Operator 197: 1.5636615804474117\n",
      "Operator 198: -0.3536031124184495\n",
      "Operator 199: 1.833660066508302\n",
      "Operator 200: -1.6492049931472619\n",
      "Total gradient norm: 18.459289545588863\n",
      "Operators under consideration (1):\n",
      "[69]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.0064496207309355)]\n",
      "Operator(s) added to ansatz: [69]\n",
      "Gradients: [np.float64(3.0064496207309355)]\n",
      "Initial energy: -24.089051963259337\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69]...\n",
      "Starting point: [np.float64(0.647024259144988), np.float64(0.21714916627121705), np.float64(0.5616465570322644), np.float64(-0.2492962169801518), np.float64(0.36532161274767844), np.float64(0.35490975762282145), np.float64(-0.7175112512654367), np.float64(0.4931514084536058), np.float64(-0.40690764090705367), np.float64(0.28581025860732695), np.float64(-0.26429011162809435), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.555190\n",
      "         Iterations: 19\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "Current energy: -24.555189588750324\n",
      "(change of -0.4661376254909868)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8274166680701547\n",
      "Operator 1: -4.16828195780931e-08\n",
      "Operator 2: -1.3939331522517187\n",
      "Operator 3: 0.9841554310547367\n",
      "Operator 4: -0.6896071169740594\n",
      "Operator 5: 1.3210960798896874e-08\n",
      "Operator 6: -0.7787531760631784\n",
      "Operator 7: 2.0995524190053687\n",
      "Operator 9: 1.341450080050063\n",
      "Operator 11: 0.7790489379387333\n",
      "Operator 14: 0.8950045910270006\n",
      "Operator 15: -1.1795043508604772\n",
      "Operator 16: 0.9272352395099677\n",
      "Operator 17: -0.6923882339761438\n",
      "Operator 18: 1.135864058601589\n",
      "Operator 19: -1.3475199157663336\n",
      "Operator 20: 0.43987716046250325\n",
      "Operator 22: 0.657471956437996\n",
      "Operator 24: 0.28582170709922095\n",
      "Operator 26: -0.16350383503004284\n",
      "Operator 27: 1.0337659212918469\n",
      "Operator 28: 0.1146505046210883\n",
      "Operator 29: 1.457159827706103\n",
      "Operator 30: 0.7142384552123092\n",
      "Operator 31: 0.79782557916159\n",
      "Operator 32: -0.6089535770201959\n",
      "Operator 33: 2.3086430550305836\n",
      "Operator 34: -1.0780815045640184\n",
      "Operator 35: 2.2137001452848204\n",
      "Operator 36: -1.6058962442159388\n",
      "Operator 37: 2.0672455149711437\n",
      "Operator 38: -1.8487682457020114\n",
      "Operator 39: 0.8859964418291782\n",
      "Operator 40: 0.6796662701947935\n",
      "Operator 41: 0.8909233185219473\n",
      "Operator 42: 0.17109110578107306\n",
      "Operator 43: 1.5268204458134997\n",
      "Operator 44: 0.26417780338783897\n",
      "Operator 45: 1.0720462057237388\n",
      "Operator 46: -1.2787710437739652\n",
      "Operator 47: 2.5247612753174753\n",
      "Operator 48: -1.7002037646293298\n",
      "Operator 49: 2.21370014528482\n",
      "Operator 50: -1.8988055048433048\n",
      "Operator 51: 2.0672455149711437\n",
      "Operator 52: -1.4074823428327683\n",
      "Operator 53: 0.47339881194948297\n",
      "Operator 54: -0.780462728989157\n",
      "Operator 55: -0.012057256908122708\n",
      "Operator 56: 0.2552741092727497\n",
      "Operator 57: 0.7439560726140997\n",
      "Operator 58: -1.4790222306352994\n",
      "Operator 59: 0.8497744333694216\n",
      "Operator 60: -2.5247612753174744\n",
      "Operator 61: 1.7002037646293298\n",
      "Operator 62: -2.2137001452848195\n",
      "Operator 63: 1.8988055048433048\n",
      "Operator 64: -2.0672455149711446\n",
      "Operator 65: 0.6903883649639153\n",
      "Operator 67: 0.5948516885430573\n",
      "Operator 68: -1.766976327677586\n",
      "Operator 69: -1.3013938810471607e-08\n",
      "Operator 70: -0.9502812197209195\n",
      "Operator 71: 0.7476003441319763\n",
      "Operator 72: -1.8349616637875141\n",
      "Operator 74: -2.0995524190053683\n",
      "Operator 76: -1.3414500800500646\n",
      "Operator 78: 0.9931556998994078\n",
      "Operator 79: -0.862437170222219\n",
      "Operator 80: -0.155780547828754\n",
      "Operator 81: 0.19885872046822534\n",
      "Operator 82: 0.6695737796546126\n",
      "Operator 83: -1.1063074601097058\n",
      "Operator 84: -0.04838482519032354\n",
      "Operator 85: -2.3086430550305828\n",
      "Operator 86: 1.0780815045640189\n",
      "Operator 87: -2.2137001452848195\n",
      "Operator 88: 1.6058962442159388\n",
      "Operator 89: -2.0672455149711446\n",
      "Operator 90: 1.8487682457020114\n",
      "Operator 91: -0.8145802176965683\n",
      "Operator 92: 0.6983347268578497\n",
      "Operator 93: -0.4588228702277699\n",
      "Operator 94: -0.35572457489212306\n",
      "Operator 95: 1.1117019618116686\n",
      "Operator 96: 1.0591266790808886\n",
      "Operator 97: -0.8025761138349724\n",
      "Operator 99: -1.2822162150110499\n",
      "Operator 101: -1.7609599700071858\n",
      "Operator 103: -1.9148333534448498\n",
      "Operator 105: 1.8460513305937205\n",
      "Operator 106: 1.1186937023362227\n",
      "Operator 107: -0.7790489379387295\n",
      "Operator 109: 0.3203568131166369\n",
      "Operator 110: -1.6146065823656088\n",
      "Operator 111: 1.0597813345515363\n",
      "Operator 112: -1.2827118374397468\n",
      "Operator 113: -1.366607571047293\n",
      "Operator 114: -1.5577088156230254\n",
      "Operator 115: 1.759075055355007\n",
      "Operator 116: -1.406025723022458\n",
      "Operator 117: 1.8238872788966463\n",
      "Operator 118: -1.8238872788966427\n",
      "Operator 119: 1.9309964934592747\n",
      "Operator 120: -1.930996493459273\n",
      "Operator 121: 1.9832592756442993\n",
      "Operator 122: -1.593708461105781\n",
      "Operator 123: -0.5079185714811908\n",
      "Operator 124: 0.3676116561930526\n",
      "Operator 125: -1.2672368950192245\n",
      "Operator 126: -1.7685144141721145\n",
      "Operator 127: -0.11883603929607245\n",
      "Operator 128: 0.1318113602852481\n",
      "Operator 129: 1.5376475327582981\n",
      "Operator 130: -1.8238872788966467\n",
      "Operator 131: 1.8238872788966414\n",
      "Operator 132: -1.9309964934592747\n",
      "Operator 133: 1.9309964934592754\n",
      "Operator 134: -1.9832592756442982\n",
      "Operator 135: -0.9153114005827929\n",
      "Operator 136: 0.948419547031119\n",
      "Operator 137: -0.7316321120488818\n",
      "Operator 138: -0.0026407030889173183\n",
      "Operator 139: 1.1142649134470541\n",
      "Operator 140: 0.8303536988688551\n",
      "Operator 141: -0.704930629060551\n",
      "Operator 143: -1.4022478872123254\n",
      "Operator 145: -1.7609599700071858\n",
      "Operator 147: -1.9148333534448498\n",
      "Operator 148: 1.1871009617123827\n",
      "Operator 149: 0.1922370434319631\n",
      "Operator 150: 0.5198855975821051\n",
      "Operator 151: 0.2600013005408176\n",
      "Operator 152: 0.6632271097772529\n",
      "Operator 153: 0.15388180476447239\n",
      "Operator 154: 0.7002970364924292\n",
      "Operator 155: -0.48374517088050534\n",
      "Operator 156: 1.4022478872123254\n",
      "Operator 158: 1.7609599700071863\n",
      "Operator 160: 1.9148333534448478\n",
      "Operator 161: 0.6144102479980154\n",
      "Operator 162: -1.372566012850369\n",
      "Operator 163: -1.269769888523831e-08\n",
      "Operator 164: -0.8381124317481725\n",
      "Operator 165: 0.7496265508606211\n",
      "Operator 166: 2.489531193074394e-08\n",
      "Operator 168: -0.6068657261219079\n",
      "Operator 169: 0.7787531760631784\n",
      "Operator 170: -0.6574719564379959\n",
      "Operator 172: -0.2858217070992207\n",
      "Operator 174: -0.6360811030866025\n",
      "Operator 175: 0.7859509861823388\n",
      "Operator 176: -0.4861575251612159\n",
      "Operator 177: 0.5172495469557736\n",
      "Operator 178: 0.17818233907835382\n",
      "Operator 179: 1.2158776836468568\n",
      "Operator 180: -0.5348651931171051\n",
      "Operator 181: 1.4920078392333367\n",
      "Operator 182: -0.9857983020417139\n",
      "Operator 183: 0.8578046380476859\n",
      "Operator 184: -1.6058962442159381\n",
      "Operator 185: 0.3196558439852607\n",
      "Operator 186: -1.8487682457020114\n",
      "Operator 187: 0.47439462037410374\n",
      "Operator 188: 0.0914925560839137\n",
      "Operator 189: -0.603796838313809\n",
      "Operator 190: -0.543389009904925\n",
      "Operator 191: 0.05387956667993121\n",
      "Operator 192: 0.24117834587137088\n",
      "Operator 193: -0.04249808170270765\n",
      "Operator 194: -1.4920078392333371\n",
      "Operator 195: 0.9857983020417139\n",
      "Operator 196: -0.8578046380476846\n",
      "Operator 197: 1.6058962442159381\n",
      "Operator 198: -0.3196558439852622\n",
      "Operator 199: 1.8487682457020114\n",
      "Operator 200: -1.7156123778700518\n",
      "Total gradient norm: 16.723091073642546\n",
      "Operators under consideration (1):\n",
      "[60]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.5247612753174744)]\n",
      "Operator(s) added to ansatz: [60]\n",
      "Gradients: [np.float64(-2.5247612753174744)]\n",
      "Initial energy: -24.555189588750324\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60]...\n",
      "Starting point: [np.float64(0.6536749796825527), np.float64(0.1786177765837806), np.float64(0.573998170975791), np.float64(-0.20838587514010556), np.float64(0.24921422100450163), np.float64(0.23678179540612876), np.float64(-0.7206599773198317), np.float64(0.5265560114250954), np.float64(-0.4384998485973535), np.float64(0.35883289360947346), np.float64(-0.3403316431338856), np.float64(-0.32171358299824265), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -24.810026\n",
      "         Iterations: 20\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n",
      "\n",
      "Current energy: -24.810025749261854\n",
      "(change of -0.25483616051153035)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8435738708822164\n",
      "Operator 2: -1.4456055990437515\n",
      "Operator 3: 0.8955727682321155\n",
      "Operator 4: -0.8017934961142357\n",
      "Operator 6: -1.2743985719270206\n",
      "Operator 7: 1.2824183741943873\n",
      "Operator 9: 1.9216924081678721\n",
      "Operator 11: 1.0804850185142345\n",
      "Operator 14: 0.8968431879261625\n",
      "Operator 15: -1.1757895439457866\n",
      "Operator 16: 0.9603075047050789\n",
      "Operator 17: -0.7229952026067925\n",
      "Operator 18: 1.2988437759603446\n",
      "Operator 19: -1.3423870916447656\n",
      "Operator 20: 0.5898942218438975\n",
      "Operator 21: -0.5299859391790374\n",
      "Operator 22: 0.7168046679360454\n",
      "Operator 23: -0.6424265258480142\n",
      "Operator 24: 0.43354753068513663\n",
      "Operator 26: -0.13375596666969541\n",
      "Operator 27: 1.0170022391272318\n",
      "Operator 28: 0.19595848672950655\n",
      "Operator 29: 1.36171652530943\n",
      "Operator 30: 0.8913367162359966\n",
      "Operator 31: 0.6576761501482953\n",
      "Operator 32: 0.029897825281107744\n",
      "Operator 33: 1.6607841164263542\n",
      "Operator 34: -0.12063858148409551\n",
      "Operator 35: 2.2291335069279596\n",
      "Operator 36: -1.1959270993502156\n",
      "Operator 37: 2.1290135899125016\n",
      "Operator 38: -1.709979476205068\n",
      "Operator 39: 0.8603148274650387\n",
      "Operator 40: 0.7402171402801866\n",
      "Operator 41: 0.8435044200878947\n",
      "Operator 42: 0.32615705089630725\n",
      "Operator 43: 1.3483354002069985\n",
      "Operator 44: 0.7498492305336395\n",
      "Operator 45: 0.7783816506083882\n",
      "Operator 46: -0.4412192536596538\n",
      "Operator 47: 1.9796179289299678\n",
      "Operator 48: -1.0422818134437362\n",
      "Operator 49: 2.4336084184367888\n",
      "Operator 50: -1.8051167563988115\n",
      "Operator 51: 2.1290135899125016\n",
      "Operator 52: -1.389668788742696\n",
      "Operator 53: 0.44125384235962606\n",
      "Operator 54: -0.7085985555212444\n",
      "Operator 55: -0.15086549243722985\n",
      "Operator 56: 0.3569023454545975\n",
      "Operator 57: 0.4852201476413345\n",
      "Operator 58: -0.9835734016636748\n",
      "Operator 59: 0.49384047598594843\n",
      "Operator 61: 1.3918441310711813\n",
      "Operator 62: -2.4336084184367897\n",
      "Operator 63: 1.8051167563988115\n",
      "Operator 64: -2.129013589912502\n",
      "Operator 65: 0.6974186756632541\n",
      "Operator 67: 0.5981334832851548\n",
      "Operator 68: -1.7497041962674102\n",
      "Operator 70: -0.9515963486336654\n",
      "Operator 71: 0.7723021277489126\n",
      "Operator 72: -1.7288750012261833\n",
      "Operator 74: -0.8526107946702979\n",
      "Operator 75: 0.642426525848014\n",
      "Operator 76: -1.9216924081678735\n",
      "Operator 78: 0.9793871391360695\n",
      "Operator 79: -0.8170632977387056\n",
      "Operator 80: -0.19354576811992258\n",
      "Operator 81: 0.21729744824309932\n",
      "Operator 82: 0.5925898032560717\n",
      "Operator 83: -0.7497671115204798\n",
      "Operator 84: -0.33140450179502495\n",
      "Operator 85: 0.1802208466829936\n",
      "Operator 86: 0.9351638074472188\n",
      "Operator 87: -2.229133506927959\n",
      "Operator 88: 1.1959270993502156\n",
      "Operator 89: -2.129013589912502\n",
      "Operator 90: 1.709979476205068\n",
      "Operator 91: -0.7969604115164717\n",
      "Operator 92: 0.7076343829467264\n",
      "Operator 93: -0.4108301322504967\n",
      "Operator 94: -0.40206592633872706\n",
      "Operator 95: 1.1174300415743095\n",
      "Operator 96: 1.0837731228871115\n",
      "Operator 97: -0.46240274024001504\n",
      "Operator 98: -0.20679276120527923\n",
      "Operator 99: 0.9682407429733857\n",
      "Operator 100: 0.6427117982816843\n",
      "Operator 101: -1.4931671784084903\n",
      "Operator 103: -1.8344620439786754\n",
      "Operator 105: 1.8483770175539362\n",
      "Operator 106: 1.1028760377864113\n",
      "Operator 107: -1.0804850185142312\n",
      "Operator 109: 0.2947019725482938\n",
      "Operator 110: -1.6054229156240083\n",
      "Operator 111: 0.9860277795147717\n",
      "Operator 112: -1.203094560990131\n",
      "Operator 113: -1.4578926478328815\n",
      "Operator 114: -1.4898292474276364\n",
      "Operator 115: 1.2496518240708578\n",
      "Operator 116: -1.4479119252734414\n",
      "Operator 117: -0.9586977834508079\n",
      "Operator 118: -1.6018663102455586\n",
      "Operator 119: 1.8642843898497221\n",
      "Operator 120: -1.8642843898497201\n",
      "Operator 121: 1.9680066560301333\n",
      "Operator 122: -1.5840837619235666\n",
      "Operator 123: -0.5690537632907229\n",
      "Operator 124: 0.4151073877695345\n",
      "Operator 125: -1.3295863103164498\n",
      "Operator 126: -1.6741178527959735\n",
      "Operator 127: -0.5925316704000804\n",
      "Operator 128: 0.5194436336222279\n",
      "Operator 129: -1.0088939588539736\n",
      "Operator 130: -2.0553494947092505\n",
      "Operator 131: 1.4672754411661226\n",
      "Operator 132: -1.864284389849721\n",
      "Operator 133: 1.8642843898497226\n",
      "Operator 134: -1.968006656030133\n",
      "Operator 135: -0.9002833164818778\n",
      "Operator 136: 0.9649230731672229\n",
      "Operator 137: -0.7135489511836943\n",
      "Operator 138: -0.030043033315708664\n",
      "Operator 139: 1.114414244078428\n",
      "Operator 140: 0.9815255133353173\n",
      "Operator 141: -0.5746338513681435\n",
      "Operator 142: -0.0256272905060134\n",
      "Operator 143: 0.9937485434051535\n",
      "Operator 144: 0.17774888994601343\n",
      "Operator 145: -1.3677093502879405\n",
      "Operator 147: -1.8344620439786754\n",
      "Operator 148: 1.174756996187216\n",
      "Operator 149: 0.20277254044408072\n",
      "Operator 150: 0.4892927339010859\n",
      "Operator 151: 0.2866476956916752\n",
      "Operator 152: 0.586158220963201\n",
      "Operator 153: 0.24032431384722383\n",
      "Operator 154: 0.47049629753354294\n",
      "Operator 155: 0.21297342367572497\n",
      "Operator 156: 0.9470831987417316\n",
      "Operator 158: 1.3677093502879416\n",
      "Operator 160: 1.8344620439786736\n",
      "Operator 161: 0.6173779571167364\n",
      "Operator 162: -1.3920811432423834\n",
      "Operator 164: -0.8454805767003855\n",
      "Operator 165: 0.7319194365819445\n",
      "Operator 168: -0.71911806387768\n",
      "Operator 169: 0.8817956733644319\n",
      "Operator 172: -0.4335475306851371\n",
      "Operator 174: -0.6182744622362661\n",
      "Operator 175: 0.779771116577147\n",
      "Operator 176: -0.46198377700504323\n",
      "Operator 177: 0.505110713325654\n",
      "Operator 178: 0.2539544401837944\n",
      "Operator 179: 1.0951025505875593\n",
      "Operator 180: -0.37854817930938334\n",
      "Operator 181: 0.4984598846217878\n",
      "Operator 182: -0.10623135017382909\n",
      "Operator 183: 1.3444369310214555\n",
      "Operator 184: -1.095443765236946\n",
      "Operator 185: 0.6469444818151484\n",
      "Operator 186: -1.709979476205068\n",
      "Operator 187: 0.446742918206335\n",
      "Operator 188: 0.09549705892377108\n",
      "Operator 189: -0.6725294163714428\n",
      "Operator 190: -0.46751582656163804\n",
      "Operator 191: -0.11524599779485362\n",
      "Operator 192: 0.3075310112522958\n",
      "Operator 193: -0.5906101989539777\n",
      "Operator 194: -0.8303285746481996\n",
      "Operator 195: 0.8234821122454383\n",
      "Operator 196: -1.3444369310214543\n",
      "Operator 197: 1.095443765236946\n",
      "Operator 198: -0.6469444818151506\n",
      "Operator 199: 1.709979476205068\n",
      "Operator 200: -1.7118855962841284\n",
      "Total gradient norm: 15.279144053903\n",
      "Operators under consideration (1):\n",
      "[49]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.4336084184367888)]\n",
      "Operator(s) added to ansatz: [49]\n",
      "Gradients: [np.float64(2.4336084184367888)]\n",
      "Initial energy: -24.810025749261854\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49]...\n",
      "Starting point: [np.float64(0.6001423271869339), np.float64(0.18051951516776063), np.float64(0.4644260465374306), np.float64(-0.2468608123958134), np.float64(0.2630474307662309), np.float64(0.24190092609897873), np.float64(-0.6958450959577972), np.float64(0.5246549541031786), np.float64(-0.35081964997419296), np.float64(0.3546732202478366), np.float64(-0.3107849576137859), np.float64(-0.3105955145976005), np.float64(0.20642792875160249), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -25.042362\n",
      "         Iterations: 21\n",
      "         Function evaluations: 30\n",
      "         Gradient evaluations: 30\n",
      "\n",
      "Current energy: -25.04236222198152\n",
      "(change of -0.23233647271966618)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8492637201427876\n",
      "Operator 2: -1.4638439580434672\n",
      "Operator 3: 0.862918081834669\n",
      "Operator 4: -0.8444515489898394\n",
      "Operator 6: -1.4147598550247547\n",
      "Operator 7: 1.126076139645254\n",
      "Operator 9: 0.8561999174084942\n",
      "Operator 10: -0.5903027595118935\n",
      "Operator 11: 1.638011575783179\n",
      "Operator 14: 0.8972595662928098\n",
      "Operator 15: -1.1736720860893142\n",
      "Operator 16: 0.9706619173916309\n",
      "Operator 17: -0.7311625897024608\n",
      "Operator 18: 1.3487025497046856\n",
      "Operator 19: -1.3391010631817912\n",
      "Operator 20: 0.6984900778531897\n",
      "Operator 21: -0.6428567904946196\n",
      "Operator 22: 1.0222918243123371\n",
      "Operator 23: -0.759970066690409\n",
      "Operator 26: -0.12349398905660736\n",
      "Operator 27: 1.0112561010455041\n",
      "Operator 28: 0.22503817730272857\n",
      "Operator 29: 1.326388359834049\n",
      "Operator 30: 0.9541133003811548\n",
      "Operator 31: 0.605851586713962\n",
      "Operator 32: 0.23738217996754923\n",
      "Operator 33: 1.4661199286967839\n",
      "Operator 34: 0.18949810650622917\n",
      "Operator 35: -0.16174589233454184\n",
      "Operator 36: -1.0720389882410148\n",
      "Operator 37: 2.1210090356089384\n",
      "Operator 38: -1.3389577194716917\n",
      "Operator 39: 0.851384381748559\n",
      "Operator 40: 0.7607450088304949\n",
      "Operator 41: 0.8257229487835636\n",
      "Operator 42: 0.37927713473892655\n",
      "Operator 43: 1.2758710027706335\n",
      "Operator 44: 0.9222731794459711\n",
      "Operator 45: 0.6732764766188523\n",
      "Operator 46: 0.019662783334705433\n",
      "Operator 47: 1.6310126603403514\n",
      "Operator 48: -0.7422564230615514\n",
      "Operator 50: -1.5485702990485988\n",
      "Operator 51: 2.2943982427530614\n",
      "Operator 52: -1.3833778423622967\n",
      "Operator 53: 0.4304902020152861\n",
      "Operator 54: -0.6825564900208896\n",
      "Operator 55: -0.19641531703254206\n",
      "Operator 56: 0.4002217913036836\n",
      "Operator 57: 0.3955402081033673\n",
      "Operator 58: -0.789629623036674\n",
      "Operator 59: 0.027173132598190545\n",
      "Operator 60: 0.13394393384066217\n",
      "Operator 61: 0.6117957495624213\n",
      "Operator 62: -2.014150107590834\n",
      "Operator 63: 1.274318206952328\n",
      "Operator 64: -2.2943982427530614\n",
      "Operator 65: 0.6998377446347301\n",
      "Operator 67: 0.599237551975171\n",
      "Operator 68: -1.7435318191245237\n",
      "Operator 70: -0.9511914498559161\n",
      "Operator 71: 0.7794982928407098\n",
      "Operator 72: -1.6629935553474\n",
      "Operator 74: -0.8927917852166533\n",
      "Operator 75: 1.1272931384602434\n",
      "Operator 76: -1.2900448462449816\n",
      "Operator 78: 0.9746984189757169\n",
      "Operator 79: -0.8014965486865677\n",
      "Operator 80: -0.20644396011715774\n",
      "Operator 81: 0.22420839715963023\n",
      "Operator 82: 0.5669633601645095\n",
      "Operator 83: -0.6003065241255614\n",
      "Operator 84: -0.42695009239136805\n",
      "Operator 85: 0.23029792931401705\n",
      "Operator 86: 0.2581122416413098\n",
      "Operator 87: -1.7225245167633485\n",
      "Operator 88: 0.28590513035325765\n",
      "Operator 89: -2.1210090356089384\n",
      "Operator 90: 1.3389577194716917\n",
      "Operator 91: -0.7909223278435842\n",
      "Operator 92: 0.7108325033322496\n",
      "Operator 93: -0.3935891487932197\n",
      "Operator 94: -0.4190819743293567\n",
      "Operator 95: 1.1167356828399138\n",
      "Operator 96: 1.0892468137401417\n",
      "Operator 97: -0.3545789597654865\n",
      "Operator 98: -0.32032652105820675\n",
      "Operator 99: 1.0645830438992512\n",
      "Operator 100: -0.2096708232055517\n",
      "Operator 101: -1.0514058901408403\n",
      "Operator 103: -1.4851932903126157\n",
      "Operator 105: 1.8490958952568675\n",
      "Operator 106: 1.097456778384899\n",
      "Operator 107: -1.6380115757831764\n",
      "Operator 109: 0.2858969053171509\n",
      "Operator 110: -1.602209262836272\n",
      "Operator 111: 0.9588920824732718\n",
      "Operator 112: -1.1746502056836114\n",
      "Operator 113: -1.489233673361435\n",
      "Operator 114: -1.462448419852385\n",
      "Operator 115: 1.0594396907294594\n",
      "Operator 116: -1.2367629435909517\n",
      "Operator 117: -1.2063952582776216\n",
      "Operator 118: 0.9172696987676381\n",
      "Operator 119: 2.118087622962192\n",
      "Operator 120: -1.5408533790018515\n",
      "Operator 121: 1.9277542049778909\n",
      "Operator 122: -1.58074285442959\n",
      "Operator 123: -0.5898199423967105\n",
      "Operator 124: 0.4311495986522627\n",
      "Operator 125: -1.3505679833366981\n",
      "Operator 126: -1.6387030335232147\n",
      "Operator 127: -0.76884804100756\n",
      "Operator 128: 0.6524283792210545\n",
      "Operator 129: -1.2179052363415828\n",
      "Operator 130: -1.5584792691971603\n",
      "Operator 131: 1.5711278707504892\n",
      "Operator 132: 0.8554684194037208\n",
      "Operator 133: 1.6668157588055637\n",
      "Operator 134: -1.9277542049778917\n",
      "Operator 135: -0.8951624546553115\n",
      "Operator 136: 0.9704807082746074\n",
      "Operator 137: -0.7074450912827923\n",
      "Operator 138: -0.040141677881532695\n",
      "Operator 139: 1.1107045344217297\n",
      "Operator 140: 1.0295401612055683\n",
      "Operator 141: -0.5338535457171238\n",
      "Operator 142: -0.10116031532840779\n",
      "Operator 143: 1.0912736485529264\n",
      "Operator 144: -0.004427020667110591\n",
      "Operator 145: -0.9332859046234712\n",
      "Operator 147: -1.606605543980429\n",
      "Operator 148: 1.1705109599869987\n",
      "Operator 149: 0.20647617585373312\n",
      "Operator 150: 0.4789387737226887\n",
      "Operator 151: 0.29584983301225853\n",
      "Operator 152: 0.5600573935907798\n",
      "Operator 153: 0.2779109181787283\n",
      "Operator 154: 0.39109309886887816\n",
      "Operator 155: 0.30772701672759684\n",
      "Operator 156: 0.6620710513856463\n",
      "Operator 157: 0.043153117097122634\n",
      "Operator 158: -0.9116572969323395\n",
      "Operator 159: -0.6356458433916947\n",
      "Operator 160: 1.6066055439804274\n",
      "Operator 161: 0.6183428440510723\n",
      "Operator 162: -1.3985665629623905\n",
      "Operator 164: -0.846689462936414\n",
      "Operator 165: 0.7232733461875168\n",
      "Operator 168: -0.7911631124820224\n",
      "Operator 169: 0.8461988013270481\n",
      "Operator 171: 0.478512954775962\n",
      "Operator 172: -0.5364636209645972\n",
      "Operator 173: 0.5903027595118935\n",
      "Operator 174: -0.6122144451057946\n",
      "Operator 175: 0.7774607380297831\n",
      "Operator 176: -0.45382959998372085\n",
      "Operator 177: 0.4998239064921176\n",
      "Operator 178: 0.27815443083168917\n",
      "Operator 179: 1.0265850910980063\n",
      "Operator 180: -0.32535952148817787\n",
      "Operator 181: 0.4843496418277306\n",
      "Operator 182: 0.44836808266913\n",
      "Operator 183: 0.8514546572369366\n",
      "Operator 184: -0.9516009814231893\n",
      "Operator 185: 1.1069337755124464\n",
      "Operator 186: -1.2377717906069101\n",
      "Operator 187: 0.43716670436117766\n",
      "Operator 188: 0.09691025580496221\n",
      "Operator 189: -0.6964933974768939\n",
      "Operator 190: -0.43931243076175563\n",
      "Operator 191: -0.17619626684927162\n",
      "Operator 192: 0.33445026855778714\n",
      "Operator 193: -0.7525433223074687\n",
      "Operator 194: -0.6652609810600598\n",
      "Operator 195: 0.6033303201501778\n",
      "Operator 196: -0.5175954676342686\n",
      "Operator 197: 0.25378517537360373\n",
      "Operator 198: -1.1069337755124482\n",
      "Operator 199: 1.2377717906069101\n",
      "Operator 200: -1.7106157595924292\n",
      "Total gradient norm: 13.799190575946046\n",
      "Operators under consideration (1):\n",
      "[64]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.2943982427530614)]\n",
      "Operator(s) added to ansatz: [64]\n",
      "Gradients: [np.float64(-2.2943982427530614)]\n",
      "Initial energy: -25.04236222198152\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64]...\n",
      "Starting point: [np.float64(0.4926228279818273), np.float64(0.18117406646087747), np.float64(0.37548828269865936), np.float64(-0.26182670972208294), np.float64(0.26823572103480675), np.float64(0.24370190115742252), np.float64(-0.6505973949494447), np.float64(0.5240080003959134), np.float64(-0.3162839772723302), np.float64(0.35325670254209035), np.float64(-0.3007037269513508), np.float64(-0.30679966177912765), np.float64(0.23928408892841527), np.float64(-0.19562994240911222), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -25.237046\n",
      "         Iterations: 24\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n",
      "\n",
      "Current energy: -25.23704556869303\n",
      "(change of -0.19468334671151055)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8511038375700811\n",
      "Operator 2: -1.4697451003499367\n",
      "Operator 3: 0.8522867253036984\n",
      "Operator 4: -0.8588686855280867\n",
      "Operator 6: -1.4621473741477053\n",
      "Operator 7: 1.048186541971885\n",
      "Operator 9: 0.9074723907726248\n",
      "Operator 10: -0.9720649836193462\n",
      "Operator 11: 1.2988733451904615\n",
      "Operator 14: 0.8973674128198974\n",
      "Operator 15: -1.1728837996776766\n",
      "Operator 16: 0.973826653787154\n",
      "Operator 17: -0.7333874758347896\n",
      "Operator 18: 1.3628961153023917\n",
      "Operator 19: -1.3342037700965004\n",
      "Operator 20: 0.7323680249910285\n",
      "Operator 21: -0.6822698299376215\n",
      "Operator 22: 1.1984760156669716\n",
      "Operator 23: -0.7507373040401177\n",
      "Operator 25: -0.3540379253044345\n",
      "Operator 26: -0.12020006694757161\n",
      "Operator 27: 1.0094166118483063\n",
      "Operator 28: 0.23449604534457197\n",
      "Operator 29: 1.314836904253681\n",
      "Operator 30: 0.9746201458737372\n",
      "Operator 31: 0.5888748146271628\n",
      "Operator 32: 0.31034453556449315\n",
      "Operator 33: 1.3782047859350097\n",
      "Operator 34: 0.28986302694303506\n",
      "Operator 35: -0.1999055162068183\n",
      "Operator 36: -0.5250328316528293\n",
      "Operator 37: 1.8128828698550428\n",
      "Operator 38: -0.6107162211750108\n",
      "Operator 39: 0.8485101710457815\n",
      "Operator 40: 0.7672862409061483\n",
      "Operator 41: 0.8198316050629456\n",
      "Operator 42: 0.3961923581936999\n",
      "Operator 43: 1.25111643090589\n",
      "Operator 44: 0.9762350371811841\n",
      "Operator 45: 0.6330392389583481\n",
      "Operator 46: 0.18245615641736038\n",
      "Operator 47: 1.498354107122555\n",
      "Operator 48: -0.3352290966114969\n",
      "Operator 49: -0.09921156018699565\n",
      "Operator 50: -0.9985478158555349\n",
      "Operator 51: 2.047288415395462\n",
      "Operator 52: -1.381341865218824\n",
      "Operator 53: 0.42707729012037776\n",
      "Operator 54: -0.6740439438051927\n",
      "Operator 55: -0.21063504971562486\n",
      "Operator 56: 0.4153792548171015\n",
      "Operator 57: 0.3681181216508581\n",
      "Operator 58: -0.7201409582779539\n",
      "Operator 59: -0.13086854411997326\n",
      "Operator 60: 0.17829151073128427\n",
      "Operator 61: 0.2139693475147765\n",
      "Operator 62: -1.7657019133264475\n",
      "Operator 63: 1.1837923977132354\n",
      "Operator 65: 0.7006134423356959\n",
      "Operator 67: 0.5995888393478728\n",
      "Operator 68: -1.7415338117565766\n",
      "Operator 70: -0.9509672159952375\n",
      "Operator 71: 0.7816210719924526\n",
      "Operator 72: -1.6393676052698967\n",
      "Operator 74: -0.8981837040512235\n",
      "Operator 75: 1.260584964902113\n",
      "Operator 76: -1.195437224525646\n",
      "Operator 78: 0.9732007210007729\n",
      "Operator 79: -0.7965201789335481\n",
      "Operator 80: -0.21056886065190844\n",
      "Operator 81: 0.22649518995186738\n",
      "Operator 82: 0.5589124602521248\n",
      "Operator 83: -0.550938451476\n",
      "Operator 84: -0.45715389078652213\n",
      "Operator 85: 0.24964406174169515\n",
      "Operator 86: 0.05128579000466531\n",
      "Operator 87: -1.5888394646446184\n",
      "Operator 88: -0.0005463436386663467\n",
      "Operator 89: 0.12560282536981493\n",
      "Operator 90: 1.3348949163346866\n",
      "Operator 91: -0.7889889235919465\n",
      "Operator 92: 0.7118583510859406\n",
      "Operator 93: -0.387968237974778\n",
      "Operator 94: -0.4246501861151306\n",
      "Operator 95: 1.1162612465186048\n",
      "Operator 96: 1.0907004973569325\n",
      "Operator 97: -0.31699404975489187\n",
      "Operator 98: -0.36615640647971553\n",
      "Operator 99: 1.0757164217027526\n",
      "Operator 100: -0.3048981458192554\n",
      "Operator 101: -0.7968182512901109\n",
      "Operator 102: -0.008874446413119914\n",
      "Operator 103: 0.7381189701458049\n",
      "Operator 105: 1.8493176077127562\n",
      "Operator 106: 1.0957220883447523\n",
      "Operator 107: -0.7724367678344888\n",
      "Operator 108: 0.48625328096239073\n",
      "Operator 109: 0.28307596898433873\n",
      "Operator 110: -1.6011726532541155\n",
      "Operator 111: 0.9499747915080932\n",
      "Operator 112: -1.165445246196199\n",
      "Operator 113: -1.49935915824791\n",
      "Operator 114: -1.4531207166400464\n",
      "Operator 115: 0.9886025670029264\n",
      "Operator 116: -1.155516426464959\n",
      "Operator 117: -1.2814641953758947\n",
      "Operator 118: 1.1094336075837683\n",
      "Operator 119: 1.7415488481801762\n",
      "Operator 120: -1.7486729494869393\n",
      "Operator 121: -0.6020737879267666\n",
      "Operator 122: -1.5796684496921594\n",
      "Operator 123: -0.5964391517750463\n",
      "Operator 124: 0.4362552330089035\n",
      "Operator 125: -1.357223021855393\n",
      "Operator 126: -1.627022958636794\n",
      "Operator 127: -0.8249563181626409\n",
      "Operator 128: 0.6934312662603945\n",
      "Operator 129: -1.2845619193352824\n",
      "Operator 130: -1.386030552013966\n",
      "Operator 131: 1.4019440276817607\n",
      "Operator 132: 1.0831752005469686\n",
      "Operator 133: -0.6998981882812918\n",
      "Operator 134: -2.2390238784736045\n",
      "Operator 135: -0.8935262961642247\n",
      "Operator 136: 0.9722482919972835\n",
      "Operator 137: -0.7055015011564433\n",
      "Operator 138: -0.0434574645129754\n",
      "Operator 139: 1.1090793818592568\n",
      "Operator 140: 1.043972990891095\n",
      "Operator 141: -0.5213776653143875\n",
      "Operator 142: -0.13034323385794666\n",
      "Operator 143: 1.0978416004197766\n",
      "Operator 144: -0.06072938018909248\n",
      "Operator 145: -0.7422161612849246\n",
      "Operator 146: -0.1643097230880228\n",
      "Operator 147: 0.7240274274627291\n",
      "Operator 148: 1.1691496158785681\n",
      "Operator 149: 0.20767180892281906\n",
      "Operator 150: 0.47564130741376875\n",
      "Operator 151: 0.2987911970089693\n",
      "Operator 152: 0.5517976902556239\n",
      "Operator 153: 0.29076577335833764\n",
      "Operator 154: 0.36642332827499396\n",
      "Operator 155: 0.3408407503983962\n",
      "Operator 156: 0.5739352070165503\n",
      "Operator 157: 0.09002119307835788\n",
      "Operator 158: -1.0291412878783675\n",
      "Operator 159: 0.10553119538048275\n",
      "Operator 160: 1.264717685062259\n",
      "Operator 161: 0.6186457500976478\n",
      "Operator 162: -1.4006167810149348\n",
      "Operator 164: -0.8469064867321272\n",
      "Operator 165: 0.7201107346141018\n",
      "Operator 168: -0.807526461969166\n",
      "Operator 169: 0.8363435928516427\n",
      "Operator 171: 0.5796805435293818\n",
      "Operator 172: -0.8125582925340633\n",
      "Operator 173: 0.6936665621381237\n",
      "Operator 174: -0.6102790266989052\n",
      "Operator 175: 0.7767027178010548\n",
      "Operator 176: -0.45123620924565216\n",
      "Operator 177: 0.4980004090180908\n",
      "Operator 178: 0.2856419839607821\n",
      "Operator 179: 1.0021496159870724\n",
      "Operator 180: -0.30886987100704744\n",
      "Operator 181: 0.47090020521247666\n",
      "Operator 182: 0.6062359498131157\n",
      "Operator 183: 0.7483132123769336\n",
      "Operator 184: -0.7443162609055138\n",
      "Operator 185: 0.47451947777954506\n",
      "Operator 186: -0.5513721324270993\n",
      "Operator 187: 0.4340886638749518\n",
      "Operator 188: 0.09736741122453414\n",
      "Operator 189: -0.70421443418849\n",
      "Operator 190: -0.43009258143699175\n",
      "Operator 191: -0.19623755147906613\n",
      "Operator 192: 0.34368741060134933\n",
      "Operator 193: -0.8056587321406806\n",
      "Operator 194: -0.5872446842360916\n",
      "Operator 195: 0.5305250897344211\n",
      "Operator 196: -0.5247493566110791\n",
      "Operator 197: -0.21720724439400704\n",
      "Operator 198: -0.8668007670128436\n",
      "Operator 199: 1.2051814428139007\n",
      "Operator 200: -1.7102100236086937\n",
      "Total gradient norm: 12.509467633881746\n",
      "Operators under consideration (1):\n",
      "[134]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.2390238784736045)]\n",
      "Operator(s) added to ansatz: [134]\n",
      "Gradients: [np.float64(-2.2390238784736045)]\n",
      "Initial energy: -25.23704556869303\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134]...\n",
      "Starting point: [np.float64(0.41731303604700487), np.float64(0.18138397056802755), np.float64(0.3427983333765153), np.float64(-0.266906742850066), np.float64(0.26995519880523877), np.float64(0.2442840700758623), np.float64(-0.5683717683795314), np.float64(0.5238013614575125), np.float64(-0.3050991332952065), np.float64(0.35280425053984193), np.float64(-0.2975157000308499), np.float64(-0.30558817875741934), np.float64(0.2515414062313206), np.float64(-0.22224660567217447), np.float64(0.17327241571982538), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -25.419046\n",
      "         Iterations: 24\n",
      "         Function evaluations: 36\n",
      "         Gradient evaluations: 36\n",
      "\n",
      "Current energy: -25.419045507428375\n",
      "(change of -0.1819999387353448)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8506639160458697\n",
      "Operator 2: -1.4683346551604062\n",
      "Operator 3: 0.8548315577715697\n",
      "Operator 4: -0.8554121053461341\n",
      "Operator 6: -1.4509757740352227\n",
      "Operator 7: 1.0694282419055994\n",
      "Operator 9: 0.8976154960027387\n",
      "Operator 10: -0.8803636877145223\n",
      "Operator 11: -0.011725609437078536\n",
      "Operator 14: 0.8973427689360771\n",
      "Operator 15: -1.1730765125456035\n",
      "Operator 16: 0.9730772578650586\n",
      "Operator 17: -0.7328719602547139\n",
      "Operator 18: 1.359555056684172\n",
      "Operator 19: -1.335361174262276\n",
      "Operator 20: 0.7239346660639002\n",
      "Operator 21: -0.6704230361151285\n",
      "Operator 22: 1.138250261262903\n",
      "Operator 23: -0.6945831454793259\n",
      "Operator 24: 0.14711127439595142\n",
      "Operator 25: -0.28106616006739804\n",
      "Operator 26: -0.12098650969489617\n",
      "Operator 27: 1.009855603332201\n",
      "Operator 28: 0.23223314332165906\n",
      "Operator 29: 1.3176041305187023\n",
      "Operator 30: 0.9697214093988412\n",
      "Operator 31: 0.5929449543829909\n",
      "Operator 32: 0.292991233057886\n",
      "Operator 33: 1.401332944276705\n",
      "Operator 34: 0.26626722667324215\n",
      "Operator 35: -0.19435870697613228\n",
      "Operator 36: -0.641893748642371\n",
      "Operator 37: -0.46468285721378305\n",
      "Operator 38: -1.21481521312252\n",
      "Operator 39: 0.8491967501827352\n",
      "Operator 40: 0.765726505663263\n",
      "Operator 41: 0.8212457400738915\n",
      "Operator 42: 0.3921589164308932\n",
      "Operator 43: 1.2570815740459427\n",
      "Operator 44: 0.963374279038817\n",
      "Operator 45: 0.6426179539796104\n",
      "Operator 46: 0.14235074274145618\n",
      "Operator 47: 1.5217153836917583\n",
      "Operator 48: -0.47539767364673335\n",
      "Operator 49: -0.255488575548639\n",
      "Operator 50: -1.2252767461892362\n",
      "Operator 51: -0.2766778517196262\n",
      "Operator 52: -1.3818286981905104\n",
      "Operator 53: 0.4278903627801175\n",
      "Operator 54: -0.6760826456864466\n",
      "Operator 55: -0.20725537329037966\n",
      "Operator 56: 0.411716637260285\n",
      "Operator 57: 0.3746376283761009\n",
      "Operator 58: -0.7367769100290495\n",
      "Operator 59: -0.09139268349866331\n",
      "Operator 60: 0.17389276057496794\n",
      "Operator 61: 0.3279556704460336\n",
      "Operator 62: -1.6275136235268026\n",
      "Operator 63: 1.0694926178042414\n",
      "Operator 64: 0.40695750256283136\n",
      "Operator 65: 0.700428270602042\n",
      "Operator 67: 0.5995051006400027\n",
      "Operator 68: -1.7420116291384276\n",
      "Operator 70: -0.9510248608006917\n",
      "Operator 71: 0.7811228357616585\n",
      "Operator 72: -1.645159368855732\n",
      "Operator 74: -0.8972303743620237\n",
      "Operator 75: 1.2355506949363046\n",
      "Operator 76: -1.2264760893784268\n",
      "Operator 77: -0.004681837338672518\n",
      "Operator 78: 0.9735579946644339\n",
      "Operator 79: -0.797707528651102\n",
      "Operator 80: -0.20958467593559282\n",
      "Operator 81: 0.22594638713677231\n",
      "Operator 82: 0.5608281420172612\n",
      "Operator 83: -0.5628475525034518\n",
      "Operator 84: -0.44997714513113884\n",
      "Operator 85: 0.24501510419262945\n",
      "Operator 86: 0.09117848391671013\n",
      "Operator 87: -1.6260517222132593\n",
      "Operator 88: 0.05174612100576079\n",
      "Operator 89: 0.5043372415756562\n",
      "Operator 90: 1.1881702535528365\n",
      "Operator 91: -0.7894503328913757\n",
      "Operator 92: 0.7116134651703053\n",
      "Operator 93: -0.3893136652054331\n",
      "Operator 94: -0.42331614575520854\n",
      "Operator 95: 1.1163873679268608\n",
      "Operator 96: 1.090374559522598\n",
      "Operator 97: -0.32589986301231993\n",
      "Operator 98: -0.35477322443710657\n",
      "Operator 99: 1.0747105155475096\n",
      "Operator 100: -0.2747889174087986\n",
      "Operator 101: -0.840209755037823\n",
      "Operator 102: -0.12340547247416266\n",
      "Operator 103: 0.9707624535784266\n",
      "Operator 105: 1.8492650674834819\n",
      "Operator 106: 1.0961360578505215\n",
      "Operator 107: -0.5510278497793558\n",
      "Operator 108: -0.1894530483176561\n",
      "Operator 109: 0.2837492589109457\n",
      "Operator 110: -1.6014203812465526\n",
      "Operator 111: 0.9521122775583528\n",
      "Operator 112: -1.1676469587576606\n",
      "Operator 113: -1.496943407149695\n",
      "Operator 114: -1.455371973999279\n",
      "Operator 115: 1.0055662389701423\n",
      "Operator 116: -1.175946736058505\n",
      "Operator 117: -1.2636555341018416\n",
      "Operator 118: 1.0606385217499854\n",
      "Operator 119: 1.7774316786362587\n",
      "Operator 120: 0.06515968404784468\n",
      "Operator 121: 0.0037918491813270574\n",
      "Operator 122: -1.5799250647620477\n",
      "Operator 123: -0.5948607021651708\n",
      "Operator 124: 0.4350380712234359\n",
      "Operator 125: -1.3556374716203747\n",
      "Operator 126: -1.62982513597652\n",
      "Operator 127: -0.8115274451570691\n",
      "Operator 128: 0.6836816630131023\n",
      "Operator 129: -1.2685971683206365\n",
      "Operator 130: -1.4185303122295716\n",
      "Operator 131: 1.4585435055831764\n",
      "Operator 132: 1.095143108169641\n",
      "Operator 133: -0.4744947960784435\n",
      "Operator 135: -0.8939166173337773\n",
      "Operator 136: 0.9718269807844072\n",
      "Operator 137: -0.7059648631683926\n",
      "Operator 138: -0.04266247589307836\n",
      "Operator 139: 1.1094862029818442\n",
      "Operator 140: 1.0405672116104296\n",
      "Operator 141: -0.5243237879815412\n",
      "Operator 142: -0.12320461393084839\n",
      "Operator 143: 1.096305835498025\n",
      "Operator 144: -0.04191610898193743\n",
      "Operator 145: -0.7549650216348365\n",
      "Operator 146: -0.17997967747775848\n",
      "Operator 147: 1.008072671860097\n",
      "Operator 148: 1.1694745832065447\n",
      "Operator 149: 0.20738603708160863\n",
      "Operator 150: 0.4764274889624589\n",
      "Operator 151: 0.2980892221320946\n",
      "Operator 152: 0.5537651550526056\n",
      "Operator 153: 0.28765904371192447\n",
      "Operator 154: 0.3722895820793305\n",
      "Operator 155: 0.33265646440415464\n",
      "Operator 156: 0.5929152786267237\n",
      "Operator 157: 0.07707854059569606\n",
      "Operator 158: -1.0355136340909268\n",
      "Operator 159: -0.009776142289241823\n",
      "Operator 160: -0.6079705095337755\n",
      "Operator 161: 0.6185737245187105\n",
      "Operator 162: -1.4001286109302629\n",
      "Operator 164: -0.846861516418441\n",
      "Operator 165: 0.7208799265939121\n",
      "Operator 168: -0.803546069134367\n",
      "Operator 169: 0.8353630969110806\n",
      "Operator 171: 0.5216034256385377\n",
      "Operator 172: -0.779504017951941\n",
      "Operator 173: 0.6592232875055563\n",
      "Operator 174: -0.610740703850733\n",
      "Operator 175: 0.7768844420015266\n",
      "Operator 176: -0.45185438459636335\n",
      "Operator 177: 0.4984412148049253\n",
      "Operator 178: 0.28386651943243557\n",
      "Operator 179: 1.0081217094314687\n",
      "Operator 180: -0.31279655662560585\n",
      "Operator 181: 0.4744228037748985\n",
      "Operator 182: 0.5744939851281567\n",
      "Operator 183: 0.7798130590700899\n",
      "Operator 184: -0.7726899448160865\n",
      "Operator 185: 0.31465797538951595\n",
      "Operator 186: -1.0985652794707417\n",
      "Operator 187: 0.43482374634133003\n",
      "Operator 188: 0.09725810654464971\n",
      "Operator 189: -0.7023701948716816\n",
      "Operator 190: -0.43230116362541554\n",
      "Operator 191: -0.19144553077863258\n",
      "Operator 192: 0.34146127279148236\n",
      "Operator 193: -0.7931322614581302\n",
      "Operator 194: -0.6079996165087138\n",
      "Operator 195: 0.5485970811967821\n",
      "Operator 196: -0.5229245965047066\n",
      "Operator 197: -0.1135714238512686\n",
      "Operator 198: 0.09154888763911995\n",
      "Operator 199: 1.0408928450213095\n",
      "Operator 200: -1.7103068183058516\n",
      "Total gradient norm: 11.794604434753744\n",
      "Operators under consideration (1):\n",
      "[105]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.8492650674834819)]\n",
      "Operator(s) added to ansatz: [105]\n",
      "Gradients: [np.float64(1.8492650674834819)]\n",
      "Initial energy: -25.419045507428375\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105]...\n",
      "Starting point: [np.float64(0.4436839985429234), np.float64(0.18133386204180243), np.float64(0.35117025653536793), np.float64(-0.2656861245886216), np.float64(0.26954256289169476), np.float64(0.24414489917783713), np.float64(-0.6290143386710287), np.float64(0.5238506541184594), np.float64(-0.3077915385897377), np.float64(0.3529121814923346), np.float64(-0.298275286360029), np.float64(-0.3058771499106074), np.float64(0.24869020758614974), np.float64(-0.22052200118074541), np.float64(0.1916814222243149), np.float64(0.161622831723117), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -25.547069\n",
      "         Iterations: 25\n",
      "         Function evaluations: 37\n",
      "         Gradient evaluations: 37\n",
      "\n",
      "Current energy: -25.54706941370662\n",
      "(change of -0.12802390627824423)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.0375452617591547\n",
      "Operator 2: -1.495247095018751\n",
      "Operator 3: 0.8536702353545665\n",
      "Operator 4: -0.8562390387252322\n",
      "Operator 6: -1.4544199251655268\n",
      "Operator 7: 1.0693922974066068\n",
      "Operator 9: 0.8971465619367659\n",
      "Operator 10: -0.8810725023624154\n",
      "Operator 11: -0.011697029098917748\n",
      "Operator 13: -0.14176341255164684\n",
      "Operator 14: 0.4110870685389121\n",
      "Operator 15: -0.9204510975924362\n",
      "Operator 16: 0.936627426162359\n",
      "Operator 17: -0.7397578073132144\n",
      "Operator 18: 1.347248131952665\n",
      "Operator 19: -1.3448097798697605\n",
      "Operator 20: 0.7172637950509358\n",
      "Operator 21: -0.6713468123916002\n",
      "Operator 22: 1.1377643724284918\n",
      "Operator 23: -0.6959955138311721\n",
      "Operator 24: 0.1471635197691664\n",
      "Operator 25: -0.2812240342705865\n",
      "Operator 26: -0.33194643123689094\n",
      "Operator 27: 0.8262178961855074\n",
      "Operator 28: 0.31353787007930667\n",
      "Operator 29: 1.2709089644056442\n",
      "Operator 30: 1.004427422158778\n",
      "Operator 31: 0.5755844371618146\n",
      "Operator 32: 0.300804829741938\n",
      "Operator 33: 1.3960150862550553\n",
      "Operator 34: 0.2703642922314775\n",
      "Operator 35: -0.19496951546709576\n",
      "Operator 36: -0.6401570653507105\n",
      "Operator 37: -0.4645760815042472\n",
      "Operator 38: -1.2140542689489366\n",
      "Operator 39: 0.2652267426303822\n",
      "Operator 40: 0.5631007042456627\n",
      "Operator 41: 0.7582415258952366\n",
      "Operator 42: 0.4131524747363533\n",
      "Operator 43: 1.2348443920400276\n",
      "Operator 44: 0.9784340834000499\n",
      "Operator 45: 0.6322784591378123\n",
      "Operator 46: 0.14527009704816946\n",
      "Operator 47: 1.5196573946143814\n",
      "Operator 48: -0.47259126264632634\n",
      "Operator 49: -0.25564170244765655\n",
      "Operator 50: -1.224493039459747\n",
      "Operator 51: -0.27634060535366645\n",
      "Operator 52: -0.2551708283023892\n",
      "Operator 53: 0.17634006753060205\n",
      "Operator 54: -0.644531042329748\n",
      "Operator 55: -0.25753268133284457\n",
      "Operator 56: 0.4185455724358236\n",
      "Operator 57: 0.37055546711911547\n",
      "Operator 58: -0.7333368130667317\n",
      "Operator 59: -0.09795083350828049\n",
      "Operator 60: 0.17431022878532207\n",
      "Operator 61: 0.3264210167655573\n",
      "Operator 62: -1.6269295118490612\n",
      "Operator 63: 1.0684585264724\n",
      "Operator 64: 0.4067022725213853\n",
      "Operator 65: 0.18010728761695918\n",
      "Operator 67: 0.23019130669333854\n",
      "Operator 68: -1.7270896497125623\n",
      "Operator 70: -0.9328437581752265\n",
      "Operator 71: 0.7935675610763135\n",
      "Operator 72: -1.6395377892587426\n",
      "Operator 74: -0.89548191370734\n",
      "Operator 75: 1.2371640318873787\n",
      "Operator 76: -1.2266363903923585\n",
      "Operator 77: -0.004671895511145525\n",
      "Operator 78: 0.8974120270918917\n",
      "Operator 79: -0.6482272584205666\n",
      "Operator 80: -0.3149885449437557\n",
      "Operator 81: 0.24611442321002608\n",
      "Operator 82: 0.5376542246998067\n",
      "Operator 83: -0.5495995083899538\n",
      "Operator 84: -0.4610691369984936\n",
      "Operator 85: 0.2473785929233337\n",
      "Operator 86: 0.08749012898252534\n",
      "Operator 87: -1.6246249394772538\n",
      "Operator 88: 0.049831892476701466\n",
      "Operator 89: 0.5042320757058708\n",
      "Operator 90: 1.1873973138246992\n",
      "Operator 91: -0.5759968652723252\n",
      "Operator 92: 0.07016028760288655\n",
      "Operator 93: -0.3233941039906484\n",
      "Operator 94: -0.43437686253387753\n",
      "Operator 95: 1.1072040009701256\n",
      "Operator 96: 1.094528029473764\n",
      "Operator 97: -0.32036405788915656\n",
      "Operator 98: -0.355942626147874\n",
      "Operator 99: 1.074321502034691\n",
      "Operator 100: -0.27571666634812475\n",
      "Operator 101: -0.8391702255290282\n",
      "Operator 102: -0.12367809949863862\n",
      "Operator 103: 0.9708947960873159\n",
      "Operator 104: -0.16231543493406886\n",
      "Operator 106: 0.4974803063891541\n",
      "Operator 107: -0.5510970743864783\n",
      "Operator 108: -0.1894651780113171\n",
      "Operator 109: 0.374100303470299\n",
      "Operator 110: -1.367186012202428\n",
      "Operator 111: 0.916991574655683\n",
      "Operator 112: -1.116579376570019\n",
      "Operator 113: -1.5112529532824879\n",
      "Operator 114: -1.4486483758236768\n",
      "Operator 115: 1.0014926467674052\n",
      "Operator 116: -1.1699104001458622\n",
      "Operator 117: -1.265340355547966\n",
      "Operator 118: 1.0615335035050852\n",
      "Operator 119: 1.776641770986983\n",
      "Operator 120: 0.06533590664383207\n",
      "Operator 121: 0.0037844057782024376\n",
      "Operator 122: -0.7755748726829287\n",
      "Operator 123: -0.656573651212817\n",
      "Operator 124: 0.5298704960339959\n",
      "Operator 125: -1.3687504669401502\n",
      "Operator 126: -1.618277114262562\n",
      "Operator 127: -0.8230857741434389\n",
      "Operator 128: 0.695790414559863\n",
      "Operator 129: -1.2702267764648603\n",
      "Operator 130: -1.4164586840809474\n",
      "Operator 131: 1.4564823917067997\n",
      "Operator 132: 1.0960278838267612\n",
      "Operator 133: -0.4749015928156544\n",
      "Operator 135: -0.8935459242630194\n",
      "Operator 136: 0.650396938413217\n",
      "Operator 137: -0.6658883202917403\n",
      "Operator 138: -0.08302648661736763\n",
      "Operator 139: 1.1102289466239157\n",
      "Operator 140: 1.0436458135477509\n",
      "Operator 141: -0.5169340640652097\n",
      "Operator 142: -0.12820222393703493\n",
      "Operator 143: 1.0965832621578473\n",
      "Operator 144: -0.04238345862867859\n",
      "Operator 145: -0.7537614734720686\n",
      "Operator 146: -0.18028349023268805\n",
      "Operator 147: 1.0082924439651062\n",
      "Operator 148: 0.8532979723230423\n",
      "Operator 149: 0.19135393973007095\n",
      "Operator 150: 0.5539456927122018\n",
      "Operator 151: 0.3296886928534035\n",
      "Operator 152: 0.5373818409459542\n",
      "Operator 153: 0.2958705768755923\n",
      "Operator 154: 0.3650123050006462\n",
      "Operator 155: 0.33620618635457156\n",
      "Operator 156: 0.590867624406986\n",
      "Operator 157: 0.07769450159871207\n",
      "Operator 158: -1.0357993202419495\n",
      "Operator 159: -0.00955056352616197\n",
      "Operator 160: -0.6078731957819109\n",
      "Operator 161: 0.15730250008800334\n",
      "Operator 162: -0.4424426308479497\n",
      "Operator 163: 0.20543858904890583\n",
      "Operator 164: -0.8358631745436936\n",
      "Operator 165: 0.7499318964661792\n",
      "Operator 168: -0.802604752832324\n",
      "Operator 169: 0.8390862551507234\n",
      "Operator 171: 0.5220221734653274\n",
      "Operator 172: -0.7796853971536546\n",
      "Operator 173: 0.6598950207584799\n",
      "Operator 174: -0.9148386614267019\n",
      "Operator 175: 0.60865393531746\n",
      "Operator 176: -0.392442331189628\n",
      "Operator 177: 0.4821077410403982\n",
      "Operator 178: 0.3084051264557421\n",
      "Operator 179: 1.0047448090776159\n",
      "Operator 180: -0.30683022379935754\n",
      "Operator 181: 0.4719074455233448\n",
      "Operator 182: 0.5773868421632128\n",
      "Operator 183: 0.7800717914080718\n",
      "Operator 184: -0.7710670147635085\n",
      "Operator 185: 0.31472652082586505\n",
      "Operator 186: -1.0977418651784587\n",
      "Operator 187: 0.36896142207271\n",
      "Operator 188: 0.26012237607855854\n",
      "Operator 189: -0.7308435434594139\n",
      "Operator 190: -0.42690777348711956\n",
      "Operator 191: -0.21371061321864424\n",
      "Operator 192: 0.35262841887629837\n",
      "Operator 193: -0.7997076753683723\n",
      "Operator 194: -0.6071309383451972\n",
      "Operator 195: 0.5452399587068983\n",
      "Operator 196: -0.5224545507202851\n",
      "Operator 197: -0.11530913899158923\n",
      "Operator 198: 0.09171140873399239\n",
      "Operator 199: 1.0400293602994835\n",
      "Operator 200: -0.5354254910916196\n",
      "Total gradient norm: 10.977320544145686\n",
      "Operators under consideration (1):\n",
      "[119]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.776641770986983)]\n",
      "Operator(s) added to ansatz: [119]\n",
      "Gradients: [np.float64(1.776641770986983)]\n",
      "Initial energy: -25.54706941370662\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119]...\n",
      "Starting point: [np.float64(0.4435033809287755), np.float64(0.1442596240865509), np.float64(0.35083349138083486), np.float64(-0.2669102560918175), np.float64(0.27271353330981773), np.float64(0.2543460936434716), np.float64(-0.6289447613177294), np.float64(0.5040911665676858), np.float64(-0.3070295428351301), np.float64(0.33114313913495647), np.float64(-0.2960317527368294), np.float64(-0.29911088281360926), np.float64(0.24904136938643576), np.float64(-0.22065248600507645), np.float64(0.19171127126971746), np.float64(0.16162271460835054), np.float64(-0.13980679139483435), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.662965\n",
      "         Iterations: 25\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 67\n",
      "\n",
      "Current energy: -25.662964740661\n",
      "(change of -0.1158953269543801)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.0362760767322698\n",
      "Operator 2: -1.4902530999584913\n",
      "Operator 3: 0.8631965432164257\n",
      "Operator 4: -0.8438778171695784\n",
      "Operator 6: -1.4167272353512375\n",
      "Operator 7: 1.1415099033618383\n",
      "Operator 8: 0.02120419278039143\n",
      "Operator 9: 0.8168093675790422\n",
      "Operator 10: -0.08325175814812069\n",
      "Operator 11: 0.10395913842567209\n",
      "Operator 13: -0.14204291770700309\n",
      "Operator 14: 0.41131414445181597\n",
      "Operator 15: -0.9209033060103451\n",
      "Operator 16: 0.9337045761609479\n",
      "Operator 17: -0.7377790210401393\n",
      "Operator 18: 1.3339402060243888\n",
      "Operator 19: -1.3450798120797758\n",
      "Operator 20: 0.6793662719031934\n",
      "Operator 21: -0.6116236537663713\n",
      "Operator 22: 1.0409132143614046\n",
      "Operator 23: -0.6710947738696644\n",
      "Operator 24: 0.1031915065479696\n",
      "Operator 25: -0.28737529002981166\n",
      "Operator 26: -0.33414463933342864\n",
      "Operator 27: 0.8276524792200175\n",
      "Operator 28: 0.30555360318528735\n",
      "Operator 29: 1.2812262990637606\n",
      "Operator 30: 0.9869311607741134\n",
      "Operator 31: 0.5906812186667807\n",
      "Operator 32: 0.24312183949402166\n",
      "Operator 33: 1.4747715837014643\n",
      "Operator 34: 0.17312352755260885\n",
      "Operator 35: -0.33532110152362815\n",
      "Operator 36: -0.6209157304771771\n",
      "Operator 37: -0.48960861198781613\n",
      "Operator 38: -1.5408671970329673\n",
      "Operator 39: 0.26705755749396864\n",
      "Operator 40: 0.5582218145969535\n",
      "Operator 41: 0.7632064716671858\n",
      "Operator 42: 0.39836654486567447\n",
      "Operator 43: 1.2566116708566837\n",
      "Operator 44: 0.9294405008204341\n",
      "Operator 45: 0.660696092754707\n",
      "Operator 46: -0.010927914015248025\n",
      "Operator 47: 1.5103682191002266\n",
      "Operator 48: -0.6308560122006814\n",
      "Operator 49: -0.37381238405949957\n",
      "Operator 50: -1.3735298495048283\n",
      "Operator 51: -0.2767235613923973\n",
      "Operator 52: -0.25702723084277085\n",
      "Operator 53: 0.17914686812214634\n",
      "Operator 54: -0.6520208891533563\n",
      "Operator 55: -0.24519198765312916\n",
      "Operator 56: 0.4053152319930059\n",
      "Operator 57: 0.39577573157391677\n",
      "Operator 58: -0.7891165344130471\n",
      "Operator 59: 0.0711165442892357\n",
      "Operator 60: 0.25120650521106574\n",
      "Operator 61: 0.6894914297175845\n",
      "Operator 62: -0.05937352446018702\n",
      "Operator 63: 1.5310040497562942\n",
      "Operator 64: 0.17449824224687646\n",
      "Operator 65: 0.18026502035021674\n",
      "Operator 67: 0.23033519344309605\n",
      "Operator 68: -1.728857251903287\n",
      "Operator 70: -0.9330255651839746\n",
      "Operator 71: 0.7917794135418981\n",
      "Operator 72: -1.6616294547255295\n",
      "Operator 74: -0.8899248450058206\n",
      "Operator 75: 1.098647282274829\n",
      "Operator 76: -0.24989298570249618\n",
      "Operator 77: -0.010216425400202955\n",
      "Operator 78: 0.8983303143471315\n",
      "Operator 79: -0.6524710166944877\n",
      "Operator 80: -0.31187368987468955\n",
      "Operator 81: 0.2440222064844647\n",
      "Operator 82: 0.544504156490364\n",
      "Operator 83: -0.5950726179273631\n",
      "Operator 84: -0.4346388906421468\n",
      "Operator 85: 0.23321457768515172\n",
      "Operator 86: 0.2809716637000274\n",
      "Operator 87: -0.12923907989562888\n",
      "Operator 88: 0.7491096416084897\n",
      "Operator 89: 0.48093367018613215\n",
      "Operator 90: 1.4356205618227502\n",
      "Operator 91: -0.5774921399205295\n",
      "Operator 92: 0.07056292900829605\n",
      "Operator 93: -0.3277735238040399\n",
      "Operator 94: -0.42939204153922017\n",
      "Operator 95: 1.1079227485259007\n",
      "Operator 96: 1.0934941042239767\n",
      "Operator 97: -0.349393574827889\n",
      "Operator 98: -0.31609343361138126\n",
      "Operator 99: 1.0776018374998795\n",
      "Operator 100: -0.13516070165585248\n",
      "Operator 101: 0.48076923785441017\n",
      "Operator 102: -0.08652163433793507\n",
      "Operator 103: 0.6463308120327196\n",
      "Operator 104: -0.16254655783023314\n",
      "Operator 105: -1.2686182697786206e-08\n",
      "Operator 106: 0.4984555745721396\n",
      "Operator 107: -0.7358031614567244\n",
      "Operator 108: -0.2536502906278457\n",
      "Operator 109: 0.37604831875029904\n",
      "Operator 110: -1.3681890794398188\n",
      "Operator 111: 0.9248907424727447\n",
      "Operator 112: -1.1246750614762513\n",
      "Operator 113: -1.5025022361044502\n",
      "Operator 114: -1.456875895819816\n",
      "Operator 115: 1.0561193239037472\n",
      "Operator 116: -1.248972535891351\n",
      "Operator 117: -1.2346332087285452\n",
      "Operator 118: 0.790237820488341\n",
      "Operator 120: 0.022377130915298554\n",
      "Operator 121: 0.30183403698985356\n",
      "Operator 122: -0.7764991542801587\n",
      "Operator 123: -0.6512821038340793\n",
      "Operator 124: 0.5259213626250832\n",
      "Operator 125: -1.362978842043075\n",
      "Operator 126: -1.6285728593484678\n",
      "Operator 127: -0.7718805695896116\n",
      "Operator 128: 0.6585032156666528\n",
      "Operator 129: -1.2083907432995886\n",
      "Operator 130: -1.5408029832276062\n",
      "Operator 131: 0.36572642502557884\n",
      "Operator 132: 0.25503297903323285\n",
      "Operator 133: -0.43348820758392365\n",
      "Operator 135: -0.8942532351892181\n",
      "Operator 136: 0.6498618495797509\n",
      "Operator 137: -0.6672679862964722\n",
      "Operator 138: -0.08007779759540506\n",
      "Operator 139: 1.1118296669267176\n",
      "Operator 140: 1.0308094408236885\n",
      "Operator 141: -0.5278243254772464\n",
      "Operator 142: -0.10362373228139654\n",
      "Operator 143: 1.078483532881825\n",
      "Operator 144: 0.25971807805122904\n",
      "Operator 145: 0.42879386120480334\n",
      "Operator 146: -0.13335397632594725\n",
      "Operator 147: 0.9600282055673611\n",
      "Operator 148: 0.854190756619595\n",
      "Operator 149: 0.19002061761890862\n",
      "Operator 150: 0.5566534581266969\n",
      "Operator 151: 0.3270171714333048\n",
      "Operator 152: 0.544515905296701\n",
      "Operator 153: 0.2842072033478853\n",
      "Operator 154: 0.38718712431715047\n",
      "Operator 155: 0.30330715425594434\n",
      "Operator 156: 0.6651589027304337\n",
      "Operator 157: 0.15002354082833214\n",
      "Operator 158: -1.0628057190044675\n",
      "Operator 159: 0.38369458531058587\n",
      "Operator 160: -0.5768114043425929\n",
      "Operator 161: 0.15752637147690235\n",
      "Operator 162: -0.4424933008879952\n",
      "Operator 163: 0.20587668407147128\n",
      "Operator 164: -0.8356404792344228\n",
      "Operator 165: 0.7528842769232487\n",
      "Operator 168: -0.7795800392738237\n",
      "Operator 169: 0.8069020176902318\n",
      "Operator 170: -0.10530147963462083\n",
      "Operator 171: 0.4140810431920285\n",
      "Operator 172: -0.21337480510735346\n",
      "Operator 173: -0.047487235254852544\n",
      "Operator 174: -0.915660966638355\n",
      "Operator 175: 0.6097828193944144\n",
      "Operator 176: -0.3942992248394094\n",
      "Operator 177: 0.4839315847409196\n",
      "Operator 178: 0.302159224836384\n",
      "Operator 179: 1.0276405533289776\n",
      "Operator 180: -0.32168127580919864\n",
      "Operator 181: 0.4813304458746501\n",
      "Operator 182: 0.4241776751352242\n",
      "Operator 183: 0.0572745345181882\n",
      "Operator 184: -0.7634763831727154\n",
      "Operator 185: 0.35340028702813575\n",
      "Operator 186: -1.3289575903158979\n",
      "Operator 187: 0.3709908414223814\n",
      "Operator 188: 0.2595931348491297\n",
      "Operator 189: -0.7249637676435671\n",
      "Operator 190: -0.4355560657803187\n",
      "Operator 191: -0.19677564894994548\n",
      "Operator 192: 0.3444425273554902\n",
      "Operator 193: -0.7564672207032216\n",
      "Operator 194: -0.6782850221810661\n",
      "Operator 195: 0.6151139484269892\n",
      "Operator 196: -0.4785464164600942\n",
      "Operator 197: 0.5852179951725065\n",
      "Operator 198: 0.09065683022070554\n",
      "Operator 199: 1.2768372716967744\n",
      "Operator 200: -0.5362403785855852\n",
      "Total gradient norm: 10.443384169192706\n",
      "Operators under consideration (1):\n",
      "[68]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.728857251903287)]\n",
      "Operator(s) added to ansatz: [68]\n",
      "Gradients: [np.float64(-1.728857251903287)]\n",
      "Initial energy: -25.662964740661\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68]...\n",
      "Starting point: [np.float64(0.5312920370537674), np.float64(0.14418242834785122), np.float64(0.3856262791829572), np.float64(-0.2625066949933139), np.float64(0.2711978545402164), np.float64(0.25385525785876356), np.float64(-0.656891137895003), np.float64(0.5041903654947406), np.float64(-0.3173877499615151), np.float64(0.3314604368015914), np.float64(-0.29885363381851693), np.float64(-0.3001403050712693), np.float64(0.24133893908945686), np.float64(-0.21568165361742805), np.float64(0.16423531565646216), np.float64(0.15976284205900215), np.float64(-0.13974786365798542), np.float64(-0.1295907955801512), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.813133\n",
      "         Iterations: 27\n",
      "         Function evaluations: 98\n",
      "         Gradient evaluations: 84\n",
      "\n",
      "Current energy: -25.81313320034247\n",
      "(change of -0.15016845968147052)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.30556041281501956\n",
      "Operator 2: -0.8930600505800191\n",
      "Operator 3: 0.8602851798885307\n",
      "Operator 4: -0.845041480867715\n",
      "Operator 5: 8.885111304248031e-08\n",
      "Operator 6: -1.422044325744821\n",
      "Operator 7: 1.1415661768743106\n",
      "Operator 8: 0.02118964727627478\n",
      "Operator 9: 0.8166827559552234\n",
      "Operator 10: -0.08377205484004491\n",
      "Operator 11: 0.10411064023358307\n",
      "Operator 12: 9.634358377752505e-08\n",
      "Operator 13: -0.2095383615490965\n",
      "Operator 14: 0.467627814241888\n",
      "Operator 15: -0.2961579945345809\n",
      "Operator 16: 0.18424502648137417\n",
      "Operator 17: -0.8377849323668182\n",
      "Operator 18: 1.343084096999841\n",
      "Operator 19: -1.3611230152378146\n",
      "Operator 20: 0.6725536254677335\n",
      "Operator 21: -0.6129705599197641\n",
      "Operator 22: 1.0408610670264822\n",
      "Operator 23: -0.6730964902518811\n",
      "Operator 24: 0.10344391559090121\n",
      "Operator 25: -0.28771243827554516\n",
      "Operator 26: -0.790499609738766\n",
      "Operator 27: -0.07455693601538987\n",
      "Operator 28: 0.2776597244660043\n",
      "Operator 29: 1.091776699472504\n",
      "Operator 30: 1.0009175583847745\n",
      "Operator 31: 0.5616557488768865\n",
      "Operator 32: 0.2513236784989162\n",
      "Operator 33: 1.4675607614028956\n",
      "Operator 34: 0.1787735283952276\n",
      "Operator 35: -0.3353823626888317\n",
      "Operator 36: -0.6184761697794479\n",
      "Operator 37: -0.4892831150708873\n",
      "Operator 38: -1.5397777623368665\n",
      "Operator 39: 0.15885063003493682\n",
      "Operator 40: -0.20112605176303627\n",
      "Operator 41: -0.09191552474701484\n",
      "Operator 42: 0.5205613368040816\n",
      "Operator 43: 1.2546867458391815\n",
      "Operator 44: 0.9531283026268451\n",
      "Operator 45: 0.6487841064326754\n",
      "Operator 46: -0.006573005946077095\n",
      "Operator 47: 1.507948416292625\n",
      "Operator 48: -0.6271891754332454\n",
      "Operator 49: -0.3732877343879038\n",
      "Operator 50: -1.3724101058147826\n",
      "Operator 51: -0.2761552723293666\n",
      "Operator 52: -0.15941452707810985\n",
      "Operator 53: 0.18491883469698164\n",
      "Operator 54: 0.13359520254682922\n",
      "Operator 55: 0.38592538476951077\n",
      "Operator 56: 0.4193434238039362\n",
      "Operator 57: 0.3911331506660778\n",
      "Operator 58: -0.7846472074990083\n",
      "Operator 59: 0.061326615792149305\n",
      "Operator 60: 0.25175932194407286\n",
      "Operator 61: 0.6869989742849604\n",
      "Operator 62: -0.06027260675791192\n",
      "Operator 63: 1.5300934879173194\n",
      "Operator 64: 0.17411668807769648\n",
      "Operator 65: 0.17956119218170263\n",
      "Operator 66: -5.1895697694295235e-08\n",
      "Operator 67: 0.3347874260201159\n",
      "Operator 69: 0.1324846398561432\n",
      "Operator 70: -0.9483383515674841\n",
      "Operator 71: 0.8132944389837746\n",
      "Operator 72: -1.6550748632469667\n",
      "Operator 73: 3.608586743230279e-08\n",
      "Operator 74: -0.8879042643617854\n",
      "Operator 75: 1.1008866661938947\n",
      "Operator 76: -0.2502995351416415\n",
      "Operator 77: -0.010218188106183579\n",
      "Operator 78: 0.7688125271621544\n",
      "Operator 79: 0.029024702900875284\n",
      "Operator 80: 0.43783326958581226\n",
      "Operator 81: 0.49015625319474665\n",
      "Operator 82: 0.5515834084874744\n",
      "Operator 83: -0.5664179051145853\n",
      "Operator 84: -0.4476565020086106\n",
      "Operator 85: 0.23653879647724224\n",
      "Operator 86: 0.2759556218393555\n",
      "Operator 87: -0.12919331331679643\n",
      "Operator 88: 0.7468201218795186\n",
      "Operator 89: 0.4807038383112986\n",
      "Operator 90: 1.43449977802792\n",
      "Operator 91: -0.7552455366401628\n",
      "Operator 92: -0.1285731479627874\n",
      "Operator 93: -0.33371289707379437\n",
      "Operator 94: 0.31971184999074764\n",
      "Operator 95: 1.1372694217244903\n",
      "Operator 96: 1.0994492914809126\n",
      "Operator 97: -0.3426412940119678\n",
      "Operator 98: -0.3175349313329008\n",
      "Operator 99: 1.0773468602949854\n",
      "Operator 100: -0.1362440686415338\n",
      "Operator 101: 0.48035479648817736\n",
      "Operator 102: -0.0867497558349857\n",
      "Operator 103: 0.6466556458816197\n",
      "Operator 104: -0.18036432322192203\n",
      "Operator 105: -1.835635704432153e-08\n",
      "Operator 106: 0.5350106653858748\n",
      "Operator 107: -0.7363406776403578\n",
      "Operator 108: -0.2538224642710958\n",
      "Operator 109: 0.623026651823647\n",
      "Operator 110: -0.13799002307187125\n",
      "Operator 111: -0.1160602236343718\n",
      "Operator 112: -1.4603465621693736\n",
      "Operator 113: -1.5205588092393045\n",
      "Operator 114: -1.4468856778412487\n",
      "Operator 115: 1.050463398072914\n",
      "Operator 116: -1.240475228898572\n",
      "Operator 117: -1.2370776967850583\n",
      "Operator 118: 0.7915732748242945\n",
      "Operator 119: 3.9114076366697503e-07\n",
      "Operator 120: 0.022308973844088253\n",
      "Operator 121: 0.30179611662103545\n",
      "Operator 122: -0.6069148263051742\n",
      "Operator 123: 0.20643243463760114\n",
      "Operator 124: -0.3681852945273395\n",
      "Operator 125: -1.4794264955744334\n",
      "Operator 126: -1.6262669981812068\n",
      "Operator 127: -0.7902947387594468\n",
      "Operator 128: 0.6723883137737003\n",
      "Operator 129: -1.2108034541237354\n",
      "Operator 130: -1.5381064520199796\n",
      "Operator 131: 0.3647357931309371\n",
      "Operator 132: 0.2554180018140896\n",
      "Operator 133: -0.43429122616428617\n",
      "Operator 134: -5.268999863083934e-07\n",
      "Operator 135: -0.8290482860664474\n",
      "Operator 136: 0.11886639384371589\n",
      "Operator 137: -0.5461777672378156\n",
      "Operator 138: 0.5533834776967168\n",
      "Operator 139: 1.1627115008798818\n",
      "Operator 140: 1.0398771906077275\n",
      "Operator 141: -0.5173094630061295\n",
      "Operator 142: -0.10909581814437774\n",
      "Operator 143: 1.0792918242519463\n",
      "Operator 144: 0.2584916219959076\n",
      "Operator 145: 0.42891795757405404\n",
      "Operator 146: -0.13357369596520308\n",
      "Operator 147: 0.9605365926953062\n",
      "Operator 148: 0.7444052014525402\n",
      "Operator 149: 0.11947956079392597\n",
      "Operator 150: 0.5411093884443804\n",
      "Operator 151: 0.49129872375104866\n",
      "Operator 152: 0.507379291627859\n",
      "Operator 153: 0.29341472251021417\n",
      "Operator 154: 0.37473457459201526\n",
      "Operator 155: 0.30767778810029933\n",
      "Operator 156: 0.6622235667994223\n",
      "Operator 157: 0.15102226920816342\n",
      "Operator 158: -1.0629613995755496\n",
      "Operator 159: 0.38438318597576643\n",
      "Operator 160: -0.5767009777365462\n",
      "Operator 161: 0.20888641130422103\n",
      "Operator 162: -0.46627860635866203\n",
      "Operator 163: 0.3221222764773926\n",
      "Operator 164: -0.22552732242036977\n",
      "Operator 165: 0.13345396274347426\n",
      "Operator 167: -1.4946247025491476e-08\n",
      "Operator 168: -0.778704414304682\n",
      "Operator 169: 0.8123567110973481\n",
      "Operator 170: -0.10537602053835718\n",
      "Operator 171: 0.4146101161728062\n",
      "Operator 172: -0.21379254861016314\n",
      "Operator 173: -0.04706304079165946\n",
      "Operator 174: -0.8136180849970109\n",
      "Operator 175: -0.2433202294518415\n",
      "Operator 176: -0.4394751428436202\n",
      "Operator 177: 0.30149280240832366\n",
      "Operator 178: 0.25370260073605577\n",
      "Operator 179: 1.0011269220642869\n",
      "Operator 180: -0.3139679304770806\n",
      "Operator 181: 0.477482305632111\n",
      "Operator 182: 0.42843797424800634\n",
      "Operator 183: 0.056996333416885266\n",
      "Operator 184: -0.761180691074534\n",
      "Operator 185: 0.35394827769428394\n",
      "Operator 186: -1.3279463811620262\n",
      "Operator 187: 0.834384256182137\n",
      "Operator 188: 0.23885215566437784\n",
      "Operator 189: -0.14221098224532355\n",
      "Operator 190: -0.1292276041169072\n",
      "Operator 191: -0.12282334539075299\n",
      "Operator 192: 0.3618113723055777\n",
      "Operator 193: -0.7606316614430637\n",
      "Operator 194: -0.6764567501007868\n",
      "Operator 195: 0.6102681538017596\n",
      "Operator 196: -0.478341844413423\n",
      "Operator 197: 0.5827072629222083\n",
      "Operator 198: 0.09075896114632227\n",
      "Operator 199: 1.275763076410266\n",
      "Operator 200: -0.5333513918706798\n",
      "Total gradient norm: 9.815777011215458\n",
      "Operators under consideration (1):\n",
      "[72]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.6550748632469667)]\n",
      "Operator(s) added to ansatz: [72]\n",
      "Gradients: [np.float64(-1.6550748632469667)]\n",
      "Initial energy: -25.81313320034247\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72]...\n",
      "Starting point: [np.float64(0.5310527687740924), np.float64(0.14410977395203745), np.float64(0.38512588336693754), np.float64(-0.2643013856956113), np.float64(0.27852914219582703), np.float64(0.1642725722801308), np.float64(-0.6567769802563308), np.float64(0.5026252825351795), np.float64(-0.31636950645708795), np.float64(0.3346304222221585), np.float64(-0.2967334075162933), np.float64(-0.3020240424807185), np.float64(0.24181397201876917), np.float64(-0.2158148011473357), np.float64(0.16426622832963367), np.float64(0.15975864480145896), np.float64(-0.1443110060079184), np.float64(-0.12956567958330698), np.float64(0.1727030214492437), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -25.954688\n",
      "         Iterations: 27\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "Current energy: -25.95468757669456\n",
      "(change of -0.14155437635209012)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72]\n",
      "On iteration 20.\n",
      "\n",
      "*** ADAPT-VQE Iteration 21 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.30688590854167314\n",
      "Operator 2: -0.886336768945918\n",
      "Operator 3: 0.9454759182936814\n",
      "Operator 4: -0.11166124582721393\n",
      "Operator 6: -0.7484837666120567\n",
      "Operator 7: 1.1395449301827143\n",
      "Operator 8: 0.02163006486325504\n",
      "Operator 9: 0.8169562136563744\n",
      "Operator 10: -0.08237514482603192\n",
      "Operator 11: 0.10421865721676143\n",
      "Operator 13: -0.20957140329700605\n",
      "Operator 14: 0.46750652935985243\n",
      "Operator 15: -0.29772390526107056\n",
      "Operator 16: 0.18491142711494868\n",
      "Operator 17: -0.8124552843826425\n",
      "Operator 18: 1.4190168730332708\n",
      "Operator 19: -0.8320275225033393\n",
      "Operator 20: 0.1364500846487888\n",
      "Operator 21: -0.69032921403127\n",
      "Operator 22: 1.0534421948886576\n",
      "Operator 23: -0.6808314283955162\n",
      "Operator 24: 0.10303501120698974\n",
      "Operator 25: -0.2874322239450321\n",
      "Operator 26: -0.7909431335892045\n",
      "Operator 27: -0.07120701962181017\n",
      "Operator 28: 0.26803833314523157\n",
      "Operator 29: 1.152983586409979\n",
      "Operator 30: 0.4782978272096414\n",
      "Operator 31: -0.11882668590377446\n",
      "Operator 32: 0.20264152338192792\n",
      "Operator 33: 1.3165202394326099\n",
      "Operator 34: 0.16358466268025057\n",
      "Operator 35: -0.3412604199218754\n",
      "Operator 36: -0.6216049539515096\n",
      "Operator 37: -0.4899152742866494\n",
      "Operator 38: -1.5413568129150796\n",
      "Operator 39: 0.16111777892474063\n",
      "Operator 40: -0.20490171875223306\n",
      "Operator 41: -0.08552839389419799\n",
      "Operator 42: 0.49633226575176514\n",
      "Operator 43: 1.141676776740125\n",
      "Operator 44: 0.29575221724705875\n",
      "Operator 45: -0.058149645161129554\n",
      "Operator 46: 0.07583028234847661\n",
      "Operator 47: 1.5261716350220322\n",
      "Operator 48: -0.6204406514149807\n",
      "Operator 49: -0.3745468102937353\n",
      "Operator 50: -1.3734604401705546\n",
      "Operator 51: -0.276573229961844\n",
      "Operator 52: -0.16167336829534523\n",
      "Operator 53: 0.188981955823445\n",
      "Operator 54: 0.12693877522496905\n",
      "Operator 55: 0.4055217447188294\n",
      "Operator 56: 0.492766370664329\n",
      "Operator 57: 0.4068845943176508\n",
      "Operator 58: 0.05668917596013217\n",
      "Operator 59: 0.7046830043341975\n",
      "Operator 60: 0.2559973023382041\n",
      "Operator 61: 0.6895118323210052\n",
      "Operator 62: -0.058697807577595074\n",
      "Operator 63: 1.5310856863593139\n",
      "Operator 64: 0.17374512602397024\n",
      "Operator 65: 0.17971909119894702\n",
      "Operator 67: 0.33572556662292985\n",
      "Operator 69: 0.13318982474054095\n",
      "Operator 70: -0.948230310118291\n",
      "Operator 71: 0.886193935450486\n",
      "Operator 73: 0.10444546844713667\n",
      "Operator 74: -0.9193585119101502\n",
      "Operator 75: 1.1069011057369027\n",
      "Operator 76: -0.2490985810107318\n",
      "Operator 77: -0.010238026254091537\n",
      "Operator 78: 0.7695528000190915\n",
      "Operator 79: 0.026283232619949796\n",
      "Operator 80: 0.4403689716811279\n",
      "Operator 81: 0.4864075964497132\n",
      "Operator 82: 0.40141017988822436\n",
      "Operator 83: 0.13056079527368733\n",
      "Operator 84: 0.3813686027460418\n",
      "Operator 85: 0.4528624378431103\n",
      "Operator 86: 0.3101678332648251\n",
      "Operator 87: -0.11988818557891251\n",
      "Operator 88: 0.7496750232311038\n",
      "Operator 89: 0.48090867413524624\n",
      "Operator 90: 1.4358761477406397\n",
      "Operator 91: -0.7561030432331886\n",
      "Operator 92: -0.12744149426012474\n",
      "Operator 93: -0.3382546774146645\n",
      "Operator 94: 0.32369638263882794\n",
      "Operator 95: 1.1453847169628169\n",
      "Operator 96: 0.525645591911742\n",
      "Operator 97: -0.42890407826741755\n",
      "Operator 98: 0.405457421662228\n",
      "Operator 99: 1.0818204473011837\n",
      "Operator 100: -0.13804262510476462\n",
      "Operator 101: 0.48055460437101816\n",
      "Operator 102: -0.08653705433683653\n",
      "Operator 103: 0.6455905794960887\n",
      "Operator 104: -0.1805100183225904\n",
      "Operator 106: 0.5358025567974858\n",
      "Operator 107: -0.7362669783184856\n",
      "Operator 108: -0.25383680209824533\n",
      "Operator 109: 0.6240365278530804\n",
      "Operator 110: -0.14199356192569776\n",
      "Operator 111: -0.10850531673392594\n",
      "Operator 112: -1.4702454200232093\n",
      "Operator 113: -1.4691498412771327\n",
      "Operator 114: -0.3155710000858044\n",
      "Operator 115: -0.040226367859452924\n",
      "Operator 116: -1.5855523811840624\n",
      "Operator 117: -1.2339056441282992\n",
      "Operator 118: 0.7899935211718357\n",
      "Operator 120: 0.022753159431178727\n",
      "Operator 121: 0.30269216227353235\n",
      "Operator 122: -0.6081490642652083\n",
      "Operator 123: 0.20947263461946702\n",
      "Operator 124: -0.3708957257721644\n",
      "Operator 125: -1.4699989009762853\n",
      "Operator 126: -1.4605026378911858\n",
      "Operator 127: -0.11464365549966116\n",
      "Operator 128: -0.29780855965486813\n",
      "Operator 129: -1.3093664209120266\n",
      "Operator 130: -1.5484541323595593\n",
      "Operator 131: 0.35578063777676405\n",
      "Operator 132: 0.2546571291345143\n",
      "Operator 133: -0.4334739763836303\n",
      "Operator 135: -0.8296513397807825\n",
      "Operator 136: 0.11849185957921962\n",
      "Operator 137: -0.5481895209462487\n",
      "Operator 138: 0.5533934231470662\n",
      "Operator 139: 1.1428143959706623\n",
      "Operator 140: 0.32252985711756244\n",
      "Operator 141: -0.3714337377362096\n",
      "Operator 142: 0.5006582834919129\n",
      "Operator 143: 1.1347157008816389\n",
      "Operator 144: 0.26128964018204565\n",
      "Operator 145: 0.4330173022740801\n",
      "Operator 146: -0.13311211480317098\n",
      "Operator 147: 0.9599258718816799\n",
      "Operator 148: 0.7454097051072629\n",
      "Operator 149: 0.11849257162482602\n",
      "Operator 150: 0.5437895856002167\n",
      "Operator 151: 0.48625648432649005\n",
      "Operator 152: 0.4900887376670879\n",
      "Operator 153: 0.3127974790246379\n",
      "Operator 154: 0.3894814078279969\n",
      "Operator 155: 0.4294810916065007\n",
      "Operator 156: 0.6355869873532284\n",
      "Operator 157: 0.1458409385615622\n",
      "Operator 158: -1.061674451895441\n",
      "Operator 159: 0.3839633541273882\n",
      "Operator 160: -0.5766790581522632\n",
      "Operator 161: 0.20892876008155217\n",
      "Operator 162: -0.4661806064991635\n",
      "Operator 163: 0.32339679690894596\n",
      "Operator 164: -0.22533413521259416\n",
      "Operator 165: 0.17338577617573445\n",
      "Operator 166: -0.03473891369370596\n",
      "Operator 167: 0.14104527466092928\n",
      "Operator 168: -0.19702191824427595\n",
      "Operator 169: 0.13342070460158767\n",
      "Operator 170: -0.10548952558120372\n",
      "Operator 171: 0.41450238054494315\n",
      "Operator 172: -0.21317509404557294\n",
      "Operator 173: -0.048146711794054387\n",
      "Operator 174: -0.8141167708458752\n",
      "Operator 175: -0.24266215858458579\n",
      "Operator 176: -0.44137646083409\n",
      "Operator 177: 0.304441750783697\n",
      "Operator 178: 0.30819336542811504\n",
      "Operator 179: -0.2354663151172612\n",
      "Operator 180: -0.37647887142082037\n",
      "Operator 181: 0.33654063050438465\n",
      "Operator 182: 0.3235060622913927\n",
      "Operator 183: 0.051303535137098746\n",
      "Operator 184: -0.759522169615203\n",
      "Operator 185: 0.35336611673293533\n",
      "Operator 186: -1.3288254989873103\n",
      "Operator 187: 0.834600070458088\n",
      "Operator 188: 0.23824758590950695\n",
      "Operator 189: -0.13468339361547002\n",
      "Operator 190: -0.18639089599357345\n",
      "Operator 191: 0.40204854581246985\n",
      "Operator 192: 0.32203803852514606\n",
      "Operator 193: -0.13038557622533653\n",
      "Operator 194: -0.3936143893301046\n",
      "Operator 195: 0.6552913837816148\n",
      "Operator 196: -0.46806456934058643\n",
      "Operator 197: 0.5832558450581258\n",
      "Operator 198: 0.09085069553385625\n",
      "Operator 199: 1.2766469424837625\n",
      "Operator 200: -0.5341684030898842\n",
      "Total gradient norm: 8.910757222864758\n",
      "Operators under consideration (1):\n",
      "[116]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.5855523811840624)]\n",
      "Operator(s) added to ansatz: [116]\n",
      "Gradients: [np.float64(-1.5855523811840624)]\n",
      "Initial energy: -25.95468757669456\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116]...\n",
      "Starting point: [np.float64(0.5314352625488867), np.float64(0.14403305189920312), np.float64(0.3857711205051941), np.float64(-0.1700452663587867), np.float64(0.2768132048409537), np.float64(0.16406144250716548), np.float64(-0.6569317649202291), np.float64(0.5027307425204459), np.float64(-0.32372353402661797), np.float64(0.3349244189324452), np.float64(-0.30242910718947397), np.float64(-0.30350796584497536), np.float64(0.24562396508484927), np.float64(-0.21591586356732384), np.float64(0.16418701308366418), np.float64(0.15977113448156416), np.float64(-0.14423122929446724), np.float64(-0.12986543358948024), np.float64(0.17238070168772648), np.float64(0.16865513591579687), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -26.041483\n",
      "         Iterations: 30\n",
      "         Function evaluations: 41\n",
      "         Gradient evaluations: 41\n",
      "\n",
      "Current energy: -26.041483089617824\n",
      "(change of -0.08679551292326337)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116]\n",
      "On iteration 21.\n",
      "\n",
      "*** ADAPT-VQE Iteration 22 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.3101573745444212\n",
      "Operator 2: -0.8669357718803588\n",
      "Operator 3: 0.9688531423476164\n",
      "Operator 4: -0.09314215162432385\n",
      "Operator 5: 0.22211831403389537\n",
      "Operator 6: 0.04241296867229652\n",
      "Operator 7: 1.1372930113394237\n",
      "Operator 8: 0.0249463236841089\n",
      "Operator 9: 0.813552912712147\n",
      "Operator 10: -0.05824788083542032\n",
      "Operator 11: 0.10033596031672226\n",
      "Operator 13: -0.20965636357294623\n",
      "Operator 14: 0.4673363871704508\n",
      "Operator 15: -0.3022146665788879\n",
      "Operator 16: 0.1877697999386375\n",
      "Operator 17: -0.813376836189442\n",
      "Operator 18: 1.3341921302547657\n",
      "Operator 19: -0.8015753131943\n",
      "Operator 20: 0.2735208897333853\n",
      "Operator 21: -0.1330742231194808\n",
      "Operator 22: 0.44134826989974085\n",
      "Operator 23: -0.6402036562470186\n",
      "Operator 24: 0.09130733257635594\n",
      "Operator 25: -0.27451119374437694\n",
      "Operator 26: -0.7921518081060972\n",
      "Operator 27: -0.06280986731268504\n",
      "Operator 28: 0.24068864425884157\n",
      "Operator 29: 1.183711596087199\n",
      "Operator 30: 0.43053744219363155\n",
      "Operator 31: 0.07175126956489707\n",
      "Operator 32: -0.38553752330628976\n",
      "Operator 33: 0.2668366893932435\n",
      "Operator 34: -0.04434678443017495\n",
      "Operator 35: -0.4209015161033137\n",
      "Operator 36: -0.7264669271945723\n",
      "Operator 37: -0.5075327962457508\n",
      "Operator 38: -1.58933197998779\n",
      "Operator 39: 0.1670588800624121\n",
      "Operator 40: -0.21345394674429258\n",
      "Operator 41: -0.06621363131775959\n",
      "Operator 42: 0.4472951168619737\n",
      "Operator 43: 1.244992405545013\n",
      "Operator 44: 0.11311456841015079\n",
      "Operator 45: -0.010026750897206005\n",
      "Operator 46: -0.4894853071392546\n",
      "Operator 47: 0.4415700186519419\n",
      "Operator 48: -0.7243353701643356\n",
      "Operator 49: -0.40445287479657843\n",
      "Operator 50: -1.4173815864171964\n",
      "Operator 51: -0.2981587880140043\n",
      "Operator 52: -0.16758630884192205\n",
      "Operator 53: 0.198439918933463\n",
      "Operator 54: 0.1063218627121237\n",
      "Operator 55: 0.43594645849005265\n",
      "Operator 56: 0.4117394454346716\n",
      "Operator 57: 0.4449295278594372\n",
      "Operator 58: -0.04274997053678669\n",
      "Operator 59: 0.5835753804935098\n",
      "Operator 60: 0.38563505579433166\n",
      "Operator 61: 0.7616658252177749\n",
      "Operator 62: -0.009408756123205991\n",
      "Operator 63: 1.5648341165749726\n",
      "Operator 64: 0.18230320492654573\n",
      "Operator 65: 0.18012284846300392\n",
      "Operator 67: 0.33765549206553513\n",
      "Operator 69: 0.1349180777501334\n",
      "Operator 70: -0.9479456671604622\n",
      "Operator 71: 0.9749451078847973\n",
      "Operator 73: 0.09351214133505167\n",
      "Operator 74: -0.23263371392959628\n",
      "Operator 75: 1.0479160812471067\n",
      "Operator 76: -0.22859136684268724\n",
      "Operator 77: -0.010433873730459009\n",
      "Operator 78: 0.7717812091233123\n",
      "Operator 79: 0.019952213431573447\n",
      "Operator 80: 0.44648445900275824\n",
      "Operator 81: 0.47441258874377473\n",
      "Operator 82: 0.331571952003622\n",
      "Operator 83: -0.0784695316751034\n",
      "Operator 84: 0.39387312848825795\n",
      "Operator 85: 0.5350021776440228\n",
      "Operator 86: 0.3526593534901375\n",
      "Operator 87: -0.03731015765057924\n",
      "Operator 88: 0.847075671308853\n",
      "Operator 89: 0.49068085590709876\n",
      "Operator 90: 1.4829814124180416\n",
      "Operator 91: -0.7583643693582972\n",
      "Operator 92: -0.12460013377510341\n",
      "Operator 93: -0.3513266479442879\n",
      "Operator 94: 0.3310373271139828\n",
      "Operator 95: 1.134112823022524\n",
      "Operator 96: 0.4843500616796675\n",
      "Operator 97: -0.6392959917948344\n",
      "Operator 98: -0.41121446485743773\n",
      "Operator 99: 1.1362501874343138\n",
      "Operator 100: -0.1215504047484608\n",
      "Operator 101: 0.5056327180075642\n",
      "Operator 102: -0.078908173478699\n",
      "Operator 103: 0.6260502770601242\n",
      "Operator 104: -0.180872959831213\n",
      "Operator 106: 0.5378211033429939\n",
      "Operator 107: -0.7182075563577102\n",
      "Operator 108: -0.2485595514191239\n",
      "Operator 109: 0.62668821071691\n",
      "Operator 110: -0.1519492518430408\n",
      "Operator 111: -0.0856360327054229\n",
      "Operator 112: -1.4883031651656096\n",
      "Operator 113: -1.42383943378245\n",
      "Operator 114: -0.3054092589075693\n",
      "Operator 115: 0.1422455462903941\n",
      "Operator 117: -1.1246517313418716\n",
      "Operator 118: 0.770305705932298\n",
      "Operator 120: 0.03121144438444501\n",
      "Operator 121: 0.3121254140828853\n",
      "Operator 122: -0.6115685539833878\n",
      "Operator 123: 0.2162122027379838\n",
      "Operator 124: -0.37792149337842373\n",
      "Operator 125: -1.4473175076078058\n",
      "Operator 126: -1.4511458876921624\n",
      "Operator 127: 0.1674669352154347\n",
      "Operator 128: -0.2534037723662689\n",
      "Operator 129: -0.5287648806613093\n",
      "Operator 130: -0.7275093084492691\n",
      "Operator 131: 0.34419919491127554\n",
      "Operator 132: 0.23892757052020253\n",
      "Operator 133: -0.4012731781690816\n",
      "Operator 135: -0.8313637110497933\n",
      "Operator 136: 0.11776762012937836\n",
      "Operator 137: -0.5532300307941853\n",
      "Operator 138: 0.5538320441083839\n",
      "Operator 139: 1.1465881279949266\n",
      "Operator 140: 0.17850385787014975\n",
      "Operator 141: -0.5291683697443714\n",
      "Operator 142: -0.08406999234326676\n",
      "Operator 143: 1.1751580769396002\n",
      "Operator 144: 0.11161586678297702\n",
      "Operator 145: 0.45178271583158985\n",
      "Operator 146: -0.12145214809519118\n",
      "Operator 147: 0.9387522610313509\n",
      "Operator 148: 0.7481777306176676\n",
      "Operator 149: 0.11611553294865891\n",
      "Operator 150: 0.5509662726772094\n",
      "Operator 151: 0.47358815065539933\n",
      "Operator 152: 0.5084355447867197\n",
      "Operator 153: 0.2610703167402155\n",
      "Operator 154: 0.4095239357041587\n",
      "Operator 155: 0.44042780903100254\n",
      "Operator 156: -0.3861420702826365\n",
      "Operator 157: 0.056821546918784535\n",
      "Operator 158: -1.045530474298761\n",
      "Operator 159: 0.3569966878477777\n",
      "Operator 160: -0.5798378587586803\n",
      "Operator 161: 0.20904537750306912\n",
      "Operator 162: -0.4660849193047881\n",
      "Operator 163: 0.32686850264648537\n",
      "Operator 164: -0.22587346574750555\n",
      "Operator 165: 0.16997876883784718\n",
      "Operator 166: -0.030993150861351078\n",
      "Operator 167: 0.21781853783163363\n",
      "Operator 168: -0.3940770679851826\n",
      "Operator 169: 0.15985020972437014\n",
      "Operator 170: -0.04182760059467615\n",
      "Operator 171: 0.39419068931049306\n",
      "Operator 172: -0.19281631523574144\n",
      "Operator 173: -0.06623621347240327\n",
      "Operator 174: -0.8157062144358546\n",
      "Operator 175: -0.24090385183824503\n",
      "Operator 176: -0.4456616129537365\n",
      "Operator 177: 0.31406329985138814\n",
      "Operator 178: 0.35107807074188563\n",
      "Operator 179: -0.21121650734604436\n",
      "Operator 180: -0.44041829634286095\n",
      "Operator 181: -0.16237693930251706\n",
      "Operator 182: 0.1468375677648757\n",
      "Operator 183: -0.010240658124509507\n",
      "Operator 184: -0.816798569743423\n",
      "Operator 185: 0.3288447586348917\n",
      "Operator 186: -1.3668444010199465\n",
      "Operator 187: 0.835216248547126\n",
      "Operator 188: 0.23668121564430714\n",
      "Operator 189: -0.113735947654435\n",
      "Operator 190: -0.2136156494549105\n",
      "Operator 191: 0.4410260336491283\n",
      "Operator 192: 0.11567806056533411\n",
      "Operator 193: 0.49214294718144547\n",
      "Operator 194: -0.44215818515303273\n",
      "Operator 195: 0.6451230782244175\n",
      "Operator 196: -0.36295899636003004\n",
      "Operator 197: 0.6558186054641089\n",
      "Operator 198: 0.0894084603184432\n",
      "Operator 199: 1.3163537918621255\n",
      "Operator 200: -0.5362711055068043\n",
      "Total gradient norm: 8.262984066828642\n",
      "Operators under consideration (1):\n",
      "[38]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.58933197998779)]\n",
      "Operator(s) added to ansatz: [38]\n",
      "Gradients: [np.float64(-1.58933197998779)]\n",
      "Initial energy: -26.041483089617824\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38]...\n",
      "Starting point: [np.float64(0.5424869820888494), np.float64(0.1438347122821669), np.float64(0.4077942498344347), np.float64(-0.14465746760265724), np.float64(0.27080463138435634), np.float64(0.16351517922137246), np.float64(-0.6619690865952992), np.float64(0.5030027185613469), np.float64(-0.39099252367324494), np.float64(0.33567396187577514), np.float64(-0.31122008538374335), np.float64(-0.3068746510482283), np.float64(0.24636851796770612), np.float64(-0.21383505098787217), np.float64(0.16256447194592172), np.float64(0.16008915591772183), np.float64(-0.14402267026600696), np.float64(-0.1332571369282511), np.float64(0.17144874153590592), np.float64(0.1767607031519247), np.float64(0.1093182318000766), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.154466\n",
      "         Iterations: 30\n",
      "         Function evaluations: 92\n",
      "         Gradient evaluations: 82\n",
      "\n",
      "Current energy: -26.154465554748885\n",
      "(change of -0.11298246513106136)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38]\n",
      "On iteration 22.\n",
      "\n",
      "*** ADAPT-VQE Iteration 23 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.3109144790737822\n",
      "Operator 1: 1.8185354056304557e-08\n",
      "Operator 2: -0.8626831163180309\n",
      "Operator 3: 0.9747069476432291\n",
      "Operator 4: -0.10169714750435496\n",
      "Operator 5: 0.22013787155324133\n",
      "Operator 6: 0.06211154236308249\n",
      "Operator 7: 1.1831508528736046\n",
      "Operator 8: 0.027497582471422893\n",
      "Operator 9: 0.6670682131069403\n",
      "Operator 10: 0.02497739577850633\n",
      "Operator 11: 0.25100863120390016\n",
      "Operator 13: -0.20967623310409597\n",
      "Operator 14: 0.46727810625311905\n",
      "Operator 15: -0.3031974587649354\n",
      "Operator 16: 0.18829947690841042\n",
      "Operator 17: -0.8128590634832065\n",
      "Operator 18: 1.3107206271880973\n",
      "Operator 19: -0.8073761133168998\n",
      "Operator 20: 0.27275194221476695\n",
      "Operator 21: -0.10723755390076009\n",
      "Operator 22: 0.37896700832674146\n",
      "Operator 23: -0.6129876490671525\n",
      "Operator 24: 0.20496701540629578\n",
      "Operator 25: -0.20391564606298418\n",
      "Operator 26: -0.792424183636798\n",
      "Operator 27: -0.06086696748042622\n",
      "Operator 28: 0.23468007540299612\n",
      "Operator 29: 1.1909787677404662\n",
      "Operator 30: 0.42855052520751363\n",
      "Operator 31: 0.09673445064643346\n",
      "Operator 32: -0.42735119240377695\n",
      "Operator 33: 0.26655371111032455\n",
      "Operator 34: -0.1428128870905196\n",
      "Operator 35: -0.5052856379618407\n",
      "Operator 36: -0.8735298010071411\n",
      "Operator 37: -0.13564688143576162\n",
      "Operator 38: -2.277203172695863e-07\n",
      "Operator 39: 0.16840133737356028\n",
      "Operator 40: -0.21553279882450788\n",
      "Operator 41: -0.06209049434402476\n",
      "Operator 42: 0.4349924717232154\n",
      "Operator 43: 1.2644015304805958\n",
      "Operator 44: 0.0719514129565708\n",
      "Operator 45: 0.02445818294201561\n",
      "Operator 46: -0.6132215066137904\n",
      "Operator 47: 0.4277587701995213\n",
      "Operator 48: -0.9698112061486324\n",
      "Operator 49: -0.4794648066416079\n",
      "Operator 50: 0.22506745756303603\n",
      "Operator 51: 0.08362966748579709\n",
      "Operator 52: -0.1689230849975693\n",
      "Operator 53: 0.20070494012401324\n",
      "Operator 54: 0.10195327708029422\n",
      "Operator 55: 0.44456130790088944\n",
      "Operator 56: 0.40045705485697125\n",
      "Operator 57: 0.4818206434866167\n",
      "Operator 58: -0.07302086616299625\n",
      "Operator 59: 0.6916777368451685\n",
      "Operator 60: 0.42014149621279717\n",
      "Operator 61: 1.0496836567350223\n",
      "Operator 62: 0.22191785719972845\n",
      "Operator 63: -0.19098812078492208\n",
      "Operator 64: -0.3321199568141372\n",
      "Operator 65: 0.18021564754501235\n",
      "Operator 66: 1.7683870537053504e-08\n",
      "Operator 67: 0.33814253951884343\n",
      "Operator 68: 3.2611659388257385e-08\n",
      "Operator 69: 0.13531334114865745\n",
      "Operator 70: -0.9477879961409554\n",
      "Operator 71: 0.9764448532277392\n",
      "Operator 73: 0.10027834679604081\n",
      "Operator 74: -0.221900763758773\n",
      "Operator 75: 0.9466287571946472\n",
      "Operator 76: -0.11918130545777833\n",
      "Operator 77: 0.058085826450565925\n",
      "Operator 78: 0.772259036711336\n",
      "Operator 79: 0.018428809208066332\n",
      "Operator 80: 0.4479152915678125\n",
      "Operator 81: 0.47189423989131196\n",
      "Operator 82: 0.3371680488421154\n",
      "Operator 83: -0.09709606704086307\n",
      "Operator 84: 0.41249221952097515\n",
      "Operator 85: 0.5432549801677646\n",
      "Operator 86: 0.5000934290907193\n",
      "Operator 87: 0.227279693687943\n",
      "Operator 88: 0.9309798649399027\n",
      "Operator 89: 0.014861758185053031\n",
      "Operator 90: -0.12324653302819222\n",
      "Operator 91: -0.7588756950883495\n",
      "Operator 92: -0.12394696374300165\n",
      "Operator 93: -0.354188589653736\n",
      "Operator 94: 0.3328703134563755\n",
      "Operator 95: 1.13294491272521\n",
      "Operator 96: 0.4982995493003468\n",
      "Operator 97: -0.6624465094938619\n",
      "Operator 98: -0.4056689454762866\n",
      "Operator 99: 1.1144866238098903\n",
      "Operator 100: -0.03741017589347408\n",
      "Operator 101: 0.6271645030883263\n",
      "Operator 102: -0.18696466244334603\n",
      "Operator 103: -0.07206970991589223\n",
      "Operator 104: -0.18095752193694056\n",
      "Operator 105: -4.062488413332886e-08\n",
      "Operator 106: 0.5382866448190342\n",
      "Operator 107: -0.5699734104447953\n",
      "Operator 108: -0.15327721195310318\n",
      "Operator 109: 0.627288716816847\n",
      "Operator 110: -0.15425953722148966\n",
      "Operator 111: -0.08073823729696206\n",
      "Operator 112: -1.4929002621888814\n",
      "Operator 113: -1.4172230250608555\n",
      "Operator 114: -0.34108893827708764\n",
      "Operator 115: 0.17554662212854877\n",
      "Operator 116: -1.517452775630715e-08\n",
      "Operator 117: -1.067611547617823\n",
      "Operator 118: 0.6007255205616575\n",
      "Operator 119: -0.12487054438877716\n",
      "Operator 120: 0.24345809439190785\n",
      "Operator 121: 0.30244073348765865\n",
      "Operator 122: -0.6123238018862186\n",
      "Operator 123: 0.21786729562189122\n",
      "Operator 124: -0.37951229908281986\n",
      "Operator 125: -1.4419475009314229\n",
      "Operator 126: -1.462074666198668\n",
      "Operator 127: 0.19074340485007388\n",
      "Operator 128: -0.2758353018715937\n",
      "Operator 129: -0.49757714195793035\n",
      "Operator 130: -0.7644371393136735\n",
      "Operator 131: 0.16871353322950638\n",
      "Operator 132: 0.22203815675492555\n",
      "Operator 133: -0.21743944252842423\n",
      "Operator 134: 0.09198294389014117\n",
      "Operator 135: -0.831740403812059\n",
      "Operator 136: 0.11757528286118846\n",
      "Operator 137: -0.5543998701146907\n",
      "Operator 138: 0.5538148193578999\n",
      "Operator 139: 1.1483017617582287\n",
      "Operator 140: 0.17295762866627268\n",
      "Operator 141: -0.5384933511253089\n",
      "Operator 142: -0.06995805940221703\n",
      "Operator 143: 1.1598793414738915\n",
      "Operator 144: 0.19203828837281567\n",
      "Operator 145: 0.5590146971251322\n",
      "Operator 146: -0.17155616451135866\n",
      "Operator 147: 0.17564218305926432\n",
      "Operator 148: 0.7487910812072847\n",
      "Operator 149: 0.11555770227311449\n",
      "Operator 150: 0.5525765137001766\n",
      "Operator 151: 0.4706918146636667\n",
      "Operator 152: 0.5148200913051673\n",
      "Operator 153: 0.2522550347536351\n",
      "Operator 154: 0.4255749142246972\n",
      "Operator 155: 0.4115512006958776\n",
      "Operator 156: -0.39363818663289585\n",
      "Operator 157: 0.05530960768036414\n",
      "Operator 158: -1.0106834436858882\n",
      "Operator 159: 0.23143542717207333\n",
      "Operator 160: -0.08671299693046325\n",
      "Operator 161: 0.2090716229101034\n",
      "Operator 162: -0.4660418439615174\n",
      "Operator 163: 0.32764418161716935\n",
      "Operator 164: -0.2259007498855587\n",
      "Operator 165: 0.16983481892801794\n",
      "Operator 166: -0.03202296247335738\n",
      "Operator 167: 0.2174551599982612\n",
      "Operator 168: -0.39020375376363436\n",
      "Operator 169: 0.1596602459571282\n",
      "Operator 170: -0.07143871654617172\n",
      "Operator 171: 0.3105208477952929\n",
      "Operator 172: -0.3107365956303123\n",
      "Operator 173: -0.03709971542322621\n",
      "Operator 174: -0.8160414729831444\n",
      "Operator 175: -0.24051153372910367\n",
      "Operator 176: -0.4467044926632328\n",
      "Operator 177: 0.31599554369053173\n",
      "Operator 178: 0.3474188406436537\n",
      "Operator 179: -0.2057189628986704\n",
      "Operator 180: -0.4509867637459427\n",
      "Operator 181: -0.15320324010468045\n",
      "Operator 182: 0.012147835705769508\n",
      "Operator 183: -0.06575927032931314\n",
      "Operator 184: -0.8569827426292198\n",
      "Operator 185: 0.2044578862281427\n",
      "Operator 186: 0.16705103929116483\n",
      "Operator 187: 0.835355434760302\n",
      "Operator 188: 0.23632728085163401\n",
      "Operator 189: -0.10908086517295085\n",
      "Operator 190: -0.2201439676769692\n",
      "Operator 191: 0.44099205296812916\n",
      "Operator 192: 0.11061555187095251\n",
      "Operator 193: 0.5207829638173029\n",
      "Operator 194: -0.49435486134064277\n",
      "Operator 195: 0.7038232986292363\n",
      "Operator 196: -0.28282988111344426\n",
      "Operator 197: 0.8143887333060029\n",
      "Operator 198: -0.011831411051703072\n",
      "Operator 199: -0.2137945365890488\n",
      "Operator 200: -0.5367536476439265\n",
      "Total gradient norm: 7.425634359431964\n",
      "Operators under consideration (1):\n",
      "[112]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.4929002621888814)]\n",
      "Operator(s) added to ansatz: [112]\n",
      "Gradients: [np.float64(-1.4929002621888814)]\n",
      "Initial energy: -26.154465554748885\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112]...\n",
      "Starting point: [np.float64(0.5974371127702667), np.float64(0.14378956489286854), np.float64(0.43739868000293897), np.float64(-0.14371199719921549), np.float64(0.26957419286186757), np.float64(0.16339101170755427), np.float64(-0.7419288269796833), np.float64(0.5030649121272953), np.float64(-0.4000904145231496), np.float64(0.33584645739823255), np.float64(-0.3136702695740473), np.float64(-0.30768455111382875), np.float64(0.23934855131580896), np.float64(-0.20530024957490436), np.float64(0.12310829760661184), np.float64(0.1776694278693482), np.float64(-0.14397546063501246), np.float64(-0.14994403322954852), np.float64(0.17124695427008618), np.float64(0.17434483954800442), np.float64(0.11122585274574406), np.float64(0.13509240071571313), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.234904\n",
      "         Iterations: 29\n",
      "         Function evaluations: 96\n",
      "         Gradient evaluations: 86\n",
      "\n",
      "Current energy: -26.234904252809905\n",
      "(change of -0.08043869806101966)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112]\n",
      "On iteration 23.\n",
      "\n",
      "*** ADAPT-VQE Iteration 24 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.30120900291461006\n",
      "Operator 1: 0.23021799402987322\n",
      "Operator 2: -0.03513775489542562\n",
      "Operator 3: 0.984720012468182\n",
      "Operator 4: -0.10959769946332967\n",
      "Operator 5: 0.22064285305281667\n",
      "Operator 6: 0.05709563270854514\n",
      "Operator 7: 1.1823742559968222\n",
      "Operator 8: 0.028295467507472638\n",
      "Operator 9: 0.6662853021350178\n",
      "Operator 10: 0.025737456630254618\n",
      "Operator 11: 0.25026361169709554\n",
      "Operator 12: 3.501842940067945e-07\n",
      "Operator 13: -0.2075343438657843\n",
      "Operator 14: 0.45973872377874825\n",
      "Operator 15: -0.30743384889892567\n",
      "Operator 16: 0.3245520821565529\n",
      "Operator 17: -0.25170000760539163\n",
      "Operator 18: 0.5651743824483422\n",
      "Operator 19: -0.7704722194356415\n",
      "Operator 20: 0.286698494363076\n",
      "Operator 21: -0.11198753655332164\n",
      "Operator 22: 0.3739596193879316\n",
      "Operator 23: -0.6084077104042543\n",
      "Operator 24: 0.20406796975106778\n",
      "Operator 25: -0.20312115318567692\n",
      "Operator 26: -0.8402033873511219\n",
      "Operator 27: 0.11005746697333192\n",
      "Operator 28: -0.36673884123327505\n",
      "Operator 29: 0.2521249157966766\n",
      "Operator 30: 0.27960516347038084\n",
      "Operator 31: 0.07463012277796707\n",
      "Operator 32: -0.4641872936201452\n",
      "Operator 33: 0.26506206856655123\n",
      "Operator 34: -0.15699143056656728\n",
      "Operator 35: -0.5068639854740924\n",
      "Operator 36: -0.8798573832835525\n",
      "Operator 37: -0.13562191969557263\n",
      "Operator 38: -4.481871984651775e-07\n",
      "Operator 39: 0.2342840039379116\n",
      "Operator 40: -0.35123127672323345\n",
      "Operator 41: -0.02110393956528288\n",
      "Operator 42: -0.15566800795880076\n",
      "Operator 43: 0.32237632479914047\n",
      "Operator 44: 0.014513452974026227\n",
      "Operator 45: 0.05101282011059158\n",
      "Operator 46: -0.6178804983058046\n",
      "Operator 47: 0.4194784293074323\n",
      "Operator 48: -0.9780620729866443\n",
      "Operator 49: -0.4813387120833742\n",
      "Operator 50: 0.22456124796248345\n",
      "Operator 51: 0.0833116172956993\n",
      "Operator 52: -0.23676137849295778\n",
      "Operator 53: 0.2230683694238812\n",
      "Operator 54: -0.026394788535472197\n",
      "Operator 55: 0.30572064012184136\n",
      "Operator 56: 0.4629779114181695\n",
      "Operator 57: 0.48492401104993826\n",
      "Operator 58: -0.0979835750114267\n",
      "Operator 59: 0.6969202625607462\n",
      "Operator 60: 0.42121777205593547\n",
      "Operator 61: 1.0558947087569956\n",
      "Operator 62: 0.22484721470613384\n",
      "Operator 63: -0.1909601369426056\n",
      "Operator 64: -0.33218270110610637\n",
      "Operator 65: 0.18210388785515688\n",
      "Operator 66: -3.2403374038631647e-07\n",
      "Operator 67: 0.4517658358506751\n",
      "Operator 69: 0.11470661082021191\n",
      "Operator 70: -0.2645062162354992\n",
      "Operator 71: 0.9313998644049835\n",
      "Operator 72: 3.0250837705311263e-07\n",
      "Operator 73: 0.11198986676413236\n",
      "Operator 74: -0.2193253830389355\n",
      "Operator 75: 0.9411094308602831\n",
      "Operator 76: -0.1182566049944277\n",
      "Operator 77: 0.05791066741727113\n",
      "Operator 78: 0.7328221260537385\n",
      "Operator 79: -0.19690703449329944\n",
      "Operator 80: 0.4445600419714133\n",
      "Operator 81: 0.4916149936041698\n",
      "Operator 82: 0.32014166623427376\n",
      "Operator 83: -0.0843984147821883\n",
      "Operator 84: 0.4486801597198564\n",
      "Operator 85: 0.5402045588990767\n",
      "Operator 86: 0.5111164560230992\n",
      "Operator 87: 0.22914071916342993\n",
      "Operator 88: 0.9369864868698548\n",
      "Operator 89: 0.014614196467668927\n",
      "Operator 90: -0.12326808849946413\n",
      "Operator 91: -0.8027393456327534\n",
      "Operator 92: -0.10693389123167256\n",
      "Operator 93: -0.5625885485278592\n",
      "Operator 94: -0.4273627062739448\n",
      "Operator 95: 1.1897721211664805\n",
      "Operator 96: 0.5154643474507536\n",
      "Operator 97: -0.6664398603790666\n",
      "Operator 98: -0.4051829267387116\n",
      "Operator 99: 1.113931248476944\n",
      "Operator 100: -0.035468561759341724\n",
      "Operator 101: 0.6285810620506402\n",
      "Operator 102: -0.18663289437440517\n",
      "Operator 103: -0.07327863613291849\n",
      "Operator 104: -0.18563774685104054\n",
      "Operator 105: -2.535204332570827e-07\n",
      "Operator 106: 0.5612557033233276\n",
      "Operator 107: -0.5681962777745779\n",
      "Operator 108: -0.15277015977065034\n",
      "Operator 109: 0.6855015503309325\n",
      "Operator 110: -0.14340331420044541\n",
      "Operator 111: 0.13514123424794192\n",
      "Operator 112: -7.45245972151495e-08\n",
      "Operator 113: -1.3207260042061262\n",
      "Operator 114: -0.32788410486203345\n",
      "Operator 115: 0.20739248055110823\n",
      "Operator 116: -1.5940592851715514e-07\n",
      "Operator 117: -1.0604035498313111\n",
      "Operator 118: 0.5977799864562248\n",
      "Operator 119: -0.12494369355644966\n",
      "Operator 120: 0.24332266449379403\n",
      "Operator 121: 0.30265198772402613\n",
      "Operator 122: -0.6077555480687598\n",
      "Operator 123: 0.502709824401271\n",
      "Operator 124: -0.3063421527772805\n",
      "Operator 125: -0.5974683781150859\n",
      "Operator 126: -0.7439896911720485\n",
      "Operator 127: 0.2194180639773452\n",
      "Operator 128: -0.3086649849704584\n",
      "Operator 129: -0.49179201784340265\n",
      "Operator 130: -0.7592558310297082\n",
      "Operator 131: 0.16971098149821262\n",
      "Operator 132: 0.22153628204731832\n",
      "Operator 133: -0.215914859583484\n",
      "Operator 134: 0.09202861915934824\n",
      "Operator 135: -0.8201463750557494\n",
      "Operator 136: 0.0336712392456993\n",
      "Operator 137: -0.736756870145493\n",
      "Operator 138: -0.05493564289765811\n",
      "Operator 139: 1.2246482699503791\n",
      "Operator 140: 0.007046207523405445\n",
      "Operator 141: -0.5589453417851629\n",
      "Operator 142: -0.055581095577089734\n",
      "Operator 143: 1.1576145444331492\n",
      "Operator 144: 0.19226933693568526\n",
      "Operator 145: 0.5592852641278457\n",
      "Operator 146: -0.17099594590858042\n",
      "Operator 147: 0.17411896607602906\n",
      "Operator 148: 0.7492836353685082\n",
      "Operator 149: 0.05984344119149991\n",
      "Operator 150: 0.5607568581429867\n",
      "Operator 151: 0.4958902402882469\n",
      "Operator 152: -0.37112093394190643\n",
      "Operator 153: 0.1860032183128666\n",
      "Operator 154: 0.44857636248506627\n",
      "Operator 155: 0.40561450268500693\n",
      "Operator 156: -0.396406443854163\n",
      "Operator 157: 0.05255586296566196\n",
      "Operator 158: -1.0099961914822786\n",
      "Operator 159: 0.22991108891510528\n",
      "Operator 160: -0.08616632321508966\n",
      "Operator 161: 0.2045688234028049\n",
      "Operator 162: -0.4533391573864416\n",
      "Operator 163: 0.4090847490194924\n",
      "Operator 164: -0.44260688610811194\n",
      "Operator 165: 0.2023588969043158\n",
      "Operator 166: 0.02350643601520398\n",
      "Operator 167: 0.2283381784896356\n",
      "Operator 168: -0.40284549184229734\n",
      "Operator 169: 0.1657500979325926\n",
      "Operator 170: -0.07066919312033183\n",
      "Operator 171: 0.3085233400242156\n",
      "Operator 172: -0.30979336650875966\n",
      "Operator 173: -0.03779155344968843\n",
      "Operator 174: -0.7711266244968269\n",
      "Operator 175: -0.23093926305337947\n",
      "Operator 176: -0.5165202351907131\n",
      "Operator 177: -0.16056288007186004\n",
      "Operator 178: 0.24360511207578897\n",
      "Operator 179: -0.2394202177328657\n",
      "Operator 180: -0.4712012455905536\n",
      "Operator 181: -0.1513976740347306\n",
      "Operator 182: 0.000490773554633353\n",
      "Operator 183: -0.06566502801243139\n",
      "Operator 184: -0.8625858146775632\n",
      "Operator 185: 0.20295493504820147\n",
      "Operator 186: 0.16718249139740404\n",
      "Operator 187: 0.8744514518380948\n",
      "Operator 188: 0.11101014304061092\n",
      "Operator 189: 0.5316524681344916\n",
      "Operator 190: -0.27372633780185107\n",
      "Operator 191: 0.4138495780429824\n",
      "Operator 192: 0.1545907204462424\n",
      "Operator 193: 0.5414135109741879\n",
      "Operator 194: -0.4819406682750373\n",
      "Operator 195: 0.7127211460397607\n",
      "Operator 196: -0.2815556705517639\n",
      "Operator 197: 0.8201510417373602\n",
      "Operator 198: -0.011416265874073933\n",
      "Operator 199: -0.21387779828285503\n",
      "Operator 200: -0.5539316336163809\n",
      "Total gradient norm: 6.651638559557051\n",
      "Operators under consideration (1):\n",
      "[113]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.3207260042061262)]\n",
      "Operator(s) added to ansatz: [113]\n",
      "Gradients: [np.float64(-1.3207260042061262)]\n",
      "Initial energy: -26.234904252809905\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113]...\n",
      "Starting point: [np.float64(0.5980956725119119), np.float64(0.14123462683084081), np.float64(0.43881026332910156), np.float64(-0.14298106350666323), np.float64(0.26787649873324965), np.float64(0.1392907499338683), np.float64(-0.7420926220409682), np.float64(0.5059552240158948), np.float64(-0.40230486423552453), np.float64(0.3432732899283768), np.float64(-0.32713103583997705), np.float64(-0.3757516344536838), np.float64(0.23857749694473843), np.float64(-0.2050610928716321), np.float64(0.12304700391110844), np.float64(0.17765072181687694), np.float64(-0.14212041910441459), np.float64(-0.15002689691647603), np.float64(0.17913961943270232), np.float64(0.17402467108479241), np.float64(0.1118755424056569), np.float64(0.13513178903006812), np.float64(0.10746899099144915), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -26.310740\n",
      "         Iterations: 33\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 50\n",
      "\n",
      "Current energy: -26.310740346714027\n",
      "(change of -0.07583609390412249)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113]\n",
      "On iteration 24.\n",
      "\n",
      "*** ADAPT-VQE Iteration 25 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.2798028541778351\n",
      "Operator 1: 0.24711438504939226\n",
      "Operator 2: -0.2610726452540115\n",
      "Operator 3: 0.04293417081503936\n",
      "Operator 4: -0.10191002345179734\n",
      "Operator 5: 0.23467193487360194\n",
      "Operator 6: 0.033436812861994715\n",
      "Operator 7: 1.1842653725731647\n",
      "Operator 8: 0.030641997468421778\n",
      "Operator 9: 0.6661385223273848\n",
      "Operator 10: 0.02602222444538782\n",
      "Operator 11: 0.24995988577515055\n",
      "Operator 13: -0.2053706300067611\n",
      "Operator 14: 0.46243815126932886\n",
      "Operator 15: -0.2933392428326973\n",
      "Operator 16: 0.31486828200055283\n",
      "Operator 17: -0.43924129988992733\n",
      "Operator 18: 0.17836409419089372\n",
      "Operator 19: -0.019188327459231283\n",
      "Operator 20: 0.2661603629287257\n",
      "Operator 21: -0.10747654407743784\n",
      "Operator 22: 0.38089511997736686\n",
      "Operator 23: -0.6053652087155567\n",
      "Operator 24: 0.20372386691016578\n",
      "Operator 25: -0.20279146885138566\n",
      "Operator 26: -0.8332965834412311\n",
      "Operator 27: 0.05621488837325215\n",
      "Operator 28: -0.1263167475503058\n",
      "Operator 29: -0.2965868047078112\n",
      "Operator 30: -0.4693628392023471\n",
      "Operator 31: -0.04573434581597534\n",
      "Operator 32: -0.5103512885069588\n",
      "Operator 33: 0.26003037222763953\n",
      "Operator 34: -0.16169381818930148\n",
      "Operator 35: -0.5062249609804432\n",
      "Operator 36: -0.882305846959957\n",
      "Operator 37: -0.13551664603743324\n",
      "Operator 38: -2.505298102306597e-07\n",
      "Operator 39: 0.19855247640009993\n",
      "Operator 40: -0.2930450377352346\n",
      "Operator 41: -0.1345753418054682\n",
      "Operator 42: -0.05808195772137576\n",
      "Operator 43: -0.24856784683299377\n",
      "Operator 44: -0.7015387990863411\n",
      "Operator 45: 0.01706698065550223\n",
      "Operator 46: -0.6244451242611586\n",
      "Operator 47: 0.43648170315583973\n",
      "Operator 48: -0.9819149140589059\n",
      "Operator 49: -0.48192345286782634\n",
      "Operator 50: 0.22430262667209444\n",
      "Operator 51: 0.08317453864447649\n",
      "Operator 52: -0.20151111005065264\n",
      "Operator 53: 0.13775770826809675\n",
      "Operator 54: 0.06367611010129304\n",
      "Operator 55: 0.3344135276013812\n",
      "Operator 56: 0.4003533799591396\n",
      "Operator 57: 0.5257296899287525\n",
      "Operator 58: -0.15361395529905175\n",
      "Operator 59: 0.6811919806457329\n",
      "Operator 60: 0.41599633121351\n",
      "Operator 61: 1.0598356994883602\n",
      "Operator 62: 0.2257691287223781\n",
      "Operator 63: -0.19086091218010548\n",
      "Operator 64: -0.3320917809525983\n",
      "Operator 65: 0.17862496952663362\n",
      "Operator 67: 0.4547971564098891\n",
      "Operator 69: 0.09703671737920191\n",
      "Operator 70: -0.27402095557133294\n",
      "Operator 71: 0.28515577085692956\n",
      "Operator 73: 0.09320207400472842\n",
      "Operator 74: -0.23302649910282003\n",
      "Operator 75: 0.9376827775141348\n",
      "Operator 76: -0.1179425745875812\n",
      "Operator 77: 0.05784151718392483\n",
      "Operator 78: 0.7072556152048532\n",
      "Operator 79: -0.1742515507013765\n",
      "Operator 80: 0.3991770673335039\n",
      "Operator 81: 0.5042891140700646\n",
      "Operator 82: 0.2867354770146081\n",
      "Operator 83: -0.10911593595643042\n",
      "Operator 84: 0.46251194321482725\n",
      "Operator 85: 0.5455461031929928\n",
      "Operator 86: 0.5175680335499191\n",
      "Operator 87: 0.22893432562051355\n",
      "Operator 88: 0.9392984813433412\n",
      "Operator 89: 0.014501792500573207\n",
      "Operator 90: -0.12323027912330087\n",
      "Operator 91: -0.7898625984785872\n",
      "Operator 92: -0.1284656198291436\n",
      "Operator 93: -0.4899427947548687\n",
      "Operator 94: -0.4287520936529092\n",
      "Operator 95: 0.4875178859935075\n",
      "Operator 96: 0.5490902828087509\n",
      "Operator 97: -0.6555252719536095\n",
      "Operator 98: -0.41255458800168165\n",
      "Operator 99: 1.1075721133191707\n",
      "Operator 100: -0.03433368333264196\n",
      "Operator 101: 0.6289171764769697\n",
      "Operator 102: -0.18646706732170218\n",
      "Operator 103: -0.07368382229126541\n",
      "Operator 104: -0.18291027440581653\n",
      "Operator 106: 0.5465669437673333\n",
      "Operator 107: -0.5674493244146199\n",
      "Operator 108: -0.1525410566900689\n",
      "Operator 109: 0.6716488748317928\n",
      "Operator 110: -0.05503768458749466\n",
      "Operator 111: 0.00016694263552277883\n",
      "Operator 114: -0.27269514751917456\n",
      "Operator 115: 0.28883447309565125\n",
      "Operator 116: 3.824410708215195e-08\n",
      "Operator 117: -1.0559446093297278\n",
      "Operator 118: 0.5957272126576032\n",
      "Operator 119: -0.12494735576879767\n",
      "Operator 120: 0.24315786772300219\n",
      "Operator 121: 0.30262097392420273\n",
      "Operator 122: -0.5798838551333374\n",
      "Operator 123: 0.4870251744782229\n",
      "Operator 124: -0.24747348075115033\n",
      "Operator 125: -0.6299613716572119\n",
      "Operator 126: 0.04895456158802865\n",
      "Operator 127: 0.7632181347787442\n",
      "Operator 128: -0.2605693412933209\n",
      "Operator 129: -0.5007143530901036\n",
      "Operator 130: -0.771884653127455\n",
      "Operator 131: 0.17053630641487333\n",
      "Operator 132: 0.22130152226366184\n",
      "Operator 133: -0.21531502212139642\n",
      "Operator 134: 0.09201075847872531\n",
      "Operator 135: -0.8034960818172886\n",
      "Operator 136: 0.02716801073090588\n",
      "Operator 137: -0.7172604936236917\n",
      "Operator 138: 0.0012133491779261034\n",
      "Operator 139: 0.6823384779516926\n",
      "Operator 140: 0.10289575571670531\n",
      "Operator 141: -0.7562324919489318\n",
      "Operator 142: -0.0720334061829209\n",
      "Operator 143: 1.1546845426543226\n",
      "Operator 144: 0.1956900568969403\n",
      "Operator 145: 0.5591922359962265\n",
      "Operator 146: -0.17078202979868862\n",
      "Operator 147: 0.17354305217911092\n",
      "Operator 148: 0.7272125917674894\n",
      "Operator 149: 0.0693158214944026\n",
      "Operator 150: 0.5123758221487579\n",
      "Operator 151: 0.5153508583629879\n",
      "Operator 152: -0.3069504224353544\n",
      "Operator 153: -0.5862629247219082\n",
      "Operator 154: 0.4084806772992852\n",
      "Operator 155: 0.40797668874764237\n",
      "Operator 156: -0.37642698746927683\n",
      "Operator 157: 0.051910088866714076\n",
      "Operator 158: -1.0098878744834157\n",
      "Operator 159: 0.22932064094621604\n",
      "Operator 160: -0.08595471652799203\n",
      "Operator 161: 0.20181861210709512\n",
      "Operator 162: -0.45440500340589385\n",
      "Operator 163: 0.41235599495581426\n",
      "Operator 164: -0.4509974236140165\n",
      "Operator 165: 0.2137694645527176\n",
      "Operator 166: 0.0008074310165811231\n",
      "Operator 167: 0.17991927241626826\n",
      "Operator 168: -0.43950591764865726\n",
      "Operator 169: 0.18590818608699683\n",
      "Operator 170: -0.07270267342143569\n",
      "Operator 171: 0.3059343519835404\n",
      "Operator 172: -0.3094722224880564\n",
      "Operator 173: -0.03807114935693612\n",
      "Operator 174: -0.749118463131394\n",
      "Operator 175: -0.2415449261797114\n",
      "Operator 176: -0.49400886748733486\n",
      "Operator 177: -0.17786008644044154\n",
      "Operator 178: -0.19890417125819887\n",
      "Operator 179: -0.3831420844904404\n",
      "Operator 180: -0.5073154307290151\n",
      "Operator 181: -0.15161439225860263\n",
      "Operator 182: -0.0072061533645174924\n",
      "Operator 183: -0.06467815102134263\n",
      "Operator 184: -0.8653068461388211\n",
      "Operator 185: 0.20240806958285457\n",
      "Operator 186: 0.16716594023450515\n",
      "Operator 187: 0.8702562265511078\n",
      "Operator 188: 0.11218335514511235\n",
      "Operator 189: 0.3402219256551332\n",
      "Operator 190: 0.40420211627472613\n",
      "Operator 191: 0.34565545514573914\n",
      "Operator 192: 0.171809433612313\n",
      "Operator 193: 0.5621166214453415\n",
      "Operator 194: -0.4574272421410375\n",
      "Operator 195: 0.7125813430010995\n",
      "Operator 196: -0.2828663843177035\n",
      "Operator 197: 0.8230302715166631\n",
      "Operator 198: -0.011318927359718955\n",
      "Operator 199: -0.21382013784385467\n",
      "Operator 200: -0.5376752334660291\n",
      "Total gradient norm: 6.190479345909763\n",
      "Operators under consideration (1):\n",
      "[7]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.1842653725731647)]\n",
      "Operator(s) added to ansatz: [7]\n",
      "Gradients: [np.float64(1.1842653725731647)]\n",
      "Initial energy: -26.310740346714027\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7]...\n",
      "Starting point: [np.float64(0.5983391012643394), np.float64(0.14242552381867687), np.float64(0.43933671158381016), np.float64(-0.15213803812735072), np.float64(0.3523695970345312), np.float64(0.1400601512179225), np.float64(-0.7421527716481324), np.float64(0.5041025096709723), np.float64(-0.4022816823964681), np.float64(0.33777221728372575), np.float64(-0.3227140006681096), np.float64(-0.36418673371685784), np.float64(0.2381946650661349), np.float64(-0.2049132367230086), np.float64(0.12302865620321889), np.float64(0.17763943726130224), np.float64(-0.14349930902106703), np.float64(-0.15002411615123593), np.float64(0.18684291138704895), np.float64(0.17175289639482438), np.float64(0.11093232589577165), np.float64(0.13514219254151044), np.float64(0.10438042438024932), np.float64(0.11408274904461559), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -26.388105\n",
      "         Iterations: 35\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 50\n",
      "\n",
      "Current energy: -26.38810512449006\n",
      "(change of -0.07736477777603312)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7]\n",
      "On iteration 25.\n",
      "\n",
      "*** ADAPT-VQE Iteration 26 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.2778449550359098\n",
      "Operator 1: 0.2485640101906082\n",
      "Operator 2: -0.26360536870135254\n",
      "Operator 3: 0.05121061244453984\n",
      "Operator 4: -0.08363111960982253\n",
      "Operator 5: 0.23507601641554787\n",
      "Operator 6: -0.17411338755798544\n",
      "Operator 8: 0.056374347563356575\n",
      "Operator 9: 0.7029953931955746\n",
      "Operator 10: 0.018037402272226832\n",
      "Operator 11: 0.25078145199167373\n",
      "Operator 13: -0.20523083219103744\n",
      "Operator 14: 0.46256875841627987\n",
      "Operator 15: -0.2911635104185093\n",
      "Operator 16: 0.3138188808988621\n",
      "Operator 17: -0.4394613909879755\n",
      "Operator 18: 0.18557843564492515\n",
      "Operator 19: -0.022655830839115893\n",
      "Operator 20: 0.28935342142430065\n",
      "Operator 21: -0.34133212388784584\n",
      "Operator 22: 0.11310824118524733\n",
      "Operator 23: 0.01690486717243847\n",
      "Operator 24: 0.21440156961780285\n",
      "Operator 25: -0.2039728118881904\n",
      "Operator 26: -0.8328683750616146\n",
      "Operator 27: 0.051758375332837225\n",
      "Operator 28: -0.11725823229271207\n",
      "Operator 29: -0.29217845405025106\n",
      "Operator 30: -0.4750822386582163\n",
      "Operator 31: -0.09596387636580145\n",
      "Operator 32: -0.25097564579132403\n",
      "Operator 33: -0.39138862839995303\n",
      "Operator 34: -0.6771165243392975\n",
      "Operator 35: -0.6443843249180032\n",
      "Operator 36: -0.8738132162554872\n",
      "Operator 37: -0.13984811758005164\n",
      "Operator 38: -1.1408967992565522e-08\n",
      "Operator 39: 0.19548784146242268\n",
      "Operator 40: -0.2886972601002985\n",
      "Operator 41: -0.14489564415425976\n",
      "Operator 42: -0.03566011559100132\n",
      "Operator 43: -0.27448904704539645\n",
      "Operator 44: -0.6256119299099215\n",
      "Operator 45: -0.09131291356777127\n",
      "Operator 46: -0.5067286239256101\n",
      "Operator 47: -0.27495862566970497\n",
      "Operator 48: -1.3323627958752713\n",
      "Operator 49: -0.4827411733932267\n",
      "Operator 50: 0.2251163787608561\n",
      "Operator 51: 0.08357458543620339\n",
      "Operator 52: -0.19849178844139836\n",
      "Operator 53: 0.13119447495191125\n",
      "Operator 54: 0.07329923362052006\n",
      "Operator 55: 0.31697592004492425\n",
      "Operator 56: 0.42834579848885657\n",
      "Operator 57: 0.4686641018526948\n",
      "Operator 58: 0.001720725471545526\n",
      "Operator 59: 0.6644297114509476\n",
      "Operator 60: 0.3856499896901443\n",
      "Operator 61: 1.017334625660404\n",
      "Operator 62: 0.21749293609579692\n",
      "Operator 63: -0.19400360193916566\n",
      "Operator 64: -0.33262560840849487\n",
      "Operator 65: 0.17835075688754687\n",
      "Operator 67: 0.4550973643915438\n",
      "Operator 69: 0.09533222076164297\n",
      "Operator 70: -0.2786498840578449\n",
      "Operator 71: 0.25299002147181526\n",
      "Operator 73: 0.07688887955596396\n",
      "Operator 74: -0.13235989772288254\n",
      "Operator 75: 0.27999980775052735\n",
      "Operator 76: -0.15780468495096084\n",
      "Operator 77: 0.05785956394350378\n",
      "Operator 78: 0.705101204782105\n",
      "Operator 79: -0.1723344793889986\n",
      "Operator 80: 0.3952897399560025\n",
      "Operator 81: 0.5066778563879113\n",
      "Operator 82: 0.30859237802619544\n",
      "Operator 83: -0.014109880996202483\n",
      "Operator 84: 0.4193190276937876\n",
      "Operator 85: 0.5437637796365274\n",
      "Operator 86: 0.40519537131736505\n",
      "Operator 87: 0.2707084897949317\n",
      "Operator 88: 0.9330513865584109\n",
      "Operator 89: 0.016719343190586544\n",
      "Operator 90: -0.123511328521408\n",
      "Operator 91: -0.7888317750822338\n",
      "Operator 92: -0.13024080979470126\n",
      "Operator 93: -0.4849819904886591\n",
      "Operator 94: -0.43027808163628245\n",
      "Operator 95: 0.48529123423439663\n",
      "Operator 96: 0.5550784634891499\n",
      "Operator 97: -0.500992704938283\n",
      "Operator 98: -0.42337898747169156\n",
      "Operator 99: 0.4875881252418408\n",
      "Operator 100: -0.14683284145315761\n",
      "Operator 101: 0.6433914390764284\n",
      "Operator 102: -0.18733575124832097\n",
      "Operator 103: -0.0705548609089347\n",
      "Operator 104: -0.18270622447636511\n",
      "Operator 106: 0.5454239466856279\n",
      "Operator 107: -0.5701423224654878\n",
      "Operator 108: -0.15373259643775228\n",
      "Operator 109: 0.6705508008977883\n",
      "Operator 110: -0.04782906734060251\n",
      "Operator 111: -0.009306314522978682\n",
      "Operator 114: -0.24058667972419223\n",
      "Operator 115: 0.08122213840841534\n",
      "Operator 116: 0.04666032048427174\n",
      "Operator 117: -0.05490334781916422\n",
      "Operator 118: 0.6837505507342811\n",
      "Operator 119: -0.12514836318853512\n",
      "Operator 120: 0.24756045428180787\n",
      "Operator 121: 0.3029806680397291\n",
      "Operator 122: -0.5774871453564798\n",
      "Operator 123: 0.48567888053666386\n",
      "Operator 124: -0.24214396904705038\n",
      "Operator 125: -0.6401989111673758\n",
      "Operator 126: 0.037965726924661936\n",
      "Operator 127: 0.6605652501705743\n",
      "Operator 128: -0.21403195205713726\n",
      "Operator 129: -0.442692742493292\n",
      "Operator 130: 0.1052344912878696\n",
      "Operator 131: 0.34107364741375945\n",
      "Operator 132: 0.23857814938087385\n",
      "Operator 133: -0.2170114546030851\n",
      "Operator 134: 0.09218923770076137\n",
      "Operator 135: -0.8021105420763222\n",
      "Operator 136: 0.026654204104675228\n",
      "Operator 137: -0.7157440582230521\n",
      "Operator 138: -0.0004867432882402709\n",
      "Operator 139: 0.6873823644565669\n",
      "Operator 140: 0.14582910452493897\n",
      "Operator 141: -0.6683187315324174\n",
      "Operator 142: -0.0600185641348069\n",
      "Operator 143: 0.583400464712343\n",
      "Operator 144: 0.010234579546806294\n",
      "Operator 145: 0.5276633163513562\n",
      "Operator 146: -0.16996954553139648\n",
      "Operator 147: 0.17495928611352485\n",
      "Operator 148: 0.7253317728442457\n",
      "Operator 149: 0.07015109627478525\n",
      "Operator 150: 0.5081903528013672\n",
      "Operator 151: 0.5223001161271239\n",
      "Operator 152: -0.3099068210917151\n",
      "Operator 153: -0.5750936626096796\n",
      "Operator 154: 0.36576980742836485\n",
      "Operator 155: 0.4334471455986241\n",
      "Operator 156: -0.2545335964975497\n",
      "Operator 157: -0.19486282070356384\n",
      "Operator 158: -1.0636405040732075\n",
      "Operator 159: 0.22488627733161293\n",
      "Operator 160: -0.08623314167349216\n",
      "Operator 161: 0.20162475729616483\n",
      "Operator 162: -0.4543900708519333\n",
      "Operator 163: 0.4113678524061748\n",
      "Operator 164: -0.452176034793856\n",
      "Operator 165: 0.21670421393703\n",
      "Operator 166: 0.002578609342332306\n",
      "Operator 167: 0.16019581732505586\n",
      "Operator 168: -0.417141508721642\n",
      "Operator 169: 0.1807735291296383\n",
      "Operator 170: -0.02889788749076356\n",
      "Operator 171: 0.29446179434334635\n",
      "Operator 172: -0.32458902436679304\n",
      "Operator 173: -0.02997449749988812\n",
      "Operator 174: -0.7472886150424063\n",
      "Operator 175: -0.24249833580870922\n",
      "Operator 176: -0.49237059590331755\n",
      "Operator 177: -0.179643877127616\n",
      "Operator 178: -0.20663144466031574\n",
      "Operator 179: -0.4002648048788029\n",
      "Operator 180: -0.4731319963764804\n",
      "Operator 181: -0.2521556242190324\n",
      "Operator 182: -0.4003467669473023\n",
      "Operator 183: -0.14108282595653687\n",
      "Operator 184: -0.9079192258974553\n",
      "Operator 185: 0.20173027380979097\n",
      "Operator 186: 0.16833208075571893\n",
      "Operator 187: 0.870073713399581\n",
      "Operator 188: 0.11225527478743431\n",
      "Operator 189: 0.33483317371304405\n",
      "Operator 190: 0.4035789689339929\n",
      "Operator 191: 0.3450670063637937\n",
      "Operator 192: 0.1946047305715848\n",
      "Operator 193: 0.407816090604458\n",
      "Operator 194: 0.4061942870264613\n",
      "Operator 195: 0.6278954318986385\n",
      "Operator 196: -0.24343291409364032\n",
      "Operator 197: 0.8314972969953955\n",
      "Operator 198: -0.009373944304359227\n",
      "Operator 199: -0.21586849450180523\n",
      "Operator 200: -0.5363863525218315\n",
      "Total gradient norm: 5.681064295980356\n",
      "Operators under consideration (1):\n",
      "[48]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.3323627958752713)]\n",
      "Operator(s) added to ansatz: [48]\n",
      "Gradients: [np.float64(-1.3323627958752713)]\n",
      "Initial energy: -26.38810512449006\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48]...\n",
      "Starting point: [np.float64(0.5978834123370429), np.float64(0.14252429033974603), np.float64(0.4397019493157684), np.float64(-0.160053931555192), np.float64(0.35360673626024813), np.float64(0.1401173439932674), np.float64(-0.7420394867534925), np.float64(0.5039524177892172), np.float64(-0.36037283744011794), np.float64(0.3373369240715685), np.float64(-0.3177817711195204), np.float64(-0.36299365497628155), np.float64(0.16860216760568528), np.float64(-0.20768721004914237), np.float64(0.12310000968859891), np.float64(0.17783476168537146), np.float64(-0.1436156282203327), np.float64(-0.1498452165158687), np.float64(0.1875320402342845), np.float64(0.17244139200526248), np.float64(0.10071206959802692), np.float64(0.13518487413761152), np.float64(0.10458472983468413), np.float64(0.1127659815109968), np.float64(-0.13091807059781424), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.456409\n",
      "         Iterations: 33\n",
      "         Function evaluations: 111\n",
      "         Gradient evaluations: 100\n",
      "\n",
      "Current energy: -26.4564093255707\n",
      "(change of -0.06830420108063961)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48]\n",
      "On iteration 26.\n",
      "\n",
      "*** ADAPT-VQE Iteration 27 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.279127908556094\n",
      "Operator 1: 0.24768051474044192\n",
      "Operator 2: -0.2624423367119817\n",
      "Operator 3: 0.04735736023544521\n",
      "Operator 4: -0.09872297239231785\n",
      "Operator 5: 0.22685916392558744\n",
      "Operator 6: -0.16170098262330418\n",
      "Operator 7: -1.8502996518635073e-08\n",
      "Operator 8: 0.010963139037420697\n",
      "Operator 9: 0.6426583556987935\n",
      "Operator 10: -0.0428223002833429\n",
      "Operator 11: 0.20612285446511308\n",
      "Operator 12: 2.7909906386014427e-08\n",
      "Operator 13: -0.2053214662052295\n",
      "Operator 14: 0.4624559678768222\n",
      "Operator 15: -0.2924822013390677\n",
      "Operator 16: 0.31432037761405723\n",
      "Operator 17: -0.43966822658265486\n",
      "Operator 18: 0.18067846075811866\n",
      "Operator 19: -0.03294198914830705\n",
      "Operator 20: 0.2805288504203486\n",
      "Operator 21: -0.31738828444404954\n",
      "Operator 22: 0.5230197825795685\n",
      "Operator 23: -0.053386095460932405\n",
      "Operator 24: 0.25950035502790886\n",
      "Operator 25: -0.17540639637614422\n",
      "Operator 26: -0.8331461554788292\n",
      "Operator 27: 0.05472168816191207\n",
      "Operator 28: -0.12261079710968742\n",
      "Operator 29: -0.29462463286547097\n",
      "Operator 30: -0.4720526573725657\n",
      "Operator 31: -0.050042621553612\n",
      "Operator 32: -0.31441084564659527\n",
      "Operator 33: -0.3829373751111543\n",
      "Operator 34: -0.6905738862740222\n",
      "Operator 35: -0.26088575916341744\n",
      "Operator 36: 0.06055386709354635\n",
      "Operator 37: -0.1339664113769744\n",
      "Operator 38: -2.303120747226828e-06\n",
      "Operator 39: 0.19746034820514444\n",
      "Operator 40: -0.2917106687226891\n",
      "Operator 41: -0.13860412221855306\n",
      "Operator 42: -0.05113285676603853\n",
      "Operator 43: -0.25750697778516607\n",
      "Operator 44: -0.6642629650039885\n",
      "Operator 45: -0.020908619751816574\n",
      "Operator 46: -0.7262238431262591\n",
      "Operator 47: -0.24726525273250574\n",
      "Operator 48: -5.1507464135589886e-08\n",
      "Operator 49: -0.18885185449264924\n",
      "Operator 50: 0.25012879107637936\n",
      "Operator 51: 0.06363169193620127\n",
      "Operator 52: -0.20043630133065288\n",
      "Operator 53: 0.1356058142744347\n",
      "Operator 54: 0.0676015376328498\n",
      "Operator 55: 0.3293061413867107\n",
      "Operator 56: 0.4126568430047262\n",
      "Operator 57: 0.5238665414866583\n",
      "Operator 58: -0.06254530274664191\n",
      "Operator 59: 0.826448568483682\n",
      "Operator 60: 0.2037523866555564\n",
      "Operator 61: 0.058534444108685815\n",
      "Operator 62: -0.016669184245861477\n",
      "Operator 63: -0.22933930418061374\n",
      "Operator 64: -0.2651182674763522\n",
      "Operator 65: 0.17852915930523602\n",
      "Operator 66: 7.733641579160464e-08\n",
      "Operator 67: 0.4549978641037227\n",
      "Operator 68: 2.5408704748383533e-07\n",
      "Operator 69: 0.09645384186250978\n",
      "Operator 70: -0.27634588691384854\n",
      "Operator 71: 0.2512160907382979\n",
      "Operator 72: -1.818660669934886e-07\n",
      "Operator 73: 0.0887969266513443\n",
      "Operator 74: -0.19964688545214007\n",
      "Operator 75: 0.20573316556802193\n",
      "Operator 76: -0.34297229822106046\n",
      "Operator 77: 0.0569630823473917\n",
      "Operator 78: 0.7064365228690759\n",
      "Operator 79: -0.17369918065317014\n",
      "Operator 80: 0.39788265491227104\n",
      "Operator 81: 0.5055872304623424\n",
      "Operator 82: 0.3200713177648478\n",
      "Operator 83: -0.039435665744665276\n",
      "Operator 84: 0.45345160343369023\n",
      "Operator 85: 0.38997343905922943\n",
      "Operator 86: 0.5550050301058108\n",
      "Operator 87: -0.014500877619535137\n",
      "Operator 88: -0.0434804286147242\n",
      "Operator 89: 0.03438832124871494\n",
      "Operator 90: -0.09856097865512567\n",
      "Operator 91: -0.7894979425625913\n",
      "Operator 92: -0.12907641143384826\n",
      "Operator 93: -0.48799189108547064\n",
      "Operator 94: -0.4293558861507198\n",
      "Operator 95: 0.48607930262016963\n",
      "Operator 96: 0.5792521370927867\n",
      "Operator 97: -0.5351491982589356\n",
      "Operator 98: -0.40566212160491893\n",
      "Operator 99: 0.49995800986256556\n",
      "Operator 100: -0.13373379484355188\n",
      "Operator 101: 0.3253862536370497\n",
      "Operator 102: -0.20532391837365396\n",
      "Operator 103: -0.04897318336982255\n",
      "Operator 104: -0.182841908045596\n",
      "Operator 105: -1.1713041536595209e-06\n",
      "Operator 106: 0.5461715496849616\n",
      "Operator 107: -0.4634040852071652\n",
      "Operator 108: -0.12166848571789046\n",
      "Operator 109: 0.6712637666406345\n",
      "Operator 110: -0.0525598677794331\n",
      "Operator 111: -0.0037127360959722765\n",
      "Operator 112: -1.6276267444979864e-08\n",
      "Operator 113: -2.3471284940483852e-08\n",
      "Operator 114: -0.30315251476219984\n",
      "Operator 115: 0.14518447180497276\n",
      "Operator 116: 0.04702374517707035\n",
      "Operator 117: -0.11328521378047322\n",
      "Operator 118: 0.5266796833191499\n",
      "Operator 119: -0.1244642512740893\n",
      "Operator 120: 0.251000075184\n",
      "Operator 121: 0.2435770766611065\n",
      "Operator 122: -0.5789955051598932\n",
      "Operator 123: 0.48672318177496376\n",
      "Operator 124: -0.24560869933024312\n",
      "Operator 125: -0.6342556345895244\n",
      "Operator 126: 0.028870965188625943\n",
      "Operator 127: 0.6766389071878922\n",
      "Operator 128: -0.25517651088475574\n",
      "Operator 129: -0.2458260329198558\n",
      "Operator 130: 0.06644731786453119\n",
      "Operator 131: 0.10157683329665163\n",
      "Operator 132: 0.2763050388393568\n",
      "Operator 133: -0.22850675655025818\n",
      "Operator 134: 0.07453492689449029\n",
      "Operator 135: -0.8029791711957563\n",
      "Operator 136: 0.026943335059486734\n",
      "Operator 137: -0.7167549081294122\n",
      "Operator 138: 0.0012200338470049277\n",
      "Operator 139: 0.6880577896839686\n",
      "Operator 140: 0.1495693336176283\n",
      "Operator 141: -0.6838847906022769\n",
      "Operator 142: -0.03915429678419324\n",
      "Operator 143: 0.5522156788858509\n",
      "Operator 144: 0.005971315706373095\n",
      "Operator 145: 0.14566326314623354\n",
      "Operator 146: -0.20664622284891793\n",
      "Operator 147: 0.18079359124867012\n",
      "Operator 148: 0.7265180382149449\n",
      "Operator 149: 0.06958554494411075\n",
      "Operator 150: 0.5108464578570066\n",
      "Operator 151: 0.5177234997258435\n",
      "Operator 152: -0.30737064002190656\n",
      "Operator 153: -0.5788140711958815\n",
      "Operator 154: 0.39557634747684084\n",
      "Operator 155: 0.34559893751487\n",
      "Operator 156: -0.23707221665727674\n",
      "Operator 157: -0.02512839474087277\n",
      "Operator 158: -0.497264523043683\n",
      "Operator 159: 0.2510960410409986\n",
      "Operator 160: -0.09092805299694391\n",
      "Operator 161: 0.2017481672297629\n",
      "Operator 162: -0.45436852949536044\n",
      "Operator 163: 0.411949906364349\n",
      "Operator 164: -0.4514512277652701\n",
      "Operator 165: 0.21641449034155627\n",
      "Operator 166: 0.0012574378385386388\n",
      "Operator 167: 0.16081533653680574\n",
      "Operator 168: -0.3903064601586786\n",
      "Operator 169: 0.1811376213320171\n",
      "Operator 170: -0.09437324388711613\n",
      "Operator 171: 0.2190670741279212\n",
      "Operator 172: -0.3159792203482151\n",
      "Operator 173: 0.028034957970652225\n",
      "Operator 174: -0.7484169951049349\n",
      "Operator 175: -0.24189259647646866\n",
      "Operator 176: -0.4935215089651383\n",
      "Operator 177: -0.17814371383298128\n",
      "Operator 178: -0.21643362644457076\n",
      "Operator 179: -0.3869575938493835\n",
      "Operator 180: -0.4904805695263829\n",
      "Operator 181: -0.17461731939192185\n",
      "Operator 182: -0.5248847464765605\n",
      "Operator 183: 0.045562409040230976\n",
      "Operator 184: 0.13496684904822256\n",
      "Operator 185: 0.21571603925600866\n",
      "Operator 186: 0.151266558191662\n",
      "Operator 187: 0.8701925663477693\n",
      "Operator 188: 0.11218265763291364\n",
      "Operator 189: 0.3379382745662404\n",
      "Operator 190: 0.40293961871532474\n",
      "Operator 191: 0.34312279397895046\n",
      "Operator 192: 0.18724621303713918\n",
      "Operator 193: 0.44579182032742826\n",
      "Operator 194: 0.37844086153505246\n",
      "Operator 195: 0.6747679552924976\n",
      "Operator 196: -0.25369084425312627\n",
      "Operator 197: -0.22035490485351325\n",
      "Operator 198: -0.05072760297681237\n",
      "Operator 199: -0.1918185094626071\n",
      "Operator 200: -0.5372234154673711\n",
      "Total gradient norm: 4.964992444890841\n",
      "Operators under consideration (1):\n",
      "[187]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.8701925663477693)]\n",
      "Operator(s) added to ansatz: [187]\n",
      "Gradients: [np.float64(0.8701925663477693)]\n",
      "Initial energy: -26.4564093255707\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187]...\n",
      "Starting point: [np.float64(0.6235973931835339), np.float64(0.14246011785418228), np.float64(0.5117881458801802), np.float64(-0.15884401143849594), np.float64(0.35300212035906225), np.float64(0.1400764927193801), np.float64(-0.7487776084422445), np.float64(0.5040499260600579), np.float64(-0.3722169059436138), np.float64(0.3376213696485042), np.float64(-0.3215065142079568), np.float64(-0.3638272972584434), np.float64(0.14811585546570336), np.float64(-0.20299766017786008), np.float64(0.12502170555600445), np.float64(0.17863516100470672), np.float64(-0.14354077017999875), np.float64(-0.12963476681180183), np.float64(0.18710473763844307), np.float64(0.16743850427396606), np.float64(0.10259045758647328), np.float64(0.13702927631951134), np.float64(0.10447010475959424), np.float64(0.1137432048017966), np.float64(-0.14150071160449756), np.float64(0.10030988314900002), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.486980\n",
      "         Iterations: 36\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 75\n",
      "\n",
      "Current energy: -26.48698032446496\n",
      "(change of -0.03057099889425885)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187]\n",
      "On iteration 27.\n",
      "\n",
      "*** ADAPT-VQE Iteration 28 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.40271116097714776\n",
      "Operator 1: 0.30719157402871045\n",
      "Operator 2: -0.2545369726310585\n",
      "Operator 3: 0.04672510157474925\n",
      "Operator 4: -0.09979166238637005\n",
      "Operator 5: 0.22674395729615315\n",
      "Operator 6: -0.1618528237559437\n",
      "Operator 7: -6.069993855552057e-08\n",
      "Operator 8: 0.010992006839183959\n",
      "Operator 9: 0.642571296777997\n",
      "Operator 10: -0.04279463259275278\n",
      "Operator 11: 0.20607910138790875\n",
      "Operator 12: 2.981140840726214e-07\n",
      "Operator 13: -0.24702874732013136\n",
      "Operator 14: 0.5676681590413204\n",
      "Operator 15: -0.30028624110646096\n",
      "Operator 16: 0.3962708687252781\n",
      "Operator 17: -0.47515413733288714\n",
      "Operator 18: 0.16090393690666055\n",
      "Operator 19: -0.03371177295809362\n",
      "Operator 20: 0.28116905196257147\n",
      "Operator 21: -0.3175761348564031\n",
      "Operator 22: 0.5228198324680139\n",
      "Operator 23: -0.053360213110175816\n",
      "Operator 24: 0.25945001998761635\n",
      "Operator 25: -0.17536729575148527\n",
      "Operator 26: -0.089514565136196\n",
      "Operator 27: 0.17974592078277382\n",
      "Operator 28: -0.14369194860514403\n",
      "Operator 29: -0.2916357116765353\n",
      "Operator 30: -0.47833002804398245\n",
      "Operator 31: -0.048844578555287906\n",
      "Operator 32: -0.31618053925840894\n",
      "Operator 33: -0.3828166214115357\n",
      "Operator 34: -0.6911178247583754\n",
      "Operator 35: -0.26081111618588765\n",
      "Operator 36: 0.06050226574589747\n",
      "Operator 37: -0.13396238609439712\n",
      "Operator 38: -2.6741768530884103e-06\n",
      "Operator 39: 0.3628274872529257\n",
      "Operator 40: -0.3031872172868209\n",
      "Operator 41: -0.026368334292651698\n",
      "Operator 42: -0.024592433305956857\n",
      "Operator 43: -0.2799946008471367\n",
      "Operator 44: -0.6646160686320938\n",
      "Operator 45: -0.019286159999196497\n",
      "Operator 46: -0.7267514050603718\n",
      "Operator 47: -0.24763177910011824\n",
      "Operator 48: -1.2300919922257843e-08\n",
      "Operator 49: -0.18885608081304855\n",
      "Operator 50: 0.2501181000933072\n",
      "Operator 51: 0.06361610043484728\n",
      "Operator 52: -0.5105819345299935\n",
      "Operator 53: 0.12573647110986594\n",
      "Operator 54: 0.03522260542713904\n",
      "Operator 55: 0.34221163382098196\n",
      "Operator 56: 0.41510449757896617\n",
      "Operator 57: 0.5234893979248606\n",
      "Operator 58: -0.06349988194213574\n",
      "Operator 59: 0.8269333961839009\n",
      "Operator 60: 0.20382652262441858\n",
      "Operator 61: 0.058591320240205635\n",
      "Operator 62: -0.016610537613861606\n",
      "Operator 63: -0.2293452384765253\n",
      "Operator 64: -0.2651104064825062\n",
      "Operator 65: 0.16941774540090648\n",
      "Operator 66: -2.971350573921505e-08\n",
      "Operator 67: 0.4371216709730984\n",
      "Operator 68: 1.3687565525444256e-08\n",
      "Operator 69: 0.16969428488592994\n",
      "Operator 70: -0.25666466254302495\n",
      "Operator 71: 0.2511186076505209\n",
      "Operator 73: 0.08946510603680422\n",
      "Operator 74: -0.19935491035298236\n",
      "Operator 75: 0.20566635893638102\n",
      "Operator 76: -0.34291222738671584\n",
      "Operator 77: 0.05695570608880963\n",
      "Operator 78: 0.06554861705141796\n",
      "Operator 79: -0.21934173549991737\n",
      "Operator 80: 0.46497257615335397\n",
      "Operator 81: 0.4924397836845887\n",
      "Operator 82: 0.3234404395451506\n",
      "Operator 83: -0.04047734997262092\n",
      "Operator 84: 0.4551727806466758\n",
      "Operator 85: 0.38963150994231155\n",
      "Operator 86: 0.5556080948556679\n",
      "Operator 87: -0.014518949810706661\n",
      "Operator 88: -0.04343910929073472\n",
      "Operator 89: 0.03438029510001793\n",
      "Operator 90: -0.09855777741985075\n",
      "Operator 91: -1.3982703345485878\n",
      "Operator 92: -0.0899529679723948\n",
      "Operator 93: -0.4703977421631513\n",
      "Operator 94: -0.38500550687637947\n",
      "Operator 95: 0.48935380632776315\n",
      "Operator 96: 0.5790969231964744\n",
      "Operator 97: -0.5354987484147751\n",
      "Operator 98: -0.40562875695958905\n",
      "Operator 99: 0.5000722390230838\n",
      "Operator 100: -0.13362585171587793\n",
      "Operator 101: 0.3253491781747088\n",
      "Operator 102: -0.2053151792398922\n",
      "Operator 103: -0.049019438498853825\n",
      "Operator 104: -0.1620693693203945\n",
      "Operator 105: -0.08599800370667252\n",
      "Operator 106: 0.6768528558980934\n",
      "Operator 107: -0.46330196386842637\n",
      "Operator 108: -0.12164050655054545\n",
      "Operator 109: 0.9429525608879249\n",
      "Operator 110: -0.18473162557124284\n",
      "Operator 111: -0.023330522999266655\n",
      "Operator 112: -2.3297777499309974e-08\n",
      "Operator 114: -0.3030649248599791\n",
      "Operator 115: 0.1463571842978841\n",
      "Operator 116: 0.04705184281408807\n",
      "Operator 117: -0.11326187394862204\n",
      "Operator 118: 0.5265043462743788\n",
      "Operator 119: -0.12446719249668634\n",
      "Operator 120: 0.2509948990216784\n",
      "Operator 121: 0.24357653315838015\n",
      "Operator 122: -0.5303801991853868\n",
      "Operator 123: 0.4704832147130919\n",
      "Operator 124: -0.41840999606314677\n",
      "Operator 125: -0.6198877747922403\n",
      "Operator 126: 0.043645437582491724\n",
      "Operator 127: 0.6767259368096082\n",
      "Operator 128: -0.25715375949687264\n",
      "Operator 129: -0.2452578264356194\n",
      "Operator 130: 0.0665569799502654\n",
      "Operator 131: 0.10156721697513496\n",
      "Operator 132: 0.2762705875559173\n",
      "Operator 133: -0.2284526242135026\n",
      "Operator 134: 0.07453401608681433\n",
      "Operator 135: -1.2812168517207527\n",
      "Operator 136: 0.06878875474276983\n",
      "Operator 137: -0.7392130132738585\n",
      "Operator 138: 0.0868598164956352\n",
      "Operator 139: 0.6916209782785352\n",
      "Operator 140: 0.14528038829947756\n",
      "Operator 141: -0.6846723277917292\n",
      "Operator 142: -0.03838933897833572\n",
      "Operator 143: 0.5521800528973715\n",
      "Operator 144: 0.00598275768202547\n",
      "Operator 145: 0.14560414466609134\n",
      "Operator 146: -0.20663326979748287\n",
      "Operator 147: 0.18073719923436937\n",
      "Operator 148: 1.0247047768076767\n",
      "Operator 149: 0.12621451218244106\n",
      "Operator 150: 0.6434355243331802\n",
      "Operator 151: 0.5075928913843067\n",
      "Operator 152: -0.3387021211820458\n",
      "Operator 153: -0.5776410240206987\n",
      "Operator 154: 0.39729649416182805\n",
      "Operator 155: 0.34528281504375125\n",
      "Operator 156: -0.23761214434783212\n",
      "Operator 157: -0.025154690468952207\n",
      "Operator 158: -0.49712690768655965\n",
      "Operator 159: 0.25105505101040465\n",
      "Operator 160: -0.09090885322462339\n",
      "Operator 161: 0.2447141151965253\n",
      "Operator 162: -0.6941907569598709\n",
      "Operator 163: 0.4531995371688827\n",
      "Operator 164: -0.46897770868293764\n",
      "Operator 165: 0.21103975241266418\n",
      "Operator 166: 0.0044718490867793415\n",
      "Operator 167: 0.16216873510275645\n",
      "Operator 168: -0.39055944965267275\n",
      "Operator 169: 0.1812841321538669\n",
      "Operator 170: -0.09431419112581249\n",
      "Operator 171: 0.21897588389615621\n",
      "Operator 172: -0.315919546041743\n",
      "Operator 173: 0.028010895594192578\n",
      "Operator 174: -0.1874183111164926\n",
      "Operator 175: 0.04503047106169414\n",
      "Operator 176: -0.4808931896107318\n",
      "Operator 177: -0.17211917379692238\n",
      "Operator 178: -0.22001953989142745\n",
      "Operator 179: -0.38692380310776286\n",
      "Operator 180: -0.4915202673327736\n",
      "Operator 181: -0.17444492445008233\n",
      "Operator 182: -0.5254536831047821\n",
      "Operator 183: 0.04555865632488805\n",
      "Operator 184: 0.13497028763466867\n",
      "Operator 185: 0.2156663318778492\n",
      "Operator 186: 0.1512671131600209\n",
      "Operator 187: -4.715681539008143e-08\n",
      "Operator 188: 0.010211778281316065\n",
      "Operator 189: 0.3968240289843138\n",
      "Operator 190: 0.38350775689867256\n",
      "Operator 191: 0.34789809006447664\n",
      "Operator 192: 0.18726019273194844\n",
      "Operator 193: 0.4472067377352425\n",
      "Operator 194: 0.3779779345890609\n",
      "Operator 195: 0.6753183616504227\n",
      "Operator 196: -0.2536047876271518\n",
      "Operator 197: -0.22033717885150672\n",
      "Operator 198: -0.05072278291518144\n",
      "Operator 199: -0.19181581221333113\n",
      "Operator 200: -0.8413279389822833\n",
      "Total gradient norm: 5.227298113007214\n",
      "Operators under consideration (1):\n",
      "[91]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.3982703345485878)]\n",
      "Operator(s) added to ansatz: [91]\n",
      "Gradients: [np.float64(-1.3982703345485878)]\n",
      "Initial energy: -26.48698032446496\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91]...\n",
      "Starting point: [np.float64(0.6236341197629817), np.float64(0.1491746483689825), np.float64(0.5118589443285138), np.float64(-0.15871605650764553), np.float64(0.352305287587398), np.float64(0.13068599466845437), np.float64(-0.7487864805746716), np.float64(0.5218589228533923), np.float64(-0.3723527214414229), np.float64(0.38529895106156514), np.float64(-0.32219642386336095), np.float64(-0.3667987174911632), np.float64(0.14811037439596497), np.float64(-0.20298663354357319), np.float64(0.1250195183483738), np.float64(0.17863455597072636), np.float64(-0.13592977353284572), np.float64(-0.12963478922874103), np.float64(0.19264914878866707), np.float64(0.16740368488931298), np.float64(0.10263777864140625), np.float64(0.1370312378482752), np.float64(0.10659491692208706), np.float64(0.11345885709584097), np.float64(-0.14145498274701965), np.float64(0.1003224622653053), np.float64(-0.06890856041216656), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.566570\n",
      "         Iterations: 35\n",
      "         Function evaluations: 93\n",
      "         Gradient evaluations: 81\n",
      "\n",
      "Current energy: -26.56657045578315\n",
      "(change of -0.07959013131819148)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91]\n",
      "On iteration 28.\n",
      "\n",
      "*** ADAPT-VQE Iteration 29 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.1735759268533098\n",
      "Operator 1: 0.5246674443104096\n",
      "Operator 2: -0.23200538735340467\n",
      "Operator 3: 0.04394877572655127\n",
      "Operator 4: -0.10309664131750922\n",
      "Operator 5: 0.226228619982169\n",
      "Operator 6: -0.16249795790499044\n",
      "Operator 7: -1.3205871248525897e-07\n",
      "Operator 8: 0.011125078609855198\n",
      "Operator 9: 0.6421227387977503\n",
      "Operator 10: -0.04265318240310063\n",
      "Operator 11: 0.2058562125997578\n",
      "Operator 12: 2.6095996563114564e-07\n",
      "Operator 13: -0.38994082971747623\n",
      "Operator 14: 0.6462755105393152\n",
      "Operator 15: -0.39218517877118153\n",
      "Operator 16: 0.5332376824185124\n",
      "Operator 17: -0.5199891576439665\n",
      "Operator 18: 0.11876010576642884\n",
      "Operator 19: -0.03721561803404391\n",
      "Operator 20: 0.28389408720793097\n",
      "Operator 21: -0.31803349410677384\n",
      "Operator 22: 0.5218238491311095\n",
      "Operator 23: -0.05323643993896426\n",
      "Operator 24: 0.2591922325391115\n",
      "Operator 25: -0.17516658271711977\n",
      "Operator 26: 0.39442863251620336\n",
      "Operator 27: 0.3727805994667203\n",
      "Operator 28: -0.22998999649237795\n",
      "Operator 29: -0.27826295742602397\n",
      "Operator 30: -0.5033188200105176\n",
      "Operator 31: -0.03869958492733701\n",
      "Operator 32: -0.3243677821183093\n",
      "Operator 33: -0.38207526748646947\n",
      "Operator 34: -0.693871034558707\n",
      "Operator 35: -0.26041192821419434\n",
      "Operator 36: 0.06024636740022405\n",
      "Operator 37: -0.13393858942036713\n",
      "Operator 38: -1.9348628633366646e-06\n",
      "Operator 39: 0.5413215181197023\n",
      "Operator 40: -0.24828244459349363\n",
      "Operator 41: 0.17603364327313048\n",
      "Operator 42: -0.009714085612977687\n",
      "Operator 43: -0.32314909684486537\n",
      "Operator 44: -0.6666026345537557\n",
      "Operator 45: -0.011886357860424357\n",
      "Operator 46: -0.7298165296204545\n",
      "Operator 47: -0.2494225640921933\n",
      "Operator 48: 3.023819155090524e-07\n",
      "Operator 49: -0.1888761005056326\n",
      "Operator 50: 0.250065156294878\n",
      "Operator 51: 0.0635387912057467\n",
      "Operator 52: 0.3281648357439782\n",
      "Operator 53: 0.3288792105219125\n",
      "Operator 54: -0.14901984291520545\n",
      "Operator 55: 0.3813750956199401\n",
      "Operator 56: 0.4198616956323785\n",
      "Operator 57: 0.5248900612997051\n",
      "Operator 58: -0.06826058487580651\n",
      "Operator 59: 0.829472134828461\n",
      "Operator 60: 0.2041995587256645\n",
      "Operator 61: 0.0588727250764533\n",
      "Operator 62: -0.016311596939830662\n",
      "Operator 63: -0.2293776599409351\n",
      "Operator 64: -0.2650731618148805\n",
      "Operator 65: 0.03157755021566899\n",
      "Operator 66: -0.058254216565964745\n",
      "Operator 67: 0.5089881685832092\n",
      "Operator 68: -5.289954677079045e-08\n",
      "Operator 69: 0.2887980458354289\n",
      "Operator 70: -0.21796321424405948\n",
      "Operator 71: 0.25072189808177936\n",
      "Operator 72: -2.5299089041649154e-07\n",
      "Operator 73: 0.09231440433316307\n",
      "Operator 74: -0.19791940652823378\n",
      "Operator 75: 0.20533643297067966\n",
      "Operator 76: -0.34260397492464917\n",
      "Operator 77: 0.05691746735374982\n",
      "Operator 78: 0.05891163584635792\n",
      "Operator 79: -0.33004868562143463\n",
      "Operator 80: 0.5973219638370326\n",
      "Operator 81: 0.45382963698155687\n",
      "Operator 82: 0.3446282288861806\n",
      "Operator 83: -0.04934790784086395\n",
      "Operator 84: 0.46284053831170247\n",
      "Operator 85: 0.38771585095455324\n",
      "Operator 86: 0.5586767224639487\n",
      "Operator 87: -0.014628011302455739\n",
      "Operator 88: -0.04323577541241473\n",
      "Operator 89: 0.03433514200210021\n",
      "Operator 90: -0.09854423752151827\n",
      "Operator 91: 5.6022521732757014e-08\n",
      "Operator 92: 0.0013212243626291864\n",
      "Operator 93: -0.5754040562908858\n",
      "Operator 94: -0.283147312452519\n",
      "Operator 95: 0.4978164803511665\n",
      "Operator 96: 0.5784164237597008\n",
      "Operator 97: -0.5383311320501902\n",
      "Operator 98: -0.4056167376660561\n",
      "Operator 99: 0.5006672649487146\n",
      "Operator 100: -0.13307038506876792\n",
      "Operator 101: 0.3251545158880593\n",
      "Operator 102: -0.20527018945313977\n",
      "Operator 103: -0.04925861726601413\n",
      "Operator 104: -0.21786209638877452\n",
      "Operator 105: -0.09749650794323028\n",
      "Operator 106: 0.7873069542317654\n",
      "Operator 107: -0.46278074975980543\n",
      "Operator 108: -0.12149602039803492\n",
      "Operator 109: -0.453891848428435\n",
      "Operator 110: -0.3414353309710284\n",
      "Operator 111: 0.035031143549758934\n",
      "Operator 112: 6.21598192813988e-08\n",
      "Operator 113: -7.812511854809892e-08\n",
      "Operator 114: -0.30678712713795075\n",
      "Operator 115: 0.15215162714236774\n",
      "Operator 116: 0.047156204557478675\n",
      "Operator 117: -0.11316181470742952\n",
      "Operator 118: 0.5256005190531617\n",
      "Operator 119: -0.12448346706011941\n",
      "Operator 120: 0.25096688866539024\n",
      "Operator 121: 0.24357521767103815\n",
      "Operator 122: -0.7848406903994185\n",
      "Operator 123: 0.4213771052996431\n",
      "Operator 124: -0.6997397008155539\n",
      "Operator 125: -0.570219267594858\n",
      "Operator 126: 0.07146146695061084\n",
      "Operator 127: 0.6779178349929202\n",
      "Operator 128: -0.2658685769317309\n",
      "Operator 129: -0.24215562540676286\n",
      "Operator 130: 0.0670675260987389\n",
      "Operator 131: 0.10152467536388834\n",
      "Operator 132: 0.2760959010681624\n",
      "Operator 133: -0.22817320369776695\n",
      "Operator 134: 0.074528649917307\n",
      "Operator 135: -0.09618363642714978\n",
      "Operator 136: 0.0823121288289171\n",
      "Operator 137: -0.7375416607266648\n",
      "Operator 138: 0.22281150337443328\n",
      "Operator 139: 0.6971390740238368\n",
      "Operator 140: 0.13590704674466134\n",
      "Operator 141: -0.6892784430280015\n",
      "Operator 142: -0.03508103643906963\n",
      "Operator 143: 0.5519264732805567\n",
      "Operator 144: 0.006051592849234424\n",
      "Operator 145: 0.14529768377919036\n",
      "Operator 146: -0.20656762695146597\n",
      "Operator 147: 0.18044527654377043\n",
      "Operator 148: 0.844726683337484\n",
      "Operator 149: 0.2185258203884538\n",
      "Operator 150: 0.7951420289905391\n",
      "Operator 151: 0.4792651398004093\n",
      "Operator 152: -0.40445096723743656\n",
      "Operator 153: -0.5728081634777276\n",
      "Operator 154: 0.4057720114107734\n",
      "Operator 155: 0.34364378674767987\n",
      "Operator 156: -0.24028900392217184\n",
      "Operator 157: -0.025287341620700975\n",
      "Operator 158: -0.49642433990881185\n",
      "Operator 159: 0.25084486289018815\n",
      "Operator 160: -0.09080870378792374\n",
      "Operator 161: 0.16483761823388576\n",
      "Operator 162: 0.0332966719833907\n",
      "Operator 163: 0.32151045917198334\n",
      "Operator 164: -0.5972169206113587\n",
      "Operator 165: 0.1965352899189023\n",
      "Operator 166: 0.014435021147357376\n",
      "Operator 167: 0.16608830952961412\n",
      "Operator 168: -0.39176986147958837\n",
      "Operator 169: 0.18190061854072156\n",
      "Operator 170: -0.09400945986760363\n",
      "Operator 171: 0.21851986037970678\n",
      "Operator 172: -0.3156133936533627\n",
      "Operator 173: 0.02788767202741346\n",
      "Operator 174: 0.21410940385606933\n",
      "Operator 175: 0.25340834015501545\n",
      "Operator 176: -0.5351711759482858\n",
      "Operator 177: -0.1457621497006653\n",
      "Operator 178: -0.24292688881333724\n",
      "Operator 179: -0.38093236795677843\n",
      "Operator 180: -0.4970907414446515\n",
      "Operator 181: -0.17337243329323593\n",
      "Operator 182: -0.5283980694338885\n",
      "Operator 183: 0.045552200180742033\n",
      "Operator 184: 0.13498534533560727\n",
      "Operator 185: 0.2154141734503983\n",
      "Operator 186: 0.15127157080133577\n",
      "Operator 187: -1.0598900215819107e-08\n",
      "Operator 188: -0.1753006481504757\n",
      "Operator 189: 0.5375582228543876\n",
      "Operator 190: 0.3256919985772289\n",
      "Operator 191: 0.3715235812464649\n",
      "Operator 192: 0.18303305889471283\n",
      "Operator 193: 0.4542353549144356\n",
      "Operator 194: 0.3754792604285684\n",
      "Operator 195: 0.6781684330265934\n",
      "Operator 196: -0.2531803908629812\n",
      "Operator 197: -0.2202444111434772\n",
      "Operator 198: -0.05069983533153005\n",
      "Operator 199: -0.191804071230603\n",
      "Operator 200: 0.06431905755093065\n",
      "Total gradient norm: 4.921726488054951\n",
      "Operators under consideration (1):\n",
      "[148]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.844726683337484)]\n",
      "Operator(s) added to ansatz: [148]\n",
      "Gradients: [np.float64(0.844726683337484)]\n",
      "Initial energy: -26.56657045578315\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91, 148]...\n",
      "Starting point: [np.float64(0.6238217909340148), np.float64(0.14259479179212323), np.float64(0.5122205789288498), np.float64(-0.1580668774804433), np.float64(0.3487933358443515), np.float64(0.11126342999671522), np.float64(-0.7488317355263658), np.float64(0.568023740993275), np.float64(-0.37304234035279604), np.float64(0.4620033291184751), np.float64(-0.3251950597402431), np.float64(-0.37745709895206014), np.float64(0.14807776395925112), np.float64(-0.2029297572610033), np.float64(0.12500817272999593), np.float64(0.17863132948753627), np.float64(-0.12829293999854627), np.float64(-0.12963450697099826), np.float64(0.19571270038957178), np.float64(0.16709782514463126), np.float64(0.1028711178358001), np.float64(0.13704141720335), np.float64(0.1116732130873067), np.float64(0.11228735046036777), np.float64(-0.14121793976108324), np.float64(0.10038695330198101), np.float64(-0.13219769933358042), np.float64(0.11146686641817265), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.593844\n",
      "         Iterations: 33\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 120\n",
      "\n",
      "Current energy: -26.593844080559883\n",
      "(change of -0.027273624776732674)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91, 148]\n",
      "On iteration 29.\n",
      "\n",
      "*** ADAPT-VQE Iteration 30 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.14365343734778208\n",
      "Operator 1: 0.541381045805801\n",
      "Operator 2: -0.27339766879017896\n",
      "Operator 3: 0.05356162852148323\n",
      "Operator 4: -0.10384407279865045\n",
      "Operator 5: 0.22527001333994062\n",
      "Operator 6: -0.16330408458002305\n",
      "Operator 7: -8.340142593418753e-07\n",
      "Operator 8: 0.011331810109079021\n",
      "Operator 9: 0.6413466064316083\n",
      "Operator 10: -0.04240770027425961\n",
      "Operator 11: 0.20547191885135865\n",
      "Operator 12: 1.3616400735827483e-06\n",
      "Operator 13: -0.22975756544830758\n",
      "Operator 14: 0.19897204291982798\n",
      "Operator 15: -0.24126866161883856\n",
      "Operator 16: 0.6024957423230463\n",
      "Operator 17: -0.4915018513271486\n",
      "Operator 18: 0.11699877749677176\n",
      "Operator 19: -0.04284342524171138\n",
      "Operator 20: 0.2878836422355652\n",
      "Operator 21: -0.3181283894036274\n",
      "Operator 22: 0.5201238143288535\n",
      "Operator 23: -0.053043108032900896\n",
      "Operator 24: 0.258749768562247\n",
      "Operator 25: -0.17481787692363016\n",
      "Operator 26: 0.1008189356882178\n",
      "Operator 27: 0.40886990096844716\n",
      "Operator 28: -0.39456344940308846\n",
      "Operator 29: -0.23280089753104813\n",
      "Operator 30: -0.5319305346807862\n",
      "Operator 31: -0.014834933291056108\n",
      "Operator 32: -0.33695266429340587\n",
      "Operator 33: -0.3806338037502756\n",
      "Operator 34: -0.6985557897357129\n",
      "Operator 35: -0.2597064879570325\n",
      "Operator 36: 0.059810156877960735\n",
      "Operator 37: -0.13389984001804794\n",
      "Operator 38: -7.194932489795747e-07\n",
      "Operator 39: -0.015049003769777443\n",
      "Operator 40: -0.4752825424940479\n",
      "Operator 41: 0.2970394749864588\n",
      "Operator 42: -0.07365089559766663\n",
      "Operator 43: -0.3108703593629188\n",
      "Operator 44: -0.6699444711915554\n",
      "Operator 45: -0.00026474113105998625\n",
      "Operator 46: -0.7355857681175437\n",
      "Operator 47: -0.2524543092443509\n",
      "Operator 48: -8.532978418565249e-07\n",
      "Operator 49: -0.18890727613874525\n",
      "Operator 50: 0.2499734477139366\n",
      "Operator 51: 0.06340260928208251\n",
      "Operator 52: 0.20063140413979774\n",
      "Operator 53: 0.40634952651491163\n",
      "Operator 54: -0.32572811350590897\n",
      "Operator 55: 0.3781537773251565\n",
      "Operator 56: 0.4090298998388304\n",
      "Operator 57: 0.5332572760034808\n",
      "Operator 58: -0.07591888474768241\n",
      "Operator 59: 0.8340099377316368\n",
      "Operator 60: 0.20485777120876303\n",
      "Operator 61: 0.059348879449328325\n",
      "Operator 62: -0.01579925868607425\n",
      "Operator 63: -0.22943285455757292\n",
      "Operator 64: -0.265006595381033\n",
      "Operator 65: 0.011781876834710638\n",
      "Operator 66: -0.08818385987510825\n",
      "Operator 67: 0.28686221745450785\n",
      "Operator 68: -0.09674007132229635\n",
      "Operator 69: 0.2739110709762738\n",
      "Operator 70: -0.22273563251342166\n",
      "Operator 71: 0.24984566524561716\n",
      "Operator 72: -1.8607966671506642e-07\n",
      "Operator 73: 0.09648183816291601\n",
      "Operator 74: -0.19547896402428055\n",
      "Operator 75: 0.2047928078770519\n",
      "Operator 76: -0.3420737502107678\n",
      "Operator 77: 0.056848128137474474\n",
      "Operator 78: 0.1498164177508733\n",
      "Operator 79: -0.5021133613400224\n",
      "Operator 80: 0.6769740629784312\n",
      "Operator 81: 0.4014314507289267\n",
      "Operator 82: 0.381595401320015\n",
      "Operator 83: -0.06967373535367363\n",
      "Operator 84: 0.4743227870502911\n",
      "Operator 85: 0.3841898282678349\n",
      "Operator 86: 0.563916770800365\n",
      "Operator 87: -0.014833018721023182\n",
      "Operator 88: -0.04288946902487567\n",
      "Operator 89: 0.034258781091309295\n",
      "Operator 90: -0.09852119493622147\n",
      "Operator 91: -8.325327470447152e-08\n",
      "Operator 92: 0.05512555321621155\n",
      "Operator 93: -0.6834498447855504\n",
      "Operator 94: -0.3206010323005177\n",
      "Operator 95: 0.5008893740397209\n",
      "Operator 96: 0.5788746079019762\n",
      "Operator 97: -0.5450887963754827\n",
      "Operator 98: -0.40578372787963524\n",
      "Operator 99: 0.5017182206551983\n",
      "Operator 100: -0.13211274764169512\n",
      "Operator 101: 0.32481480749593733\n",
      "Operator 102: -0.20519358991976974\n",
      "Operator 103: -0.04966751907960423\n",
      "Operator 104: -0.04872890454050795\n",
      "Operator 105: 0.02719216203650907\n",
      "Operator 106: 0.22700690001869311\n",
      "Operator 107: -0.4618835333313621\n",
      "Operator 108: -0.1212488465320023\n",
      "Operator 109: -0.1417020759882242\n",
      "Operator 110: -0.3882893755699271\n",
      "Operator 111: 0.2581937926711769\n",
      "Operator 112: -4.641161727819458e-07\n",
      "Operator 114: -0.32003281367064046\n",
      "Operator 115: 0.1613868634081272\n",
      "Operator 116: 0.0472446528502009\n",
      "Operator 117: -0.11303251486826682\n",
      "Operator 118: 0.5240388042234524\n",
      "Operator 119: -0.12451131791377046\n",
      "Operator 120: 0.2509203726695239\n",
      "Operator 121: 0.24357134889348908\n",
      "Operator 122: -0.03595993222397048\n",
      "Operator 123: 0.5636572133348878\n",
      "Operator 124: -0.7308400184197996\n",
      "Operator 125: -0.5164620520418273\n",
      "Operator 126: 0.06068688128673905\n",
      "Operator 127: 0.680542658877899\n",
      "Operator 128: -0.2791125143386157\n",
      "Operator 129: -0.23652986982121965\n",
      "Operator 130: 0.06792784185629777\n",
      "Operator 131: 0.10145830620028465\n",
      "Operator 132: 0.2757945781861163\n",
      "Operator 133: -0.22769380885620033\n",
      "Operator 134: 0.07452212602073038\n",
      "Operator 135: 0.017544773478021206\n",
      "Operator 136: -0.23234828975799682\n",
      "Operator 137: -0.8779660945961336\n",
      "Operator 138: 0.22998356207602982\n",
      "Operator 139: 0.6885647082043477\n",
      "Operator 140: 0.13553520918899487\n",
      "Operator 141: -0.6980816098422367\n",
      "Operator 142: -0.030163382405523197\n",
      "Operator 143: 0.5514061234181328\n",
      "Operator 144: 0.006171522203742822\n",
      "Operator 145: 0.14476731605437307\n",
      "Operator 146: -0.206455083723078\n",
      "Operator 147: 0.1799454159714949\n",
      "Operator 148: 5.558375813664242e-08\n",
      "Operator 149: 0.20152571179642992\n",
      "Operator 150: 0.8797086891147002\n",
      "Operator 151: 0.428987259923014\n",
      "Operator 152: -0.41166146686354305\n",
      "Operator 153: -0.5642481086356014\n",
      "Operator 154: 0.4205986087623662\n",
      "Operator 155: 0.3408142259331408\n",
      "Operator 156: -0.24485849809496485\n",
      "Operator 157: -0.02551174821492492\n",
      "Operator 158: -0.4952178924726109\n",
      "Operator 159: 0.25048454201831144\n",
      "Operator 160: -0.09063965010664105\n",
      "Operator 161: 0.16079929743257\n",
      "Operator 162: -0.06715152053495038\n",
      "Operator 163: 0.3016183304369891\n",
      "Operator 164: -0.7026037819485194\n",
      "Operator 165: 0.23214510249454653\n",
      "Operator 166: 0.015295577728684284\n",
      "Operator 167: 0.16706010814462036\n",
      "Operator 168: -0.393532567191309\n",
      "Operator 169: 0.18266353901853943\n",
      "Operator 170: -0.093476173100526\n",
      "Operator 171: 0.21775260904413624\n",
      "Operator 172: -0.3150861569460158\n",
      "Operator 173: 0.027673692466656297\n",
      "Operator 174: -0.17334930311682634\n",
      "Operator 175: 0.36612165008618436\n",
      "Operator 176: -0.6529964415864046\n",
      "Operator 177: -0.11055364121857159\n",
      "Operator 178: -0.28018369668878457\n",
      "Operator 179: -0.3635163827111494\n",
      "Operator 180: -0.506960712637379\n",
      "Operator 181: -0.17127943913370552\n",
      "Operator 182: -0.5335119703798785\n",
      "Operator 183: 0.045553339034646895\n",
      "Operator 184: 0.1350028995982291\n",
      "Operator 185: 0.21498121752882304\n",
      "Operator 186: 0.1512783875658954\n",
      "Operator 187: 0.026668971945443088\n",
      "Operator 188: -0.08986157620619159\n",
      "Operator 189: 0.6229390860692992\n",
      "Operator 190: 0.2700256203665522\n",
      "Operator 191: 0.40808157264640155\n",
      "Operator 192: 0.17019071825568732\n",
      "Operator 193: 0.4660214371409114\n",
      "Operator 194: 0.3710097345632988\n",
      "Operator 195: 0.6831308081532739\n",
      "Operator 196: -0.2524652124251925\n",
      "Operator 197: -0.22007682907302728\n",
      "Operator 198: -0.05066029613031525\n",
      "Operator 199: -0.1917834549870816\n",
      "Operator 200: -0.06685342238632559\n",
      "Total gradient norm: 4.787163077257661\n",
      "Operators under consideration (1):\n",
      "[150]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.8797086891147002)]\n",
      "Operator(s) added to ansatz: [150]\n",
      "Gradients: [np.float64(0.8797086891147002)]\n",
      "Initial energy: -26.593844080559883\n",
      "Optimizing energy with indices [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91, 148, 150]...\n",
      "Starting point: [np.float64(0.6241442713263305), np.float64(0.134327045570132), np.float64(0.5128404668511827), np.float64(-0.15691238223422796), np.float64(0.3422810587839151), np.float64(0.11207375462734244), np.float64(-0.7489096003122757), np.float64(0.6032999303940151), np.float64(-0.3742191145531954), np.float64(0.49644751832562267), np.float64(-0.3296107719607109), np.float64(-0.38801474776856537), np.float64(0.1480172262293348), np.float64(-0.20283143147884458), np.float64(0.12498879913203917), np.float64(0.1786259665754202), np.float64(-0.10810089509223994), np.float64(-0.12963360833795168), np.float64(0.17722334446189125), np.float64(0.16640828492950868), np.float64(0.10326281346732814), np.float64(0.13705885711228072), np.float64(0.11310931555644752), np.float64(0.11032011427267503), np.float64(-0.14080617984524685), np.float64(0.10049729450012308), np.float64(-0.13885468125579195), np.float64(0.10201295403987092), np.float64(-0.06447261989093965), np.float64(0.0)]\n",
      "         Current function value: -26.614177\n",
      "         Iterations: 41\n",
      "         Function evaluations: 169\n",
      "         Gradient evaluations: 152\n",
      "\n",
      "Current energy: -26.614177087675365\n",
      "(change of -0.02033300711548236)\n",
      "Current ansatz: [197, 200, 195, 179, 190, 164, 108, 104, 73, 53, 167, 69, 60, 49, 64, 134, 105, 119, 68, 72, 116, 38, 112, 113, 7, 48, 187, 91, 148, 150]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 4 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(30):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "23\n",
      "30\n",
      "37\n",
      "44\n",
      "49\n",
      "54\n",
      "59\n",
      "66\n",
      "73\n",
      "78\n",
      "85\n",
      "92\n",
      "99\n",
      "106\n",
      "111\n",
      "118\n",
      "123\n",
      "128\n",
      "135\n",
      "142\n",
      "149\n",
      "156\n",
      "161\n",
      "168\n",
      "175\n",
      "182\n",
      "189\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGwCAYAAAC+Qv9QAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQepJREFUeJzt3Qd4VFX+//HvpJOeQAglCUlowSC9CtKk2BVxFXV/YsGyf3tBsYu6WFfFtlgRcVcFBRV1QanSpEtvgYQkpJAQ0nsy/+eckDGBUJJMcqe8X89zn7n3zmTmcLhkPpx7islsNpsFAADAjrkYXQAAAIDGItAAAAC7R6ABAAB2j0ADAADsHoEGAADYPQINAACwewQaAABg99zEwVVWVkpKSor4+fmJyWQyujgAAOAcqGny8vLypF27duLicvb2F4cPNCrMhIeHG10MAADQAElJSRIWFnbW1zl8oFEtM9UV4u/vb3RxAADAOcjNzdUNEtXf4+Lsgab6NpMKMwQaAADsy7l2F6FTMAAAsHsEGgAAYPcINAAAwO4RaAAAgN0zPNCUlpbK1KlTxc3NTRISEizny8vL5ZNPPpGRI0fKqFGjpG/fvjJ58mTJzMw0tLwAAMD2GBpoVIAZPny4pKamSkVFRa3n0tLS5L777pMZM2bIsmXLZO3atRIfHy/XXnutYeUFAAC2ydBAk5+fL3PmzJFbb731lOc8PDzktttukx49euhjT09P+cc//iErV67UAQgAAMAm5qHp3r27fkxOTj7ludatW8v7779f65yXl5d+LCkpOe17qudqPq8m5gEAAI7N8D409bFu3Trp37+/REZGnvY1L7/8sgQEBFg2lj0AAMDx2U2gUZ2BP/30U3nvvffO+LonnnhCcnJyLJta8gAAADg2u1j6QI14uuGGG+Sll16SAQMGnPG1qq+N2gAAgPOw+RaayspKmTRpkowePVoP2wYAALC7QHPPPfdIRESEPP744/p4yZIlcujQIaOLJWazWZbvOyoVlWajiwIAgNOz6UCjJtzbu3evTJgwQTZt2qS3uXPnSmJiotFFk3u/2iq3ztoo/91gfFkAAHB2bkbPEjx27FjJzs7WxxMnTtSjkubNmye7du2SV199VZ9XI5tquvHGG8VoAyKD5eftqfL6or1yafc20tKXfjsAABjFZFb3ThyYmodGDd9WI578/f2t9r7qVtOV762WXSm5cl2/MHnt2p5We28AAJxdbj2/v236lpMtc3UxyQtXVU0MOHdTsmw+nGV0kQAAcFoEmkbo2yFIt84oz3y/S8orKq319wIAAOqBQNNIj18cI/5ebrI7NVf+s54OwgAAGIFA00iqM/CUi2P0/hu/7pOMvNOvMwUAAJoGgcYKbhwQIee3D5C84nJ5+X97rPGWAACgHgg0Vuog/OLV3cVkEpm/5YhsiKeDMAAAzYlAYyW9wgNlYv9wvf/sDzvpIAwAQDMi0FjRlHExEujtLnvT8uSLdYet+dYAAOAMCDRWFOzjoUc9KW/9tl+O5hZb8+0BAMBpEGis7Pp+4dIzPFDySspl+i90EAYAoDkQaKxdoaqD8FWxuoPw93+myB+Hjln7IwAAwEkINE2gR1igHspd3UG4jBmEAQBoUgSaJjJlXFfdp2Z/er7MXpvQVB8DAAAINE0n0NtDptboIJxOB2EAAJoMLTRN6Nq+YdI7IlAKSivkpZ/pIAwAQFMh0DR5B+Hu4mISWbgtRdbGZTblxwEA4LQINE2se/sA+fugDnr/2R93SWl5ZVN/JAAATodA0wweGdtVWvp4SNzRfJm1Jr45PhIAAKdCoGkGAS3c5YlLu+n9GUsPSGpOUXN8LAAAToNA00yu6d1e+nUIkkLVQfgnOggDAGBNBJpm7CD8wokOwj/vSJVVBzKa66MBAHB4BJpmdF47f7l5cKTef+6HXVJSXtGcHw8AgMMi0DSzh8d2kVa+nnIos0A+XU0HYQAArIFA08z8vdzlqcuqZhB+d2mcHGUGYQAAGo1AY4Cre7WXPhGBUlRWoUc9AQCAxiHQGMBkMsnjJ9Z5+npjksRnFhhRDAAAHAaBxiADo1vKyK4hUlFpln/9us+oYgAA4BAINAaaMi5GTCaRn7anyo7kHCOLAgCAXSPQGDyM+6qe7fT+a4v3GlkUAADsGoHGBtZ5cnc1yaoDmazGDQBAAxFoDBYe7C03DaxajfvVRXvFbDYbXSQAAOwOgcYG3DOyk3h7uMq25BxZtDPN6OIAAGB3CDQ2IMTPUyZfGK33X/91n5RXVBpdJAAA7AqBxkbccWGUBPt4yKGMApm3Odno4gAAYFcINDbCz8td33pS3l6yX4rLWLgSAIBzRaCxIX8fFCHtA1tIem6JfL42wejiAABgNwg0NsTTzVUeHtNF73+wPE5yCsuMLhIAAHaBQGNjru7dXrqG+klucbnM/P2g0cUBAMAuEGhsjKuLSaaM66r3Z62Jl/TcYqOLBACAzSPQ2KCLurWWfh2CpLisUt5ecsDo4gAAYPMINDbIZDLJ45fE6P25m5LkUEa+0UUCAMCmEWhsVP/IYLkoprVUVJrlX7/uN7o4AADYNAKNDZtycVcxmUR+3pEq25OzjS4OAAA2i0Bjw2La+Mv4Xu0tC1cCAIC6EWhs3ENjuoiHq4usiTsmqw9kGl0cAABsEoHGxoUHe8tNgyIsrTSVlWajiwQAgM0h0NiBe0d2Eh8PV9lxJEd+2ZlqdHEAALA5BBo70NLXU+4YFq331YinsopKo4sEAIBNIdDYickXRktLHw+JzyzQc9MAAIC/EGjshK+nm9w3qpPen7HkgBSVVhhdJAAAbIbhgaa0tFSmTp0qbm5ukpCQcMrzH374ofTt21eGDBkil112mRw5ckSc1Q0DIyQsqIUczSuRWWvjjS4OAAA2w9BAowLM8OHDJTU1VSoqTm1xmD9/vkybNk0WL14sa9askYEDB8rll18ulZXO2YfE081VHhnbRe//e8VByS4sNbpIAADYBEMDTX5+vsyZM0duvfXWOp9/6aWXZNKkSdKqVSt9/MADD8jOnTvl559/Fmd1Vc/2EtPGT/KKy+XdZXFGFwcAAJtgaKDp3r27dOpU1S/kZFlZWbJ161bp16+f5VxAQIB06dJFlixZIs7KxcUkT1zaTe9/vjZB9qXlGV0kAAAMZ3gfmtOJj6/qIxIaGlrrfJs2bSzP1aWkpERyc3NrbY5meJcQGRcbqheufOaHnWI2M9keAMC52WygKSws1I+enp61zqvj6ufq8vLLL+uWnOotPDxcHNEzl58nXu4usiE+S374M8Xo4gAAYCibDTTe3t6WFpea1HH1c3V54oknJCcnx7IlJTnmnC1hQd5y36jOev+fv+yR3OIyo4sEAIBhbDbQREdXzYybnp5e63xaWprlubqoFhx/f/9am6OafGGURLfykYy8Enn7twNGFwcAAMPYbKAJCgqS3r17y+bNmy3nVH+Y/fv3y+jRow0tmy0N437+yli9P3tdguxJdbz+QgAAnAubDTTK008/LbNnz5Zjx47p43feeUePjLr00kuNLprNGNYlRC49v01VB+Hv6SAMAHBObkbPEjx27FjJzs7WxxMnTtSdeOfNm6ePr7nmGjl69KiMGTNGvLy8dKvNwoULxcXFpnNYs3v6svNkxb4M2XT4uMzfckQm9A0zukgAADQrk9nBx/yq21RqtJPqIOzI/Wlmrjwor/xvr7Ty9ZClj4yQgBbuRhcJAIBm+/6mqcNB3DYkSjqG+Ehmfqm8+es+o4sDAECzItA4CA83F3nhqu56f84fh2XnkRyjiwQAQLMh0DiQIZ1ayeU92kqlWeTZH3ZKpdoBAMAJEGgcsIOwj4erbEnMlm+3JBtdHAAAmgWBxsG0CfCSB0d30fuqk3B2YanRRQIAoMkRaBzQLUMipXNrX8kqKJU36CAMAHACBBoH5O76Vwfh/6xPlB3JdBAGADg2Ao2DGtyxpVzVq52oWYaepoMwAMDBEWgc2FOXdhNfTzfZlpQt32xyzFXHAQBQCDQOrLW/lzw0pqqD8KuL9srxAjoIAwAcE4HGwU0a3EFi2vhJdmGZvLaYGYQBAI6JQOPg3Gp0EP56Y6L8mVS1ECgAAI6EQOMEBkQFyzW92+sOws98v1MqmEEYAOBgCDRO4olLu4mfp5vsOJIjX21INLo4AABYFYHGSYT4ecojY6s6CL++eJ8cyy8xukgAAFgNgcaJ/H1QB+nW1l9yisrktUV0EAYAOA4CjZN1EH7xqli9r+al2XmEGYQBAI6BQONk+kUG6xmEq+emAQDAERBonNAjY7qKu6tJVh3IlLVxmUYXBwCARiPQOKGIlt5y44AISyuNWY3nBgDAjhFonNS9ozqLt4erbEvOkUU704wuDgAAjUKgceJh3JOHRun913/dJ+UVlUYXCQCABiPQOLE7hkVLsI+HHMookG83JxtdHAAAGoxA48T8vNzlnpGd9P7bSw5IcVmF0UUCAKBBCDRO7qaBEdI+sIWk5RbL52sTjC4OAAANQqBxcl7urvLQmKolET5YHic5hWVGFwkAgHoj0EDG924vXUJ9Jbe4XGb+fpAaAQDYHQINxNXFJFPGxeiamLUmXtJzi6kVAIBdIdBAG92ttfTrECTFZZUyY+kBagUAYFcINNBMJpM8fklVK803G5PkUEY+NQMAsBsEGlj0jwyWUTGtpaLSLP/6dT81AwCwGwQa1PLYxV3FZBL5eUeqbE/OpnYAAHaBQINaYtr4y/he7fX+a4v2UTsAALtAoMEp1Lw07q4mWR2XKasPZFJDAACbR6DBKcKDveWmgR30/muL94rZbKaWAAA2jUCDOt07qpP4eLjK9uQc+WVHGrUEALBpBBrUqZWvp16NW3nj131SVlFJTQEAbBaBBqc1+cJoaenjIfGZBTJvUzI1BQCwWQQanJavp5u+9aS8vWS/FJVWUFsAAJtEoMEZ3TgwQsKCWsjRvBL5fG0CtQUAsEkEGpyRp5urPDymi97/94o4ySkso8YAADaHQIOzuqpXe4lp4ye5xeXywco4agwAYHMINDgrVxeTXhJB+XxNgqTlFFNrAACbQqDBORnZtbX0jwySkvJKmbGUhSsBALaFQINzYjKZZOolMXp/7qZkOZiRT80BAGwGgQbnrG+HYBndLVQqKs3y9pID1BwAwGYQaFAvD43prB9/2ZEqR7KLqD0AgE0g0KBeYtsFyAUdW+pWmtnMSwMAsBEEGtTb5Auj9ONXGxIlv6ScGgQAGI5Ag3ob0aW1RIf4SF5xuczblEQNAgAMZ/OBpqSkRB566CHp2bOnDB8+XAYOHCgLFiwwulhOzcXFJLcNqWql+WxNvL79BACAkWw+0Lz00kvy/fffy++//y4rV66UmTNnysSJE2Xbtm1GF82pTegTJoHe7pKUVSS/7U43ujgAACdn84Hmzz//lP79+0tAQIA+7t27t95ftmyZ0UVzai08XOXvAzvo/U9XHzK6OAAAJ2fzgWbChAmyatUqSUxM1MeLFy+WjIwMCQ0NNbpoTu/mwR3E3dUkGxOOy7akbKevDwCAcdzExt1yyy1SWFgoPXr0kLZt28r+/fvl2muvleuuu+60fW7UVi03N7cZS+tcWvt7yRU928n8LUfk09Xx8s4NvY0uEgDASdl8C80nn3wir7zyimzevFn27NkjW7ZskUGDBomLS91Ff/nll/UtqeotPDy82cvsTG4fWtU5+OcdqZLCRHsAAIPYdKAxm83y2GOPyV133SUdO3bU59Rop19++UWmT59e58888cQTkpOTY9mSkhhW3NQT7Q2OZqI9AICxbDrQqL4yx48fl8jIyFrno6Ki5LvvvqvzZzw9PcXf37/WhuaZaO+/GxKlgIn2AAAGsOlA06pVKx1QUlNTa51Xx97e3oaVC7WN7Npaolsx0R4AwI4Czfbt22XXrl3SHFQ/mUmTJul+NKqlRlF9aH777bfTdgqGMRPt3XqiL81naxKYaA8AYPuBplevXvLWW29Jc1GfdeWVV8pFF10kQ4cOlVtvvVV3Er7//vubrQw4uwl92uuJ9hKzCploDwDQ7Exm1fO2HoYNG6Zn7bUXati2Gu2kOgjTn6ZpvbZor3yw4qAMiAyWuXcPbuJPAwA4stx6fn/Xu4Wme/fukpKSUudzqiUFzmvSBZF6or0NCVlMtAcAsO2J9fz8/OSCCy7Qt4DCwsLE1dXV8tzOnTutXT7YkVA10V6PdjJ/KxPtAQBs/JZTUFCQ7kdTF7VgZFZWltgSbjk1r51HcuTyd1eLm4tJfn9spLQLbNHMJQAAOIL6fn/Xu4VGdcxduHBhnc/dcMMN9X07OJju7QNkUHSw/HEoS2avS5AnLulmdJEAAE6g3n1oThdmlK+++qqx5YEDmDw0Wj/+dz0T7QEAbHhivcOHD+th0yNHjtSb2lfnAGVUTGuJOjHR3rebk6kUAIDtBZoVK1ZITEyMrFq1Ss/kq7bVq1dLt27dZOXKlU1TStjdRHu3DalaruKzNfFMtAcAaHL17kPz5JNPyo8//ihjxoypdX7JkiUydepUWbdunTXLBzs1oW+YvPHrfjl8rFCW7EmXcbFtjC4SAMCB1buFRg2KOjnMKKNHj9bPAYq3h5vcODBC73+6Op5KAQDYVqApKCiQzMzMOlfGLiwstFa54AAmDY7Uw7c3xGfJ9uRso4sDAHBg9b7lpBaL7Nu3r15TqWPHjvpcXFyczJ49m/WVUEubAC+5omc7WXBior0ZE3tTQwAA2wg0jzzyiJ4tePr06ZKYmKjPRUREyFNPPSV33HFHU5QRduz2oVE60Py8PVWmXhIjbQOYaA8AYAO3nNTMfWoCvYSEBL2vNrVPmMHpJtobGBUs5ZVmmb2Wof0AABsJNIGBgTJhwgS97+vrqzfgTCZfWD3R3mEpKCmnsgAAxgea/v37y6+//mr9ksBhXRTTWiJbeksuE+0BAGwl0HTt2lXy8vLqfO7OO++0RpngiBPtDY3S+7OYaA8AYAudgnv06CEjRoyQq6++WsLCwsTV1dXynJoxGKjLtX3D5F+/7peEY4WydE+6jGWiPQCAFZnM9ZwNr0WLFtKmTd2zvqanp9vcXDT1XX4cTeeV/+2VmSsPyoCoYJl712CqGgBgte/verfQDBo0SJYvX17nc2qhSuB0Jl3QQT5ZdUhPtLcnNVe6tSVgAgAM6kMzefJk+eWXX+p87nRBB1DUHDRqJW7lfztSqRQAgHGBRs0QvHnzZuuVAE7lkvOrblf+b2ea0UUBADhzoBk2bJg888wzdT5na/1nYHtGxYTq9Z0OHM2XuKP5RhcHAODM89Ds2LGjzucuv/xya5QJDiyghbtc0KmV3l+8i1YaAIB11LtTcEpKih623atXr1OGbe/du9dKxYIjuzi2jfy+P0MHmntGdjK6OAAAZ2yhUbMEX3nllXpBShcXF1Gjvqs34FyMjQ0Vk0lke3KOJB/nNiUAwIAWGnVb6eOPP67zuYceesgKRYKja+XrKf0jg/Xw7cW70vWK3AAANGsLzenCjPLWW281qjBwrttOymJGOwEAjAg0yjfffCPDhw+XIUOG6OMXX3xR5syZY43ywEmM614VaDYezpKMvBKjiwMAcLZA8+GHH8qjjz4qPXv2lKKiIn3ummuukQULFsiMGTOaooxwQO0DW0jPsABRXa9+3c1oJwBAMwca1RKzbds2eeedd/QaC0psbKxutfnuu+8aWRw4YyvNIm47AQCaO9CokU3BwcF636SGqpzg7u4upaWljS0PnLAfzbqDxySnsMzo4gAAnCnQlJSUyM6dO085v2TJEqmoqLBWueAEokN8pUuor5RXmmXp3nSjiwMAcKZh288//7xecXvUqFFy4MABvbbTvn37ZMuWLbJw4cKmKSUcupVmf3qcXtvpmj5hRhcHAOAsLTSXXHKJrF+/Xt92Cg0N1csgdOnSRbZu3SpjxoxpmlLCYV3cva1+VDMHF5SUG10cAICztNBUdwL+/PPPrV8aOJ1ubf0kIthbErMKZeX+DLn0/KqAAwBAk89DA1iL6lh+MaOdAACNRKCB4cadGO20bO9RKSmnYzkAoP4INDBc7/BACfX3lPySclkTl2l0cQAAdohAA8O5uJgsrTRMsgcAaJZAM2zYsAZ9EHAuk+z9tjtdyisqqSwAQNMGmt27d8uAAQNk2rRpcvjw4fr+OFCnAVHBEuTtLscLy2RDQha1BABo2kBz++23y9q1a6VHjx7ywAMPyLhx4+TLL7+U4uLi+r4VYOHm6iJjzgvV+9x2AgA0eaB59dVXxc3NTcaPHy/ff/+9Xqxy06ZN0rZtW7nrrrvkjz/+qHchAKV6+PbiXWlSWWmmUgAATRdo5s2bpx/Lyspk7ty5MmnSJHnvvfekZcuW0r59e5k1a5YMHTpUVqxYUd+3hpO7oGMr8fV0k/TcEvkzOdvo4gAAHHmmYNV3ZtWqVfKf//xHr6597bXXyrJly2p1Fs7OzpaxY8fKhg0brF1eODAvd1cZGdNaFm5LkcU706RPRJDRRQIAOHKn4G3btskbb7whaWlpukXm5JFPe/bskZSUFGuWE07ikhO3ndRilWYzt50AAE3UQnPjjTfqTsBnolpuPvjgg/q+NSDDu4SIp5uLXttpT2qenNfOn1oBAFg/0ERHR5/1NcOHD6/v2wKaj6ebDOsSouejWbQrjUADAGiaQKNGNbm7u9d5O0Cdj4yMlEsuuUQCAwPr+9aAZZI9FWhUP5qHx3ShVgAA1g80HTp0kBdeeEEP046IiNCrJScmJsqxY8ekX79+kpqaquenWbx4sfTu3bu+bw/I6G6h4uZikn3peXIoI1+iQ3ypFQCAdTsFDx48WL766isdYlavXq1HPKkZg2fPni0XX3yx7Nu3T/exmTJlSn3fGtACvN1lcMeWel/ddgIAwOqBRg3FVkO1TzZhwgQ9fFtRQ7ZVx2BrOXTokH7/kSNHSmxsrAwaNEhP5gcnmGRvJ4EGANAEgebgwYN6npmTZWVl6dYZa8vIyJCLLrpI38Zavny5HjLu7e0tcXFxVv8s2A61DILJJLItOUeOZBcZXRwAgKP1obniiiukb9++eobgqKgoSwvKF198oZdDUDMIv/zyy+Lp6WmVAqqlFtRtruq5btSyCx999JEONXBcrf28pF+HINmYcFy30tw2tOpaAwDAKoHm7bff1kscvPvuu7oDsKI6CN9///3y6KOPSlFRkV4GQYUaa5g/f7489thjtc516tTptK8vKSnRW7Xc3FyrlAPN7+LubXWgUf1oCDQAgDMxmes5HasKCGpkk5+fnyUs+Ps3zeRnBQUF4uvrq9eKUit8JyQk6OMHH3xQDw2vy/PPP6+XZzhZTk5Ok5UTTSP5eKEMfXW5vvW04cnREuJnnVY/AIDtUxkjICDgnL+/692HRs0vozroKuoDmjIkVPfVeeaZZ3QrzZo1a/Sjuu3122+/1fkzTzzxhP7DV29JSUlNVj40rbAgbzm/fYCoyL1kTzrVDQCwXqDp37+//Prrr9IcXF1d9aMKMD179tT7qoPwqFGjZMaMGXX+jOq7Ux20mjpwoflGO6m1nQAAsFqg6dq1q+Tl5dX53J133inWFBISogOK6rNz8uR+8fHxVv0s2HagWRuXKTlFZUYXBwDgKJ2Ce/ToISNGjJCrr75awsLCLK0oippoz5rUew8ZMsTS+bhaenq6nqUYjq9jiK90bu0rB47my7K96TK+d5jRRQIAOEKgUf1Z2rRpI5999tkpz6mgYW2PP/64TJw4Uc9MrELM7t279S2vb7/91uqfBdttpTmwLE4W7Uwj0AAArBNo1Cy9aoK7uqiZfK1NzTr8zjvvyFVXXaVHOJWXl+tlFi6//HKrfxZs07jYNvLusjhZuT9DCkvLxduj3pctAMDB1XvYthpK7ePjI4467Au2R12iw15fLklZRfLvm/rIJee3NbpIAAB7H7atwowaCv3cc8/Jww8/rM8tWLBADhw40LASA2eh5j26OLaqczCLVQIArBJoVMdfNdJJhZhFixbpc2q5A7XswdKlS+v7dkC9Rjst23NUSsorqDUAQOMCjeoUrILL9u3bJTQ0VJ+77rrrdL+af/7zn/V9O+Cc9A4PktZ+npJXUi5rDx6j1gAAjQs0qj+DWiyy+lZAzTljKir4nzOahouLSXcOVn7aVnsYPwAA9Q40qnNOXRPrqX41mZmZ1CiazBU92+nH+VuTZUN8FjUNAGh4oLnxxhtl4MCB8uabb0pGRoZ88cUX8uSTT+rh3HfccUd93w44ZwOigmVCnzC9ttND3/wpucXMHAwAaOCwbeWjjz6S6dOn68nuFDXh3VNPPWWTgYZh244lr7hMLpmxSpKPF8k1fdrLm9f1MrpIAAAb+P5uUKCplp+frx/VhHe2ikDjeDYmZMn1H66TSrPI+zf2kct6MC8NADiaJp+HpiYVZGqGmSlTpjTm7YBz0j8yWP7fiE56/8kFOyQtp5iaAwAnV+8WGjXnzH//+1/5888/dXqq+eNqXpqUlBSxJbTQOKayikqZ8O+1sj05R4Z2aiVf3DZAj4QCADiGJm+hmTRpkjz99NO6/4wapq0CTfUGNBd3Vxd56/pe4uXuIqvjMmXW2gQqHwCcWL1X+VMtM2qZAy8vr1OeU6OdgObSMcRXnr7sPHn6+53y6qK9MqRTS4lpw3pdAOCM6t1CExMTU2eYUW6++WZrlAk4ZzcNjJBRMa2ltLxSHvz6TykuY3JHAHBG9Q40EydOlHvvvVfWrl0r8fHx+tZT9Xbbbbc1TSmB01CzVb86oYe09PGQvWl58q9f91FXAOCE6t0p2MXlrwxUc+kD9Tbq2NaWP6BTsHP4bXe63PHFJlGX5H9uHygXdGpldJEAALbcKVjNEqxaZtR26NChWtuAAQMaWm6gUcacFyo3DIjQswg/Mm+b5BQyizAAOJN6dwp+4403pEOHDnU+N3PmTGuUCWiQZy7vJn8cOibxmQXy1Pc75N0betdqRQQAOK56t9AMGTLktM/17NmzseUBGszbw00P5XZ1MclP21Plhz9ta04kAIDBgSYqKkqio6Nl1apVdT4/d+5c/Rpvb29rlw+ol17hgfLARZ31/jPf75Tk44XUIAA4gXPqFDxy5EhZvny53p82bVqtZvxnn33Wsj948GBZt26d2BI6BTuf8opKue7DdbIlMVuv0P3VHYN0qw0AwMk7BdcMMJGRkboPzddff633T/c6wChuJ2YR9vFwlQ3xWfLR74f4ywAAB9egpQ/UFhoaykR6sFkdWvrIc1fG6v03f9snO4/kGF0kAEATavBq27TGwNb9rW+YXBzbRsoqzPLgN8wiDADi7MO2U1NTZc6cObUWoExLSzvlXEZGRtOUEmhg6J5+zfmyOfG4xB3Nl1f+t1eeP9FqAwBwwk7BNWcHPuObMVMwbNDK/Rky6bMNev/zW/vLiK6tjS4SAMCITsHDhw+XysrKs27MFAxbNLxLiNxyQVUH9infbpfM/BKjiwQAsLJzCjSvvfbaOb3Z22+/3djyAE1i6iUx0rm1r2TklcjtszdJUaltrTkGAGiGQNO/f/9zXucJsEVe7q7y4f/1lUBvd9mWlC33f71VKirrtS4rAMARRzkB9iY6xFc+vrmfeLi56NW5X1i4q1andgCA/SLQwKn0jwyWt67rpfdnrzssn66ON7pIAAArINDA6VzWo608dWk3vf/Sz3vk5+2pRhcJANBIBBo4pckXRsmkwR30/kNz/5RNCVlGFwkA0AgEGjglNWfSs1fEypjzQqW0vFImf7FJDmXkG10sAEADEWjgtNQK3O9M7C09wwMlu7BMbpm1kTlqAMBOEWjg1Fp4uMqnk/pJRLC3JGYVMkcNANgpAg2cXitfT70kAnPUAID9ItAAJ+ao+YQ5agDAbhFogBP6RQbL29f3EpOJOWoAwN4QaIAaLj2fOWoAwB4RaICT3D40yrI6N3PUAIB9INAAdcxR88zl5zFHDQDYEQINUAfmqAEA+0KgAU6DOWoAwH4QaIB6zFFz55xNklNURp0BgI0h0ADnOEeNp5uLrDqQKVe+t1r2puVSbwBgQwg0wDnOUfPdPy6Q9oEt5PCxQhn//lr5cVsKdQcANoJAA5yj7u0D5Kf7hsqFnVtJUVmF3P/VVnnpp91SXlFJHQKAwQg0QD0E+XjI57cOkH+M6KiPP1kdL3//dD2rdAOAwQg0QAOGdD9+cYzM/Hsf8fFwlT8OZckV766WP5OyqUsAMAiBBmigi7u3lR/uHSLRIT6SmlMs181cJ19tSKQ+AcAAdhVo3nvvPT2L64oVK4wuCqB1au0nP9wzRMaeFyqlFZXyxPwdMvW77VJSXkENAUAzsptAk5KSIq+//rrRxQBO4eflLjP/3lemjOuqV+r+emOSXPfhH5KSXURtAUAzsZtAc99998mTTz5pdDGAOrm4mOSekZ1k9q0DLJPwqX416w4eo8YAoBnYRaBZuHChuLu7y7hx48762pKSEsnNza21Ac1lWJcQWXjvUDmvrb8cKyjVI6A+WXVIzGYzfwkA4MyBpqCgQJ566il56623zun1L7/8sgQEBFi28PDwJi8jUFN4sLeehO+a3u2lotIsL/28R+77aqsUlpZTUQDgrIHmmWeekbvvvlvatm17Tq9/4oknJCcnx7IlJSU1eRmBuha2/Nd1PeWFq2LFzcUkP21P1bMLx2cWUFkA4GyBZsuWLbJ+/XodaM6Vp6en+Pv719oAI6gReTcPjpSv7xwkIX6esi89T/er+WVHKn8hAOBMgebnn3+WoqIiGTVqlIwYMUImTpyozz/44IP6OC4uzugiAue0DtTP9w2VAVHBkl9SLv/vP1tk2sJdUlrOkgkAYC0msx31VkxISJCoqChZvny5DjTnQnUKVn1p1O0nWmtgJLXm0xu/7peZKw/q494RgfL+jX2kXWAL/mIAoJHf3zbdQgM4EjdXF5l6SYx8fHM/8fdyk62J2XLZO6tk5f4Mo4sGAHbPbgKNus1U85ZT9T5gb8acFyo/33+hdG/vL8cLy+SWWRvkzd/26xFRAAAnuOXUENxygq0qLquQF3/aLf9ZX7X+09BOrWTGxF7S0tfT6KIBgOG45QTYCS93V/nn+PPlret7Sgt3V1kdlymXvbNaNiVkGV00ALA7dnPLCXBU43uH6VW7O4b4SFpusUz86A9mFwaAeiLQADagS6if/HjvULmiZzspPzG78N1fbpbc4jKjiwYAdoFAA9gIH083eWdiL3nxqlhxdzXJ4l3peiK+XSk5RhcNAGwegQawsdmF/29wpHx79wXSPrCFHD5WKOM/WCvfbExkgUsAOAMCDWCDeoYHys/3D5VRMa31jMKPf7dDpn63Q8oqmF0YAOpCoAFsVKC3h3xycz+ZMq6ruJhEvtmUJP/4cose7g0AqI1AA9gwFxeT3DOyk55d2NPNRZbsSZdbZ23Ua0IBAP5CoAHswEXdQmX2bQPE19NN1h06Jjd9/IccLyg1ulgAYDMINICdGBTdUv57x0AJ8naXbck5ct2H6yQtp9joYgGATSDQAHakR1igzL1rsLTx95IDR/Plbx+ulcPHCowuFgAYjkAD2JnOoX4y7+7B0qGltyRlFcm1M9fJvrQ8o4sFAIYi0AB2KDzYW4eamDZ+kpFXom8/bU08bnSxAMAwBBrATrX285Jv7hwsfSICJaeoTG76ZL2sics0ulgAYAgCDWDHArzd5cvJA+XCzq2ksLRCD+letDPN6GIBQLMj0AB2ztvDTT6Z1E8ujm0jpRWV8v/+s1m+3ZxsdLEAoFkRaAAH4OnmKu/d2Fv+1jdMKs0ij87bJp+tjje6WADQbAg0gINwc3WRVyf0kNuHRunjF37aLW/9tp9FLQE4BQIN4GBLJTx9WTd5ZEwXfTxj6QGZtnC3VKpmGwBwYAQawMGYTCa576LOMu3KWH38+doEmfLtdlbqBuDQCDSAg5p0QaS8eV1PcXUxyXdbkmX0mytl/pZkqaC1BoADItAADuyaPmHy4d/7SrCPhxw+VigPz90mY99aKQu3pXAbCoBDMZnNZoe+uZ6bmysBAQGSk5Mj/v7+RhcHMERBSbnMXpcgH/1+SLILy/S5rqF+8tCYzjIuto2+TQUA9vz9TaABnEhecZl8tjpBPll9SPKKy/W52Hb+8tDoLnJRt9YEGwA2g0DTyAoBnEFOYZkONWqumoLSCn2uZ3igPDymiwzr3IpgA8BwBJpGVgjgTLIKSvVtqNlrE6SorCrY9OsQpIPNBZ1aGV08AE4sl1tOjasQwBmpFbtnrjwoX/5xWErKK/W5QdHB8sjYrtI/Mtjo4gFwQrkEmsZVCODM0nOL5f3lcfL1hiS9LpSiFr58cHQX6dshyOjiAXAiuQSaxlUIAJEj2UXy3rI4mbcpScpPzFszMCpY7h7RUUZ0CaGPDYAmR6BpZIUA+EtSVqG8u+yAzN9yxBJsurX1l7uHR8tl57fV60cBQFMg0DSyQgCcKjWnSD5dFS//3ZAohSdGRYUFtZA7LoyW6/qFSwsPV6oNgFURaBpZIQBOL7uwVOasO6zXhzpWUKrPqVmIb7kgUm4e3EECvT2oPgBWQaBpZIUAOLvisgrdv+ajVYckKatIn/P2cJWJ/SNk8oVR0i6wBdUIoFEINI2sEADnrryiUn7ZmSb/XnFQ9qTm6nNuLia5slc7uXt4R+kS6kd1AmgQAk0jKwRA/akl4X4/kCkzVxyUdYeOWc6P7tZaB5t+zGUDoJ4INI2sEACNsy0pW0/St2hXmlQvfdu9vb9c3y9cruzVXgJauFPFAM6KQNPICgFgHYcy8uXjVYfku81HLJP0ebq56OHe1/cPlwFRwcxnA+C0CDSNrBAA1nW8oFS+//OInn14X3qe5XxUKx8dbK7p015a+3lR7QBqIdCchEAD2E4/m23JOfLNxkT58c8Uyyrfri4muSimtUwcEC7DOocwWR8AjUBzEgINYHsKSsrl5+2p8vXGRNmSmG0538bfS/7WL0xP1hce7G1oGQEYi0DTyAoB0LwOpOfJNxuT5LstyXK8sMxyfkinlnJ9/wgZe16oeLkzEzHgbHJZnLJxFQLAGCXlFbJk91HdarM6LtMyQkqNirq8R1vd16ZPRBAdiQEnkUugaVyFALCNRTHnbU6WbzclSUpOseV8h5beMr53e711aOljaBkBNC0CTSMrBIDtqKg0y7qDx2T+1mRZtDPNsjCm0q9DkIzv014uP7+dBHgztw3gaAg0jawQALapsLRcFu9Kk/lbjsiauEypPHFLysPVRS7q1lqu6RMmw7uEiIebi9FFBWAFBJpGVggA25eeWyw//HlEh5u9aX/NbRPk7S5X9Gynw03PsAD62wB2jEDTyAoBYF92p+TKgq3J8v2fKZKRV2I5H93KR/e1GRMbKl1a+4mLi8nQcgKoHwJNIysEgP2u/L3m4DFZsCVZFu9Kl6Kyv/rbtPTxkEEdW8rg6JZyQceWepZik4mAA9gyAk0jKwSA/csvKdediBduS5EN8Vm1wk31BH6DVcDpWBVwwoKYxA+wNQSaRlYIAMdSWl4p25OzZe3BY7L2YKZsOZxtWSyzWnhwC7kgupVc0KmqFae1P2tLAUZzuEAzd+5c+eSTT6SiokL/4SIjI+X111/Xj+eCQAOgpuKyCtly+Lgl4Kj1pdTw8Jo6hvhUteBEt5L+kUEEHMAADhdoPDw8ZOHChTJu3DiprKyUW265RTZs2CDbtm0TT0/Ps/48gQbA2W5PbUzI0vPdqG1nSo5lluJqEcHeet6bfpHB0i8ySDqF+NLJGGhiDhdo/va3v8m8efMsx5s2bZL+/fvL2rVrZfDgwWf9eQINgPrIKSyTP+Krws36+CzZm5Z7SsBRyzH07RCkt/6RwdIjLID1pgArq+/3t5vYuJphRvHyqrq3XVLy1/BMALAWNevwuNg2elNyi8tka2K2bE7Iko0Jx+XPpGzJKSqTZXuP6k1xdzVJ9/YBOtyokKNac1r6nr0FGYD12HygOdm6deukXbt2MmTIkDqfV0GnZthRCQ8AGsrfy13PQKw2payiUvak5upws/lwVchR89+o0KO2mvPgqE7GF3UL1R2NWTEcaFo2f8upJhVUzj//fHn11Vdl/Pjxdb7m+eefl2nTpp1ynlFOAJqC+hWalFWk++FsOnxcNiVkyYGj+bVe4+3hKhd2bqXDzaiY1tKK1hvA+frQ1KQ6BIeHh8uLL7542tfU1UKjfoZAA6C5ZBeW6pab5fuOytI96ZKe+9fvJDWfX6/wQBndLVSvQdU11I9J/gBnCjRTp07Vf7gPPvigXj9Hp2AARlK/YnceyZUle9Jl6d50vV9TWFALuSimtW69GRgdLJ5uroaVFbAlDhloXnnlFdmxY4fMmTNHXFxcZPPmzfp83759z/qzBBoAtiQtp1gHm6V7jupVw0vK/5rkz8fDVYZ1CdHhZmTXEDoWw6nlOlqgmTlzprz33nt6cj03t6o+zD/99JOeWE/dgjobAg0AW1VUWiGr4zL1bamle4/WWlxT3ZrqGRaoOyOP6BoiPcICxZUFNuFEch0p0OTl5UlgYKCeUO9ks2bNItAAcBiVlWbZcSRHh5sle47K7tTat6aCvN3lws5V4UY9hvgxLByOLdeRAo010EIDwF5vTf2+P0NW7D8qqw5kSl5xea3nz28fYGm9UZ2M3VxdDCsr0BQINI2sEACwNeUVlbI1KVtW7DsqK/dnnNKx2N/LTbfaDO9aNV9OKItrwgEQaBpZIQBg647mqdabTB1uVh3IkOzCslrPd2vrLwMig6RrG3/p2sZXuoT6iZ+Xu2HlBRqCQNPICgEAe6JWCt+WrFpvMmTlvqOy/cipi2sq7QNbSNc2fjrcxJx47Njah2HisFkEmkZWCADYs2P5JXrk1K6UXNmXlqe3tNziOl+rRk1FtfLRQUdN8Ff9qFYXd2FEFQxGoGlkhQCAI64gvi89r2pLy5X9afl6FfHckzoa11yqQa0gXr2ieJ+IIAn09mj2csO55TLKqXEVAgDOQA1wVUsyqGCzPz1P9p5ozVHrUJXWmOyvWscQH0vAUVt0K19acdCkCDSNrBAAcPY+OQcz8mXLYbWa+HHZnHhcDmUUnPK6gBbu0icisKoFp0OQngTQx7Nq8lPAGgg0jawQAEBtWQWlsjXxRMA5fFx3Qi4uqzylP063tn7SNyJIzg8LlE6tffXmS8hBAxFoGlkhAIAzK6uolD2puZaAo1pzUnLq7njcNsBLB5uOIVUBp/OJoNPSl5mOcWYEmkZWCACg/lKyi2TLiVacval5EpeRX2ttqpOppRw6t1ZDx6sCTnXYUQHIpBaygtPLpVNwbQQaADBudFVcRp7EHc3X24ETj8nHi077M2rF8agQH93pOFo9hvhKdCv16CPeHvTRcSa5BJrGVQgAoOlXGVcdj6uDjt4y8iUhs0DKK0+/vKBqvVHz5kTXCDzqVla7wBasRO6ACDSNrBAAgHF9cw4fK9Cjqg5lqsd8y77qmHw6Hm4uEtXyRNA5EXLUpvZZ8sF5vr9pvwMA2AR3Vxfp1NpPbyfLLiyVgxknQk6NsHP4WKGeN6d64sCTtfbztISbmo9qKQhmQ3YsJrOaXcmB0UIDAI49b86R40VyMPNEa05Gvr6dpcLPmTole6pWnVY+ulNyVYtOVdDp0NKbVh0bwS2nRlYIAMAx5BaX6ZBz8Khq1cmXg0cLdNjRrToVp86GXC3Yx0OvZ6XCTYdgb4lo6WPZD/HzZBRWMyHQNLJCAACOrbyiUo+0UuFGB54aj8fO0FdHaeHuqsNOxImAo4KODjzB3rpzsurPA+sg0DSyQgAAziuvuEwSswol8VihHM4q1K05iVlVfXXUXDtnGIRlCTz+Ldz0bSt/Lzfxb+Fea9/fSx1X79d49HLXkw2qGZdRhU7BAAA0kAofse0C9HYy1fn4SHaRHomlQo8KOWq/KvQUSkl5pRSVVehNLfxZX6pfT/SJ/jzVkw2qTfX18XRz5e/0LBjlBADAOdDDw1v56O1klZVm3Wcnr7hccorKLPu5er9ct/zkFpXr8+qcfq649utUIFJLSqitJtVoo25zVS8fUXN2ZdWygyoEGgAAGkkNAQ/09tBbeANHayVlFepJBi2TDp54VIEn4Vih3pbuPXrKsHQVbDq09NbhRi0G6uvlph/VrS1fT/eTjt3E28PVITs2M2wbAAAbpWZWUcPPqwPOwRpBpyG3tapbfHxUwKkRftSxl7urDjuqH1DN/Rbq8cS+Oqeeq9p3kxYeLtLCw02CvT30a6yJPjQAADgI1ZLS2t9Lbxd0alXrOXW7Sgeco/m6b09+cbnkl5RLXkm5FJSU/3V84lFtqiVIdWxW59QmOdYp5wtXxcrNgyPFSNxyAgDADqlbTL0jgvR2rq09xWWVkldSZgk76lH18Skuq5DC0qoOzUWl5fpRHavzau2tv5476fHE61SLjdEINAAAOElrT4sTt4/qWF2iUWxh0QFmAAIAAI1iC52MCTQAAMDuEWgAAIDdI9AAAAC7R6ABAAB2j0ADAADsHoEGAADYPQINAACwewQaAABg9wg0AADA7hFoAACA3SPQAAAAu0egAQAAdo9AAwAA7J6bOLjqJc1zc3ONLgoAADhH1d/b1d/j4uyBJi8vTz+Gh4cbXRQAANCA7/GAgICzvs5kPtfoY6cqKyslJSVF/Pz8xGQyWTU5qpCUlJQk/v7+VntfR0e9UW9cb7aNf6PUm61cbyqeqDDTrl07cXE5ew8Zh2+hUZUQFhbWZO+v/gIINNRbc+F6o9641mwb/0atW2/n0jJTjU7BAADA7hFoAACA3SPQNJCnp6c899xz+hHUW1PjeqPemgvXGvVmr9ebw3cKBgAAjo8WGgAAYPcINAAAwO4RaAAAgN1z+HlomsqCBQtk+vTp4uXlpee6+eCDDyQ2NtboYtms559/Xr7//nsJDAy0nAsODpb58+cbWi5bVFpaKs8++6y88cYbEhcXJ5GRkbWe//DDD+Wjjz7S156qT7Xfvn17cXZnqrdbbrlF9u7dq+us2nnnnaf/3TqzuXPnyieffCIVFRV6gjNVZ6+//rql7lQXyxdffFH/23Vzc5MuXbrI+++/X6+5QZytzkaMGHHKz4waNUpfm87qhx9+kJkzZ+p/oyUlJVJYWChTpkyRG264wfIaq1xrqlMw6mf9+vVmPz8/8/79+/Xx7Nmzze3btzfn5uZSlafx3HPPmZcvX079nEV8fLx50KBB5ptvvll11tfHNX333Xfmtm3bmjMyMvTxtGnTzL169TJXVFQ4dd2erd4mTZp0yjmYze7u7uZFixbpqlDX0P/93/+Zu3btai4uLtbn/vWvf5l79OhhLiws1Me33nqr+YorrnDqqjtbnQ0fPtzgEtqecePG6e/Jaj/++KPZZDKZt23bZjlnjWuNQNMA48ePN0+cONFyrC7q0NBQ8zvvvNOQt3MKBJpzs2PHDvOBAwd0+Kvri7l3797mqVOnWo6zs7PNbm5u+heEMztbvRFo6nbttdfWOt64caOuv7Vr15rLy8vNISEh5pkzZ1qe37Vrl35++/btZmd1pjpTCDSn2rRpk7msrMxyrP7zr+pswYIF+tha1xp9aBpg6dKl0q9fP8uxuuXUt29fWbJkSUPeDrDo3r27dOrUqc4aycrKkq1bt9a69lRzrGqadfZr70z1htObN29erePqW3LqtsD27dslIyOj1vXWrVs38fHxcerr7Ux1hrqp70d1G0kpKyvTt4XVLd/Ro0frc9a61gg09XTs2DF93zQ0NLTW+TZt2kh8fHx9386pfPbZZ/r+8pAhQ2TSpEly8OBBo4tkV6qvL669hnn55Zf19Td06FC55557JD093ap/P45g3bp1eiFA9W/00KFDp1xvaoFfdczvurrrrNoDDzwgw4cPl2HDhsnUqVP1AosQ/e8uJCREh5TFixeLr6+vrhZrXWsEmnpSnZmUk2c1VMfVz+FUERER0rt3b30hr1q1SqKionRqP3LkCNXFtdfkVCuW+nJZtmyZLF++XP9vetCgQZKfn8/1d4KqE9W59b333hN3d3d+1zWgzpRevXrJZZddJitXrpRffvlFduzYIWPGjNGdiJ3d+++/L5mZmZb/2Kamplr1e5VAU0/e3t51Ni+q4+rncKrbbrtNHnroId3sqG7RPfPMM7qp1tlHmdQH117DPfnkk3LTTTfpa0998bz55puSmJgoX331lRX/huzbXXfdJddff72MHz9eH3O91b/OlLffflvGjh2r91ULxGuvvSbr16/XYRqivwPUaKbKykr979Ca1xqBpp5atmyp+y2c3FydlpYm0dHRXK/nyNXVVQ9z5LbTuau+vrj2Gs/f3183fXP9VVG3RdQXh/qiOdv1po75XVd3ndWlY8eO+tGZr7XS0tJax+o/FqrVdPfu3Va91gg0DaDmFNi8ebPlWI0W27Jli6WDE06l7imfLCUlRd+KwrkJCgrSt+1qXnuqP9f+/fu59up5/an/+an+cFx/Iq+88ookJSXp2yaKur7U1qNHDx36al5ve/bskYKCAqe/3k5XZ0ePHpV//vOfta616tvqznyt9enT55Rz6naT6nukWO1aa8CoNaen5qHx9/fXw0SVOXPmMA/NWURGRpp/+OEHy/HHH39s9vLyMu/Zs8fpr6e6nG74sZqHpl27dubMzEx9/OKLLzIPzTnUm4eHhx5eW+3pp5/Ww0SPHj3q1Nffv//9b3NsbKx53bp1un7UpqZYmDVrlmVukJ49e1rmBrn99tudfh6aM9WZuu6Cg4Mt158ajqymDIiJiTEXFRWZnZXJZDL/9NNPlmP1neni4mJetWqV5Zw1rjVmCm6AAQMGyOeffy4TJ06UFi1a6OYz1WPbz8+vIW/nFNT/WtS9ZXXPVDU/qs5eqoNwTEyM0UWzKapu1P337OxsfayusfDwcMtQ0WuuuUb/L1B1MlR9kFSrzcKFC/U16MzOVm9qmGh1Hy7VyVD9b1B1DlaPzkqNvFGjTlRfhsGDB9d6btasWfpR1ZnqOK06cKq669y5s3zxxRfirM5WZ2q06yOPPKJnwFW/41QLg6oz9f1Qc5ZqZzNjxgz9HaBGGqq6UyOYfvzxRz3isJo1rjWTSjVNUH4AAIBm49z/rQMAAA6BQAMAAOwegQYAANg9Ag0AALB7BBoAAGD3CDQAAMDuEWgAAIDdI9AAAAC7R6ABYBUbNmyQESNG6FlA1QzQL7zwgp659/nnn7fM4NscEhIS9Gee7Oqrr5a33nqr2coBoHkxUzAA6/5SMZn0NPC33HKLDhdRUVESHx+vV1dvDitWrJCRI0fqRWNrUlOrq2VL1LT0ABwPazkBcAq0zgCOjVtOAJrE7t279SKRinpUt6MWLFigj9UidHfccYf07t1bhg8frm8HJSYm6udWr14tgwYN0i09anHJq666Sjp16iS9evXSz3/wwQcycOBA3QrTv39/vehddWvMsmXL5MEHH9T76vPUtm7dOnnsscd0C5E6rmnOnDn6fdX7qbJUL2apTJ48WS82ePPNN8vjjz+uy9m1a1e90CAAG2S9BcIBQCcL86xZs3RVxMfH62P1WNMNN9ygt4qKCn08ffp083nnnWcuLy+v9XO33Xabfk1eXp55xIgR+rn+/fubd+zYoffz8/PNPXr0MM+ePdvy3suXL9c/e7LnnnvOPHz4cMvx4sWLzb6+vua9e/fq4+3bt5u9vLzMa9assbxm0qRJ5qCgIPOePXv08YwZM8wRERH8NQM2iBYaAM3q0KFD8vXXX8vDDz8sLi5Vv4LuvPNO3aKj+r/UpFpH1Gt8fX1l+fLl+pxqRenevbve9/HxkUsvvVT+97//1bscqmVHtQypVhfl/PPPl3Hjxsn06dNrvU613KhOzopq4VEtScePH2/gnx5AU6EPDYBmtWvXLn2L6IEHHhB3d3fL+Q4dOkhGRkat14aFhZ3y88nJyXL//fdLZmam/vnqjsf1tXPnThk1alStc+rWVs3bTkq7du0s+35+fvoxNzdXgoKC6v2ZAJoOgQaAIb788suzBhFXV9dax4cPH5YxY8boIeGPPvqoPqeGaJ/csmNNNcug+vUoJ4+gAmA8bjkBaLpfMCduKSmVlZVSUFAgsbGx+njfvn21Xvvss8/K3r17z/h+mzZtkqKiIrn++ust50pLS0/7meXl5fr1dVG3reLi4mqdO3jwoL71BMD+EGgANJmWLVvqgKH6nKgwouamiY6O1nPBvPbaa1JcXKxft3btWvnuu+/0LZ8zUX1ZVCvJ0qVL9bEKKyf3nwkJCdGP6jPnz5+vg1JdnnrqKfnhhx/kwIEDllthixYtkieffNIqf3YAzczoXskAHMP69ev1KCL1a6Vr167madOm6fOPPfaYOTY21jxw4EDz6tWr9Tk1aunOO+/Ur1Ojl6644grzgQMH9HNbt27Vr1Xvox7ffffdWp8zc+ZMc2RkpPnCCy80X3vtteYJEyaYAwICzDfeeKPlNWq/V69e5sGDB+tRTFOmTDF36NBBv+6yyy6zvE6NjurZs6d5wIAB+vXffPON5bkHHnjAHBoaqjf18+p9apZLjYoCYDuYKRgAANg9bjkBAAC7R6ABAAB2j0ADAADsHoEGAADYPQINAACwewQaAABg9wg0AADA7hFoAACA3SPQAAAAu0egAQAAdo9AAwAAxN79f4HyL9e0PqReAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'x': 7, 'cx': 4, 'rx': 2, 'barrier': 2, 'u2': 1, 'rz': 1, 'h': 1})\n",
      "On circuit 1/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'cx': 8, 'x': 7, 'rx': 4, 'h': 3, 'barrier': 3, 'rz': 2, 'u2': 1})\n",
      "On circuit 2/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 16, 'cx': 12, 'x': 7, 'rx': 6, 'h': 5, 'barrier': 4, 'rz': 3, 'u2': 1})\n",
      "On circuit 3/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 16, 'measure': 16, 'rx': 8, 'x': 7, 'h': 7, 'barrier': 5, 'rz': 4, 'u2': 1})\n",
      "On circuit 4/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 20, 'measure': 16, 'rx': 10, 'h': 9, 'x': 7, 'barrier': 6, 'rz': 5, 'u2': 1})\n",
      "On circuit 5/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 26, 'measure': 16, 'rx': 12, 'h': 11, 'x': 7, 'barrier': 7, 'rz': 6, 'u2': 1})\n",
      "On circuit 6/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 28, 'measure': 16, 'rx': 14, 'h': 13, 'barrier': 8, 'x': 7, 'rz': 7, 'u2': 1})\n",
      "On circuit 7/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 30, 'rx': 16, 'measure': 16, 'h': 15, 'barrier': 9, 'rz': 8, 'x': 7, 'u2': 1})\n",
      "On circuit 8/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 32, 'rx': 18, 'h': 17, 'measure': 16, 'barrier': 10, 'rz': 9, 'x': 7, 'u2': 1})\n",
      "On circuit 9/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'rx': 20, 'h': 19, 'measure': 16, 'barrier': 11, 'rz': 10, 'x': 7, 'u2': 1})\n",
      "On circuit 10/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 42, 'rx': 22, 'h': 21, 'measure': 16, 'barrier': 12, 'rz': 11, 'x': 7, 'u2': 1})\n",
      "On circuit 11/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 44, 'rx': 24, 'h': 23, 'measure': 16, 'barrier': 13, 'rz': 12, 'x': 7, 'u2': 1})\n",
      "On circuit 12/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 48, 'rx': 26, 'h': 25, 'measure': 16, 'barrier': 14, 'rz': 13, 'x': 7, 'u2': 1})\n",
      "On circuit 13/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 52, 'rx': 28, 'h': 27, 'measure': 16, 'barrier': 15, 'rz': 14, 'x': 7, 'u2': 1})\n",
      "On circuit 14/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 56, 'rx': 30, 'h': 29, 'barrier': 16, 'measure': 16, 'rz': 15, 'x': 7, 'u2': 1})\n",
      "On circuit 15/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 60, 'rx': 32, 'h': 31, 'barrier': 17, 'rz': 16, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 16/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 62, 'rx': 34, 'h': 33, 'barrier': 18, 'rz': 17, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 17/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 66, 'rx': 36, 'h': 35, 'barrier': 19, 'rz': 18, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 18/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 68, 'rx': 38, 'h': 37, 'barrier': 20, 'rz': 19, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 19/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 70, 'rx': 40, 'h': 39, 'barrier': 21, 'rz': 20, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 20/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 74, 'rx': 42, 'h': 41, 'barrier': 22, 'rz': 21, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 21/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 78, 'rx': 44, 'h': 43, 'barrier': 23, 'rz': 22, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 22/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 82, 'rx': 46, 'h': 45, 'barrier': 24, 'rz': 23, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 23/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 86, 'rx': 48, 'h': 47, 'barrier': 25, 'rz': 24, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 24/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 88, 'rx': 50, 'h': 49, 'barrier': 26, 'rz': 25, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 25/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 92, 'rx': 52, 'h': 51, 'barrier': 27, 'rz': 26, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 26/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 96, 'rx': 54, 'h': 53, 'barrier': 28, 'rz': 27, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 27/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 100, 'rx': 56, 'h': 55, 'barrier': 29, 'rz': 28, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 28/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 104, 'rx': 58, 'h': 57, 'barrier': 30, 'rz': 29, 'measure': 16, 'x': 7, 'u2': 1})\n",
      "On circuit 29/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 108, 'rx': 60, 'h': 59, 'barrier': 31, 'rz': 30, 'measure': 16, 'x': 7, 'u2': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sampler.run([to_run], shots=30_000)\n",
    "    job = sim.run(to_run)\n",
    "    # bit_array = job.result()[0].data.meas\n",
    "    # bit_array = job.result().data().meas\n",
    "    counts = job.result().data()['counts']\n",
    "    bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAARAVJREFUeJzt3Qd41EX+x/HvppAQUggtAZJAAAEpAaQLEkCaIIKACuqB5dT7nwW9U09QBCxgr6joWQ6xYkFAFBQEBYUgvfdAAimUkAIhff/PTEgkECBlN7/fb/f9ep59tiY7DCv7ceY7Mza73W4XAAAAi/IwugEAAACVQZgBAACWRpgBAACWRpgBAACWRpgBAACWRpgBAACWRpgBAACW5iUurqCgQBISEiQgIEBsNpvRzQEAAGWgtsHLyMiQBg0aiIeHh3uHGRVkwsPDjW4GAACogPj4eAkLC3PvMKNGZIo6IzAw0OjmAACAMkhPT9eDEUXf424dZoqmllSQIcwAAGAtZSkRoQAYAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYGmEGAABYmuFhJicnRx577DHx8vKSAwcOlLqd8Z133skhkQAAwHxhRoWX6OhoSUxMlPz8/POe37Bhg/Tp00efmgkAAGC6MHPy5EmZPXu23H777aU+n52dLQsXLpTBgwdXedsAAIA1GHrQZJs2bfT1oUOHSn2+W7duYmY7k9Il2K+ahAT6Gt0UAADcluE1M46mRnNUnc3ZF2d45vvtMui1FTLrj/PrfAAAQNVxuTAzffp0CQoKKr6Eh4c75X06NQ7W13PWHpLc/AKnvAcAAHDDMDNhwgRJS0srvsTHxzvlfa6+PETq+PvIsZPZsnTHEae8BwAAcMMw4+PjI4GBgSUuzuDt6SE3dArTtz9fE+eU9wAAAG4YZqrS6M6FU1i/7Tkqh05kGt0cAADcEmGmEhrVriE9mtUWu11kzp/Omc4CAAAmDjNq99/evXvLgw8+qO+PHj1abrjhhuLn4+Li9PPPPfecvq9u33///WImY7pEFBcC51EIDACAe+0zU61aNVm+fPkFn4+IiLjo82bQv1WI1KpRTZLSs2T5rqPSr1WI0U0CAMCtMM1UST5enjLyiob69hd/UggMAEBVI8w4wOgzU02/7DwiSWlZjviVAACgjAgzDtC0rr90iawlBaoQeC2FwAAAVCXCjIPcfGZ05ss/4yVfpRoAAFAlCDMOMqhNqARV95bDqadlxZ6jjvq1AADgEggzDuLr7SkjigqB1zDVBABAVSHMOGHPmSU7kuVIBoXAAABUBcKMAzUPCZCOjYIlr8AuX6875MhfDQAALoAw46TzmtRUUwGFwAAAOB1hxsGujWogAb5eEpeSKav2H3f0rwcAAOcgzDhY9WqeMrx9YSHwZ2vYERgAAGcjzDjB6C6FU00/bUuS4yeznfEWAADgDMKME7RuECTtwoIkN98u36ynEBgAAGcizDj5vCZVCGy3syMwAADOQphxkqHtGkiNap6y/9gpiYlNcdbbAADg9ggzTuLv4yXXtW+gb39BITAAAE5DmKmCHYF/2JokqZk5znwrAADcFmHGido2DJJW9QMlJ69Avl1/2JlvBQCA2yLMOJHNZpMxXQtHZz5fE0chMAAATkCYcbJh7RtIdW9P2XPkpKyPO+HstwMAwO0QZpws0Ndbro2qr29/FhPv7LcDAMDtEGaqcM+ZhVsSJO10blW8JQAAboMwUwWuiKgpzUP8JSu3QOZtpBAYAABHIsxUVSHwmdGZz2IoBAYAwJEIM1Xk+g4NpZqXh+xMypBNh9Kq6m0BAHB5hJkqUtOvmgxpW1gIzI7AAAA4DmGmCo3uHK6v529KkJPZeVX51gAAuCzCTBXqEllLmtStIZk5+TJ/Y0JVvjUAAC6LMFPVhcCd/9oRGAAAuECYycnJkccee0y8vLzkwIED5z3/7rvvSseOHaVHjx4yZMgQOXzY2kubR3YMk2qeHrLlcJpsPUwhMAAAlg4zKrxER0dLYmKi5Ofnn/f8t99+K1OnTpXFixfL77//Ll27dpVrr71WCgoKxKpq1agmA1qH6NuMzgAAYPEwc/LkSZk9e7bcfvvtpT7/zDPPyLhx46ROnTr6/vjx42Xr1q2ycOFCsbKbz+w5M29jgmTmUAgMAIBlw0ybNm2kWbNmpT6XkpIiGzZskE6dOhU/FhQUJM2bN5clS5aIlXVrUlsa1fbTK5q+35RodHMAALA0w2tmLiQ2NlZfh4QUTskUCQ0NLX6uNNnZ2ZKenl7iYjYeHja56cwy7VmrDojdbje6SQAAWJZpw0xmZqa+9vHxKfG4ul/0XGmmT5+uR3CKLuHhhaHBbEZ3jpDq3p6yLSFdVu49ZnRzAACwLNOGGT8/v+KRlrOp+0XPlWbChAmSlpZWfImPjxezFgKP7lIYtN5Zvs/o5gAAYFmmDTNNmjTR18nJySUeT0pKKn6uNGrkJjAwsMTFrP5+VRPx8rDJH/uOy8b4VKObAwCAJZk2zAQHB0uHDh1k3bp1xY+p+pfdu3dLv379xBU0rFldhrVvqG/PZHQGAADXCjPKE088IbNmzZLjx4/r+2+88YZeATV48GBxFf+ILhxlWrw9SfYdPWl0cwAAsBwvo3f/HTBggKSmFk6xjB49WhfsfvXVV/r+iBEj5MiRI9K/f3/x9fXVozULFiwQDw9TZ7ByuSwkQPpdHiJLdiTLe7/ul+dHRRndJAAALMVmd/F1wWpqSq1qUsXAZq2fWXfwhIx85w/x9rTJikf7SmiQr9FNAgDAMt/frjPEYWEdGwXrE7Vz8+3ywcr9RjcHAABLIcyYxP/1bqqvP4uJk7TMXKObAwCAZRBmTKJ387rSMjRATuXky+zV558eDgAASkeYMQmbzVY8OvPR7wfkdM75p4gDAIDzEWZMZEjb+hIWXF2On8qRr9aZc+diAADMhjBjIl6eHnJPr8J9Z979db/k5hcY3SQAAEyPMGMyN3QKl9o1qsnh1NOycHOi0c0BAMD0CDMm4+vtKXf0jNS3Z/66T1x8GyAAACqNMGNCt3ZrJP4+XrIzKUOW7zpqdHMAADA1wowJBVX3lpu7Rujb73AAJQAAF0WYMak7e0ZKNU8PWXMgRdYeSDG6OQAAmBZhxqRCAn1lxBUNi2tnAABA6QgzJnZ3ryZis4ks2XFEdidnGN0cAABMiTBjYk3q+ss1bUL1bUZnAAAoHWHG5P4RXXjEwfyNCXLoRKbRzQEAwHQIMyYXFVZTejSrLXkFdnl/RazRzQEAwHQIMxbwf9HN9PUXf8ZJyqkco5sDAICpEGYsQI3MtG0YJFm5BTLrjwNGNwcAAFMhzFiAzWaT/+tdWDsza9UBOZWdZ3STAAAwDcKMRQxsHSqRdWpIamaufPFnvNHNAQDANAgzFuHpYZN7ejXRt99fsV9y8gqMbhIAAKZAmLGQ669oKPUCfCQxLUvmbTxsdHMAADAFwoyF+Hh56jOblHd/2y8FBXajmwQAgOEIMxajTtMO8PWSvUdOypIdyUY3BwAAwxFmLCbA11vGdm+kb7+9fJ/Y7YzOAADcG2HGgm67MlJ8vDxkY3yqxMSmGN0cAAAMRZixoLoBPnJjp3B9+91f9xndHAAADEWYsai/XxUpNpvI8t1HJe44B1ACANwXYcaiGtWuIb0uqyuqZObzP+OMbg4AAIYhzFjYLV0j9PWcP+PZRA8A4LZMH2ZycnJk6tSp0q1bN+nevbv07NlT1q5da3SzTKFvy3oSGugrx0/lyKJtSUY3BwAAQ5g+zEyYMEG+/vpr+fnnn2XVqlVy3333yYABA+TIkSPi7rw8PWR0l8JC4E9XHzS6OQAAGMLUYaagoEBmzpwpd9xxhwQEBOjHRo8eLT4+PvLuu+8a3TxTGN05Qp/bpJZo7z2SYXRzAACocqYOM8eOHZPMzEwJCQkp8XhoaKj89ttvhrXLTEKDfOXqlvX07U9jKAQGALgfU4eZunXrSo0aNSQu7q8vabXjbWJiohw6dKjUn8nOzpb09PQSF1d3S7fCHYG/WXdITufkG90cAACqlKnDjM1m0zUy7733nhw+XHhK9DvvvCMpKSmSn1/6l/b06dMlKCio+BIeXlhT4squalZHwmtVl/SsPPl+c4LRzQEAoErZ7CY/3EeFlldeeUXmzZunw010dLTEx8fLzp07JSYmptSRGXUpokZmVKBJS0uTwMBAcVXvLN8nzy/aKe3Da8p39/YwujkAAFSK+v5WgxJl+f42fZgpzbXXXqvrZt5//32HdoaVHTuZLd2nL5XcfLt8f39PadMwyOgmAQBQYeX5/jb1NJOyefNmPa1UJDc3V1avXi2jRo0ytF1mU8ffRwa1qa9vUwgMAHAnpg8zql5mxowZxfefeeYZ6dy5swwaNMjQdpnRrWd2BJ638bBkZOUa3RwAAKqE6cNM165d5fPPP9fXavff1NRUvYkeztclspY0q+cvmTn58t1GCoEBAO7BkjUz5eEuNTNFPvo9VqYu2C4tQwPkx/FX6aJpAACsxqVqZlA+IzqEia+3h+xMypD1cal0HwDA5RFmXEyQn7cMjWqgb38aw3lNAADXR5hx4R2Bv9+cKKmZOUY3BwAApyLMuKB2YUHSukGg5OQVyNfrSj/2AQAAV0GYcUGq6PeWro2K95xx8RpvAICbI8y4qOvaNxB/Hy+JPXZKVu07bnRzAABwGsKMi1JBZniHokLgv04dBwDA1RBmXFjRVNPibUlyJCPL6OYAAOAUhBkXdnn9QOnYKFjyCuzy1VoKgQEArokw4+JuOXNe02cxcZJfQCEwAMD1EGZc3OC29aWmn7ccTj0tv+0+anRzAABwOMKMi/P19pRRV4Tp25+sZkdgAIDrIcy4gTFnppp+2XVEDp3INLo5AAA4FGHGDTSt6y9XNq0tau+8L/+MN7o5AAA4FGHGzZZpf/FnvOTmFxjdHAAAHIYw4yb6twqROv4+cjQjW5ZsTza6OQAAOAxhxk1U8/KQmzoXFgKzIzAAwJUQZtzI6M4RYrOJrNx7TJ/ZBACAKyDMuJHwWn7Su3ldffuzGJZpAwBcA2HGzdzarbAQ+Kt1hyQrN9/o5gAAUGmEGTfTu0U9aRDkK6mZufLj1kSjmwMAQKURZtyMp4dNxnQp3ETv09VxRjcHAIBKI8y4oZs6h+tQs/bgCdmZlG50cwAAqBTCjBuqF+grA1qFFJ+mDQCAlRFm3HxH4LkbDkt2HoXAAADrIsy4KXVWU0igj2Rk5cnKPceMbg4AABVGmHFTHh42uaZNfX174WZWNQEArIsw48aGRBWGmZ+3JzPVBACwLMKMG+sYEVw41ZTNVBMAwLoIM26MqSYAgCswfZjJzs6Whx56SNq1ayfR0dHStWtXmTt3rtHNchlMNQEArM5LTO6ZZ56R7777TjZu3ChBQUGyYcMG6datm6xZs0YHHDhmqik5PVuvarr68sL9ZwAAsArTj8yoENO5c2cdZJQOHTro27/88ovRTXMJTDUBANwuzGzevFm2bdsmVWXkyJGyYsUKiYsr3Kl28eLFcvToUQkJCbngtFR6enqJCy6OqSYAgFuFmfbt28urr74qVeW2226TSZMmSVRUlFx++eUyePBgGTVqlNx4442lvn769Ol65KboEh4eXmVttSpWNQEA3CrM9OzZU95//32pKuq9nnvuOVm3bp3s2LFD1q9fr2tmPDxKb/qECRMkLS2t+BIfH19lbbUqppoAAG4VZtq0aSMJCQmlPnfdddeJI9ntdnn00UflnnvukaZNm+rHVNHvDz/8INOmTSv1Z3x8fCQwMLDEBZfGVBMAwG1WMwUEBMiVV14pV199tYSFhYmnp2fxc1u3bnVo41RtzIkTJ6Rx48YlHo+MjJRvvvlGnnjiCYe+nztjVRMAwG3CzHvvvafrZvbv368vZ0tNTXVk26ROnTp6pCUxseTZQeq+n5+fQ9/L3RVNNf3vjwP6rCaWaAMAXDbMqJqZBQsWlPrcmDFjxJFUXcy4ceN03cydd94pwcHBumbm559/lhdffNGh74XCqSYVZorOavLx+mvUDQAAs7LZVWGKiWVmZsqUKVNkyZIlejQmIyNDBxy1K7DNZrvkz6ul2WpVkyoGpn7m4goK7NL9uaV6A70PxnVidAYAYJjyfH9XaAfggwcPyssvvyxbtmzR99u2bSv//ve/pVGjRuJoKsC88MILDv+9OB9TTQAAt1jNtHz5cmnZsqXeyE7VtKjLypUr9R4wv/76q3NaiSrDqiYAgNWUe2Rm4sSJMn/+fOnfv3+Jx9U00GOPPSarVq1yZPtg4KqmFbuPSb9WnNUEAHCxkRlVYnNukFH69eunn4PrbKD3w5aSq8gAAHCJMHPq1Ck5duxYqXvCqGJdWB9TTQAAl55mUiuJOnbsKLfffnvxrrx79+6VWbNmyQMPPOCMNqKKMdUEAHDpMKNWLaldgNVxAkUnWUdERMjjjz8ud911lzPaCANXNampJupmAAAutc+MWvet9ndRgebkyZP6MX9/fzEr9pmpmD8PpMgNM1dJgI+XrJ3Ujw30AACm/f4ud81MzZo1ZeTIkcUhxsxBBpWfasrIztOrmgAAMKtyh5nOnTvLTz/95JzWwDRY1QQAcNkw06JFC32kQGnuvvtuR7QJJsGqJgCASxYAR0VFSe/evWX48OESFhYmnp5/HUaodgKG62BVEwDAJQuAq1evLqGhoaU+l5ycbLq9ZigArpwp87fpVU0jOjSUV25q76C/FQAADDxoslu3brJs2bJSn+vTp095fx0sMNWkwszP25MlOy+fVU0AAOvXzPz973+XH374odTnLhRyYF2sagIAuFyYUTv/rlu3zjmtgemwqgkA4HJhplevXjJp0qRSnzNbvQwcg1VNAACX22dmy5YtpT537bXXOqJNMBmmmgAAZlbuAuCEhAS9NLt9+/bnLc3euXOno9sHE+CsJgCAS43MqN1/r7vuOn24pIeHh6iV3UUXuC6mmgAALjMyo6aS/vvf/5b63EMPPeSINsGE2EAPAOAyIzMXCjLKq6++Wtn2wKRY1QQAcJkwo3z55ZcSHR0tPXr00PeffvppmT17tqPbBpO5Nqq+vi7aQA8AAEuGmXfffVcefvhhadeunZw+fVo/NmLECJk7d668/vrrzmgjTOKKiGAJDfSVjOw8WbH7mNHNAQCgYmFGjcBs2rRJ3njjDX1mgtK6dWs9WvPNN9+U99fBYlNNg9oUnsv1w5ZEo5sDAEDFwoxawVSrVi1922azFT/u7e0tOTk55f11sBimmgAAlg8z2dnZsnXr1vMeX7JkieTnU0fh6phqAgBYPsxMmTJFn5yt9prZs2ePPqvpyiuv1Eu2p02b5pxWwjSYagIAWD7MXHPNNRITE6OnmkJCQvTRBs2bN5cNGzZI//79ndNKmApTTQAAS2+aV1Tw+7///c/xrYGlppqS0rP0qqZ+rUKMbhIAwI1VKMxUpZYtW0poaOEKmiKHDh2SBg0ayG+//WZYu9xZ0VTT//44oFc1EWYAAEYyfZhRQWb58uUlHhs1apT06dPHsDahcKpJhRm1gV5ufoF4e1Zo/0UAACrN9N9AH330UYn7KSkp8vPPP8vNN99sWJtQONUU7OetN9DbfCiNLgEAGMb0YSYyMrLE/c8//1wXIQcHBxvWJhRONXVuXLjfUEzscboEAGCdMNOrVy8xkio8VsvBL7YPTnp6eokLnKNrk9r6OmZ/Cl0MALBOmNm+fbt06dJFpk6dKgcPHnROqy7y3klJSRddAj59+nR9zELRJTw8vErb6E66RhaOzKw9kCJ5+QVGNwcA4KbKHWbuvPNO+eOPPyQqKkrGjx8vAwcOlE8++USysrKkKkZlxo4dq49UuJAJEyZIWlpa8SU+Pt7p7XJXl9cPlABfLzmVky/bEhgBAwBYJMw8//zz4uXlJddff7189913+uDJtWvXSv369eWee+6R1atXO6Wh6qiETz/99KJTTIqPj48EBgaWuMA5PD1s0oW6GQCA1cLMV199pa9zc3Nlzpw5Mm7cOJkxY4bUrl1bGjZsqFcf9ezZ87zl1JX1008/SdOmTaVZs2YO/b2onK5NzhQBUzcDALDKPjOqVmbFihV6lESdkq32fPnll19KFAanpqbKgAEDZM2aNVVW+AtjdI0sLAJecyBF8gvserQGAABThxlVhKtGYV566SW58cYbpUaNGue9ZseOHZKQkOCoNupwtHTpUvnggw8c9jvhGK0bBIq/j5dkZOXJjsR0adMwiK4FAJg7zKjN6lTB78WoEZu3335bHKVmzZpy7Ngxh/0+OI6Xp4d0ahwsy3cdlZjYFMIMAMD8NTNNmjS55Guio6Pluuuuq2ibYNGpppj9bJ4HALDAyIxaveTt7S12u/2859TjjRs31jv0qtEUuFcRsKqbKSiw692BAQAwbZhp1KiRPPXUU3opdkREhNhsNomLi5Pjx49Lp06dJDExUe8/s3jxYunQoYNzWg1TadswSPyqeUpqZq7sPpIhLUNZDg8AMPE0U/fu3fX5SCrArFy5Uq9sUjsBz5o1SwYNGiS7du3SNTWPPPKIc1oM01EnZndsVHhWFku0AQCmDzNqubVajn2ukSNH6iXailqWrYqA4X5HG3DoJADA9GFm3759eqn0uVJSUvSoDNz70Mk1sSml1lMBAGCampmhQ4dKx44d9c6/kZGR+rH9+/fLxx9/rI84UDsDq8Me1bECcB9RYUHi4+Uhx07myL6jJ6VZvQCjmwQAcBPlDjOvvfaaPrbgzTff1MW+iioGfuCBB+Thhx+W06dP6031VKCB+/Dx8pQrIoJl1f7jsnp/CmEGAFBlbPZyzgmkp6frFUwBAQH6tmLmwxxVG4OCgvQJ2mZupyt4bclueW3JHhnaroG8OYaVbACAqvn+LnfNjNo/RhX7KpxKjQttnkfdDACgqpQ7zHTu3FmfYA2cq0NETanm6SFHMrLlwPFMOggAYM4w06JFC8nIyCj1ubvvvtsRbYJF+Xp7Svvwwp2fOdoAAGDaAuCoqCjp3bu3DB8+XMLCwsTT07P4ObWJHtybOtpAHWugDp0c3SXC6OYAANxAucPMpEmTJDQ0VD788MPznktOTnZUu2Dhupk3ZW9x3YwqFgcAwFRhplu3brJs2bJSn+vTp48j2gQLu6JRTfHysElCWpYcOnFawmv5Gd0kAICLK3fNzPfff3/B5y4UcuA+/Kp56Q30lNX7jxvdHACAGyh3mKlRo4bEx8fL5MmT5V//+pd+bO7cubJnzx5ntA8WPtpA1c0AAGC6MKOKfNWKJhVgFi1apB9TRxioowyWLl3qjDbCYjh0EgBg6jCjCoBVaNm8ebOEhITox2688UY9xfTss886o42wmE6Na4mnh03iU05LQuppo5sDAHBx5Q4zaoVK9+7d9e2zV6rUrVtX8vPzHds6WJK/j5e0aVC49XRMLHUzAACThRl1RkJpm+apOppjx445ql1wlbqZ/dTNAABMFmZuvvlm6dq1q7zyyity9OhR+fjjj2XixIl6yfZdd93lnFbCsnUzrGgCAJhun5lHHnlEn2I5bdo0iYuLk9tuu00iIiJkypQphBmUqJtRs5DqjKbk9CwJCfSldwAA5hiZKTqD6cCBA/p4bnVRtxmVwdmCqntLq/qFdTOMzgAATBdmivj7++vL2aM2wNlHGyjsNwMAMNU0k9pT5rPPPpONGzfqURm1uqmI2nfmxRdfdHQbYeFDJz/8PZYTtAEA5goz48aNkxUrVkiXLl0kICCAgwRxQV0aFxYB7zt6So5mZEvdAB96CwBgfJhRIzLq6AJf3/MLOtWqJqBIcI1q0jI0QHYmZcia2BQZElWfzgEAGF8z07Jly1KDjDJ27FhHtAkupFvxOU1sngcAMEmYGT16tNx3333yxx9/SGxsrF6eXXS54447nNNKWP+cJjbPAwCYZZpJhRnl7bffLlEvowqBz77vSPv379crpVJSUuTIkSO6VmfGjBnSqVMnp7wfHKfLmTCzKzlDUk7lSK0a1eheAICxIzNq9181IqMuKmScfVFFwY6mdhm++uqrZfz48fowy02bNomfn5/s3bvX4e8Fx6vt7yOX1Stcvq/qZgAAMHxk5qWXXpJGjRqV+tzMmTPF0Z5//nl9sGWvXr30fS8vL3nvvfd0oIF1lmjvOXJS180MahNqdHMAAO4+MtOjR48LPteuXTtxtG+//bY4yBRp1qyZNGjQoNTXZ2dnF+9MXHSBSTbPo24GAGBUmImMjJQmTZro/WVKM2fOHP0aR4+WnDp1Sk9n5efnyy233KKD1MCBA+XHH3+84M9Mnz5dnx1VdAkPD3dom1CxkRllR1K6pGXm0oUAAIey2c/ewvcC+vTpo+tVlKlTp5Yo9H3yySeLb6vpoFWrVjmscYcPH5awsDAJDg7W769GfpYuXVocaPr371/qyIy6FFEjMyrQpKWlSWBg4VlBqHp9X1ou+4+dkvfHdpJ+rUL4KwAAXJT6/laDEmX5/i7TyMzZ4aVx48a6ZuaLL77Qty/0Okfw9PTU10OHDi2ewlLFwH379pXXX3+91J/x8fHRf+izLzDP6Az7zQAADK+ZUccZqEtISIjTN8mrW7euDicNGzYs8bgKU2r6CdbBoZMAANOdmu2sPWXOHZlRdTKJiYklHk9OTpaIiAinvz8cPzKz9XCaZGRRNwMAqOKl2SpMzJ49u8QJ2UlJSec9pvaEcbT//Oc/eqM+tcOwCjDbt2+Xn376Sb7++muHvxecp35QdYmo5SdxKZmy9uAJ6dOiHt0NAKi6MLNr1y49tXSucx9zxmjNgAED5I033pBhw4aJv7+/5OXlyaxZs+Taa691+HvB+UcbqDCjlmgTZgAAVRpmoqOji1czXYxazeQMt956q77A2ro2qS1frTtEETAAoOprZl544YUy/bLXXnutsu2BGxw6ueVQmmTm5BndHACAO4WZzp07l/ncJuBCwmv5ScOa1SWvwC7rDp6gowAAxq5mAiozOsPRBgAARyHMoEqxeR4AwNEIMzBk87xN8WmSlZtP7wMAKo0wgyrVqLafhAT6SE5+gayPo24GAFB5hBlUKbUXUfHRBvtT6H0AQKURZlDlqJsBADgSYQZVrmhkZkNcqmTnUTcDAKgcwgyqXNO6NaSOv49k5xXoQmAAACqDMAOD6maK9ps5zt8AAKBSCDMwuG6GImAAQOUQZmBo3Yw61uB0DnUzAICKI8zAEJfV85f6Qb5yOjdfHvhig+QX2PmbAABUCGEGhvDwsMnroztINS8P+Xl7skyev1XsdgINAKD8CDMwTJfIWvLaTe3FZhP5ZHWcvPPrPv42AADlRpiBoQa3rS+ThrTSt19YtEu+XX+IvxEAQLkQZmC4O3pGyl1XRerbj369WVbuOWZ0kwAAFkKYgSlMuOZyGdqugeQV2OUfn6yTbQlspgcAKBvCDExTEPzSDVHSrUktOZmdJ7d/9KccOpFpdLMAABZAmIFp+Hh5yrt/6yQtQgLkSEa23PbRn5KamWN0swAAJkeYgakEVfeWj27vLKGBvrL3yEm5++N1kpXLpnoAgAsjzMB0GtSsLv+7o7ME+HjJmgMp8q85G6WATfUAABdAmIEptQwNlHfHdpRqnh7yw5YkeXrhdjbVAwCUijAD07qyaR156cZ2+vZHvx+Q91fEGt0kAIAJEWZgate1ayATB7fUt5/9YYfM35RgdJMAACZDmIHp3XVVE7ntysb69sNzNsmqfceNbhIAwEQIMzA9m80mk65tJde0CZWc/AK5e/Za2ZmUbnSzAAAmQZiBJXh62OTVm9pL58bBkpFVuKleYtppo5sFADAB04eZKVOmSPv27aV3797FlxEjRhjdLBjA19tT/ju2kzSr5y+JaVk60KRn5fJ3AQBuzkss4LXXXtMhBqjpV03+d3tnGfH2H7IzKUPu+Xid3mRPBR0AgHsy/cgMcK6wYD8dYGpU85RV+4/LvZ+ul5y8AjoKANwUYQaW1LpBkPx3XCfx8fKQpTuPyAOfb5DcfAINALgjS4SZDz/8UE8z9ejRQ8aNGyf79u274Guzs7MlPT29xAWuu6nee2M76V2CF21Lkn/N2ST5HHsAAG7H9GEmIiJCOnToIEuWLJEVK1ZIZGSkdOzYUQ4fPlzq66dPny5BQUHFl/Dw8CpvM6pOdPO68s6tV4iXh00WbEqQR7/ezDlOAOBmbHa73S4Wkp+fLw0bNpQ777xTnn322VJHZtSliBqZUYEmLS1NAgMDq7i1qCqLtibKvZ9t0CMzY7qEy7Tr2+r9aQAA1qS+v9WgRFm+v00/MnMuT09Pady48QWnmnx8fPQf+uwLXN+gNvX1PjQeNpHP18TLlPnbOJgSANyE6cPM+PHjz3ssISFBTz8B557j9MKodqIGZGatOijTfthBoAEAN2D6MDN//nx9KfL+++/L0aNH5Y477jC0XTCnUR3D9BST8t8VsfLyT7uNbhIAwN03zVN1MWrTvFdeeUVycnL0NJIqBm7ZsvAkZeBcY7pE6H1nJs/fJjOW7ZVqXh7ywNWX0VEA4KIsVwDszAIiuJb//rZfnv1hh7792DUt5R/RTY1uEgCgjFy6ABgoq7t6NZFHBrbQt5/7cad8uDKWzgMAF0SYgUu7t08zeaBvM337qe+3yyerDxrdJACAgxFm4PIe6t9c7oluom8/8d1WmfNnvNFNAgA4EGEGLk9tnvfYoJZye4/G+v5/vt0s320ofQdpAID1EGbgNoHmyWtbyS1dI0SVvP9rzkZZuDnR6GYBAByAMAO3CjRPD2sjN3QME3Ue5fgvNshP25KMbhYAoJIIM3ArHh42eW5klAxv30DyCuxy72fr5au11NAAgJURZuB2PD1s8tIN7WRIVH3JzbfLI19vlolzt0h2Xr7RTQMAVABhBm7Jy9ND3hzdQR7q11yf5fRZTJzc+O5qSUg9bXTTAADlRJiBW085je93mXx4W2cJqu4tm+JT5do3V8ofe48Z3TQAQDkQZuD2+rSoJwvu6ymt6gdKyqkcufWDGJn56z5O3AYAiyDMACISUdtPvv3nlTLyisKVTur4g//7ZL1kZOXSPwBgcoQZ4Axfb0956YYoeWZ4G/H2tMmibUky7K3fZU9yBn0EACZGmAHO2Yvm1m6NZM493SU00Ff2Hz2lAw0b7AGAeRFmgFJ0iAiW7x/oKd2b1JbMnHy9H82zC7dLXn4B/QUAJkOYAS6gjr+PzL6zS/Ehlf9dESu3vB8jRzOy6TMAMBHCDHCJ/WgmXHO5vHPLFVKjmqfExKbItW+ukHUHT9BvAGAShBmgDK5pW1/m3ddTmtatIcnp2TL6vVUye9UBlm8DgAkQZoAyalbPXweawW1D9TEIk+Ztk3/P2SSpmTn0IQAYiDADlIO/j5e8dfMVMnFwS/GwiXy74bD0eO4Xmf7DDjmSnkVfAoABbHa73S4uLD09XYKCgiQtLU0CAwONbg5cyOr9x2XK/G2yM6lwH5pqXh5yY6cwuadXUwmv5Wd08wDAbb6/CTNAJaj/F1i264jM+GWvrI9LLT6Ve1i7BvJ/vZvKZSEB9C8AVABhpoKdAVQm1KiVTm8t2ysr9vx1UOWAViFyb59m0i68Jp0LAOVAmKlgZwCOsPlQqry9bJ8+DqFIz2Z15J99mupN+NQuwwCAiyPMVLAzAEdSZzq98+s+mbcxQfLV6ZV6Z+Gacm/vZnL15fUINQBwEYSZCnYG4AzxKZny3m/75cu18ZKTV3gcQsvQAF1TM6Rtfb0xHwCgJMJMBTsDcKYjGVnywcpY+XR1nJzMztOPNartJ88Obys9L6tD5wPAWQgzFewMoCqkZebKx6sOyIe/x8qJzFy9X82jg1rKPb2aMPUEABX4/mZ8G6hiQX7ecv/Vl8nK//SVGzqGiSqnee7HnXLfZxvk1JkRGwBA2RFmAIPU8PGSF0ZFydPD24i3p00WbkmU4W/9LrHHTvF3AgCuGmZmzJihh+GXL19udFMAh1Cf5791ayRf3N1N6gX4yJ4jJ+W6GStl6Y5kehgAXC3MJCQkyIsvvmh0MwCn6Niolnx/f0/p1ChYMrLy5M5Za+W1Jbul4MySbgCAC4SZ+++/XyZOnGh0MwCnqRfoK5/d1U3Gdm+k77+2ZI/cPXutpGfl0usAYPUws2DBAvH29paBAwde8rXZ2dm6AvrsC2AV6rDKp4a1kRdHRenbS3YckWEzftcb8AEALBpmTp06JY8//ri8+uqrZXr99OnT9VKuokt4eLjT2wg42g2dwuXrf3SXBkG+uiB42Fu/yw9bEuloALBimJk0aZL84x//kPr165fp9RMmTNBr0osu8fHxTm8j4AxRYTVlwf095cqmtSUzJ1/++el6eX7RzuKjEQAAFggz69evl5iYGB1mysrHx0dvrnP2BbCq2v4+8vEdXeSuqyL1/XeW75PbPlojJ07lGN00ADANU4eZhQsXyunTp6Vv377Su3dvGT16tH78wQcf1Pf37t1rdBMBp1NnNz0+pJW8MaaDVPf2lBV7jsnQGStlW0IavQ8AapsLu91umTHrAwcOSGRkpCxbtkyHmbLgOAO4kh2J6XLP7HUSl5Ipvt4e8tyIKBneoaHRzQIAh+M4A8BFXV4/UBbc11N6t6grWbkF8uCXG+WRrzYx7QTArZl6mulsamrp7GmmotuAO57t9MG4znJ/32b6/lfrDknfl5fLV2vjxUIDrQDgntNMFcE0E1zZ2gMp8vjcrbLrzD40XSNrybPXt5Fm9QKMbhoAVArTTICb6NS4lnz/QE957JqWuoYmJjZFrnl9hby0eJdk5eYb3TwAqBKWmWYCUDpvTw/5R3RT+fmhaLm6ZT3JzbfLjGV7ZcCrv8mvu4/SbQBcHmEGcBHhtfzk/XGdZOatHSU00FeveBr34Rq577P1ciQ9y+jmAYDTEGYAF2Kz2WRQm1BZ8u9oubNnpHjYRL7fnChXv/yrfLzqALsHA3BJFAADLmzr4TR5fO4W2XSocIO9dmFB8uz1baVNwyCjmwYAF0UBMABNhZZv/9lDnh7WWgJ8vHSouW7GSnlqwXY5mZ1HLwFwCUwzAS7O08Mmf+veWJb+O1qGtmsg6pzKD3+PlX4v/yqLtiayNw0Ay2OaCXAzaoXTpO+26gJhxcvDJtW8PPTFp/jaU6p5nv/YX7f/uu7RrI70blHP6D8WADeeZiLMAG5I7UHz1rK98u6v+yUnv6DSv6/f5fVk8tDWekUVADgCYaaCnQG4m9M5+ZJ2Oldy8gokOy9fsvMKdLjJzi26ztfXhc8XXefra3VJTs+Wb9YfkrwCux6lUUcs3NWriR7FAYDKIMxUsDMAlN+e5AyZNG+rrN6fou83qVNDnhrWRnpeVofuBFBhrGYCUGUuCwmQz+/qJq+Pbi91/H1k/7FTcusHMXqzvqQ0c2/Wl5B6Wv48kCKHU0+zBw9gYdTMAHCY9KxceeWn3XqDPrVqqkY1T3mof3O57crG4uXpYZp6oZ+2J+tTxlfuPSZFR+2qQuj6NX0lrKafhAVXl7DgouvqElbLT++qrFaGAagaTDNVsDMAOG6zPjX1tCEuVd9vGRogTw9vI50b1zKsi7clpMmcP+Plu40Juk6oSMOa1eVIRpY+0+piLhR2WtYPkNYN2IQQcDTCTAU7A4DjFBTYZc7aeHlu0U5JzSwMD6M6hsmEa1pKbX+fKunqtMxcmbfpsHz5Z7xsS0gvfrx+kK/c0DFMRnUMl4jafnqKSQWaQydOy6ETmXIoRV2flkOpmfpaTUddLOwMb99AnhzaWmrVqFYlfy7AHaSzNLtinQHA8VJO5cgLi3bKF3/G6/tB1b3l0UEtZHTnCKdM26gQ9ce+4zpILdqWpFddKWrfnP6tQ+TGTuHSs1mdcr33hcKO2qsnJva4nlKrXaOaLnwe3DZUn5EFoHIIMxXsDADOs+7gCb1Z3/bE9OJzop4Z3lbahjlmikaFjK/XHZKv1h7SBb1F1BTXTZ3DZXj7hhLshJGTjfGp8ujXm2R38kl9f2DrEHl6WBupF+jr8PcC3Ek6IzMV6wwAzpWXXyCzVx/URcIZ2XmiBjC6NK4lAb7eUr2ap/h5e+rrs2/7VfOS6tU8pLq3l/idea66t3rcU3y9PXVImnNOMW+Ar5cOL2oUpk3DQKePlKi9d95atk/eXrZX77kT6Oulp51GXtGQURqggggzFewMAFXjSHqWPPvDDpm3McGhv/fKprX1KMzA1qE66FS17Qnp8ug3m2Tr4cLRp+jmdWXaiLa6yBhA+RBmKtgZAKp+1dO+oyclMydfX07n5Mnp3KLb+Re4XfI1IYG+egTkhk7hpjhOQY0+/XdFrLy6ZLeu11HL0x8bfLnc0iVCPFjaDZQZYaaCnQEAjqJC2qNfb9bTYErXyFry/MgoaVynBp0MlAE7AAOAwZrW9Zc593SXyUNb6RqfmNgUGfT6b/Lf3/az2zDgYObYkhMAXJBa/n17j0hZ/GAv6dGstmTlFuhaoZHv/CG7kzOMbh7gMggzAOBkamO+T+7sKs+NaCsBPl56OfeQN1bIG0v3SG5+4T44sCYVSoumEmEczmYCgCqUmHZanpi7VZbuPFK8D86D/S6T/q1COfvJYj6NOSiT523Ty/FHXNFQJg9trTeFhGNQAFzBzgCAqmC32/Wy9KkLtsmJM0c9hNeqLrddGSk3dgrT++7AvNRo2tPfb5ePVx0s8bg6jPS5kW2ld4t6hrXNlRBmKtgZAFCVjp/Mlg9/j5VPY+KKz69S01A3dg7XJ42bYak5SjpxKkfu/Wy9PjJD7cX48IAW0q1JbXnkq02y/9gp/ZoxXcLl8SGtxN/Hi+6rBMJMBTsDAIyg9sv5dsMh+XBlrOw7WviFqLakGdQmVO7sGSlXRAQ7ZSdhdYDmjsR0Pb2lvnhr+HgVX9fw8RQfr6rfeNDM9iRnyN8/XisHj2fq/YNeG91B+rcKKf47fHHxLh1OFbVR4oujouTKZnUMbrV1EWYq2BkAYCR1SOave47qULNiz7Hix9uF19Sh5po2oeLt6VHhzfx2JhUWq649eELWHUiRhLSsi/6Mt6etMNhUKwo5nsX3C4OPp54S69goWK5sVtulw8/SHcky/ouNcjI7T8KCq8v74zpJy9Dzv1NW7z8uj3y9SeJTCs8HG9u9kTx2TUt9LAfcOMzMmzdPZs6cKTk5OZKdnS2ZmZnyyCOPyJgxY8r084QZAFa0KylDh5q5Gw8Xn/xdP8hXxl3ZWMZ0jpAgv4vX1WRk5cqGuNTC4HIwRTbGpcqpnPwSr1EjMpfV8xcPm01O5eTJqew8/WWtlpCXl5oe63t5PR24opvX02douQL1FTnz1/3ywuKd+uwvtfnhO7d2lFoXObRU9eO0H3bo6UOlUW0/eemGdtK5cS2HT3kt2ZEsB46fksg6/tIiJECa1fN3mb53qTAzaNAgufnmm2Xs2LH6/oIFC2TYsGGyceNGiYqKuuTPE2YAWNmxk9nyyeqD+nLsZI5+TB2yeUPHML2HjdpRWP0zrk4K16MuBwpHXnYlpUuB/fzA0aFRsHQ6c1EjPmqEpbRRHBV81JdyUcA5lZ1/5jpPB5+i28cycmTZriNyJCO7+Od9vT2kd/N6ck3bUOnbsp5lC5qzcvNlwrdbZO6Gw/r+zV0jZMrQ1lLNq2yjYyv2HJX/fL1Zj4CpWcI7e0TKwwNbVOrcMHU6/E/bkuWn7UmyJjblvL9jm00kopafXFYvQFqE+kvzkAB9aVK3huVGzlwqzKxbt07atWsnXl6F/8FlZGToP9TcuXNl+PDhl/x5wgwAV6C+WOdvStCjNWq6qOiLS4USNaWRlH7+lJFaIdWpUS09DdSpcbD+glOjMc6YHtsQf0IWbU2SH7cmyaEThVMsSjVPD71h4DVt6uv6kuCLjGiYSXJ6ltw9e51sik/VfTZlaCv5W/fG5f496Vm58sz322XO2kP6ftO6NfQoTYeI4DL9vPqK3pWcoQPM4m1Jsi2h8BDTIpfXD5T24UFy4Fim3vPm+KnCwHsu9WdoXNtPWoQGnAk6KuT4S+PaNcSrglOXzuZSYeZsubm58swzz8jXX38tMTEx4u/vf8mfIcwAcCXqn2y1kuaDlbHyy5m9ahQvD5u0bhgkHSMKg4sKOfUCfQ1pn/rCLQw2icUFzUVfqGqaRk1FqZPNjWhfWWw+lCp3fbxWktOzpaaft7x98xWVLuT9ZWeyPPbNFj2CpfLkP6Kbyvh+l5U6WpKvwmHcCR1eftqerAuOi6if7dS4lgxoFaL78NwVb8dOZutQsyf5pA5BqmhZTVmmZ+WV2i4VNiPr1JAGNX2lfs3q0iDIV0KDCq/VfTW1acQJ9C4bZu6991759NNPpXXr1vLll19KWFhYqa9TdTXqcnZnhIeHUwAMwCUPs1y177iuk2gXVtOUtRLqy1SN1qhwsz3xr1EFNaqkgpdasaVGCdRUVICvlwSeuTbqC3TexsP6gNDsvALdrx+M6ySNajvmcNDUzByZMn+bfLcxoXjDRDVK06ZhkGTn5csfe4/r6aOftycXTykqalrrqmZ1dHi5+vJ6Utvfp1zva7fbdYhSoUYFHXXZlXxS/92o0+cvJdjPW+qrgFNTBR3fv24H/vWYM6awXDLMKHl5eTJ58mT55JNPZPXq1VK/fv3zXjNlyhSZOnXqeY+zmgkAjHXw+CkdahZtS9LFyRejRgxUqCm8eJdy21sCzzwWEuirA4daZVTR1V5qquzln3fJW8v26fuq1uf10e2dUu+zaGuiPD53q54SUiNqPZrVkbUHUkoUaKs/19Ut68mA1qqgum6ptU2VVVBQWGul9sdJTD2ta3vUtZqyVMv2E9OyyhR2lLuuitR76ziSy4YZpaCgQBo1aiSjR4+WF1988bznGZkBAGsc67B4a5I+1uFIerZefZWRlScZ2aVPh5SFmsZS+7uo1UP6UquGvlZF0qoo9kKjPaqY+cEvNuqVQco90U3k0YEtnXq8hNow8YnvtupRqyL1AnxkQOvC6aOukbXLXGjsLCoepJ/Ok8T005KYmiUJaaclKU0FnSz996fCjgo9ahTrkYEt5N4+zRz6/i4VZtSS7GrVShaMXX311eLr6ysLFy685M9TMwMA1qFGC07m5BUGm6KAc+Y6vbTHTufqL1W1PPlSS8rVcQPq0E9VCKtGclTQUUusp87frutLVHh4fmRbub5D6WUMjqa+fpfuOKLf+8qmtfVUoYcTA5Sz/gxq92q1vP9S2wWUV3m+v02/i88VV1whW7duLfFYYmKi9OjRw7A2AQCcQ32Zq7oZdRGpXq4v1aMZ2XLgeKYONnFnrg+euVbBR02fqIta0nwuNSry7t86lnmVkSOoXZ37tQrRF6uy2WymWKFm+jCzfft2PQIzZMgQfV/Vy+zatUvee+89o5sGADDRl6paHaUuXSJrlTp6cDAlU9ftqGXMB1MKg05cSqYuxH1xVDtdyAprMv0005tvvimff/65eHh46HoZ9YGdOHFicbi5FKaZAACwHpeqmakswgwAAK79/W3Obf8AAADKiDADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAsjTADAAAszcvoBjhb0aHg6vRNAABgDUXf20Xf424dZjIyMvR1eHi40U0BAAAV+B4PCgq66Gts9rJEHgsrKCiQhIQECQgIEJvN5vDUqEJSfHy8BAYGOvR3uwP6jz40Gp9B+tBofAYvTMUTFWQaNGggHh4e7j0yozogLCzMqe+hggxhhv4zEp9B+s9ofAbpP2e41IhMEQqAAQCApRFmAACApRFmKsHHx0cmT56sr0H/GYHPIP1nND6D9J8ZuHwBMAAAcG2MzAAAAEsjzAAAAEsjzAAAAEtz+X1mnGXu3Lkybdo08fX11XvZvP3229K6dWujm2UJU6ZMke+++05q1qxZ/FitWrXk22+/NbRdZpaTkyNPPvmkvPTSS7J3715p3Lhxieffffddee+99/TnUfWrut2wYUPD2mu1Przttttk586duv+KtGrVSv93jUJz5syR999/X/Lz8/VGb6r/XnzxxeJ+VOWXTz/9tP5v28vLS5o3by5vvfVWmfcJcff+692793k/07dvX/2ZRRmoAmCUT0xMjD0gIMC+e/dufX/WrFn2hg0b2tPT0+nKMpg8ebJ92bJl9FUZxcbG2rt162YfO3asKtbX98/2zTff2OvXr28/evSovj916lR7+/bt7fn5+fRxGftw3Lhx5z2Gkry9ve2LFi3St9Vn629/+5u9RYsW9qysLP3Yyy+/bI+KirJnZmbq+7fffrt96NChdGMZ+y86Opq+qgSmmSrgueeekyFDhshll12m7996662Sl5cn//vf/yry64CLOnnypMyePVtuv/32Up9/5plnZNy4cVKnTh19f/z48bJ161ZZuHAhPVvGPsSlDRs2TAYOHKhvq9HoBx54QHbt2iXr16/Xow3q38V//vOfUr16df2ahx9+WBYsWCBbtmyhey/Rf6g8wkwFLF26VDp16vRXJ3p4SMeOHWXJkiUO+CsBSmrTpo00a9as1G5JSUmRDRs2lPg8qmF9NcTP57FsfYiy+eqrr0rcL5qSy87Ols2bN8vRo0dLfA4vv/xyqVGjBp/DMvQfKo8wU07Hjx/X850hISElHg8NDZXY2FgH/JW4hw8//FDPEffo0UOPKuzbt8/oJllS0WeOz2PlTZ8+XX8me/bsKffee68kJyc74Le6rlWrVukDANV/w/v37z/vc6gO9lX3+Xfx0v1XRI2qRkdHS69eveSxxx7ThyyibAgz5ZSZmamvz931V90veg4XFxERIR06dND/x7ZixQqJjIzUI1uHDx+m6/g8GkKNZKkvkF9++UWWLVum/2+5W7duenoK51P9o4pXZ8yYId7e3vy7WMn+U9q3b6/LF3799Vf54Ycf9PRc//799RQeLo0wU05+fn6lDg2q+0XP4eLuuOMOeeihh/SKBzVFN2nSJD3kysqR8uPz6BgTJ06UW265RX8e1ZfLK6+8InFxcfL555876B1cyz333CM33XSTXH/99fo+n8PK9Z/y2muvyYABA/Rtf39/eeGFFyQmJkYHbFwaYaacateurWsSzh2CTkpKkiZNmpT310FEPD099fJEpprKr+gzx+fRsQIDA6Vu3bp8Jkuhpj9UeFHLsC/1OVT3+Xfx0v1XmqZNm+pr/l0sG8JMBai1/+vWrSu+r/ZXUBXp/fr1q8ivcztqXvhcCQkJevoJ5RMcHKyn7M7+PKqart27d/N5rMRnUo20qvo4PpMlqRVL8fHxenpEUZ87dYmKitLh7+zP4Y4dO+TUqVN8DsvQf0eOHJFnn322RF8XTbvzGSwbwkwFk7Va9qo23lI+/fRTPbqgCllxafPnz9eXImojKbUSQk0/ofyeeOIJmTVrlv7yVd544w29emfw4MF0ZxnNnDlT1q5dW2K5uwqKN9xwA314Vh998skncv/99+v/eVP9VbT0Wv37p/5dVFPFp0+f1q9/+eWXZejQofqziIv3n6q3VFObBw4c0F2l6mTUyE3Lli31/zzj0tgBuAK6dOmi95QZPXq03lNBzbMvXrxYAgICKvLr3I76PxA1P6z+41W7sqriaVUMrP7DxflUH6m59NTUVH1ffe7Cw8OLl3qOGDFC/5+dKhZUtUfqS1j9I6k+lyhbH6pdgYvquNQXixplUIXA6hqiV9WoFV4FBQXSvXv3El3y0Ucf6WvVf6pgWq3OUf2o9uH6+OOP6b4y9J9aDfvvf/9bxowZo/89VCNaqv/U98rZu1Ljwmxq57yLPA8AAGBq/K8bAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAACwNMIMAIdYs2aN9O7dW2w2m97N+amnntI77k6ZMqV4592qoLaEV+95ruHDh8urr75aZe0AUHXYARiAY/9Rsdn0Fu233XabDhaRkZESGxurT0avCsuXL5c+ffroA2DPprbbV0eRqC3jAbgWzmYC4BYYlQFcF9NMAJxi+/bt+kBHRV2rKai5c+fq++pAwrvuuks6dOgg0dHRegooLi5OP7dy5Urp1q2bHuFRB0EOGzZMmjVrJu3bt9fPq5OZu3btqkdfOnfurA8uLRqF+eWXX+TBBx/Ut9X7qcuqVavk0Ucf1SND6v7ZZs+erX+v+n2qLUUHTyp///vf9QGAY8eOlf/85z+6nS1atNCH/wEwGXXQJAA4ivpn5aOPPtK3Y2Nj9X11fbYxY8boS35+vr4/bdo0e6tWrex5eXklfu6OO+7Qr8nIyLD37t1bP9e5c2f7li1b9O2TJ0/ao6Ki7LNmzSr+3cuWLdM/e67Jkyfbo6Oji+8vXrzY7u/vb9+5c6e+v3nzZruvr6/9999/L37NuHHj7MHBwfYdO3bo+6+//ro9IiKCDwtgMozMAKhS+/fvly+++EL+9a9/iYdH4T9Bd999tx7JUfUuZ1OjIuo1/v7+smzZMv2YGj1p06aNvl2jRg0ZPHiw/Pjjj+VuhxrRUSNCarRFadu2rQwcOFCmTZtW4nVqxEYVNCtqZEeNIJ04caKCf3oAzkDNDIAqtW3bNj0tNH78ePH29i5+vFGjRnL06NESrw0LCzvv5w8dOiQPPPCAHDt2TP98UZFxeW3dulX69u1b4jE1nXX2VJPSoEGD4tsBAQH6Oj09XYKDg8v9ngCcgzADwBCffPLJJUOIp6dnifsHDx6U/v3762XfDz/8sH5MLcM+d0THkc5ug6rjUc5dKQXAWEwzAXDePzBnppGUgoICOXXqlLRu3Vrf37VrV4nXPvnkk7Jz586L/r61a9fK6dOn5aabbip+LCcn54LvmZeXp19fGjVVtXfv3hKP7du3T083AbAWwgwAp6ldu7YOF6rGRAURtfdMkyZN9F4vL7zwgmRlZenX/fHHH/LNN9/oaZ6LUbUranRk6dKl+r4KKufWy9StW1dfq/f89ttvdUgqzeOPPy7z5s2TPXv2FE9/LVq0SCZOnOiQPzuAKmR0BTIA1xATE6NXC6l/Vlq0aGGfOnWqfvzRRx+1t27d2t61a1f7ypUr9WNqddLdd9+tX6dWKQ0dOtS+Z88e/dyGDRv0a9XvUddvvvlmifeZOXOmvXHjxvarrrrKPmrUKPvIkSPtQUFB9ptvvrn4Nep2+/bt7d27d9erlR555BF7o0aN9OuGDBlS/Dq1Cqpdu3b2Ll266Nd/+eWXxc+NHz/eHhISoi/q59XvObtdavUTAHNgB2AAAGBpTDMBAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAABLI8wAAACxsv8H2utwRzW4HcgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    all_counts = collections.Counter()\n",
    "    tuple_of_counts = tuple(counts_list[:i])\n",
    "    assert len(tuple_of_counts) == i\n",
    "    for counts in tuple_of_counts:\n",
    "        for bitstring, count in counts.items():\n",
    "            all_counts[bitstring] += count\n",
    "\n",
    "    bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x33733b390>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"SQD\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"iSQD\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
