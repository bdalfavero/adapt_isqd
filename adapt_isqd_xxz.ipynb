{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "268707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048479704)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481971\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [244, 31, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531929\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894929267349487)]\n",
      "Initial energy: -6.32727615481971\n",
      "Optimizing energy with indices [244, 31, 225, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981631072092), np.float64(-0.7853981620731963), np.float64(-0.1635701974084101), np.float64(0.16356963668286922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134975\n",
      "(change of -0.136825460315265)\n",
      "Current ansatz: [244, 31, 225, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.710313281228145e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531929 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617646\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797047)]\n",
      "Initial energy: -6.123105625617646\n",
      "Optimizing energy with indices [241, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.785398163997711), np.float64(-0.7853981625399766), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206213)\n",
      "Current ansatz: [241, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531922\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267349487)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [241, 74, 225, 210, 147]...\n",
      "Starting point: [np.float64(-0.7853981657335386), np.float64(-0.7853981641901568), np.float64(-0.16357019740840956), np.float64(0.16356963668286917), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615122723\n",
      "(change of -0.13682546030301435)\n",
      "Current ansatz: [241, 74, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 5.3335584305691664e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531922 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.000000000000006)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 210]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752619964306\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071047541058)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 210, 225]...\n",
      "Starting point: [np.float64(0.7853981908695737), np.float64(0.7853981903042655), np.float64(0.12248927937223411), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819498\n",
      "(change of -0.2041705292018472)\n",
      "Current ansatz: [244, 79, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580347369\n",
      "Operator(s) added to ansatz: [156]\n",
      "Gradients: [np.float64(-2.089492927047892)]\n",
      "Initial energy: -6.327276154819498\n",
      "Optimizing energy with indices [244, 79, 210, 225, 156]...\n",
      "Starting point: [np.float64(0.7853982318859356), np.float64(0.7853984111666573), np.float64(0.16357019742529944), np.float64(-0.1635696366118327), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614684\n",
      "(change of -0.136825459864502)\n",
      "Current ansatz: [244, 79, 210, 225, 156]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00033606692738294376\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580347369 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562482\n",
      "(change of -0.1231056256329639)\n",
      "Current ansatz: [228, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201775814\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484682814)]\n",
      "Initial energy: -6.123105625562482\n",
      "Optimizing energy with indices [228, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853947065772693), np.float64(0.7853985308794288), np.float64(0.1224892796166995), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765011\n",
      "(change of -0.20417052920252932)\n",
      "Current ansatz: [228, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069117\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643863324)]\n",
      "Initial energy: -6.327276154765011\n",
      "Optimizing energy with indices [228, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.78539470657713), np.float64(0.7853982468601272), np.float64(0.16357028648716854), np.float64(0.16356997194353076), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615018015\n",
      "(change of -0.13682546025300368)\n",
      "Current ansatz: [228, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016418668721381277\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069117 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297173)\n",
      "Current ansatz: [225, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647983\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019367)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [225, 31, 228]...\n",
      "Starting point: [np.float64(0.7853947065773541), np.float64(-0.7853993777262479), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562488\n",
      "(change of -0.1231056256329719)\n",
      "Current ansatz: [225, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201775825\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048468285)]\n",
      "Initial energy: -6.123105625562488\n",
      "Optimizing energy with indices [225, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853947065772433), np.float64(-0.7853985308794607), np.float64(0.1224892796166997), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765022\n",
      "(change of -0.20417052920253376)\n",
      "Current ansatz: [225, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069173\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.089491643823333)]\n",
      "Initial energy: -6.327276154765022\n",
      "Optimizing energy with indices [225, 31, 228, 210, 108]...\n",
      "Starting point: [np.float64(0.7853947065771169), np.float64(-0.7853982468601591), np.float64(0.1635702864871716), np.float64(0.16356997194354156), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615013047\n",
      "(change of -0.13682546024802544)\n",
      "Current ansatz: [225, 31, 228, 210, 108]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00017023652441954795\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069173 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [225, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.999999999994421)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [225, 79, 225]...\n",
      "Starting point: [np.float64(0.7853985607314237), np.float64(0.7853989420959421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610644\n",
      "(change of -0.12310562561245586)\n",
      "Current ansatz: [225, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955813\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850687898689876)]\n",
      "Initial energy: -6.123105625610644\n",
      "Optimizing energy with indices [225, 79, 225, 198]...\n",
      "Starting point: [np.float64(0.7853983869831812), np.float64(0.7853991695302517), np.float64(-0.12248869758311735), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816648\n",
      "(change of -0.20417052920600387)\n",
      "Current ansatz: [225, 79, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964056413545\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.089491640728177)]\n",
      "Initial energy: -6.327276154816648\n",
      "Optimizing energy with indices [225, 79, 225, 198, 45]...\n",
      "Starting point: [np.float64(0.7853984379181708), np.float64(0.7853991591701385), np.float64(-0.16357028929925446), np.float64(-0.16356997348665897), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614714432\n",
      "(change of -0.13682545989778383)\n",
      "Current ansatz: [225, 79, 225, 198, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00028938959056029335\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964056413545 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [241, 74, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853981639976342), np.float64(-0.7853981625399249), np.float64(0.12248927961411428), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819971\n",
      "(change of -0.2041705292023286)\n",
      "Current ansatz: [241, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964057733311\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.089491629507777)]\n",
      "Initial energy: -6.327276154819971\n",
      "Optimizing energy with indices [241, 74, 228, 210, 57]...\n",
      "Starting point: [np.float64(-0.785398162447052), np.float64(-0.7853981711958521), np.float64(0.16357028748210503), np.float64(0.16356997569593643), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615067938\n",
      "(change of -0.13682546024796682)\n",
      "Current ansatz: [241, 74, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013840069558915347\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964057733311 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297208)\n",
      "Current ansatz: [228, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-1.999999999952204)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [228, 26, 201]...\n",
      "Starting point: [np.float64(-0.7853947065773501), np.float64(0.785399377726245), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625548022\n",
      "(change of -0.12310562561850613)\n",
      "Current ansatz: [228, 26, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917506431664028\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.485060029388178)]\n",
      "Initial energy: -6.123105625548022\n",
      "Optimizing energy with indices [228, 26, 201, 228]...\n",
      "Starting point: [np.float64(-0.7853977521552514), np.float64(0.7853977933764661), np.float64(0.12248644006210563), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816646\n",
      "(change of -0.204170529268624)\n",
      "Current ansatz: [228, 26, 201, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962578717635\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0894929267475435)]\n",
      "Initial energy: -6.327276154816646\n",
      "Optimizing energy with indices [228, 26, 201, 228, 216]...\n",
      "Starting point: [np.float64(-0.7853977521552578), np.float64(0.785398879885591), np.float64(0.16357019699115474), np.float64(0.16356963656316748), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615068982\n",
      "(change of -0.13682546025233577)\n",
      "Current ansatz: [228, 26, 201, 228, 216]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013734916060266638\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962578717635 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048479704)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481971\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [244, 31, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531929\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894929267349487)]\n",
      "Initial energy: -6.32727615481971\n",
      "Optimizing energy with indices [244, 31, 225, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981631072092), np.float64(-0.7853981620731963), np.float64(-0.1635701974084101), np.float64(0.16356963668286922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134975\n",
      "(change of -0.136825460315265)\n",
      "Current ansatz: [244, 31, 225, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.710313281228145e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531929 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297173)\n",
      "Current ansatz: [225, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647983\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.000000000019367)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [225, 31, 225]...\n",
      "Starting point: [np.float64(0.7853947065773541), np.float64(-0.7853993777262479), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562486\n",
      "(change of -0.12310562563297012)\n",
      "Current ansatz: [225, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201775823\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.4850710484412666)]\n",
      "Initial energy: -6.123105625562486\n",
      "Optimizing energy with indices [225, 31, 225, 45]...\n",
      "Starting point: [np.float64(0.7853947065772626), np.float64(-0.7853985308794541), np.float64(-0.1224892796166998), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154759888\n",
      "(change of -0.20417052919740186)\n",
      "Current ansatz: [225, 31, 225, 45]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962581358221\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267112037)]\n",
      "Initial energy: -6.327276154759888\n",
      "Optimizing energy with indices [225, 31, 225, 45, 198]...\n",
      "Starting point: [np.float64(0.7853947065771469), np.float64(-0.7853982468607564), np.float64(-0.16357019741020742), np.float64(0.16356963668509739), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615075615\n",
      "(change of -0.13682546031572684)\n",
      "Current ansatz: [225, 31, 225, 45, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00010684194441140021\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962581358221 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071048479702)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [241, 74, 228, 147]...\n",
      "Starting point: [np.float64(-0.7853981639976342), np.float64(-0.7853981625399249), np.float64(0.12248927961411428), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819698\n",
      "(change of -0.20417052920205503)\n",
      "Current ansatz: [241, 74, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531963\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734873)]\n",
      "Initial energy: -6.327276154819698\n",
      "Optimizing energy with indices [241, 74, 228, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981657327621), np.float64(-0.7853981589235528), np.float64(0.16357019740840664), np.float64(-0.1635696366828855), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615097817\n",
      "(change of -0.1368254602781196)\n",
      "Current ansatz: [241, 74, 228, 147, 210]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.424965606542188e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531963 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998192\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [225, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140577\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999999944222)]\n",
      "Initial energy: -5.999999999998192\n",
      "Optimizing energy with indices [225, 26, 228]...\n",
      "Starting point: [np.float64(0.7853985607314246), np.float64(0.7853989420959444), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610641\n",
      "(change of -0.12310562561244964)\n",
      "Current ansatz: [225, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917522148955772\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868967)]\n",
      "Initial energy: -6.123105625610641\n",
      "Optimizing energy with indices [225, 26, 228, 210]...\n",
      "Starting point: [np.float64(0.7853983869831882), np.float64(0.7853991695302422), np.float64(0.12248869758311211), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816647\n",
      "(change of -0.20417052920600565)\n",
      "Current ansatz: [225, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964056416012\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916407310475)]\n",
      "Initial energy: -6.327276154816647\n",
      "Optimizing energy with indices [225, 26, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853984379184572), np.float64(0.7853991591701212), np.float64(0.16357028929940762), np.float64(0.16356997348722374), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615069497\n",
      "(change of -0.13682546025285003)\n",
      "Current ansatz: [225, 26, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013629210843883166\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964056416012 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 225]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.12310562561763927)\n",
      "Current ansatz: [244, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327167\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048479704)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 31, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981639978266), np.float64(-0.7853981625399236), np.float64(-0.1224892796141142), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481971\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [244, 31, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531929\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089492926734949)]\n",
      "Initial energy: -6.32727615481971\n",
      "Optimizing energy with indices [244, 31, 225, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981631072092), np.float64(-0.7853981620731963), np.float64(-0.1635701974084101), np.float64(0.16356963668286922), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135274\n",
      "(change of -0.13682546031556342)\n",
      "Current ansatz: [244, 31, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.6155413873611976e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531929 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474305875)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 198]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615482008\n",
      "(change of -0.20417052920242895)\n",
      "Current ansatz: [244, 79, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963610610418\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894920221535056)]\n",
      "Initial energy: -6.32727615482008\n",
      "Optimizing energy with indices [244, 79, 225, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981493735948), np.float64(0.7853981777037257), np.float64(-0.16357026021874255), np.float64(-0.16356987308330864), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615050827\n",
      "(change of -0.13682546023074682)\n",
      "Current ansatz: [244, 79, 225, 198, 210]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00014811055823501704\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963610610418 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819709\n",
      "(change of -0.20417052920205858)\n",
      "Current ansatz: [244, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580530028\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089492926737891)]\n",
      "Initial energy: -6.327276154819709\n",
      "Optimizing energy with indices [244, 79, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981700346647), np.float64(0.7853981900506334), np.float64(-0.1635701974085294), np.float64(0.16356963668219085), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614835283\n",
      "(change of -0.13682546001557316)\n",
      "Current ansatz: [244, 79, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00027271975089248316\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580530028 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 244]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634967\n",
      "(change of -0.79785741488877)\n",
      "Current ansatz: [211, 244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273103219\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.725288012410362)]\n",
      "Initial energy: -4.626284539634967\n",
      "Optimizing energy with indices [211, 244, 79]...\n",
      "Starting point: [np.float64(-0.2651612351142246), np.float64(0.44546905146305454), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999417229\n",
      "(change of -1.3737154597822618)\n",
      "Current ansatz: [211, 244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958970181085\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999998057472)]\n",
      "Initial energy: -5.999999999417229\n",
      "Optimizing energy with indices [211, 244, 79, 228]...\n",
      "Starting point: [np.float64(-9.855379305101342e-06), np.float64(0.7853981714334987), np.float64(0.7853981407341446), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625041492\n",
      "(change of -0.1231056256242633)\n",
      "Current ansatz: [211, 244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620050184\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710482493232)]\n",
      "Initial energy: -6.123105625041492\n",
      "Optimizing energy with indices [211, 244, 79, 228, 210]...\n",
      "Starting point: [np.float64(-9.693301808235352e-06), np.float64(0.785398307409238), np.float64(0.7853977639811249), np.float64(0.12248927961503829), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154271405\n",
      "(change of -0.20417052922991274)\n",
      "Current ansatz: [211, 244, 79, 228, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 6.240962748731634\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 8.91752620050184 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819709\n",
      "(change of -0.20417052920205858)\n",
      "Current ansatz: [244, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580530028\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089492926737891)]\n",
      "Initial energy: -6.327276154819709\n",
      "Optimizing energy with indices [244, 79, 225, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981700346647), np.float64(0.7853981900506334), np.float64(-0.1635701974085294), np.float64(0.16356963668219085), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614835283\n",
      "(change of -0.13682546001557316)\n",
      "Current ansatz: [244, 79, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00027271978716130764\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580530028 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000004)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 216]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617655\n",
      "(change of -0.12310562561764904)\n",
      "Current ansatz: [244, 31, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620132718\n",
      "Operator(s) added to ansatz: [180]\n",
      "Gradients: [np.float64(2.485071048479707)]\n",
      "Initial energy: -6.123105625617655\n",
      "Optimizing energy with indices [244, 31, 216, 180]...\n",
      "Starting point: [np.float64(0.7853981646238432), np.float64(-0.7853981621539974), np.float64(-0.1224892796141143), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.3272761548199945\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 31, 216, 180]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041689943\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894916435963014)]\n",
      "Initial energy: -6.3272761548199945\n",
      "Optimizing energy with indices [244, 31, 216, 180, 201]...\n",
      "Starting point: [np.float64(0.7853981625535668), np.float64(-0.7853981690429753), np.float64(-0.16357028650384128), np.float64(-0.16356997201408663), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072654\n",
      "(change of -0.13682546025265907)\n",
      "Current ansatz: [244, 31, 216, 180, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013511960648838098\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041689943 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071048479702)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [241, 74, 228, 198]...\n",
      "Starting point: [np.float64(-0.7853981639976342), np.float64(-0.7853981625399249), np.float64(0.12248927961411428), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.3272761548197165\n",
      "(change of -0.20417052920207368)\n",
      "Current ansatz: [241, 74, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531976\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894929267348807)]\n",
      "Initial energy: -6.3272761548197165\n",
      "Optimizing energy with indices [241, 74, 228, 198, 210]...\n",
      "Starting point: [np.float64(-0.7853981657328081), np.float64(-0.7853981589236129), np.float64(0.16357019740840653), np.float64(-0.16356963668288546), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615097822\n",
      "(change of -0.13682546027810538)\n",
      "Current ansatz: [241, 74, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 8.424922649146182e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531976 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.4850710474292024)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 108]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [244, 79, 228, 108]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531789\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929267348794)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [244, 79, 228, 108, 120]...\n",
      "Starting point: [np.float64(0.7853981655442128), np.float64(0.7853981656707975), np.float64(0.1635701974083666), np.float64(-0.16356963668287375), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615107881\n",
      "(change of -0.13682546028817644)\n",
      "Current ansatz: [244, 79, 228, 108, 120]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 7.742895848061765e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531789 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 225]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617648\n",
      "(change of -0.12310562561763749)\n",
      "Current ansatz: [241, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752619944177\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.4850710474288826)]\n",
      "Initial energy: -6.123105625617648\n",
      "Optimizing energy with indices [241, 31, 225, 135]...\n",
      "Starting point: [np.float64(-0.7853981605940241), np.float64(-0.7853981659075159), np.float64(-0.12248927934332374), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206036)\n",
      "Current ansatz: [241, 31, 225, 135]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531515\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089492926735362)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [241, 31, 225, 135, 198]...\n",
      "Starting point: [np.float64(-0.785398148954906), np.float64(-0.7853981642599093), np.float64(-0.1635701974083929), np.float64(0.1635696366827643), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614819276\n",
      "(change of -0.1368254599995673)\n",
      "Current ansatz: [241, 31, 225, 135, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00028089153692768706\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531515 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474292965)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 198]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819986\n",
      "(change of -0.20417052920233836)\n",
      "Current ansatz: [241, 79, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964058377264\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089491628942173)]\n",
      "Initial energy: -6.327276154819986\n",
      "Optimizing energy with indices [241, 79, 225, 198, 210]...\n",
      "Starting point: [np.float64(-0.7853981662933148), np.float64(0.7853981545803549), np.float64(-0.16357028752134434), np.float64(-0.16356997584374114), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072628\n",
      "(change of -0.1368254602526422)\n",
      "Current ansatz: [241, 79, 225, 198, 210]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013512941823076596\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964058377264 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929513\n",
      "(change of -1.7639320224297146)\n",
      "Current ansatz: [225, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647981\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019366)]\n",
      "Initial energy: -5.999999999929513\n",
      "Optimizing energy with indices [225, 74, 228]...\n",
      "Starting point: [np.float64(0.7853947065773572), np.float64(-0.78539937772625), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.12310562556249\n",
      "(change of -0.12310562563297633)\n",
      "Current ansatz: [225, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526201775825\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.485071048468284)]\n",
      "Initial energy: -6.12310562556249\n",
      "Optimizing energy with indices [225, 74, 228, 135]...\n",
      "Starting point: [np.float64(0.7853947065772676), np.float64(-0.7853985308794365), np.float64(0.1224892796166996), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765016\n",
      "(change of -0.20417052920252665)\n",
      "Current ansatz: [225, 74, 228, 135]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640420691685\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643863285)]\n",
      "Initial energy: -6.327276154765016\n",
      "Optimizing energy with indices [225, 74, 228, 135, 198]...\n",
      "Starting point: [np.float64(0.7853947065771415), np.float64(-0.7853982468601413), np.float64(0.16357028648717137), np.float64(0.16356997194354145), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615018006\n",
      "(change of -0.13682546025298947)\n",
      "Current ansatz: [225, 74, 228, 135, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016418668802244734\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640420691685 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047767142)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 198]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819957\n",
      "(change of -0.20417052920230994)\n",
      "Current ansatz: [244, 74, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640413812895\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.0894916438671864)]\n",
      "Initial energy: -6.327276154819957\n",
      "Optimizing energy with indices [244, 74, 225, 198, 135]...\n",
      "Starting point: [np.float64(0.7853981633827103), np.float64(-0.7853981633905597), np.float64(-0.1635702864850013), np.float64(-0.16356997194328274), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150727975\n",
      "(change of -0.13682546025284026)\n",
      "Current ansatz: [244, 74, 225, 198, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350387836355389\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640413812895 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000005)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 216]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056256176455\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200768382\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071048168266)]\n",
      "Initial energy: -6.1231056256176455\n",
      "Optimizing energy with indices [244, 79, 216, 225]...\n",
      "Starting point: [np.float64(0.7853981633974504), np.float64(0.7853981633974519), np.float64(-0.1224892795338588), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920233658)\n",
      "Current ansatz: [244, 79, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041381378\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894916438671958)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [244, 79, 216, 225, 201]...\n",
      "Starting point: [np.float64(0.7853981633972181), np.float64(0.7853981633972502), np.float64(-0.16357028648501623), np.float64(-0.16356997194328662), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072787\n",
      "(change of -0.13682546025280473)\n",
      "Current ansatz: [244, 79, 216, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503877244586837\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041381378 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.763932022498393)\n",
      "Current ansatz: [228, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897114057\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999944205)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [228, 31, 225]...\n",
      "Starting point: [np.float64(-0.7853985607314264), np.float64(-0.7853989420959467), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.12310562561064\n",
      "(change of -0.12310562561245231)\n",
      "Current ansatz: [228, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955662\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.4850687898619763)]\n",
      "Initial energy: -6.12310562561064\n",
      "Optimizing energy with indices [228, 31, 225, 108]...\n",
      "Starting point: [np.float64(-0.7853983869831765), np.float64(-0.7853991695302354), np.float64(-0.12248869758309638), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548153245\n",
      "(change of -0.20417052920468404)\n",
      "Current ansatz: [228, 31, 225, 108]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409641716539275\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894916448440033)]\n",
      "Initial energy: -6.3272761548153245\n",
      "Optimizing energy with indices [228, 31, 225, 108, 210]...\n",
      "Starting point: [np.float64(-0.7853982921528802), np.float64(-0.7853991637528451), np.float64(-0.16357031624283225), np.float64(-0.16356998000681872), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614392212\n",
      "(change of -0.13682545957688763)\n",
      "Current ansatz: [228, 31, 225, 108, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00037206868511146244\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409641716539275 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 228]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [244, 31, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981639978813), np.float64(-0.7853981625399111), np.float64(0.12248927961411445), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819977\n",
      "(change of -0.20417052920233392)\n",
      "Current ansatz: [244, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964044667907\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(2.089491640981466)]\n",
      "Initial energy: -6.327276154819977\n",
      "Optimizing energy with indices [244, 31, 228, 210, 108]...\n",
      "Starting point: [np.float64(0.7853981417276958), np.float64(-0.7853981819240415), np.float64(0.1635702866854681), np.float64(0.16356997269745452), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615071398\n",
      "(change of -0.13682546025142095)\n",
      "Current ansatz: [244, 31, 228, 210, 108]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.0001360053146680995\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964044667907 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 147]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154820074\n",
      "(change of -0.20417052920242362)\n",
      "Current ansatz: [244, 79, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963655680355\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0894919825745366)]\n",
      "Initial energy: -6.327276154820074\n",
      "Optimizing energy with indices [244, 79, 225, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981479801436), np.float64(0.7853981834326691), np.float64(-0.16357026296685157), np.float64(-0.16356988342672032), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615050255\n",
      "(change of -0.13682546023018016)\n",
      "Current ansatz: [244, 79, 225, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001482896166931572\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963655680355 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562482\n",
      "(change of -0.1231056256329639)\n",
      "Current ansatz: [228, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526201775814\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484682814)]\n",
      "Initial energy: -6.123105625562482\n",
      "Optimizing energy with indices [228, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853947065772693), np.float64(0.7853985308794288), np.float64(0.1224892796166995), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765011\n",
      "(change of -0.20417052920252932)\n",
      "Current ansatz: [228, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069117\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089491643863324)]\n",
      "Initial energy: -6.327276154765011\n",
      "Optimizing energy with indices [228, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.78539470657713), np.float64(0.7853982468601272), np.float64(0.16357028648716854), np.float64(0.16356997194353076), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615018015\n",
      "(change of -0.13682546025300368)\n",
      "Current ansatz: [228, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016418668721381277\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069117 > 1e-05)\n",
      "Pool will be tiled from 21 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 12\n",
      "Got DMRG energy -2.05684e+01\n",
      "Tiled pool has 150 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -11.000000000000036\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -4.000000000000014\n",
      "Operator 1: 4.00000000000002\n",
      "Operator 2: -4.000000000000008\n",
      "Operator 3: 4.000000000000013\n",
      "Operator 4: -4.0\n",
      "Operator 5: 3.9999999999999996\n",
      "Operator 6: -3.9999999999999973\n",
      "Operator 7: 3.999999999999998\n",
      "Operator 8: -4.0\n",
      "Operator 9: 4.000000000000052\n",
      "Operator 10: -4.000000000000022\n",
      "Operator 11: 4.000000000000014\n",
      "Operator 12: -4.00000000000002\n",
      "Operator 13: 4.000000000000008\n",
      "Operator 14: -4.000000000000013\n",
      "Operator 15: 4.0\n",
      "Operator 16: -3.9999999999999996\n",
      "Operator 17: 3.9999999999999973\n",
      "Operator 18: -4.000000000000014\n",
      "Operator 19: -4.00000000000002\n",
      "Operator 20: -4.000000000000008\n",
      "Operator 21: -4.000000000000013\n",
      "Operator 22: -4.0\n",
      "Operator 23: -3.9999999999999996\n",
      "Operator 24: -3.9999999999999973\n",
      "Operator 25: -3.999999999999998\n",
      "Operator 26: -4.0\n",
      "Operator 27: -4.000000000000022\n",
      "Operator 28: -4.000000000000014\n",
      "Operator 29: -4.00000000000002\n",
      "Operator 30: -4.000000000000008\n",
      "Operator 31: -4.000000000000013\n",
      "Operator 32: -4.0\n",
      "Operator 33: -3.9999999999999996\n",
      "Operator 34: -3.9999999999999973\n",
      "Operator 35: -3.999999999999998\n",
      "Operator 45: -3.999999999999998\n",
      "Operator 46: 4.0\n",
      "Operator 47: 4.000000000000052\n",
      "Operator 48: -4.000000000000022\n",
      "Operator 49: 4.000000000000014\n",
      "Operator 50: -4.00000000000002\n",
      "Operator 51: 4.000000000000008\n",
      "Operator 52: -4.000000000000013\n",
      "Operator 53: 4.0\n",
      "Operator 54: -3.9999999999999996\n",
      "Operator 55: 3.9999999999999973\n",
      "Operator 56: 4.000000000000022\n",
      "Operator 57: 4.000000000000014\n",
      "Operator 58: 4.00000000000002\n",
      "Operator 59: 4.000000000000008\n",
      "Operator 60: 4.000000000000013\n",
      "Operator 61: 4.0\n",
      "Operator 62: 3.9999999999999996\n",
      "Operator 63: 3.9999999999999973\n",
      "Operator 64: 3.999999999999998\n",
      "Operator 65: -4.000000000000052\n",
      "Operator 66: 4.000000000000022\n",
      "Operator 76: 4.0\n",
      "Operator 77: 4.000000000000022\n",
      "Operator 78: 4.000000000000014\n",
      "Operator 79: 4.00000000000002\n",
      "Operator 80: 4.000000000000008\n",
      "Operator 81: 4.000000000000013\n",
      "Operator 82: 4.0\n",
      "Operator 83: 3.9999999999999996\n",
      "Operator 84: 3.9999999999999973\n",
      "Operator 85: 3.999999999999998\n",
      "Operator 104: -4.000000000000052\n",
      "Operator 105: 4.000000000000022\n",
      "Operator 106: -4.000000000000014\n",
      "Operator 107: 4.00000000000002\n",
      "Operator 108: -4.000000000000008\n",
      "Operator 109: 4.000000000000013\n",
      "Operator 110: -4.0\n",
      "Operator 111: 3.9999999999999996\n",
      "Operator 112: -3.9999999999999973\n",
      "Operator 131: -4.000000000000014\n",
      "Operator 132: -4.00000000000002\n",
      "Operator 133: -4.000000000000008\n",
      "Operator 134: -4.000000000000013\n",
      "Operator 135: -4.0\n",
      "Operator 136: -3.9999999999999996\n",
      "Operator 137: -3.9999999999999973\n",
      "Operator 138: -3.999999999999998\n",
      "Operator 139: -4.0\n",
      "Operator 149: -4.000000000000022\n",
      "Total gradient norm: 37.309516212355355\n",
      "Operators under consideration (1):\n",
      "[149]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000022)]\n",
      "Operator(s) added to ansatz: [149]\n",
      "Gradients: [np.float64(-4.000000000000022)]\n",
      "Initial energy: -11.000000000000036\n",
      "Optimizing energy with indices [149]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -11.828427\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -11.828427124746232\n",
      "(change of -0.828427124746197)\n",
      "Current ansatz: [149]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135623691905\n",
      "Operator 1: 4.000000000000021\n",
      "Operator 2: -4.00000000000001\n",
      "Operator 3: 4.000000000000014\n",
      "Operator 4: -4.000000000000002\n",
      "Operator 5: 4.000000000000003\n",
      "Operator 6: -3.9999999999999996\n",
      "Operator 7: 4.000000000000001\n",
      "Operator 8: -4.0000000000000036\n",
      "Operator 9: 3.4142135623692242\n",
      "Operator 11: 3.414213562369194\n",
      "Operator 12: -4.000000000000021\n",
      "Operator 13: 4.00000000000001\n",
      "Operator 14: -4.000000000000014\n",
      "Operator 15: 4.000000000000002\n",
      "Operator 16: -4.000000000000003\n",
      "Operator 17: 3.9999999999999996\n",
      "Operator 18: -3.4142135623691905\n",
      "Operator 19: -2.8284271247383748\n",
      "Operator 20: -4.00000000000001\n",
      "Operator 21: -4.000000000000014\n",
      "Operator 22: -4.000000000000002\n",
      "Operator 23: -4.000000000000003\n",
      "Operator 24: -3.9999999999999996\n",
      "Operator 25: -4.000000000000001\n",
      "Operator 26: -4.0000000000000036\n",
      "Operator 28: -3.4142135623691905\n",
      "Operator 29: -4.000000000000021\n",
      "Operator 30: -4.00000000000001\n",
      "Operator 31: -4.000000000000014\n",
      "Operator 32: -4.000000000000002\n",
      "Operator 33: -4.000000000000003\n",
      "Operator 34: -3.9999999999999996\n",
      "Operator 35: -4.000000000000001\n",
      "Operator 45: -4.000000000000001\n",
      "Operator 46: 4.0000000000000036\n",
      "Operator 47: 3.4142135623692242\n",
      "Operator 49: 3.4142135623691905\n",
      "Operator 50: -4.000000000000021\n",
      "Operator 51: 4.00000000000001\n",
      "Operator 52: -4.000000000000014\n",
      "Operator 53: 4.000000000000002\n",
      "Operator 54: -4.000000000000003\n",
      "Operator 55: 3.9999999999999996\n",
      "Operator 57: 3.414213562369194\n",
      "Operator 58: 2.8284271247383748\n",
      "Operator 59: 4.00000000000001\n",
      "Operator 60: 4.000000000000014\n",
      "Operator 61: 4.000000000000002\n",
      "Operator 62: 4.000000000000003\n",
      "Operator 63: 3.9999999999999996\n",
      "Operator 64: 4.000000000000001\n",
      "Operator 65: -3.4142135623692247\n",
      "Operator 67: 1.414213562377012\n",
      "Operator 76: 4.0000000000000036\n",
      "Operator 78: 3.414213562369194\n",
      "Operator 79: 4.000000000000021\n",
      "Operator 80: 4.00000000000001\n",
      "Operator 81: 4.000000000000014\n",
      "Operator 82: 4.000000000000002\n",
      "Operator 83: 4.000000000000003\n",
      "Operator 84: 3.9999999999999996\n",
      "Operator 85: 4.000000000000001\n",
      "Operator 104: -3.4142135623692247\n",
      "Operator 106: -3.414213562369194\n",
      "Operator 107: 4.000000000000021\n",
      "Operator 108: -4.00000000000001\n",
      "Operator 109: 4.000000000000014\n",
      "Operator 110: -4.000000000000002\n",
      "Operator 111: 4.000000000000003\n",
      "Operator 112: -3.9999999999999996\n",
      "Operator 113: 1.414213562377012\n",
      "Operator 122: -1.41421356237702\n",
      "Operator 131: -3.4142135623691905\n",
      "Operator 132: -2.8284271247383748\n",
      "Operator 133: -2.8284271247383668\n",
      "Operator 134: -4.000000000000014\n",
      "Operator 135: -4.000000000000002\n",
      "Operator 136: -4.000000000000003\n",
      "Operator 137: -3.9999999999999996\n",
      "Operator 138: -4.000000000000001\n",
      "Operator 139: -4.0000000000000036\n",
      "Total gradient norm: 34.37352331727015\n",
      "Operators under consideration (1):\n",
      "[139]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.0000000000000036)]\n",
      "Operator(s) added to ansatz: [139]\n",
      "Gradients: [np.float64(-4.0000000000000036)]\n",
      "Initial energy: -11.828427124746232\n",
      "Optimizing energy with indices [149, 139]...\n",
      "Starting point: [np.float64(0.3926990817001086), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -13.064495\n",
      "         Iterations: 9\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "\n",
      "Current energy: -13.064495102246013\n",
      "(change of -1.236067977499781)\n",
      "Current ansatz: [149, 139]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135622661263\n",
      "Operator 1: 4.0000000000000195\n",
      "Operator 2: -4.000000000000008\n",
      "Operator 3: 4.000000000000012\n",
      "Operator 4: -4.0\n",
      "Operator 5: 3.999999999999999\n",
      "Operator 6: -3.9999999999999964\n",
      "Operator 7: 2.8944271911310833\n",
      "Operator 9: 3.414213562266159\n",
      "Operator 11: 3.41421356226613\n",
      "Operator 12: -4.0000000000000195\n",
      "Operator 13: 4.000000000000008\n",
      "Operator 14: -4.000000000000012\n",
      "Operator 15: 4.0\n",
      "Operator 16: -3.999999999999999\n",
      "Operator 17: 3.9999999999999964\n",
      "Operator 18: -3.4142135622661263\n",
      "Operator 19: -2.8284271245322503\n",
      "Operator 20: -4.000000000000008\n",
      "Operator 21: -4.000000000000012\n",
      "Operator 22: -4.0\n",
      "Operator 23: -3.999999999999999\n",
      "Operator 24: -3.9999999999999964\n",
      "Operator 25: -2.8944271911310846\n",
      "Operator 28: -3.4142135622661263\n",
      "Operator 29: -4.0000000000000195\n",
      "Operator 30: -4.000000000000008\n",
      "Operator 31: -4.000000000000012\n",
      "Operator 32: -4.0\n",
      "Operator 33: -3.999999999999999\n",
      "Operator 34: -1.7888543822621716\n",
      "Operator 35: -2.8944271911310846\n",
      "Operator 45: -2.8944271911310846\n",
      "Operator 47: 3.41421356226616\n",
      "Operator 49: 3.4142135622661263\n",
      "Operator 50: -4.0000000000000195\n",
      "Operator 51: 4.000000000000008\n",
      "Operator 52: -4.000000000000012\n",
      "Operator 53: 4.0\n",
      "Operator 54: -1.7888543822621743\n",
      "Operator 55: 3.9999999999999964\n",
      "Operator 57: 3.4142135622661294\n",
      "Operator 58: 2.8284271245322503\n",
      "Operator 59: 4.000000000000008\n",
      "Operator 60: 4.000000000000012\n",
      "Operator 61: 4.0\n",
      "Operator 62: 3.999999999999999\n",
      "Operator 63: 3.9999999999999964\n",
      "Operator 64: 2.8944271911310833\n",
      "Operator 65: -3.4142135622661605\n",
      "Operator 67: 1.4142135624800731\n",
      "Operator 75: -1.7888543819342446\n",
      "Operator 78: 3.41421356226613\n",
      "Operator 79: 4.0000000000000195\n",
      "Operator 80: 4.000000000000008\n",
      "Operator 81: 4.000000000000012\n",
      "Operator 82: 4.0\n",
      "Operator 83: 3.999999999999999\n",
      "Operator 84: 1.7888543822621716\n",
      "Operator 85: 2.8944271911310833\n",
      "Operator 104: -3.4142135622661605\n",
      "Operator 106: -3.41421356226613\n",
      "Operator 107: 4.0000000000000195\n",
      "Operator 108: -4.000000000000008\n",
      "Operator 109: 4.000000000000012\n",
      "Operator 110: -4.0\n",
      "Operator 111: 1.7888543822621743\n",
      "Operator 112: -3.9999999999999964\n",
      "Operator 113: 1.4142135624800738\n",
      "Operator 121: -1.7888543819342446\n",
      "Operator 122: -1.4142135624800813\n",
      "Operator 130: 1.7888543819342468\n",
      "Operator 131: -3.4142135622661263\n",
      "Operator 132: -2.8284271245322503\n",
      "Operator 133: -2.8284271245322414\n",
      "Operator 134: -4.000000000000012\n",
      "Operator 135: -4.0\n",
      "Operator 136: -3.999999999999999\n",
      "Operator 137: -3.9999999999999964\n",
      "Operator 138: -2.8944271911310833\n",
      "Total gradient norm: 31.726693281625476\n",
      "Operators under consideration (1):\n",
      "[112]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.9999999999999964)]\n",
      "Operator(s) added to ansatz: [112]\n",
      "Gradients: [np.float64(-3.9999999999999964)]\n",
      "Initial energy: -13.064495102246013\n",
      "Optimizing energy with indices [149, 139, 112]...\n",
      "Starting point: [np.float64(0.3926990817365464), np.float64(0.5535743588603823), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -14.123463\n",
      "         Iterations: 7\n",
      "         Function evaluations: 61\n",
      "         Gradient evaluations: 49\n",
      "\n",
      "Current energy: -14.123462682928345\n",
      "(change of -1.0589675806823315)\n",
      "Current ansatz: [149, 139, 112]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135312834743\n",
      "Operator 1: 4.000000000000024\n",
      "Operator 2: -4.0000000000000115\n",
      "Operator 3: 4.000000000000015\n",
      "Operator 4: -4.0000000000000036\n",
      "Operator 5: 3.0643400666326044\n",
      "Operator 6: -2.2320761350336227e-08\n",
      "Operator 7: 1.5786157892198847\n",
      "Operator 8: -9.982455217993902e-08\n",
      "Operator 9: 3.4142135312835085\n",
      "Operator 10: 1.2435854817666948e-07\n",
      "Operator 11: 3.4142135312834783\n",
      "Operator 12: -4.000000000000024\n",
      "Operator 13: 4.0000000000000115\n",
      "Operator 14: -4.000000000000015\n",
      "Operator 15: 4.0000000000000036\n",
      "Operator 16: -3.0643400666326017\n",
      "Operator 17: 2.2320761350336227e-08\n",
      "Operator 18: -3.4142135312834743\n",
      "Operator 19: -2.8284270625669405\n",
      "Operator 20: -4.0000000000000115\n",
      "Operator 21: -4.000000000000015\n",
      "Operator 22: -4.0000000000000036\n",
      "Operator 23: -3.0643400666326017\n",
      "Operator 24: -2.2320761350336227e-08\n",
      "Operator 25: -2.273682128423044\n",
      "Operator 26: 1.3853852047131117\n",
      "Operator 27: 1.2435854817666948e-07\n",
      "Operator 28: -3.4142135312834743\n",
      "Operator 29: -4.000000000000024\n",
      "Operator 30: -4.0000000000000115\n",
      "Operator 31: -4.000000000000015\n",
      "Operator 32: -2.128680133265204\n",
      "Operator 33: -3.064340066632602\n",
      "Operator 34: 1.5813158540983392\n",
      "Operator 35: -2.273682128423044\n",
      "Operator 44: 1.6363381713863485\n",
      "Operator 45: -1.5786157892198875\n",
      "Operator 46: 9.982455217993902e-08\n",
      "Operator 47: 3.4142135312835085\n",
      "Operator 48: 1.2435854817666948e-07\n",
      "Operator 49: 3.4142135312834743\n",
      "Operator 50: -4.000000000000024\n",
      "Operator 51: 4.0000000000000115\n",
      "Operator 52: -2.1286801332652106\n",
      "Operator 53: 4.0000000000000036\n",
      "Operator 54: -0.7879578510103296\n",
      "Operator 55: 2.2320761350336227e-08\n",
      "Operator 56: -1.2435854817666948e-07\n",
      "Operator 57: 3.4142135312834787\n",
      "Operator 58: 2.8284270625669405\n",
      "Operator 59: 4.0000000000000115\n",
      "Operator 60: 4.000000000000015\n",
      "Operator 61: 4.0000000000000036\n",
      "Operator 62: 3.0643400666326044\n",
      "Operator 63: 2.2320761350336227e-08\n",
      "Operator 64: 2.273682128423044\n",
      "Operator 65: -3.4142135312835085\n",
      "Operator 66: -1.2435854817666948e-07\n",
      "Operator 67: 1.4142135934627293\n",
      "Operator 73: -1.6932749990951055\n",
      "Operator 74: 0.4354051118493083\n",
      "Operator 75: -1.932749461559189\n",
      "Operator 76: -1.3853852047131117\n",
      "Operator 77: -1.2435854817666948e-07\n",
      "Operator 78: 3.4142135312834783\n",
      "Operator 79: 4.000000000000024\n",
      "Operator 80: 4.0000000000000115\n",
      "Operator 81: 4.000000000000015\n",
      "Operator 82: 2.1286801332652043\n",
      "Operator 83: 3.064340066632604\n",
      "Operator 84: -1.5813158540983387\n",
      "Operator 85: 2.2736821284230433\n",
      "Operator 94: 1.636338171386348\n",
      "Operator 103: -1.636338171386348\n",
      "Operator 104: -3.414213531283508\n",
      "Operator 105: -1.2435854817666948e-07\n",
      "Operator 106: -3.4142135312834783\n",
      "Operator 107: 4.000000000000024\n",
      "Operator 108: -4.0000000000000115\n",
      "Operator 109: 2.1286801332652106\n",
      "Operator 110: -4.0000000000000036\n",
      "Operator 111: 0.7879578510103298\n",
      "Operator 112: -2.2320761350336227e-08\n",
      "Operator 113: 1.4142135934627293\n",
      "Operator 119: -1.6932749990951053\n",
      "Operator 120: 1.6932749990951046\n",
      "Operator 121: -1.932749461559189\n",
      "Operator 122: -1.4142135934627373\n",
      "Operator 128: 1.693274999095102\n",
      "Operator 129: -1.6932749990951055\n",
      "Operator 130: 1.9327494615591903\n",
      "Operator 131: -3.4142135312834743\n",
      "Operator 132: -2.8284270625669405\n",
      "Operator 133: -2.8284270625669325\n",
      "Operator 134: -4.000000000000015\n",
      "Operator 135: -4.0000000000000036\n",
      "Operator 136: -3.0643400666326044\n",
      "Operator 137: -2.2320761350336227e-08\n",
      "Operator 138: -1.5786157892198847\n",
      "Operator 139: 1.3853852047131126\n",
      "Operator 148: -1.636338171386348\n",
      "Operator 149: 1.2435854817666948e-07\n",
      "Total gradient norm: 28.248774931330516\n",
      "Operators under consideration (1):\n",
      "[134]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000015)]\n",
      "Operator(s) added to ansatz: [134]\n",
      "Gradients: [np.float64(-4.000000000000015)]\n",
      "Initial energy: -14.123462682928345\n",
      "Optimizing energy with indices [149, 139, 112, 134]...\n",
      "Starting point: [np.float64(0.392699092690569), np.float64(0.6553685118825965), np.float64(0.504817350140072), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -14.951890\n",
      "         Iterations: 9\n",
      "         Function evaluations: 89\n",
      "         Gradient evaluations: 76\n",
      "\n",
      "Current energy: -14.951889807674572\n",
      "(change of -0.8284271247462272)\n",
      "Current ansatz: [149, 139, 112, 134]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.4142135360318915\n",
      "Operator 1: 4.000000000000032\n",
      "Operator 2: -3.4142136240494376\n",
      "Operator 3: 2.4670530939460104e-07\n",
      "Operator 4: -3.4142136240494314\n",
      "Operator 5: 3.064340092075494\n",
      "Operator 6: -4.442194057041604e-08\n",
      "Operator 7: 1.5786158724082564\n",
      "Operator 8: -1.989050613104837e-07\n",
      "Operator 9: 3.414213536031926\n",
      "Operator 10: 1.0536490641554181e-07\n",
      "Operator 11: 3.4142135360318946\n",
      "Operator 12: -4.000000000000032\n",
      "Operator 13: 3.4142136240494376\n",
      "Operator 14: -2.4670530963643525e-07\n",
      "Operator 15: 3.4142136240494305\n",
      "Operator 16: -3.0643400920754926\n",
      "Operator 17: 4.442194057041604e-08\n",
      "Operator 18: -3.414213536031892\n",
      "Operator 19: -2.8284270720637683\n",
      "Operator 20: -3.4142136240494376\n",
      "Operator 21: -2.4670530963643525e-07\n",
      "Operator 22: -3.414213624049431\n",
      "Operator 23: -2.1668157534670054\n",
      "Operator 24: -4.442194057041604e-08\n",
      "Operator 25: -2.273682165695795\n",
      "Operator 26: 1.385385114802414\n",
      "Operator 27: 1.0536490599356368e-07\n",
      "Operator 28: -3.4142135360318915\n",
      "Operator 29: -2.828427248098862\n",
      "Operator 30: -3.414213624049439\n",
      "Operator 31: -2.4670530939460104e-07\n",
      "Operator 32: -1.8169422214930724\n",
      "Operator 33: -3.0643400920754926\n",
      "Operator 34: 1.5813158083374308\n",
      "Operator 35: -2.273682165695795\n",
      "Operator 44: 1.6363381429227384\n",
      "Operator 45: -1.5786158724082586\n",
      "Operator 46: 1.989050613104837e-07\n",
      "Operator 47: 3.414213536031926\n",
      "Operator 48: 1.0536490599356368e-07\n",
      "Operator 49: 2.4142136490351227\n",
      "Operator 50: -4.000000000000032\n",
      "Operator 51: 3.414213624049439\n",
      "Operator 52: -1.3128917572617562e-07\n",
      "Operator 53: 3.4142136240494314\n",
      "Operator 54: -0.7879579460285615\n",
      "Operator 55: 4.442194057041604e-08\n",
      "Operator 56: -1.0536490599356368e-07\n",
      "Operator 57: 3.414213536031895\n",
      "Operator 58: 2.8284270720637683\n",
      "Operator 59: 3.4142136240494376\n",
      "Operator 60: 2.4670530939460104e-07\n",
      "Operator 61: 3.414213624049431\n",
      "Operator 62: 2.1668157534670063\n",
      "Operator 63: 4.442194057041604e-08\n",
      "Operator 64: 2.273682165695794\n",
      "Operator 65: -3.4142135360319266\n",
      "Operator 66: -1.0536490599356368e-07\n",
      "Operator 67: 1.4142135887143223\n",
      "Operator 70: -1.4142135006967744\n",
      "Operator 71: 1.414213500696774\n",
      "Operator 73: -1.1973262751828044\n",
      "Operator 74: 0.43540515662649637\n",
      "Operator 75: -1.9327494461939831\n",
      "Operator 76: -1.385385114802414\n",
      "Operator 77: -1.0536490641554181e-07\n",
      "Operator 78: 3.4142135360318946\n",
      "Operator 79: 2.828427248098862\n",
      "Operator 80: 3.414213624049437\n",
      "Operator 81: 2.4670530963643525e-07\n",
      "Operator 82: 1.8169422214930715\n",
      "Operator 83: 3.0643400920754935\n",
      "Operator 84: -1.5813158083374312\n",
      "Operator 85: 2.2736821656957944\n",
      "Operator 94: 1.6363381429227384\n",
      "Operator 103: -1.636338142922738\n",
      "Operator 104: -3.4142135360319266\n",
      "Operator 105: -1.0536490641554181e-07\n",
      "Operator 106: -2.414213649035126\n",
      "Operator 107: 4.000000000000032\n",
      "Operator 108: -3.414213624049437\n",
      "Operator 109: 1.3128917707008841e-07\n",
      "Operator 110: -3.4142136240494305\n",
      "Operator 111: 0.7879579460285613\n",
      "Operator 112: -4.442194057041604e-08\n",
      "Operator 113: 1.4142135887143223\n",
      "Operator 116: -1.4142135006967744\n",
      "Operator 117: 1.414213500696774\n",
      "Operator 119: -1.6932749831025036\n",
      "Operator 120: 1.6932749831025022\n",
      "Operator 121: -1.9327494461939831\n",
      "Operator 122: -1.41421358871433\n",
      "Operator 125: 1.4142135006967762\n",
      "Operator 126: -1.4142135006967718\n",
      "Operator 128: 1.6932749831025002\n",
      "Operator 129: -1.6932749831025036\n",
      "Operator 130: 1.9327494461939847\n",
      "Operator 131: -3.4142135360318915\n",
      "Operator 132: -2.8284270720637683\n",
      "Operator 133: -2.4142135610175757\n",
      "Operator 134: -2.4670530939460104e-07\n",
      "Operator 135: -3.4142136240494314\n",
      "Operator 136: -2.1668157534670063\n",
      "Operator 137: -3.1411057643815354e-08\n",
      "Operator 138: -1.5786158724082564\n",
      "Operator 139: 1.385385114802415\n",
      "Operator 148: -1.636338142922738\n",
      "Operator 149: 1.0536490641554181e-07\n",
      "Total gradient norm: 23.99950549760343\n",
      "Operators under consideration (1):\n",
      "[50]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000032)]\n",
      "Operator(s) added to ansatz: [50]\n",
      "Gradients: [np.float64(-4.000000000000032)]\n",
      "Initial energy: -14.951889807674572\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50]...\n",
      "Starting point: [np.float64(0.3926990910117525), np.float64(0.6553684969439084), np.float64(0.5048173426271508), np.float64(0.39269905989285026), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -16.027480\n",
      "         Iterations: 10\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 64\n",
      "\n",
      "Current energy: -16.027480122926548\n",
      "(change of -1.0755903152519757)\n",
      "Current ansatz: [149, 139, 112, 134, 50]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.2432916907621347\n",
      "Operator 1: 1.6653057115823294e-08\n",
      "Operator 2: -2.243291691974471\n",
      "Operator 3: 2.554343097996875e-08\n",
      "Operator 4: -3.208780228467157\n",
      "Operator 5: 3.064340042724396\n",
      "Operator 6: 1.682781813894252e-08\n",
      "Operator 7: 1.5786157327592822\n",
      "Operator 8: -5.321456342244346e-08\n",
      "Operator 9: 3.208780227254862\n",
      "Operator 10: -2.1723304475607183e-08\n",
      "Operator 11: 2.243291690762145\n",
      "Operator 12: -1.6653057196836443e-08\n",
      "Operator 13: 2.2432916919744743\n",
      "Operator 14: -2.5543431561800632e-08\n",
      "Operator 15: 3.2087802284671563\n",
      "Operator 16: -3.064340042724395\n",
      "Operator 17: -1.682781813894252e-08\n",
      "Operator 18: -2.6252485009780577\n",
      "Operator 19: 1.0864132692667567\n",
      "Operator 20: -2.625248501605141\n",
      "Operator 21: 1.1670634281806163\n",
      "Operator 22: -3.208780228467157\n",
      "Operator 23: -1.8520568284727092\n",
      "Operator 24: 1.682781813894252e-08\n",
      "Operator 25: -2.273682104951935\n",
      "Operator 26: 1.3853852603170518\n",
      "Operator 27: 1.167063430830238\n",
      "Operator 28: -2.6252485009780577\n",
      "Operator 29: 1.0864132680125844\n",
      "Operator 30: -2.625248501605141\n",
      "Operator 31: -2.554343097996875e-08\n",
      "Operator 32: -1.7076166427299493\n",
      "Operator 33: -3.064340042724395\n",
      "Operator 34: 1.5813158923727162\n",
      "Operator 35: -2.273682104951935\n",
      "Operator 37: 1.363661278340433\n",
      "Operator 39: 1.3636612775533168\n",
      "Operator 44: 1.636338193242318\n",
      "Operator 45: -1.5786157327592845\n",
      "Operator 46: 5.321456342244346e-08\n",
      "Operator 47: 1.6597599644853815\n",
      "Operator 48: -2.172330393067514e-08\n",
      "Operator 49: 1.5866742413976476\n",
      "Operator 50: -1.6653057205038855e-08\n",
      "Operator 51: 2.625248501605141\n",
      "Operator 52: -1.3593449832208278e-08\n",
      "Operator 53: 3.208780228467157\n",
      "Operator 54: -0.7879577949868255\n",
      "Operator 55: -1.682781813894252e-08\n",
      "Operator 56: 2.1723303955631888e-08\n",
      "Operator 57: 2.6252485009780546\n",
      "Operator 58: -1.0864132692667567\n",
      "Operator 59: 2.6252485016051406\n",
      "Operator 60: -1.167063428180617\n",
      "Operator 61: 3.2087802284671563\n",
      "Operator 62: 1.8520568284727097\n",
      "Operator 63: -1.682781813894252e-08\n",
      "Operator 64: 2.2736821049519347\n",
      "Operator 65: -3.208780227254861\n",
      "Operator 66: 2.1723303955631888e-08\n",
      "Operator 67: 0.8241834020124159\n",
      "Operator 68: -1.7116617749637264\n",
      "Operator 69: 0.6252484962591154\n",
      "Operator 70: -1.5933770298541825\n",
      "Operator 71: 0.8241834015366927\n",
      "Operator 73: -1.0233986792146974\n",
      "Operator 74: 0.43540508815349166\n",
      "Operator 75: -1.9327494702208976\n",
      "Operator 76: -1.3853852603170518\n",
      "Operator 77: -1.1670634308302374\n",
      "Operator 78: 2.6252485009780546\n",
      "Operator 79: -1.0864132680125842\n",
      "Operator 80: 2.6252485016051406\n",
      "Operator 81: 2.5543431561800632e-08\n",
      "Operator 82: 1.7076166427299488\n",
      "Operator 83: 3.064340042724396\n",
      "Operator 84: -1.5813158923727162\n",
      "Operator 85: 2.2736821049519347\n",
      "Operator 87: 1.363661278340433\n",
      "Operator 89: 1.3636612775533168\n",
      "Operator 94: 1.6363381932423176\n",
      "Operator 96: -1.3636612783404356\n",
      "Operator 98: -1.3636612775533168\n",
      "Operator 103: -1.6363381932423176\n",
      "Operator 104: -1.6597599644853813\n",
      "Operator 105: 2.1723304451373478e-08\n",
      "Operator 106: -1.5866742413976451\n",
      "Operator 107: 1.665305729364311e-08\n",
      "Operator 108: -2.6252485016051406\n",
      "Operator 109: 1.359345056525131e-08\n",
      "Operator 110: -3.2087802284671563\n",
      "Operator 111: 0.7879577949868257\n",
      "Operator 112: 1.682781813894252e-08\n",
      "Operator 113: 1.5933770307738935\n",
      "Operator 114: -1.7116617749637264\n",
      "Operator 115: 1.71166177496373\n",
      "Operator 116: -1.5933770298541825\n",
      "Operator 117: 1.5933770298541807\n",
      "Operator 119: -1.6932750141230706\n",
      "Operator 120: 1.6932750141230697\n",
      "Operator 121: -1.9327494702208978\n",
      "Operator 122: -1.5933770307738966\n",
      "Operator 123: 1.7116617749637357\n",
      "Operator 124: -1.71166177496373\n",
      "Operator 125: 1.5933770298541836\n",
      "Operator 126: -1.5933770298541785\n",
      "Operator 128: 1.693275014123068\n",
      "Operator 129: -1.6932750141230706\n",
      "Operator 130: 1.9327494702208987\n",
      "Operator 131: -2.2432916907621347\n",
      "Operator 132: 1.0864132692667567\n",
      "Operator 133: -1.3558233206118673\n",
      "Operator 134: 1.167063428180617\n",
      "Operator 135: -1.6597599651124455\n",
      "Operator 136: -1.8520568284727097\n",
      "Operator 137: 1.0170565112321839e-08\n",
      "Operator 138: -1.5786157327592822\n",
      "Operator 139: 1.3853852603170518\n",
      "Operator 141: -1.3636612783404356\n",
      "Operator 143: -1.3636612775533168\n",
      "Operator 148: -1.6363381932423173\n",
      "Operator 149: -2.1723304475607183e-08\n",
      "Total gradient norm: 19.53992159282786\n",
      "Operators under consideration (1):\n",
      "[110]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.2087802284671563)]\n",
      "Operator(s) added to ansatz: [110]\n",
      "Gradients: [np.float64(-3.2087802284671563)]\n",
      "Initial energy: -16.027480122926548\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110]...\n",
      "Starting point: [np.float64(0.46089810430885453), np.float64(0.6553685203038622), np.float64(0.5048173571998272), np.float64(0.4608981039284251), np.float64(0.513577524580443), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -16.607123\n",
      "         Iterations: 11\n",
      "         Function evaluations: 56\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "Current energy: -16.60712258504065\n",
      "(change of -0.5796424621141014)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.386739272746647\n",
      "Operator 2: -2.721568214198333\n",
      "Operator 4: -1.2144396260490915\n",
      "Operator 5: 2.3798533238912656\n",
      "Operator 7: 1.2961847987013413\n",
      "Operator 8: -1.123069992559067e-08\n",
      "Operator 9: 3.2372526985594994\n",
      "Operator 11: 2.386739272746656\n",
      "Operator 13: 2.721568214198335\n",
      "Operator 14: -1.289240150784368\n",
      "Operator 16: -2.379853323891264\n",
      "Operator 18: -2.711102682935495\n",
      "Operator 19: 1.0103172272790364\n",
      "Operator 20: -2.9035433693595007\n",
      "Operator 21: -0.11626041570591936\n",
      "Operator 22: -1.2144396260490924\n",
      "Operator 23: -2.088534687329655\n",
      "Operator 24: 0.7754057421066741\n",
      "Operator 25: -2.185220212450062\n",
      "Operator 26: 1.58379048906093\n",
      "Operator 27: 1.0523000369085982\n",
      "Operator 28: -2.7111026829354943\n",
      "Operator 29: 0.6254358544310146\n",
      "Operator 30: -2.1908508959984383\n",
      "Operator 31: -1.289240150784368\n",
      "Operator 32: 0.3073688634995585\n",
      "Operator 33: -2.6570308235580757\n",
      "Operator 34: 1.7190269643226819\n",
      "Operator 35: -2.185220212450062\n",
      "Operator 37: 1.285905673581393\n",
      "Operator 39: 0.7634069761361733\n",
      "Operator 40: -0.46632006302111484\n",
      "Operator 41: -0.8113536486511013\n",
      "Operator 42: 1.1815713918766841\n",
      "Operator 44: 1.7592874536345264\n",
      "Operator 45: -1.2961847987013437\n",
      "Operator 46: 1.123069992559067e-08\n",
      "Operator 47: 1.8605892571226936\n",
      "Operator 49: 2.131037376013908\n",
      "Operator 51: 2.9035433693595007\n",
      "Operator 52: -0.5613129561919858\n",
      "Operator 53: 1.2144396260490906\n",
      "Operator 54: -0.5651760248593554\n",
      "Operator 57: 2.711102682935492\n",
      "Operator 58: -1.0103172272790364\n",
      "Operator 59: 2.9035433693595007\n",
      "Operator 60: -0.8279461547947269\n",
      "Operator 62: 2.0885346873296555\n",
      "Operator 63: -0.7754057421066736\n",
      "Operator 64: 2.185220212450062\n",
      "Operator 65: -3.237252698559497\n",
      "Operator 67: 0.9031347399799009\n",
      "Operator 68: -1.6366675336681626\n",
      "Operator 69: 0.795855059481833\n",
      "Operator 70: -1.236349188999581\n",
      "Operator 71: 1.395307142664222\n",
      "Operator 72: 0.571439537098883\n",
      "Operator 73: -1.4152593153666226\n",
      "Operator 74: 0.2889765249485833\n",
      "Operator 75: -1.9542309139610365\n",
      "Operator 76: -1.58379048906093\n",
      "Operator 77: -1.0523000369085977\n",
      "Operator 78: 2.7111026829354925\n",
      "Operator 79: -0.6254358544310139\n",
      "Operator 80: 2.1908508959984383\n",
      "Operator 82: -1.0637042507916443\n",
      "Operator 83: 2.657030823558076\n",
      "Operator 84: -1.7190269643226819\n",
      "Operator 85: 2.1852202124500617\n",
      "Operator 87: 1.285905673581393\n",
      "Operator 89: 1.0117462889562852\n",
      "Operator 90: -0.8113536486511022\n",
      "Operator 91: -0.35324940412724837\n",
      "Operator 92: 0.928763345765867\n",
      "Operator 94: 1.7592874536345264\n",
      "Operator 96: -1.285905673581395\n",
      "Operator 98: -0.7634069761361737\n",
      "Operator 101: -1.1815713918766855\n",
      "Operator 103: -1.7592874536345255\n",
      "Operator 104: -1.8605892571226936\n",
      "Operator 106: -2.1310373760139067\n",
      "Operator 108: -2.903543369359502\n",
      "Operator 111: 0.5651760248593558\n",
      "Operator 113: 1.5713706627996418\n",
      "Operator 114: -1.636667533668162\n",
      "Operator 115: 1.6366675336681653\n",
      "Operator 116: -0.932879721057579\n",
      "Operator 117: 1.4823449686204493\n",
      "Operator 118: 1.3124991804421038\n",
      "Operator 119: -1.8004908642742\n",
      "Operator 120: 1.8004908642741992\n",
      "Operator 121: -1.954230913961036\n",
      "Operator 122: -1.5713706627996449\n",
      "Operator 123: 1.6366675336681715\n",
      "Operator 124: -1.6366675336681658\n",
      "Operator 125: 1.2363491889995815\n",
      "Operator 126: 1.5442225121628723\n",
      "Operator 127: -1.0316779320513427\n",
      "Operator 128: 1.800490864274198\n",
      "Operator 129: -1.8004908642742006\n",
      "Operator 130: 1.954230913961037\n",
      "Operator 131: -2.386739272746647\n",
      "Operator 132: 1.0103172272790364\n",
      "Operator 133: -1.683633808665255\n",
      "Operator 134: 0.8279461547947269\n",
      "Operator 135: -0.6979910226521677\n",
      "Operator 136: -1.8706618582044632\n",
      "Operator 137: 0.6095005653625525\n",
      "Operator 138: -0.9780283145007116\n",
      "Operator 139: 1.5837904890609291\n",
      "Operator 141: -1.285905673581395\n",
      "Operator 143: -1.0117462889562856\n",
      "Operator 146: -0.9287633457658682\n",
      "Operator 148: -1.7592874536345255\n",
      "Total gradient norm: 18.072847608310074\n",
      "Operators under consideration (1):\n",
      "[65]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.237252698559497)]\n",
      "Operator(s) added to ansatz: [65]\n",
      "Gradients: [np.float64(-3.237252698559497)]\n",
      "Initial energy: -16.60712258504065\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65]...\n",
      "Starting point: [np.float64(0.45190158834751465), np.float64(0.6782245581097973), np.float64(0.56016645132361), np.float64(0.3332091491729178), np.float64(0.4792529576383789), np.float64(0.3579187585716581), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -17.335898\n",
      "         Iterations: 11\n",
      "         Function evaluations: 54\n",
      "         Gradient evaluations: 45\n",
      "\n",
      "Current energy: -17.335897915572804\n",
      "(change of -0.7287753305321552)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.901301448715434\n",
      "Operator 1: 9.14780640529534e-08\n",
      "Operator 2: -2.845492337063895\n",
      "Operator 3: -1.1922625987446841e-08\n",
      "Operator 4: -1.1918955172115688\n",
      "Operator 5: 2.3700644750155764\n",
      "Operator 6: 1.7676249453035808e-07\n",
      "Operator 7: 1.2917963798021546\n",
      "Operator 8: -2.767084339438952e-08\n",
      "Operator 9: 1.1323805647625926\n",
      "Operator 10: -3.0506020005669286e-07\n",
      "Operator 11: 2.90130144871544\n",
      "Operator 12: -9.147806307409843e-08\n",
      "Operator 13: 2.845492337063898\n",
      "Operator 14: -1.2948189330782534\n",
      "Operator 15: 2.8014509105612537e-07\n",
      "Operator 16: -2.3700644750155746\n",
      "Operator 17: -1.767624950854696e-07\n",
      "Operator 18: -1.9796322506128963\n",
      "Operator 19: 0.5042307958939996\n",
      "Operator 20: -2.998384409242177\n",
      "Operator 21: -0.2616334224594636\n",
      "Operator 22: -1.1918955172115693\n",
      "Operator 23: -2.106065262100215\n",
      "Operator 24: 0.7852478081499111\n",
      "Operator 25: -2.183977969414731\n",
      "Operator 26: 1.5865783707740695\n",
      "Operator 27: -0.4977142679318224\n",
      "Operator 28: -3.0334632947285884\n",
      "Operator 29: 0.5743885668668134\n",
      "Operator 30: -2.2522233253913795\n",
      "Operator 31: -1.2948189330782536\n",
      "Operator 32: 0.3340084032761548\n",
      "Operator 33: -2.6518240626643985\n",
      "Operator 34: 1.7209399946492303\n",
      "Operator 35: -2.183977969414731\n",
      "Operator 36: -0.8627885355996696\n",
      "Operator 37: 0.8856350626230398\n",
      "Operator 39: 0.7100143858046233\n",
      "Operator 40: -0.5042748170046161\n",
      "Operator 41: -0.8022811778377532\n",
      "Operator 42: 1.189523116040651\n",
      "Operator 44: 1.7609718578157167\n",
      "Operator 45: -1.2917963798021572\n",
      "Operator 46: 2.767084339438952e-08\n",
      "Operator 47: 0.7117591909787027\n",
      "Operator 48: -1.5070149058707756\n",
      "Operator 49: 2.4091612105159737\n",
      "Operator 50: -6.871334751068719e-08\n",
      "Operator 51: 2.9983844092421768\n",
      "Operator 52: -0.5618046031367268\n",
      "Operator 53: 1.1918955172115688\n",
      "Operator 54: -0.5622173974722768\n",
      "Operator 55: -1.7676249453035808e-07\n",
      "Operator 56: 3.050601995773088e-07\n",
      "Operator 57: 1.9796322506128932\n",
      "Operator 58: -0.504230795893998\n",
      "Operator 59: 2.998384409242178\n",
      "Operator 60: -0.7351791617082775\n",
      "Operator 61: 2.801450899646899e-07\n",
      "Operator 62: 2.1060652621002145\n",
      "Operator 63: -0.7852478081499106\n",
      "Operator 64: 2.183977969414731\n",
      "Operator 65: -8.771886369110188e-08\n",
      "Operator 66: 1.5070149058707751\n",
      "Operator 67: 0.7157235412463494\n",
      "Operator 68: -1.0151400187120008\n",
      "Operator 69: 1.015621004884756\n",
      "Operator 70: -1.2153261149686685\n",
      "Operator 71: 1.4113806823987876\n",
      "Operator 72: 0.572848817717238\n",
      "Operator 73: -1.4310876392093967\n",
      "Operator 74: 0.28696096701300966\n",
      "Operator 75: -1.9545343096035235\n",
      "Operator 76: -1.5865783707740695\n",
      "Operator 77: -0.68881858811743\n",
      "Operator 78: 3.033463294728585\n",
      "Operator 79: -0.5743885668668143\n",
      "Operator 80: 2.252223325391381\n",
      "Operator 81: -1.1922625236116388e-08\n",
      "Operator 82: -1.0717215265552227\n",
      "Operator 83: 2.651824062664399\n",
      "Operator 84: -1.7209399946492303\n",
      "Operator 85: 2.183977969414731\n",
      "Operator 86: -0.5423067908383481\n",
      "Operator 87: 0.5779637206386598\n",
      "Operator 89: 0.9452419929823321\n",
      "Operator 90: -0.8022811778377539\n",
      "Operator 91: -0.3480990640503426\n",
      "Operator 92: 0.944713244151395\n",
      "Operator 94: 1.7609718578157167\n",
      "Operator 96: -0.8856350626230415\n",
      "Operator 98: -0.7100143858046238\n",
      "Operator 101: -1.1895231160406525\n",
      "Operator 103: -1.7609718578157156\n",
      "Operator 104: -5.513579874769499e-08\n",
      "Operator 105: 3.050602001216103e-07\n",
      "Operator 106: -2.4091612105159705\n",
      "Operator 107: 6.871334613225641e-08\n",
      "Operator 108: -2.9983844092421785\n",
      "Operator 110: -2.8014509105612537e-07\n",
      "Operator 111: 0.5622173974722768\n",
      "Operator 112: 1.767624950854696e-07\n",
      "Operator 113: 1.1386876883682957\n",
      "Operator 114: -1.5555363804664049\n",
      "Operator 115: 1.555536380466409\n",
      "Operator 116: -0.912886891905073\n",
      "Operator 117: 1.4733279036574536\n",
      "Operator 118: 1.3202730821898556\n",
      "Operator 119: -1.801934966465672\n",
      "Operator 120: 1.8019349664656725\n",
      "Operator 121: -1.9545343096035235\n",
      "Operator 122: -0.7431053723929826\n",
      "Operator 123: 1.5555363804664168\n",
      "Operator 124: -1.5555363804664104\n",
      "Operator 125: 1.2153261149686694\n",
      "Operator 126: 1.5577664321728766\n",
      "Operator 127: -1.0485542061535795\n",
      "Operator 128: 1.8019349664656703\n",
      "Operator 129: -1.8019349664656739\n",
      "Operator 130: 1.9545343096035244\n",
      "Operator 131: -1.89338368676087\n",
      "Operator 132: 0.3290600374293569\n",
      "Operator 133: -2.339277826151157\n",
      "Operator 134: 0.7351791617082775\n",
      "Operator 135: -0.7491673872374391\n",
      "Operator 136: -1.8822932222558195\n",
      "Operator 137: 0.6236398387694049\n",
      "Operator 138: -0.9703271966324993\n",
      "Operator 139: 1.5865783707740695\n",
      "Operator 141: -0.5779637206386607\n",
      "Operator 143: -0.9452419929823326\n",
      "Operator 146: -0.9447132441513963\n",
      "Operator 148: -1.7609718578157156\n",
      "Operator 149: -1.5070149058707758\n",
      "Total gradient norm: 17.416224725195566\n",
      "Operators under consideration (1):\n",
      "[28]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.0334632947285884)]\n",
      "Operator(s) added to ansatz: [28]\n",
      "Gradients: [np.float64(-3.0334632947285884)]\n",
      "Initial energy: -17.335897915572804\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28]...\n",
      "Starting point: [np.float64(0.3028537439717878), np.float64(0.6785817284495919), np.float64(0.5609970924774158), np.float64(0.32655736832810384), np.float64(0.44555362717029934), np.float64(0.36050026174141275), np.float64(0.42989403934362447), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -17.762176\n",
      "         Iterations: 14\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 47\n",
      "\n",
      "Current energy: -17.762175809201274\n",
      "(change of -0.4262778936284697)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.1559620102218124\n",
      "Operator 1: -1.1010767984080806e-07\n",
      "Operator 2: -3.168553784240608\n",
      "Operator 3: 1.198823227795521e-08\n",
      "Operator 4: -1.1350957611780752\n",
      "Operator 5: 2.345478548846052\n",
      "Operator 7: 1.2807314945094694\n",
      "Operator 9: 0.8227925097252419\n",
      "Operator 10: 7.312272987771776e-08\n",
      "Operator 11: 0.6374351821629438\n",
      "Operator 12: -0.9919825446726032\n",
      "Operator 13: 3.1685537842406086\n",
      "Operator 14: -1.3080533081651406\n",
      "Operator 16: -2.3454785488460494\n",
      "Operator 18: 0.764607987391948\n",
      "Operator 19: 0.3416069785924712\n",
      "Operator 20: -2.7239868947823105\n",
      "Operator 21: -0.657087093957101\n",
      "Operator 22: -1.1350957611780759\n",
      "Operator 23: -2.1474570182086596\n",
      "Operator 24: 0.8098054352148479\n",
      "Operator 25: -2.1808639078301204\n",
      "Operator 26: 1.5935672635703577\n",
      "Operator 27: -0.9863115390544585\n",
      "Operator 28: 5.7905620253103676e-08\n",
      "Operator 29: 0.4305936573030099\n",
      "Operator 30: -2.4165201521989568\n",
      "Operator 31: -1.3080533081651406\n",
      "Operator 32: 0.40013465679549426\n",
      "Operator 33: -2.638827811036527\n",
      "Operator 34: 1.7257332416276427\n",
      "Operator 35: -2.1808639078301204\n",
      "Operator 36: 0.9554153105642239\n",
      "Operator 37: 0.44415875930392895\n",
      "Operator 39: 0.4606035492972196\n",
      "Operator 40: -0.5997535405325942\n",
      "Operator 41: -0.7784105350372807\n",
      "Operator 42: 1.2091908836915906\n",
      "Operator 44: 1.7651895715326449\n",
      "Operator 45: -1.2807314945094725\n",
      "Operator 47: 0.6339491805667523\n",
      "Operator 48: -1.6527826645090864\n",
      "Operator 49: -4.7123131855111964e-08\n",
      "Operator 50: 8.176861344924822e-08\n",
      "Operator 51: 3.254028880811767\n",
      "Operator 52: -0.562613623742415\n",
      "Operator 53: 1.1350957611780759\n",
      "Operator 54: -0.554814964166513\n",
      "Operator 56: 0.8378676833832333\n",
      "Operator 57: 0.37528859056762054\n",
      "Operator 58: 0.7171493134823032\n",
      "Operator 59: 2.723986894782312\n",
      "Operator 60: -0.4723092077151197\n",
      "Operator 62: 2.14745701820866\n",
      "Operator 63: -0.8098054352148479\n",
      "Operator 64: 2.180863907830121\n",
      "Operator 65: 1.69394093251369e-08\n",
      "Operator 66: 1.8433466952623063\n",
      "Operator 67: -0.8494388807411694\n",
      "Operator 68: -0.9449056008721922\n",
      "Operator 69: 0.7897311437688244\n",
      "Operator 70: -0.9729857792145498\n",
      "Operator 71: 1.4338743942293055\n",
      "Operator 72: 0.5761036403439841\n",
      "Operator 73: -1.4693405661944774\n",
      "Operator 74: 0.28191304534558004\n",
      "Operator 75: -1.9552950792305803\n",
      "Operator 76: -1.5935672635703577\n",
      "Operator 77: 0.36243169326634506\n",
      "Operator 78: 1.6600660675044925\n",
      "Operator 79: 0.44681240171250247\n",
      "Operator 80: 2.4165201521989568\n",
      "Operator 81: 1.1988232278955173e-08\n",
      "Operator 82: -1.091626101171214\n",
      "Operator 83: 2.638827811036527\n",
      "Operator 84: -1.7257332416276427\n",
      "Operator 85: 2.180863907830121\n",
      "Operator 86: 0.5491814039170699\n",
      "Operator 87: 0.24373340799819232\n",
      "Operator 89: 0.7409256869157694\n",
      "Operator 90: -0.7784105350372816\n",
      "Operator 91: -0.33480621098762214\n",
      "Operator 92: 0.9840298933780997\n",
      "Operator 94: 1.765189571532645\n",
      "Operator 95: -0.664762422606397\n",
      "Operator 96: 1.158098948354165\n",
      "Operator 97: 0.5160691710982541\n",
      "Operator 98: -0.4606035492972197\n",
      "Operator 101: -1.2091908836915923\n",
      "Operator 103: -1.7651895715326438\n",
      "Operator 104: 1.3051558558154827e-08\n",
      "Operator 105: 0.8378676833832333\n",
      "Operator 106: -1.3509485205676524\n",
      "Operator 107: 0.7366701088507064\n",
      "Operator 108: -3.254028880811767\n",
      "Operator 111: 0.5548149641665128\n",
      "Operator 113: -1.392719574615287\n",
      "Operator 114: -1.7219145409308834\n",
      "Operator 115: 1.0672480639751767\n",
      "Operator 116: -0.8631611803194319\n",
      "Operator 117: 1.4504696077994548\n",
      "Operator 118: 1.3394170364828293\n",
      "Operator 119: -1.80554801194228\n",
      "Operator 120: 1.8055480119422807\n",
      "Operator 121: -1.9552950792305803\n",
      "Operator 122: -0.13262904851722335\n",
      "Operator 123: -1.39719150316235\n",
      "Operator 124: -1.274916567997368\n",
      "Operator 125: 1.1623124297139147\n",
      "Operator 126: 1.589472602188299\n",
      "Operator 127: -1.0900068974843293\n",
      "Operator 128: 1.805548011942279\n",
      "Operator 129: -1.8055480119422822\n",
      "Operator 130: 1.9552950792305803\n",
      "Operator 131: -0.04521864549214476\n",
      "Operator 132: 0.18745782075488704\n",
      "Operator 133: -2.4118214649717062\n",
      "Operator 134: 0.39537574472296844\n",
      "Operator 135: -0.8745741109188422\n",
      "Operator 136: -1.908731729183522\n",
      "Operator 137: 0.65901320198401\n",
      "Operator 138: -0.9511020274859847\n",
      "Operator 139: 1.5935672635703577\n",
      "Operator 140: -0.5121893893074252\n",
      "Operator 141: 0.8383547522093571\n",
      "Operator 142: 0.6974194041575148\n",
      "Operator 143: -0.7409256869157697\n",
      "Operator 146: -0.9840298933781007\n",
      "Operator 148: -1.765189571532644\n",
      "Operator 149: -1.6527826645090866\n",
      "Total gradient norm: 16.13425604782015\n",
      "Operators under consideration (1):\n",
      "[108]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.254028880811767)]\n",
      "Operator(s) added to ansatz: [108]\n",
      "Gradients: [np.float64(-3.254028880811767)]\n",
      "Initial energy: -17.762175809201274\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108]...\n",
      "Starting point: [np.float64(0.214615672427471), np.float64(0.6794825544991614), np.float64(0.5630879678136274), np.float64(0.3100743727437863), np.float64(0.3455974464646159), np.float64(0.36690810775544), np.float64(0.4949622962006173), np.float64(0.28940694040619747), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18.332100\n",
      "         Iterations: 19\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 54\n",
      "\n",
      "Current energy: -18.332099531903676\n",
      "(change of -0.5699237227024021)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.7665731035594869\n",
      "Operator 2: -0.7595578093224401\n",
      "Operator 3: 0.854684308933099\n",
      "Operator 4: -0.605938998784916\n",
      "Operator 5: 1.865284487561767\n",
      "Operator 7: 1.0525086430129185\n",
      "Operator 9: 0.6584238829952687\n",
      "Operator 11: 0.5785777114018918\n",
      "Operator 12: -1.8113625242015456\n",
      "Operator 13: 0.4890300079019183\n",
      "Operator 14: -1.2603462452758234\n",
      "Operator 16: -1.8652844875617638\n",
      "Operator 18: 1.0589279029181042\n",
      "Operator 19: -1.050516643283643\n",
      "Operator 20: -0.3193948366782645\n",
      "Operator 21: -1.74390100073428\n",
      "Operator 22: 0.26055572644634173\n",
      "Operator 23: -2.216619168833539\n",
      "Operator 24: 1.2419038156226692\n",
      "Operator 25: -2.1224532965528935\n",
      "Operator 26: 1.72471436293797\n",
      "Operator 27: -1.477356722511658\n",
      "Operator 28: 0.6438007104249983\n",
      "Operator 29: -1.0434854403450073\n",
      "Operator 30: 0.07267976003994224\n",
      "Operator 31: -1.9866812425607638\n",
      "Operator 32: 1.0989984991486792\n",
      "Operator 33: -2.4090556143614417\n",
      "Operator 34: 1.8151084512397504\n",
      "Operator 35: -2.1224532965528935\n",
      "Operator 36: 1.2433430195142896\n",
      "Operator 37: 0.1485533901247656\n",
      "Operator 38: 0.6174237272684532\n",
      "Operator 39: -0.8375938590523702\n",
      "Operator 40: -0.9219298914976494\n",
      "Operator 41: -0.6380573746908015\n",
      "Operator 42: 1.5245532898353606\n",
      "Operator 44: 1.8431043248908479\n",
      "Operator 45: -1.0525086430129218\n",
      "Operator 47: 0.5973742732526331\n",
      "Operator 48: -1.2637016808607342\n",
      "Operator 50: -0.717072555221605\n",
      "Operator 51: 1.1821203340951196\n",
      "Operator 52: -0.7005467161499394\n",
      "Operator 53: 0.6059389987849157\n",
      "Operator 54: -0.41829088536107206\n",
      "Operator 56: 1.1921998134707734\n",
      "Operator 57: -0.2944998349536221\n",
      "Operator 58: 1.1128529718612765\n",
      "Operator 59: -0.6462626950122758\n",
      "Operator 60: -0.23332932247678168\n",
      "Operator 61: -1.0622108092220182\n",
      "Operator 62: 2.216619168833539\n",
      "Operator 63: -1.2419038156226692\n",
      "Operator 64: 2.1224532965528944\n",
      "Operator 66: 1.8304499520548405\n",
      "Operator 67: -1.3093433703149921\n",
      "Operator 68: -0.6736292792155812\n",
      "Operator 69: 0.14027658997530748\n",
      "Operator 70: 1.0878873721189017\n",
      "Operator 71: 1.0042663807157828\n",
      "Operator 72: 0.4188283400504031\n",
      "Operator 73: -1.7220331836945704\n",
      "Operator 74: 0.18848304298725627\n",
      "Operator 75: -1.9696209560437397\n",
      "Operator 76: -1.72471436293797\n",
      "Operator 77: 0.9959687635639082\n",
      "Operator 78: 0.3797279096152948\n",
      "Operator 79: 0.9945453330257796\n",
      "Operator 80: -1.264158303930987\n",
      "Operator 82: -1.4266251348502794\n",
      "Operator 83: 2.4090556143614412\n",
      "Operator 84: -1.8151084512397508\n",
      "Operator 85: 2.1224532965528944\n",
      "Operator 86: 0.9745918910624599\n",
      "Operator 87: 0.09759680898903322\n",
      "Operator 88: 0.8436517443183347\n",
      "Operator 89: -0.9244774362717012\n",
      "Operator 90: -1.026518265802669\n",
      "Operator 91: -0.16402892761672308\n",
      "Operator 92: 1.4027712876412965\n",
      "Operator 94: 1.843104324890848\n",
      "Operator 95: -0.9868287940278027\n",
      "Operator 96: 0.8667776330754395\n",
      "Operator 97: -0.6494733475773878\n",
      "Operator 98: -0.10120116498297496\n",
      "Operator 99: -1.026063438796552\n",
      "Operator 101: -1.5245532898353626\n",
      "Operator 103: -1.8431043248908474\n",
      "Operator 105: 0.8691622434561923\n",
      "Operator 106: -1.1742619592664738\n",
      "Operator 107: 0.7264629679614818\n",
      "Operator 111: 0.41829088536107173\n",
      "Operator 113: -1.6358454506837843\n",
      "Operator 114: -0.2245155031713349\n",
      "Operator 115: 0.2494015956748394\n",
      "Operator 116: 1.930564205694715\n",
      "Operator 117: 0.897403939780482\n",
      "Operator 118: 1.6292035495294002\n",
      "Operator 119: -1.8715320013582581\n",
      "Operator 120: 1.8715320013582593\n",
      "Operator 121: -1.9696209560437399\n",
      "Operator 122: 0.4374022410115286\n",
      "Operator 123: -1.653859932651673\n",
      "Operator 124: 1.6387246324531144\n",
      "Operator 125: 1.0592605115158027\n",
      "Operator 126: 2.100075127573376\n",
      "Operator 127: -1.4990620375427661\n",
      "Operator 128: 1.8715320013582575\n",
      "Operator 129: -1.8715320013582604\n",
      "Operator 130: 1.9696209560437392\n",
      "Operator 131: 0.5935806322639409\n",
      "Operator 132: 0.07556455197624605\n",
      "Operator 133: 0.032492950760359865\n",
      "Operator 134: -0.49003959466418334\n",
      "Operator 135: 0.23639678290468707\n",
      "Operator 136: -1.251241498581753\n",
      "Operator 137: 1.1426999805010318\n",
      "Operator 138: -0.6104767419194643\n",
      "Operator 139: 1.7247143629379718\n",
      "Operator 140: -0.8953292079494142\n",
      "Operator 141: 0.6586336621353592\n",
      "Operator 142: -0.838557008614175\n",
      "Operator 143: -0.24013859996415002\n",
      "Operator 144: -0.9309259838416166\n",
      "Operator 146: -1.4027712876412985\n",
      "Operator 148: -1.8431043248908474\n",
      "Operator 149: -1.7333759255510097\n",
      "Total gradient norm: 14.493874914003792\n",
      "Operators under consideration (1):\n",
      "[33]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.4090556143614417)]\n",
      "Operator(s) added to ansatz: [33]\n",
      "Gradients: [np.float64(-2.4090556143614417)]\n",
      "Initial energy: -18.332099531903676\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33]...\n",
      "Starting point: [np.float64(0.1699779541289512), np.float64(0.6981395474995092), np.float64(0.6052125714399629), np.float64(0.2012053465913035), np.float64(0.21701406657269834), np.float64(0.4760211463799883), np.float64(0.5356598410749226), np.float64(0.378736652411002), np.float64(0.3769384380862807), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18.559242\n",
      "         Iterations: 17\n",
      "         Function evaluations: 23\n",
      "         Gradient evaluations: 23\n",
      "\n",
      "Current energy: -18.559242451391825\n",
      "(change of -0.22714291948814846)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8257641547595296\n",
      "Operator 2: -0.7915617799860648\n",
      "Operator 3: 0.8931574881124155\n",
      "Operator 4: -1.1009182250306755\n",
      "Operator 5: 1.2820529550443034\n",
      "Operator 7: 1.6199010447693454\n",
      "Operator 9: 0.686040677721081\n",
      "Operator 11: 0.5927534112842013\n",
      "Operator 12: -1.7555057679091508\n",
      "Operator 13: 0.651651864267157\n",
      "Operator 14: -1.1232972769567424\n",
      "Operator 16: -0.8629740637848804\n",
      "Operator 17: 0.5776039696617972\n",
      "Operator 18: 1.0031937900053398\n",
      "Operator 19: -0.8755477665318818\n",
      "Operator 20: -0.5795355526647279\n",
      "Operator 21: -1.5465836379147784\n",
      "Operator 22: -0.0972793417566318\n",
      "Operator 23: 0.1559067542435424\n",
      "Operator 24: 1.1077816196368966\n",
      "Operator 25: -2.1188741173712096\n",
      "Operator 26: 1.3532852387151095\n",
      "Operator 27: -1.4134804286806997\n",
      "Operator 28: 0.5140598769111502\n",
      "Operator 29: -0.8203696750773386\n",
      "Operator 30: -0.5002416108127385\n",
      "Operator 31: -1.6445395678539145\n",
      "Operator 32: 0.7900212277457463\n",
      "Operator 34: 1.5586997112061778\n",
      "Operator 35: -2.2880046148093665\n",
      "Operator 36: 1.1948224410449726\n",
      "Operator 37: 0.18497812276819758\n",
      "Operator 38: 0.48927560997377784\n",
      "Operator 39: -0.9335785091296559\n",
      "Operator 40: -1.0715219234970423\n",
      "Operator 41: 0.19640942862732236\n",
      "Operator 42: 1.0749029969028676\n",
      "Operator 44: 1.4963810219716882\n",
      "Operator 45: -1.6199010447693483\n",
      "Operator 47: 0.6112087239499103\n",
      "Operator 48: -1.3441018435612897\n",
      "Operator 50: -0.7828560178013066\n",
      "Operator 51: 1.2523437033003\n",
      "Operator 52: -1.013381589023103\n",
      "Operator 53: 0.736812118454458\n",
      "Operator 56: 1.1298036981464996\n",
      "Operator 57: -0.17817069102629035\n",
      "Operator 58: 1.0469907819397395\n",
      "Operator 59: -0.5464963192369372\n",
      "Operator 60: -0.3043057094728142\n",
      "Operator 61: -0.35690508291052914\n",
      "Operator 62: 1.7376038339023885\n",
      "Operator 63: -0.3283934938335845\n",
      "Operator 64: 2.1188741173712096\n",
      "Operator 66: 1.843925236234461\n",
      "Operator 67: -1.2389225903837175\n",
      "Operator 68: -0.7190184464722573\n",
      "Operator 69: 0.30562900846645435\n",
      "Operator 70: 1.205226622971805\n",
      "Operator 71: 1.2394973333893877\n",
      "Operator 72: -0.19659160602707385\n",
      "Operator 73: -1.8971535608316814\n",
      "Operator 74: 0.29475425770817426\n",
      "Operator 75: -1.786680012479171\n",
      "Operator 76: -1.3532852387151095\n",
      "Operator 77: 0.8991718627225108\n",
      "Operator 78: 0.6185393246102122\n",
      "Operator 79: 0.8444237952204002\n",
      "Operator 80: -0.870186667061286\n",
      "Operator 81: -0.12942759628022965\n",
      "Operator 82: -0.6434874057799214\n",
      "Operator 83: 2.0197097880409354\n",
      "Operator 84: -1.2912554536172407\n",
      "Operator 85: 2.288004614809367\n",
      "Operator 86: 0.9024286849840581\n",
      "Operator 87: 0.11647865750850028\n",
      "Operator 88: 0.7160104043481961\n",
      "Operator 89: -1.0294855377924919\n",
      "Operator 90: -1.1291304688772292\n",
      "Operator 91: -0.001135712710156952\n",
      "Operator 92: 0.9574526899303926\n",
      "Operator 94: 1.6158235431333885\n",
      "Operator 95: -0.9260321697676601\n",
      "Operator 96: 0.9351711903424207\n",
      "Operator 97: -0.5218561230052577\n",
      "Operator 98: -0.17445997326604223\n",
      "Operator 99: -0.7690307630146085\n",
      "Operator 100: -0.2208969897665693\n",
      "Operator 101: 0.9081473441121368\n",
      "Operator 102: 0.12009680235018208\n",
      "Operator 103: -1.4963810219716878\n",
      "Operator 105: 0.8837803364887862\n",
      "Operator 106: -1.1933393200685005\n",
      "Operator 107: 0.8682136927960915\n",
      "Operator 110: -0.4699176413041949\n",
      "Operator 111: 0.5322834450655137\n",
      "Operator 112: -0.5776039696617972\n",
      "Operator 113: -1.5996300671638926\n",
      "Operator 114: -0.46756302728735855\n",
      "Operator 115: 0.46988679065042716\n",
      "Operator 116: 1.790929740581252\n",
      "Operator 117: 1.1687766576584955\n",
      "Operator 118: -0.8979136334034794\n",
      "Operator 119: -2.129876566821465\n",
      "Operator 120: 1.551220973814314\n",
      "Operator 121: -1.92929446833383\n",
      "Operator 122: 0.33557603497669464\n",
      "Operator 123: -1.6199077663134074\n",
      "Operator 124: 1.552812724194252\n",
      "Operator 125: 1.303474824118894\n",
      "Operator 126: 1.6034282119404417\n",
      "Operator 127: -1.6020322613845028\n",
      "Operator 128: -0.8364230866612149\n",
      "Operator 129: -1.675040870799613\n",
      "Operator 130: 1.9292944683338311\n",
      "Operator 131: 0.4870213433368896\n",
      "Operator 132: 0.08979187184554863\n",
      "Operator 133: -0.09957827358693357\n",
      "Operator 134: -0.4863137737145387\n",
      "Operator 135: -0.3719714062486139\n",
      "Operator 136: -0.8266579649769519\n",
      "Operator 137: 0.9867387984151637\n",
      "Operator 138: -1.0816062186482385\n",
      "Operator 139: 1.2532496862873934\n",
      "Operator 140: -0.8250224209742462\n",
      "Operator 141: 0.6960945530598045\n",
      "Operator 142: -0.7476137698199529\n",
      "Operator 143: -0.32294935057556784\n",
      "Operator 144: -0.6345000343809728\n",
      "Operator 145: -0.03803046029160456\n",
      "Operator 146: 0.8993984544740414\n",
      "Operator 147: 0.6320406705645569\n",
      "Operator 148: -1.6158235431333878\n",
      "Operator 149: -1.7182677310679675\n",
      "Total gradient norm: 12.929076635962883\n",
      "Operators under consideration (1):\n",
      "[85]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.288004614809367)]\n",
      "Operator(s) added to ansatz: [85]\n",
      "Gradients: [np.float64(2.288004614809367)]\n",
      "Initial energy: -18.559242451391825\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85]...\n",
      "Starting point: [np.float64(0.17743468405782709), np.float64(0.652050768976599), np.float64(0.49636468040929876), np.float64(0.2359193705799676), np.float64(0.23571253628398284), np.float64(0.38277937185641636), np.float64(0.5278783763270049), np.float64(0.36193083572944784), np.float64(0.33626956042635586), np.float64(0.1934549304244466), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18.752519\n",
      "         Iterations: 18\n",
      "         Function evaluations: 28\n",
      "         Gradient evaluations: 28\n",
      "\n",
      "Current energy: -18.752518894658877\n",
      "(change of -0.1932764432670524)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8455613359262691\n",
      "Operator 2: -0.7998616156078302\n",
      "Operator 3: 0.8959106586234216\n",
      "Operator 4: -1.2408112355901448\n",
      "Operator 5: 1.1936105208417904\n",
      "Operator 7: 0.7726115671757052\n",
      "Operator 8: -0.48161104865860693\n",
      "Operator 9: 0.6946954492430111\n",
      "Operator 11: 0.5968624158098239\n",
      "Operator 12: -1.7324596304580313\n",
      "Operator 13: 0.7103568025010443\n",
      "Operator 14: -1.0475696753039585\n",
      "Operator 16: -0.9167991249022094\n",
      "Operator 17: 0.9606235835785801\n",
      "Operator 18: 0.986221911635766\n",
      "Operator 19: -0.8174250836152945\n",
      "Operator 20: -0.6647086974467402\n",
      "Operator 21: -1.4541486021297436\n",
      "Operator 22: -0.21320553403514084\n",
      "Operator 23: 0.1946804280204154\n",
      "Operator 24: 0.5541488555614776\n",
      "Operator 25: -1.8177360858592797\n",
      "Operator 26: 0.6271014046664298\n",
      "Operator 27: -1.3916912623049646\n",
      "Operator 28: 0.4752529717513937\n",
      "Operator 29: -0.740449577422091\n",
      "Operator 30: -0.6959115147943418\n",
      "Operator 31: -1.5088650032058895\n",
      "Operator 32: 0.3716820667451887\n",
      "Operator 33: 0.0976130551723958\n",
      "Operator 34: 1.010236093881673\n",
      "Operator 35: -2.047650033314942\n",
      "Operator 36: 1.1796232041409986\n",
      "Operator 37: 0.19755388397705248\n",
      "Operator 38: 0.44996943325374206\n",
      "Operator 39: -0.9598763247203511\n",
      "Operator 40: -1.1001386391575554\n",
      "Operator 41: 0.29382158642495243\n",
      "Operator 42: 0.8135406562744228\n",
      "Operator 43: 0.006924979437960391\n",
      "Operator 44: -0.7316746704867311\n",
      "Operator 45: -1.2933980833316134\n",
      "Operator 47: 0.6149197836214639\n",
      "Operator 48: -1.3676907688997202\n",
      "Operator 50: -0.7944434818514647\n",
      "Operator 51: 1.2756184597121305\n",
      "Operator 52: -1.1916757720613678\n",
      "Operator 53: 0.7339623884142198\n",
      "Operator 55: 0.350910022480574\n",
      "Operator 56: 1.1103803995701664\n",
      "Operator 57: -0.1415262372835242\n",
      "Operator 58: 1.0264123705366752\n",
      "Operator 59: -0.5163153838748501\n",
      "Operator 60: -0.33146488177955513\n",
      "Operator 61: -0.13630314189330422\n",
      "Operator 62: 1.6050301832423681\n",
      "Operator 63: -0.030450209973226672\n",
      "Operator 64: -0.12307426550005618\n",
      "Operator 66: 1.8470085680757307\n",
      "Operator 67: -1.2158179692974922\n",
      "Operator 68: -0.7333904439698833\n",
      "Operator 69: 0.35249604636599996\n",
      "Operator 70: 1.2342171944969784\n",
      "Operator 71: 1.2936221629140425\n",
      "Operator 72: -0.36652825050855103\n",
      "Operator 73: -1.7452848508906829\n",
      "Operator 74: 0.6875150821287517\n",
      "Operator 75: 0.650025190001496\n",
      "Operator 76: -1.348842712472993\n",
      "Operator 77: 0.8677000011231554\n",
      "Operator 78: 0.6919171794146723\n",
      "Operator 79: 0.7877574916995146\n",
      "Operator 80: -0.7331502076913137\n",
      "Operator 81: -0.17380987426708283\n",
      "Operator 82: -0.23723204176508092\n",
      "Operator 83: 1.7760602260832985\n",
      "Operator 84: -1.2024104259536066\n",
      "Operator 86: 0.8795040767827429\n",
      "Operator 87: 0.12293655784090556\n",
      "Operator 88: 0.6763177372001932\n",
      "Operator 89: -1.0588435845971644\n",
      "Operator 90: -1.1326368971861516\n",
      "Operator 91: 0.05276169483861897\n",
      "Operator 92: 0.7594759111587921\n",
      "Operator 93: 0.15950486958867482\n",
      "Operator 94: -0.7168745700424682\n",
      "Operator 95: -0.9074963377232146\n",
      "Operator 96: 0.9554896380869895\n",
      "Operator 97: -0.4803486303971797\n",
      "Operator 98: -0.205729850200471\n",
      "Operator 99: -0.6792728406728573\n",
      "Operator 100: -0.2975754058931949\n",
      "Operator 101: 1.044098498085273\n",
      "Operator 102: 0.030261737332407057\n",
      "Operator 103: -1.1518280592379004\n",
      "Operator 105: 0.8862025897460367\n",
      "Operator 106: -1.1950666185219627\n",
      "Operator 107: 0.9123962466866377\n",
      "Operator 110: -0.5734786191101713\n",
      "Operator 111: 0.8110133442007209\n",
      "Operator 112: -0.6835283799669729\n",
      "Operator 113: -1.5878698482187885\n",
      "Operator 114: -0.5438832853066884\n",
      "Operator 115: 0.5388678304544181\n",
      "Operator 116: 1.7350744311473174\n",
      "Operator 117: 1.2522044734491984\n",
      "Operator 118: -1.0958888872384704\n",
      "Operator 119: -1.7547319337754712\n",
      "Operator 120: 1.7604907109059682\n",
      "Operator 121: 0.5950552206453373\n",
      "Operator 122: 0.3039345512547936\n",
      "Operator 123: -1.608631262586069\n",
      "Operator 124: 1.5216902033503423\n",
      "Operator 125: 1.3819302416220354\n",
      "Operator 126: 1.4256765479127393\n",
      "Operator 127: -1.4277426060893093\n",
      "Operator 128: -1.0693976593526946\n",
      "Operator 129: 0.6922098851151807\n",
      "Operator 130: 2.242867457419841\n",
      "Operator 131: 0.4530145780403023\n",
      "Operator 132: 0.09466885949268994\n",
      "Operator 133: -0.13822728983320234\n",
      "Operator 134: -0.47472732972937437\n",
      "Operator 135: -0.5489091950373609\n",
      "Operator 136: -0.7332116946051548\n",
      "Operator 137: 0.7685821203872589\n",
      "Operator 138: -0.47299554879895844\n",
      "Operator 139: 0.567201597303832\n",
      "Operator 140: -0.8032835859772884\n",
      "Operator 141: 0.7076324874093123\n",
      "Operator 142: -0.7186046742331779\n",
      "Operator 143: -0.3548002314877261\n",
      "Operator 144: -0.5438359762079366\n",
      "Operator 145: -0.08420996617272244\n",
      "Operator 146: 1.0242005732079915\n",
      "Operator 147: -0.10084687249341545\n",
      "Operator 148: -1.2734678416206524\n",
      "Operator 149: -1.7136680032660563\n",
      "Total gradient norm: 11.403313429068094\n",
      "Operators under consideration (1):\n",
      "[130]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.242867457419841)]\n",
      "Operator(s) added to ansatz: [130]\n",
      "Gradients: [np.float64(2.242867457419841)]\n",
      "Initial energy: -18.752518894658877\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130]...\n",
      "Starting point: [np.float64(0.1797748112054255), np.float64(0.5697994072454183), np.float64(0.42011576260996686), np.float64(0.2490329363393999), np.float64(0.24197438928520587), np.float64(0.34832574970250196), np.float64(0.525526621885844), np.float64(0.3568091758438257), np.float64(0.3233245883071755), np.float64(0.22031696554674238), np.float64(-0.172542835653139), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -18.934856\n",
      "         Iterations: 18\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 54\n",
      "\n",
      "Current energy: -18.93485595064815\n",
      "(change of -0.1823370559892723)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.8407673340078962\n",
      "Operator 1: 7.708783157945747e-08\n",
      "Operator 2: -0.7979601946210018\n",
      "Operator 3: 0.8956236943342597\n",
      "Operator 4: -1.2136952536729382\n",
      "Operator 5: 1.223418312578524\n",
      "Operator 6: 0.004870901530149871\n",
      "Operator 7: 0.5497507374365225\n",
      "Operator 8: 0.18925209789706365\n",
      "Operator 9: 0.692611476017384\n",
      "Operator 10: -8.047032240554586e-08\n",
      "Operator 11: 0.5958877628822584\n",
      "Operator 12: -1.7382456224655225\n",
      "Operator 13: 0.6961961550256497\n",
      "Operator 14: -1.0686772359425922\n",
      "Operator 15: 2.964051371171827e-08\n",
      "Operator 16: -0.9062403242334601\n",
      "Operator 17: 0.8682804007596763\n",
      "Operator 18: 0.9902895755724976\n",
      "Operator 19: -0.8315888183336818\n",
      "Operator 20: -0.6442834499426466\n",
      "Operator 21: -1.478798092701075\n",
      "Operator 22: -0.18546530701934555\n",
      "Operator 23: 0.1887008318264791\n",
      "Operator 24: 0.6733556611007175\n",
      "Operator 25: 0.46692721485529076\n",
      "Operator 26: 1.2282029770356604\n",
      "Operator 27: -1.3970031199710873\n",
      "Operator 28: 0.48456088298604894\n",
      "Operator 29: -0.7598681899437743\n",
      "Operator 30: -0.6472458875184766\n",
      "Operator 31: -1.5338831289509849\n",
      "Operator 32: 0.5160207433186457\n",
      "Operator 33: 0.2530267214411419\n",
      "Operator 34: 1.2390282297497137\n",
      "Operator 35: 0.2826547352908629\n",
      "Operator 36: 1.1832829276000103\n",
      "Operator 37: 0.1944728890763392\n",
      "Operator 38: 0.4593313633474392\n",
      "Operator 39: -0.9540142679813591\n",
      "Operator 40: -1.0947063876101029\n",
      "Operator 41: 0.26264764381529476\n",
      "Operator 42: 0.8586528627951359\n",
      "Operator 43: 0.1186555492962204\n",
      "Operator 44: -0.968412036056105\n",
      "Operator 45: 0.012266055489528176\n",
      "Operator 46: 5.5914408699411825e-08\n",
      "Operator 47: 0.6140511121477505\n",
      "Operator 48: -1.3620404906717658\n",
      "Operator 49: -2.7295055937681667e-08\n",
      "Operator 50: -0.7916809921142072\n",
      "Operator 51: 1.2652956132213888\n",
      "Operator 52: -1.130710198229236\n",
      "Operator 53: 0.6776113019946931\n",
      "Operator 54: -0.14619607247172173\n",
      "Operator 55: 0.2783006948987638\n",
      "Operator 56: 1.1150531158920036\n",
      "Operator 57: -0.15034336311137275\n",
      "Operator 58: 1.0313730032243522\n",
      "Operator 59: -0.5235640684504311\n",
      "Operator 60: -0.32489833249073374\n",
      "Operator 61: -0.18045056305546187\n",
      "Operator 62: 1.6424274797400358\n",
      "Operator 63: -0.08555084531959378\n",
      "Operator 64: -0.5063989295234747\n",
      "Operator 65: -1.0409737871697189e-07\n",
      "Operator 66: 1.846312397488428\n",
      "Operator 67: -1.2214117690708963\n",
      "Operator 68: -0.7299270771708593\n",
      "Operator 69: 0.3413604109490425\n",
      "Operator 70: 1.2275555680860366\n",
      "Operator 71: 1.2808890287257686\n",
      "Operator 72: -0.31605876289087126\n",
      "Operator 73: -1.7276536253732144\n",
      "Operator 74: 0.6899202939795661\n",
      "Operator 75: 0.09526631234810803\n",
      "Operator 76: -1.201774050563206\n",
      "Operator 77: 0.8753187750435113\n",
      "Operator 78: 0.6742680430998977\n",
      "Operator 79: 0.8014386880166712\n",
      "Operator 80: -0.7679897680459238\n",
      "Operator 81: -0.16904556812814997\n",
      "Operator 82: -0.35473963773976663\n",
      "Operator 83: 1.6378086305466757\n",
      "Operator 84: -1.087217556060488\n",
      "Operator 85: -0.4114865036361979\n",
      "Operator 86: 0.8850257282040275\n",
      "Operator 87: 0.12136018426572062\n",
      "Operator 88: 0.6858916760776996\n",
      "Operator 89: -1.0521377348221488\n",
      "Operator 90: -1.1319568060469365\n",
      "Operator 91: 0.034414865140731964\n",
      "Operator 92: 0.7735171538236578\n",
      "Operator 93: 0.17450882334877357\n",
      "Operator 94: -1.0042119948766648\n",
      "Operator 95: -0.9119392241581643\n",
      "Operator 96: 0.9506467804095988\n",
      "Operator 97: -0.4902614170785002\n",
      "Operator 98: -0.19777510632875955\n",
      "Operator 99: -0.7017620897332439\n",
      "Operator 100: -0.28403190188516525\n",
      "Operator 101: 1.004034198751742\n",
      "Operator 102: 0.5094822946229609\n",
      "Operator 103: 0.7465658426823968\n",
      "Operator 104: -9.228999520161096e-08\n",
      "Operator 105: 0.8856856250901677\n",
      "Operator 106: -1.1947083573589983\n",
      "Operator 107: 0.9012941734153129\n",
      "Operator 108: -1.8767484996166859e-07\n",
      "Operator 109: -1.107868476369387e-07\n",
      "Operator 110: -0.5147625668068346\n",
      "Operator 111: 0.7764237491255456\n",
      "Operator 112: -0.6478795027434914\n",
      "Operator 113: -1.590719471605379\n",
      "Operator 114: -0.5254603091114083\n",
      "Operator 115: 0.5224200056439512\n",
      "Operator 116: 1.7496528415845902\n",
      "Operator 117: 1.232124676444649\n",
      "Operator 118: -1.044843821397579\n",
      "Operator 119: -1.791507550944077\n",
      "Operator 120: -0.06246926472586136\n",
      "Operator 121: -0.003933301605426408\n",
      "Operator 122: 0.31153704638905105\n",
      "Operator 123: -1.6113760287529864\n",
      "Operator 124: 1.5293333216377196\n",
      "Operator 125: 1.363045514676926\n",
      "Operator 126: 1.4604555254605511\n",
      "Operator 127: -1.486522039551865\n",
      "Operator 128: -1.079600675673578\n",
      "Operator 129: 0.4673347830816336\n",
      "Operator 130: -3.6538593435416013e-07\n",
      "Operator 131: 0.461213304359949\n",
      "Operator 132: 0.09347806974144338\n",
      "Operator 133: -0.1290371685089368\n",
      "Operator 134: -0.47790527889586915\n",
      "Operator 135: -0.5122001703577007\n",
      "Operator 136: -0.7626683550093681\n",
      "Operator 137: 0.7992287046257261\n",
      "Operator 138: -0.31309781550361887\n",
      "Operator 139: 1.1128773213166239\n",
      "Operator 140: -0.8085013231738938\n",
      "Operator 141: 0.7048767715953949\n",
      "Operator 142: -0.7255600059375638\n",
      "Operator 143: -0.3470397766592943\n",
      "Operator 144: -0.563743943183737\n",
      "Operator 145: -0.07116904183846057\n",
      "Operator 146: 1.0294628673019164\n",
      "Operator 147: 0.01368200395097005\n",
      "Operator 148: 0.6096781909708174\n",
      "Operator 149: -1.7147703994179773\n",
      "Total gradient norm: 10.619041940472531\n",
      "Operators under consideration (1):\n",
      "[66]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.846312397488428)]\n",
      "Operator(s) added to ansatz: [66]\n",
      "Gradients: [np.float64(1.846312397488428)]\n",
      "Initial energy: -18.93485595064815\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66]...\n",
      "Starting point: [np.float64(0.17921114988774975), np.float64(0.6302460938893445), np.float64(0.44686596101233667), np.float64(0.24592047325473), np.float64(0.24045569863214605), np.float64(0.35723903858726247), np.float64(0.526089485628609), np.float64(0.3580370972528822), np.float64(0.32647170161986877), np.float64(0.2184093544722972), np.float64(-0.1911680731107306), np.float64(-0.16163900793807973), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.062091\n",
      "         Iterations: 20\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n",
      "\n",
      "Current energy: -19.062090549427182\n",
      "(change of -0.1272345987790331)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.0327093465827675\n",
      "Operator 2: -0.8307308592826925\n",
      "Operator 3: 0.8735370342694228\n",
      "Operator 4: -1.2318256301676804\n",
      "Operator 5: 1.2253413254352672\n",
      "Operator 6: 0.004759319685732066\n",
      "Operator 7: 0.5505780887151475\n",
      "Operator 8: 0.1894024908944818\n",
      "Operator 9: 0.18223644242924614\n",
      "Operator 11: 0.23180640009640535\n",
      "Operator 12: -1.7201137142121983\n",
      "Operator 13: 0.7038061586191204\n",
      "Operator 14: -1.0685964691744962\n",
      "Operator 16: -0.9011850960925917\n",
      "Operator 17: 0.8762663054235604\n",
      "Operator 18: 0.9075451944158739\n",
      "Operator 19: -0.6752717317515686\n",
      "Operator 20: -0.7701560740338634\n",
      "Operator 21: -1.4276475532020836\n",
      "Operator 22: -0.23051640335984042\n",
      "Operator 23: 0.1954930499306565\n",
      "Operator 24: 0.653734383420161\n",
      "Operator 25: 0.46565580263946155\n",
      "Operator 26: 1.2195846727779749\n",
      "Operator 27: -0.2705027908496407\n",
      "Operator 28: 0.22813123443330457\n",
      "Operator 29: -0.721174104858687\n",
      "Operator 30: -0.7027062585298536\n",
      "Operator 31: -1.5092021612514457\n",
      "Operator 32: 0.48451586750016973\n",
      "Operator 33: 0.25473637557803386\n",
      "Operator 34: 1.2301661358655207\n",
      "Operator 35: 0.2788210221983781\n",
      "Operator 36: 0.8620359401659086\n",
      "Operator 37: 0.1770401276019584\n",
      "Operator 38: 0.5234651645868301\n",
      "Operator 39: -0.9926039044038278\n",
      "Operator 40: -1.0964395144829497\n",
      "Operator 41: 0.2729761285216729\n",
      "Operator 42: 0.8469408257625525\n",
      "Operator 43: 0.12167904102050037\n",
      "Operator 44: -0.9699374147137649\n",
      "Operator 45: 0.011942381712069405\n",
      "Operator 47: 0.1596445968437788\n",
      "Operator 48: -0.4381404576269499\n",
      "Operator 49: 0.21183700560541013\n",
      "Operator 50: -0.7789922267997265\n",
      "Operator 51: 1.2953686279383632\n",
      "Operator 52: -1.1241362519555191\n",
      "Operator 53: 0.6933848631898929\n",
      "Operator 54: -0.1467921058226548\n",
      "Operator 55: 0.28008867908726187\n",
      "Operator 56: 0.508004963833014\n",
      "Operator 57: -0.34844374002602696\n",
      "Operator 58: 0.8423512842801198\n",
      "Operator 59: -0.4581981482272692\n",
      "Operator 60: -0.355445804155772\n",
      "Operator 61: -0.140122273318765\n",
      "Operator 62: 1.626627379373337\n",
      "Operator 63: -0.06396840983599361\n",
      "Operator 64: -0.5051683578595635\n",
      "Operator 65: -0.16485082962696812\n",
      "Operator 67: -0.496973019132344\n",
      "Operator 68: -0.8002554488344191\n",
      "Operator 69: 0.44297444448099543\n",
      "Operator 70: 1.2303780326121065\n",
      "Operator 71: 1.2993844099031788\n",
      "Operator 72: -0.32788609576818956\n",
      "Operator 73: -1.7148957793333395\n",
      "Operator 74: 0.6875720987714616\n",
      "Operator 75: 0.09650561258164592\n",
      "Operator 76: -1.193016662443613\n",
      "Operator 77: 0.2805330522496124\n",
      "Operator 78: 0.49188965812324253\n",
      "Operator 79: 0.7240661990467198\n",
      "Operator 80: -0.7422014713058187\n",
      "Operator 81: -0.1736759014945464\n",
      "Operator 82: -0.33738891303198004\n",
      "Operator 83: 1.6313966316781436\n",
      "Operator 84: -1.0755822512175632\n",
      "Operator 85: -0.40859062167026394\n",
      "Operator 86: 0.6342217526530711\n",
      "Operator 87: 0.35374921287535704\n",
      "Operator 88: 0.5428571112297224\n",
      "Operator 89: -1.0591233547906909\n",
      "Operator 90: -1.1367994471773386\n",
      "Operator 91: 0.03938145432301356\n",
      "Operator 92: 0.759929233527165\n",
      "Operator 93: 0.17793834840139988\n",
      "Operator 94: -1.0067117363273903\n",
      "Operator 95: -0.901107630230336\n",
      "Operator 96: 0.6427939472249092\n",
      "Operator 97: -0.4442216734558646\n",
      "Operator 98: -0.22151681969546325\n",
      "Operator 99: -0.6780834750811068\n",
      "Operator 100: -0.29279743659454815\n",
      "Operator 101: 1.0099415978850315\n",
      "Operator 102: 0.5056049141981913\n",
      "Operator 103: 0.7469204622779749\n",
      "Operator 104: -0.14441427787078204\n",
      "Operator 105: 0.4083950101146772\n",
      "Operator 106: -0.9397951368896049\n",
      "Operator 107: 0.8671595178532504\n",
      "Operator 110: -0.5194967983442976\n",
      "Operator 111: 0.7785829055666206\n",
      "Operator 112: -0.6554195921016435\n",
      "Operator 113: -0.7849903388617593\n",
      "Operator 114: -0.6003110482863918\n",
      "Operator 115: 0.6346392434893736\n",
      "Operator 116: 1.7224472422047143\n",
      "Operator 117: 1.2510253778841194\n",
      "Operator 118: -1.0550338381058997\n",
      "Operator 119: -1.7826678554476265\n",
      "Operator 120: -0.06439672982581168\n",
      "Operator 121: -0.0038503621247802933\n",
      "Operator 122: 0.3885564972990459\n",
      "Operator 123: -1.3754804692110278\n",
      "Operator 124: 1.5090415755871855\n",
      "Operator 125: 1.3898418251790288\n",
      "Operator 126: 1.4379244399739923\n",
      "Operator 127: -1.4635431901761988\n",
      "Operator 128: -1.0896228959050855\n",
      "Operator 129: 0.4719438730033838\n",
      "Operator 131: 0.3843944302599095\n",
      "Operator 132: 0.2565878268176912\n",
      "Operator 133: -0.19612459362062268\n",
      "Operator 134: -0.4595226146247044\n",
      "Operator 135: -0.5478456403344594\n",
      "Operator 136: -0.7656543982625379\n",
      "Operator 137: 0.7808395979685421\n",
      "Operator 138: -0.3139611552313281\n",
      "Operator 139: 1.1035574164689193\n",
      "Operator 140: -0.5874142785113996\n",
      "Operator 141: 0.07297466858693699\n",
      "Operator 142: -0.6321156626971034\n",
      "Operator 143: -0.3715508768831418\n",
      "Operator 144: -0.5375082477748558\n",
      "Operator 145: -0.07833462199341158\n",
      "Operator 146: 1.0329406121682325\n",
      "Operator 147: 0.011166732319565276\n",
      "Operator 148: 0.6085789200665153\n",
      "Operator 149: -0.5450055015805884\n",
      "Total gradient norm: 9.602570139425884\n",
      "Operators under consideration (1):\n",
      "[119]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.7826678554476265)]\n",
      "Operator(s) added to ansatz: [119]\n",
      "Gradients: [np.float64(-1.7826678554476265)]\n",
      "Initial energy: -19.062090549427182\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119]...\n",
      "Starting point: [np.float64(0.14356892596535537), np.float64(0.6294538478109837), np.float64(0.4448172231499766), np.float64(0.24985949100575278), np.float64(0.25161438653686424), np.float64(0.35345687037841444), np.float64(0.5051368582368844), np.float64(0.33478747208939014), np.float64(0.3184700739197432), np.float64(0.2198689111416589), np.float64(-0.19150347910354684), np.float64(-0.16163587187198827), np.float64(-0.13921660177503137), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.178768\n",
      "         Iterations: 20\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 61\n",
      "\n",
      "Current energy: -19.178767570322606\n",
      "(change of -0.11667702089542331)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.0202150443870832\n",
      "Operator 1: -7.26920799912199e-08\n",
      "Operator 2: -0.8250808490875886\n",
      "Operator 3: 0.8695678733829799\n",
      "Operator 4: -1.0899974650937212\n",
      "Operator 5: 0.24636303673256157\n",
      "Operator 6: 0.010222776436472847\n",
      "Operator 7: 0.7323817735954049\n",
      "Operator 8: 0.2525854674053054\n",
      "Operator 9: 0.18400455235970875\n",
      "Operator 10: -6.994203353910907e-08\n",
      "Operator 11: 0.23333191185816474\n",
      "Operator 12: -1.7405717853427858\n",
      "Operator 13: 0.6596404244449587\n",
      "Operator 14: -1.1392992137204145\n",
      "Operator 15: -0.02166052036214449\n",
      "Operator 16: -0.8177304227849012\n",
      "Operator 17: 0.07887871344659303\n",
      "Operator 18: 0.9175176290882905\n",
      "Operator 19: -0.7243644116688515\n",
      "Operator 20: -0.7078663227927109\n",
      "Operator 21: -1.5096511010139095\n",
      "Operator 22: -0.12573691183349792\n",
      "Operator 23: 0.33996544897848924\n",
      "Operator 24: 0.6386827339965615\n",
      "Operator 25: 0.492169950276622\n",
      "Operator 26: 1.5489555315804626\n",
      "Operator 27: -0.289845475716382\n",
      "Operator 28: 0.2606181132835331\n",
      "Operator 29: -0.7832834346774374\n",
      "Operator 30: -0.517058831062295\n",
      "Operator 31: -1.5033786261377484\n",
      "Operator 32: 0.6477242379903201\n",
      "Operator 33: 0.378145904239221\n",
      "Operator 34: 1.3813532686219319\n",
      "Operator 35: 0.28059979716390704\n",
      "Operator 36: 0.8715733292800258\n",
      "Operator 37: 0.162587158739329\n",
      "Operator 38: 0.5545046396914384\n",
      "Operator 39: -0.9777737144896517\n",
      "Operator 40: -1.092522724194731\n",
      "Operator 41: 0.13000012938326277\n",
      "Operator 42: -0.4835046866558431\n",
      "Operator 43: 0.08494377003456315\n",
      "Operator 44: -0.6434090020558718\n",
      "Operator 45: -0.10311150768371902\n",
      "Operator 46: 3.0453058741386485e-08\n",
      "Operator 47: 0.16205735772594013\n",
      "Operator 48: -0.4379344928035698\n",
      "Operator 49: 0.21442920471685759\n",
      "Operator 50: -0.7598079989261992\n",
      "Operator 51: 1.219400737082664\n",
      "Operator 52: -1.0266530465005093\n",
      "Operator 53: 0.6653889699840302\n",
      "Operator 54: -0.1012841452631667\n",
      "Operator 55: 0.2850460477575768\n",
      "Operator 56: 0.5186706984550357\n",
      "Operator 57: -0.37115326182157604\n",
      "Operator 58: 0.8577995054456218\n",
      "Operator 59: -0.48026967917072666\n",
      "Operator 60: -0.3362370511450632\n",
      "Operator 61: -0.34067275681586473\n",
      "Operator 62: 0.12155078143206433\n",
      "Operator 63: -0.7656939099412612\n",
      "Operator 64: -0.4825400072521046\n",
      "Operator 65: -0.1673390224477299\n",
      "Operator 66: 7.683465203483197e-08\n",
      "Operator 67: -0.512264849073589\n",
      "Operator 68: -0.7914520979236912\n",
      "Operator 69: 0.409104475984206\n",
      "Operator 70: 1.203204401381316\n",
      "Operator 71: 1.235843664643089\n",
      "Operator 72: -0.07679336034354388\n",
      "Operator 73: -0.1551158328048068\n",
      "Operator 74: 0.7779487421953548\n",
      "Operator 75: 0.01637221695177349\n",
      "Operator 76: -1.4437338136375777\n",
      "Operator 77: 0.29962796690664983\n",
      "Operator 78: 0.43788649202119967\n",
      "Operator 79: 0.7604039313395533\n",
      "Operator 80: -0.8850304672689131\n",
      "Operator 81: -0.251420761628428\n",
      "Operator 82: -0.7070603409654013\n",
      "Operator 83: 0.05241930847031512\n",
      "Operator 84: -1.537462626845464\n",
      "Operator 85: -0.17654946908635644\n",
      "Operator 86: 0.6476092539145886\n",
      "Operator 87: 0.3479327045541267\n",
      "Operator 88: 0.5727921679447187\n",
      "Operator 89: -1.0369453500918286\n",
      "Operator 90: -1.1277531292798548\n",
      "Operator 91: -0.2695985100769733\n",
      "Operator 92: -0.43151534535136415\n",
      "Operator 93: 0.1316069314917786\n",
      "Operator 94: -0.9563598360831832\n",
      "Operator 95: -0.9088250711559078\n",
      "Operator 96: 0.6363140391665715\n",
      "Operator 97: -0.4732685113323516\n",
      "Operator 98: -0.19315582144453383\n",
      "Operator 99: -0.7362605997619541\n",
      "Operator 100: -0.2598774226447979\n",
      "Operator 101: 1.0762157863590338\n",
      "Operator 102: -0.13877754974766868\n",
      "Operator 103: 0.6785785821071515\n",
      "Operator 104: -0.14737961357231597\n",
      "Operator 105: 0.4099403860290883\n",
      "Operator 106: -0.9354899974240763\n",
      "Operator 107: 0.8182486172036444\n",
      "Operator 108: 6.559762240727946e-08\n",
      "Operator 109: 0.1049262062438169\n",
      "Operator 110: -0.4107090884764585\n",
      "Operator 111: 0.21025724582823352\n",
      "Operator 112: 0.050994566048883305\n",
      "Operator 113: -0.7949085054013907\n",
      "Operator 114: -0.5396023318794011\n",
      "Operator 115: 0.584440837213056\n",
      "Operator 116: 1.779801214009156\n",
      "Operator 117: 1.2168632792009513\n",
      "Operator 118: -0.780607846712457\n",
      "Operator 119: 1.0047825010905953e-07\n",
      "Operator 120: -0.023187551208838947\n",
      "Operator 121: -0.3028260717156299\n",
      "Operator 122: 0.40863196866571583\n",
      "Operator 123: -1.385809643861702\n",
      "Operator 124: 1.5342597692740796\n",
      "Operator 125: 1.324271546653946\n",
      "Operator 126: 1.5660820691654138\n",
      "Operator 127: -0.3641538885310336\n",
      "Operator 128: -0.2520265607990166\n",
      "Operator 129: 0.42782368840980867\n",
      "Operator 130: 9.529581923573005e-08\n",
      "Operator 131: 0.4054464320029525\n",
      "Operator 132: 0.2512555549668642\n",
      "Operator 133: -0.16970080196477477\n",
      "Operator 134: -0.46868111986718847\n",
      "Operator 135: -0.38507256809517304\n",
      "Operator 136: -0.054152287616561594\n",
      "Operator 137: 0.7762863578110766\n",
      "Operator 138: -0.34948753481541095\n",
      "Operator 139: 1.335965610734502\n",
      "Operator 140: -0.6029263641646685\n",
      "Operator 141: 0.0769657344642108\n",
      "Operator 142: -0.6514087041944722\n",
      "Operator 143: -0.3479374822101268\n",
      "Operator 144: -0.6099414302840371\n",
      "Operator 145: -0.14797198197318742\n",
      "Operator 146: 1.0607004310945747\n",
      "Operator 147: -0.37906801474936236\n",
      "Operator 148: 0.5774734253157305\n",
      "Operator 149: -0.5540898067160968\n",
      "Total gradient norm: 8.76286944773751\n",
      "Operators under consideration (1):\n",
      "[116]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.779801214009156)]\n",
      "Operator(s) added to ansatz: [116]\n",
      "Gradients: [np.float64(1.779801214009156)]\n",
      "Initial energy: -19.178767570322606\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116]...\n",
      "Starting point: [np.float64(0.14275659926886625), np.float64(0.6577326492772131), np.float64(0.533103165156622), np.float64(0.2416670527752049), np.float64(0.2467030767956236), np.float64(0.38926822960936946), np.float64(0.5062201817111986), np.float64(0.3382555577698923), np.float64(0.32969396731143463), np.float64(0.21494674496318716), np.float64(-0.1639771952345786), np.float64(-0.15979953113184148), np.float64(-0.13858352084518444), np.float64(0.13000032360871164), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.291112\n",
      "         Iterations: 20\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 61\n",
      "\n",
      "Current energy: -19.291112058411862\n",
      "(change of -0.11234448808925634)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.9696059836621015\n",
      "Operator 1: 2.9898297253770634e-08\n",
      "Operator 2: -0.7454918581881903\n",
      "Operator 3: 0.048466592056795016\n",
      "Operator 4: -1.0398539193775864\n",
      "Operator 5: 0.2224476123631688\n",
      "Operator 6: 0.010486021326256534\n",
      "Operator 7: 0.7123932174156341\n",
      "Operator 8: 0.2468436828329903\n",
      "Operator 9: 0.19341725541392452\n",
      "Operator 10: -1.4519533538911953e-08\n",
      "Operator 11: 0.24064765081764752\n",
      "Operator 12: -1.838327012786439\n",
      "Operator 13: -0.2525902755877319\n",
      "Operator 14: -1.1015539293480714\n",
      "Operator 15: -0.005886691946787065\n",
      "Operator 16: -0.8110630712149067\n",
      "Operator 17: 0.0513787285183378\n",
      "Operator 18: 0.9677768406151195\n",
      "Operator 19: -0.9842605234602707\n",
      "Operator 20: 0.07828790533842364\n",
      "Operator 21: -0.2571653460132049\n",
      "Operator 22: 0.12100029568887458\n",
      "Operator 23: 0.44635013567440734\n",
      "Operator 24: 0.7581151581546397\n",
      "Operator 25: 0.5130736902616114\n",
      "Operator 26: 1.6039231425424356\n",
      "Operator 27: -0.37938028712840566\n",
      "Operator 28: 0.4426079710714206\n",
      "Operator 29: -0.9292750291417156\n",
      "Operator 30: 0.22922542661754353\n",
      "Operator 31: -0.17711824055119413\n",
      "Operator 32: 0.7507157965495801\n",
      "Operator 33: 0.41391166598206514\n",
      "Operator 34: 1.4306148687142115\n",
      "Operator 35: 0.3047354730820162\n",
      "Operator 36: 0.9181407802762713\n",
      "Operator 37: 0.09171475705290047\n",
      "Operator 38: 0.7243278123041854\n",
      "Operator 39: -0.22543780280684034\n",
      "Operator 40: -1.1950615484021099\n",
      "Operator 41: 0.11724374156772657\n",
      "Operator 42: -0.5138469475389711\n",
      "Operator 43: 0.07681888274999452\n",
      "Operator 44: -0.6199748104333658\n",
      "Operator 45: -0.09914977923381962\n",
      "Operator 46: 9.031567316242217e-08\n",
      "Operator 47: 0.1739943867095985\n",
      "Operator 48: -0.42950112711593624\n",
      "Operator 49: 0.22681833994777728\n",
      "Operator 50: -0.8798235400964407\n",
      "Operator 51: 0.48732948049343977\n",
      "Operator 52: -0.2947774549308804\n",
      "Operator 53: 0.6372511131809239\n",
      "Operator 54: -0.08776047236564449\n",
      "Operator 55: 0.2705602459689733\n",
      "Operator 56: 0.5731964335031501\n",
      "Operator 57: -0.47429576530742695\n",
      "Operator 58: 0.9342208478100535\n",
      "Operator 59: -0.5630051391241839\n",
      "Operator 60: -0.46618656131035313\n",
      "Operator 61: -0.39332835085542217\n",
      "Operator 62: 0.011638994090843707\n",
      "Operator 63: -0.8762830571050748\n",
      "Operator 64: -0.4937129973361901\n",
      "Operator 65: -0.17962730050376308\n",
      "Operator 66: -1.3154777930048653e-07\n",
      "Operator 67: -0.5835693600204465\n",
      "Operator 68: -0.7502565659708266\n",
      "Operator 69: 0.2415530372057254\n",
      "Operator 70: 0.23442475248885541\n",
      "Operator 71: 1.1548120320833837\n",
      "Operator 72: -0.03677090059219833\n",
      "Operator 73: -0.10283427686372804\n",
      "Operator 74: 0.7655653704507293\n",
      "Operator 75: 0.014110095891692653\n",
      "Operator 76: -1.497288147797228\n",
      "Operator 77: 0.38821400465200656\n",
      "Operator 78: 0.1571112855938951\n",
      "Operator 79: 0.7539060138235976\n",
      "Operator 80: -0.7850534057890811\n",
      "Operator 81: -0.4561821313187586\n",
      "Operator 82: -0.7731815318344717\n",
      "Operator 83: -0.0064763628576485766\n",
      "Operator 84: -1.5746610102601588\n",
      "Operator 85: -0.1848155840709259\n",
      "Operator 86: 0.7101040828402836\n",
      "Operator 87: 0.3208677606432005\n",
      "Operator 88: 0.6861093249194203\n",
      "Operator 89: -0.5037366463774214\n",
      "Operator 90: -1.2015661897274559\n",
      "Operator 91: -0.08294758480066007\n",
      "Operator 92: -0.4574778942237485\n",
      "Operator 93: 0.11795989827600056\n",
      "Operator 94: -0.9322891167823678\n",
      "Operator 95: -0.9480999463590338\n",
      "Operator 96: 0.5975222185522439\n",
      "Operator 97: -0.8118882325670311\n",
      "Operator 98: 0.008651446045182486\n",
      "Operator 99: 0.6329108350213102\n",
      "Operator 100: -0.2043191788408378\n",
      "Operator 101: 1.0774910863336293\n",
      "Operator 102: -0.1270926258896815\n",
      "Operator 103: 0.6817842921336519\n",
      "Operator 104: -0.16158921250623326\n",
      "Operator 105: 0.40880019456469907\n",
      "Operator 106: -0.86688650597016\n",
      "Operator 107: 0.8737496175249353\n",
      "Operator 108: 1.2248064585744815e-08\n",
      "Operator 109: 0.004661791514183174\n",
      "Operator 110: -0.40484144992845916\n",
      "Operator 111: 0.1865093896466275\n",
      "Operator 112: 0.07120474316139092\n",
      "Operator 113: -0.8437944769994099\n",
      "Operator 114: -0.21157659935365292\n",
      "Operator 115: 0.30544313822422364\n",
      "Operator 116: 4.2372227183548605e-08\n",
      "Operator 117: 1.1083328073341736\n",
      "Operator 118: -0.7661642790539726\n",
      "Operator 119: -2.1869824573528263e-07\n",
      "Operator 120: -0.034058883103239124\n",
      "Operator 121: -0.315099442333139\n",
      "Operator 122: 0.4993163356305854\n",
      "Operator 123: -1.4322464316637162\n",
      "Operator 124: 1.535482981554994\n",
      "Operator 125: 0.4226528588095136\n",
      "Operator 126: 0.5544530708089106\n",
      "Operator 127: -0.3384457998318769\n",
      "Operator 128: -0.23449990430954049\n",
      "Operator 129: 0.39150583495119906\n",
      "Operator 130: -2.8531121420982686e-07\n",
      "Operator 131: 0.5019354260517739\n",
      "Operator 132: 0.22841177089049347\n",
      "Operator 133: -0.02524041561028797\n",
      "Operator 134: 0.09237055941361969\n",
      "Operator 135: -0.14923641475735905\n",
      "Operator 136: 0.026165040187021565\n",
      "Operator 137: 0.8329650533793873\n",
      "Operator 138: -0.3211253800754715\n",
      "Operator 139: 1.378226059722338\n",
      "Operator 140: -0.6740773978490764\n",
      "Operator 141: 0.09331970366765832\n",
      "Operator 142: -0.6918861771327769\n",
      "Operator 143: -0.37478919283105694\n",
      "Operator 144: 0.6257235315290285\n",
      "Operator 145: -0.03985896059992659\n",
      "Operator 146: 1.0392501107275014\n",
      "Operator 147: -0.3486934703493323\n",
      "Operator 148: 0.5807735740444507\n",
      "Operator 149: -0.6022221063534117\n",
      "Total gradient norm: 7.8241154092415\n",
      "Operators under consideration (1):\n",
      "[12]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.838327012786439)]\n",
      "Operator(s) added to ansatz: [12]\n",
      "Gradients: [np.float64(-1.838327012786439)]\n",
      "Initial energy: -19.291112058411862\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12]...\n",
      "Starting point: [np.float64(0.13891068160385883), np.float64(0.6635377166545895), np.float64(0.545906030828576), np.float64(0.24631570654052642), np.float64(0.22599415569495515), np.float64(0.41443013464433714), np.float64(0.5117522673509072), np.float64(0.3559412251164458), np.float64(0.41441134560215714), np.float64(0.21331639695597643), np.float64(-0.16207416575616315), np.float64(-0.16019186178959618), np.float64(-0.13545339877365228), np.float64(0.13431445086999524), np.float64(-0.12646278003648267), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.436936\n",
      "         Iterations: 24\n",
      "         Function evaluations: 37\n",
      "         Gradient evaluations: 37\n",
      "\n",
      "Current energy: -19.436936420857606\n",
      "(change of -0.14582436244574382)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.3839306987496036\n",
      "Operator 2: -0.1220230915694249\n",
      "Operator 3: 0.08808138627291176\n",
      "Operator 4: -1.078707130530216\n",
      "Operator 5: 0.22839758738341795\n",
      "Operator 6: 0.010536691470394821\n",
      "Operator 7: 0.7211503387920856\n",
      "Operator 8: 0.24969118271798685\n",
      "Operator 9: 0.18743515441529837\n",
      "Operator 10: 3.2662308748306794e-08\n",
      "Operator 11: 0.38139584419947326\n",
      "Operator 13: -0.10919684479447586\n",
      "Operator 14: -1.1249288138271298\n",
      "Operator 15: 0.010886636468633942\n",
      "Operator 16: -0.8111697917523689\n",
      "Operator 17: 0.05838632380043007\n",
      "Operator 18: 0.7993211911546861\n",
      "Operator 19: -0.10841188085535965\n",
      "Operator 20: 0.691397681049099\n",
      "Operator 21: -0.06442411438836809\n",
      "Operator 22: 0.04303844401962503\n",
      "Operator 23: 0.44787155530736666\n",
      "Operator 24: 0.7222363343276816\n",
      "Operator 25: 0.5080924983148303\n",
      "Operator 26: 1.5877884721140223\n",
      "Operator 27: -0.25104741383593465\n",
      "Operator 28: 0.42057529223010004\n",
      "Operator 29: -0.011472094842997722\n",
      "Operator 30: 0.6466086071133907\n",
      "Operator 31: -0.19819962995309104\n",
      "Operator 32: 0.6955705629133411\n",
      "Operator 33: 0.40584685070896276\n",
      "Operator 34: 1.414106854945521\n",
      "Operator 35: 0.2962043677498075\n",
      "Operator 36: 0.7857590105665984\n",
      "Operator 37: 0.07407553305424316\n",
      "Operator 38: 0.6498739437112899\n",
      "Operator 39: 0.06836599593759973\n",
      "Operator 40: -1.2071770319381367\n",
      "Operator 41: 0.13279260641369406\n",
      "Operator 42: -0.5077674685165201\n",
      "Operator 43: 0.07987063834415657\n",
      "Operator 44: -0.6249236462158669\n",
      "Operator 45: -0.10145851888535878\n",
      "Operator 47: 0.21118870336190354\n",
      "Operator 48: -0.44903571899386596\n",
      "Operator 49: 0.370795092275209\n",
      "Operator 50: -0.2937204498863143\n",
      "Operator 51: -0.051347478967991605\n",
      "Operator 52: -0.3185120191919052\n",
      "Operator 53: 0.6718172168501146\n",
      "Operator 54: -0.09139395222993607\n",
      "Operator 55: 0.275642220086175\n",
      "Operator 56: 0.5723014517652733\n",
      "Operator 57: -0.7974677141694321\n",
      "Operator 58: 0.1121333119523769\n",
      "Operator 59: -0.5269276929728068\n",
      "Operator 60: -0.5624931927124928\n",
      "Operator 61: -0.3428582476591926\n",
      "Operator 62: 0.011952198209719031\n",
      "Operator 63: -0.8430531532082615\n",
      "Operator 64: -0.49017105278563255\n",
      "Operator 65: -0.18736997154061769\n",
      "Operator 67: -0.7784432847065249\n",
      "Operator 68: -0.001121827759078606\n",
      "Operator 69: -0.3891680037374827\n",
      "Operator 70: 0.12398434897509363\n",
      "Operator 71: 1.1507648935103405\n",
      "Operator 72: -0.05784163345928135\n",
      "Operator 73: -0.09690559278616728\n",
      "Operator 74: 0.768900800471974\n",
      "Operator 75: 0.014925445631711387\n",
      "Operator 76: -1.4806448860539332\n",
      "Operator 77: 0.25109443182300334\n",
      "Operator 78: -0.4195174980905798\n",
      "Operator 79: 0.0064221396491803105\n",
      "Operator 80: -0.5952010179372127\n",
      "Operator 81: -0.4562168932384266\n",
      "Operator 82: -0.7289002449847332\n",
      "Operator 83: 0.007770317088605122\n",
      "Operator 84: -1.5616127522991237\n",
      "Operator 85: -0.17907555193836155\n",
      "Operator 86: 0.8543405540153347\n",
      "Operator 87: -0.09953561138772152\n",
      "Operator 88: 0.6195446386509262\n",
      "Operator 89: -0.15394557204918138\n",
      "Operator 90: -1.2115599216598842\n",
      "Operator 91: -0.07148416398310187\n",
      "Operator 92: -0.4586919339774943\n",
      "Operator 93: 0.12100569725812695\n",
      "Operator 94: -0.9398764542287261\n",
      "Operator 95: -0.8552355028060457\n",
      "Operator 96: 0.10154521268189474\n",
      "Operator 97: -0.6581296321043435\n",
      "Operator 98: 0.4538004925994622\n",
      "Operator 99: 0.6565851620188462\n",
      "Operator 100: -0.22428750993304258\n",
      "Operator 101: 1.084478851431794\n",
      "Operator 102: -0.13104027984704275\n",
      "Operator 103: 0.6815835198285506\n",
      "Operator 104: -0.21113398022315408\n",
      "Operator 105: 0.44894041692206454\n",
      "Operator 106: -0.3730274405400952\n",
      "Operator 107: 0.2753644761156387\n",
      "Operator 108: -0.14649273162690762\n",
      "Operator 109: 0.009966139281043871\n",
      "Operator 110: -0.42683886276545013\n",
      "Operator 111: 0.1927133393725781\n",
      "Operator 112: 0.06571110786239265\n",
      "Operator 113: -0.6578328971373026\n",
      "Operator 114: 0.36413898014814505\n",
      "Operator 115: -0.4208993207718219\n",
      "Operator 116: -0.14411218028247935\n",
      "Operator 117: 1.1556472373786448\n",
      "Operator 118: -0.7869802751343166\n",
      "Operator 120: -0.03283339278489383\n",
      "Operator 121: -0.31461524420154774\n",
      "Operator 122: 0.6564660532646384\n",
      "Operator 123: -0.3695144754711304\n",
      "Operator 124: 0.36768040111677736\n",
      "Operator 125: 0.2742211018187196\n",
      "Operator 126: 0.5419497740664736\n",
      "Operator 127: -0.3249921975153288\n",
      "Operator 128: -0.23972291617920394\n",
      "Operator 129: 0.40338447275343703\n",
      "Operator 131: 0.8332574940505799\n",
      "Operator 132: 0.21780304624747998\n",
      "Operator 133: 0.5405046559617163\n",
      "Operator 134: 0.24020706694908256\n",
      "Operator 135: -0.12403695672345194\n",
      "Operator 136: 0.031614011567372806\n",
      "Operator 137: 0.7996573135491261\n",
      "Operator 138: -0.32968668025770975\n",
      "Operator 139: 1.3632448344259425\n",
      "Operator 140: -0.7848263207652412\n",
      "Operator 141: -0.07343508396783757\n",
      "Operator 142: -0.6121754134595785\n",
      "Operator 143: -0.11025610094166768\n",
      "Operator 144: 0.7089373970036925\n",
      "Operator 145: -0.04528620878953974\n",
      "Operator 146: 1.0413959123849676\n",
      "Operator 147: -0.3589880399836413\n",
      "Operator 148: 0.5791400277723133\n",
      "Operator 149: -0.5724358930243485\n",
      "Total gradient norm: 6.863131691983931\n",
      "Operators under consideration (1):\n",
      "[26]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.5877884721140223)]\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(1.5877884721140223)]\n",
      "Initial energy: -19.436936420857606\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26]...\n",
      "Starting point: [np.float64(0.14091272761803705), np.float64(0.661771622053981), np.float64(0.5422575432532457), np.float64(0.25237840743254497), np.float64(0.16021771110388092), np.float64(0.40719007061345247), np.float64(0.507383812045199), np.float64(0.34846510564114386), np.float64(0.40920925913880885), np.float64(0.21525083237817635), np.float64(-0.16251939842087545), np.float64(-0.16012730888670212), np.float64(-0.1408963824878501), np.float64(0.13395610732144295), np.float64(-0.1286515261002295), np.float64(0.15948480564377687), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.549932\n",
      "         Iterations: 24\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 40\n",
      "\n",
      "Current energy: -19.549932404657714\n",
      "(change of -0.11299598380010778)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.39324921620306885\n",
      "Operator 2: -0.12896950670891974\n",
      "Operator 3: 0.08784062958675705\n",
      "Operator 4: -0.9780368301262924\n",
      "Operator 5: 0.11791970273064323\n",
      "Operator 6: -0.0581877979699835\n",
      "Operator 7: 0.5721242978693576\n",
      "Operator 8: 0.15425782960837756\n",
      "Operator 9: 0.18893479230762505\n",
      "Operator 10: 1.9118247197828352e-07\n",
      "Operator 11: 0.3904537059965085\n",
      "Operator 13: -0.1210394611767404\n",
      "Operator 14: -1.1747193574955659\n",
      "Operator 15: 0.007809064608035393\n",
      "Operator 16: -0.6644664900731208\n",
      "Operator 17: -0.025507649550251005\n",
      "Operator 18: 0.8050020701618749\n",
      "Operator 19: -0.13604840602697976\n",
      "Operator 20: 0.72302049920388\n",
      "Operator 21: -0.07144436548099661\n",
      "Operator 22: 0.14677543580180846\n",
      "Operator 23: 0.5304800681392079\n",
      "Operator 24: 0.8707908356960524\n",
      "Operator 25: 0.13774307159735696\n",
      "Operator 27: -0.26902521388920697\n",
      "Operator 28: 0.45845222306225053\n",
      "Operator 29: -0.05193113616143509\n",
      "Operator 30: 0.7588288294898033\n",
      "Operator 31: -0.18889169684128146\n",
      "Operator 32: 0.9448968642769571\n",
      "Operator 33: 0.4819349319736232\n",
      "Operator 34: -0.22687545427545616\n",
      "Operator 35: -0.08412446956441123\n",
      "Operator 36: 0.7936238173702753\n",
      "Operator 37: 0.0657887306838947\n",
      "Operator 38: 0.6726915507700324\n",
      "Operator 39: 0.05660784191191171\n",
      "Operator 40: -1.1868827075046942\n",
      "Operator 41: 0.04631905812911805\n",
      "Operator 42: -0.6300591448608459\n",
      "Operator 43: 0.18790976825441952\n",
      "Operator 44: 0.07334920577431613\n",
      "Operator 45: -0.2516898697229667\n",
      "Operator 47: 0.2117132183183699\n",
      "Operator 48: -0.4456505003583895\n",
      "Operator 49: 0.3752518460756742\n",
      "Operator 50: -0.293946142821449\n",
      "Operator 51: -0.06488209051819871\n",
      "Operator 52: -0.28122479257255273\n",
      "Operator 53: 0.6448231482506553\n",
      "Operator 54: -0.20518548299466913\n",
      "Operator 55: 0.20476602767405822\n",
      "Operator 56: 0.5801712531878325\n",
      "Operator 57: -0.8029650016875569\n",
      "Operator 58: 0.14012423096871487\n",
      "Operator 59: -0.5468049565398949\n",
      "Operator 60: -0.5735144431971388\n",
      "Operator 61: -0.49201402679069367\n",
      "Operator 62: -0.250451011176828\n",
      "Operator 63: -0.9285200202573021\n",
      "Operator 64: -0.01522025131638165\n",
      "Operator 65: -0.18886393546795038\n",
      "Operator 66: -6.641267808058674e-08\n",
      "Operator 67: -0.7828398048219413\n",
      "Operator 68: -0.010477944054032766\n",
      "Operator 69: -0.4026465181790742\n",
      "Operator 70: 0.1349677695812217\n",
      "Operator 71: 1.0963975972536595\n",
      "Operator 72: 0.0856737403811291\n",
      "Operator 73: 0.06159144323433139\n",
      "Operator 74: 0.40744305617370535\n",
      "Operator 75: 0.02647307385657163\n",
      "Operator 76: 0.12440874263593127\n",
      "Operator 77: 0.26907658417479624\n",
      "Operator 78: -0.45734680052648774\n",
      "Operator 79: 0.04616309189863381\n",
      "Operator 80: -0.7053977878469273\n",
      "Operator 81: -0.49382232484862665\n",
      "Operator 82: -1.0201843785073341\n",
      "Operator 83: -0.22510076842683144\n",
      "Operator 84: 0.19301227163491713\n",
      "Operator 85: 0.3350168560890117\n",
      "Operator 86: 0.8594004667970994\n",
      "Operator 87: -0.09457686364325336\n",
      "Operator 88: 0.6359023713820556\n",
      "Operator 89: -0.16654279980848471\n",
      "Operator 90: -1.1972483530646942\n",
      "Operator 91: -0.14973266334085908\n",
      "Operator 92: -0.5668610653898986\n",
      "Operator 93: 0.17180655104875678\n",
      "Operator 94: -0.1759396469255793\n",
      "Operator 95: -0.8603864891919404\n",
      "Operator 96: 0.09677112184669741\n",
      "Operator 97: -0.676793412562782\n",
      "Operator 98: 0.48317587342133494\n",
      "Operator 99: 0.6720034650285305\n",
      "Operator 100: -0.1841451558755191\n",
      "Operator 101: 1.0406469608127948\n",
      "Operator 102: -0.2156314639600131\n",
      "Operator 103: 0.19578061517285908\n",
      "Operator 104: -0.21165336458491774\n",
      "Operator 105: 0.44554953546962023\n",
      "Operator 106: -0.37773251203821345\n",
      "Operator 107: 0.2745379756968773\n",
      "Operator 108: -0.14844999508242468\n",
      "Operator 109: 0.04623659699894717\n",
      "Operator 110: -0.343235144287338\n",
      "Operator 111: 0.31032953157514365\n",
      "Operator 112: 0.037126063274597186\n",
      "Operator 113: -0.6671766060851309\n",
      "Operator 114: 0.3955835214071166\n",
      "Operator 115: -0.4486662603026893\n",
      "Operator 116: -0.13240788066147763\n",
      "Operator 117: 1.0982697718885024\n",
      "Operator 118: -0.6172478157562692\n",
      "Operator 119: 0.1254423899243806\n",
      "Operator 120: -0.24603270654733334\n",
      "Operator 121: -0.30521048739648\n",
      "Operator 122: 0.665676890604869\n",
      "Operator 123: -0.401427676743189\n",
      "Operator 124: 0.3934102133892333\n",
      "Operator 125: 0.26410451031798643\n",
      "Operator 126: 0.5790472210539076\n",
      "Operator 127: -0.1513648090446817\n",
      "Operator 128: -0.22283417876935754\n",
      "Operator 129: 0.21832192650059612\n",
      "Operator 130: -0.09285820334475157\n",
      "Operator 131: 0.8372185344554478\n",
      "Operator 132: 0.21334992215007548\n",
      "Operator 133: 0.555668553280149\n",
      "Operator 134: 0.22144481659335213\n",
      "Operator 135: 0.013680060161670286\n",
      "Operator 136: 0.084416987839134\n",
      "Operator 137: 0.8404888242745118\n",
      "Operator 138: -0.20443327168544617\n",
      "Operator 139: -0.16862118739443105\n",
      "Operator 140: -0.792597412485678\n",
      "Operator 141: -0.06510647547058858\n",
      "Operator 142: -0.632699093656662\n",
      "Operator 143: -0.09688904648758224\n",
      "Operator 144: 0.7195671934866971\n",
      "Operator 145: -0.045076426940895406\n",
      "Operator 146: 1.0078951179310953\n",
      "Operator 147: -0.23235347529553724\n",
      "Operator 148: 0.0869601615597707\n",
      "Operator 149: -0.5803172566360771\n",
      "Total gradient norm: 5.896865558381906\n",
      "Operators under consideration (1):\n",
      "[90]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.1972483530646942)]\n",
      "Operator(s) added to ansatz: [90]\n",
      "Gradients: [np.float64(-1.1972483530646942)]\n",
      "Initial energy: -19.549932404657714\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90]...\n",
      "Starting point: [np.float64(0.14026175652243106), np.float64(0.7419410459189941), np.float64(0.5974527997980511), np.float64(0.24522071557344938), np.float64(0.15839221860816713), np.float64(0.437169252250321), np.float64(0.5083472365567816), np.float64(0.35124463083459195), np.float64(0.41790028006373825), np.float64(0.2065999664743494), np.float64(-0.12299480655859664), np.float64(-0.1777660582963097), np.float64(-0.1402439857503503), np.float64(0.15081328933273577), np.float64(-0.12974459921186246), np.float64(0.15758386678826974), np.float64(-0.13519345523840276), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.596693\n",
      "         Iterations: 28\n",
      "         Function evaluations: 139\n",
      "         Gradient evaluations: 125\n",
      "\n",
      "Current energy: -19.596693058197765\n",
      "(change of -0.04676065354005132)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.4062751697196044\n",
      "Operator 1: 0.019855899954091934\n",
      "Operator 2: 0.02990891371672575\n",
      "Operator 3: -0.45969856015613225\n",
      "Operator 4: -0.8198839282550319\n",
      "Operator 5: 0.1818661010403278\n",
      "Operator 6: -0.057412826961064325\n",
      "Operator 7: 0.5571188276948613\n",
      "Operator 8: 0.14901451416258604\n",
      "Operator 9: 0.19030095394590207\n",
      "Operator 10: 9.920372909543448e-08\n",
      "Operator 11: 0.3948723821164511\n",
      "Operator 13: -0.3569128121483619\n",
      "Operator 14: -0.6537928083248324\n",
      "Operator 15: -0.08236281481820565\n",
      "Operator 16: -0.7331258892077068\n",
      "Operator 17: -0.047097877155046444\n",
      "Operator 18: 0.8085824062125706\n",
      "Operator 19: -0.1489542642183876\n",
      "Operator 20: 0.9924168223684858\n",
      "Operator 21: 0.0970708356416229\n",
      "Operator 22: -0.0041327758495676575\n",
      "Operator 23: 0.3480605720942862\n",
      "Operator 24: 0.9022174134180982\n",
      "Operator 25: 0.1315061143609024\n",
      "Operator 26: 2.215268046779073e-08\n",
      "Operator 27: -0.27299786503606865\n",
      "Operator 28: 0.5074442397733953\n",
      "Operator 29: 0.0077595101064293695\n",
      "Operator 30: 0.6151936730183876\n",
      "Operator 31: -0.029169303656514416\n",
      "Operator 32: 0.993838503246554\n",
      "Operator 33: 0.37071196904806464\n",
      "Operator 34: -0.2217016595686324\n",
      "Operator 35: -0.08157070583196165\n",
      "Operator 36: 0.7966180732551891\n",
      "Operator 37: 0.06214885850600542\n",
      "Operator 38: 0.7410312700814214\n",
      "Operator 39: 0.048907110929251804\n",
      "Operator 40: -0.4918880120810498\n",
      "Operator 41: 0.5843846250699615\n",
      "Operator 42: -0.6198612709721262\n",
      "Operator 43: 0.18075210221333643\n",
      "Operator 44: 0.08055656808282734\n",
      "Operator 45: -0.24599284884227743\n",
      "Operator 46: 1.0373653269724059e-08\n",
      "Operator 47: 0.21290624594677732\n",
      "Operator 48: -0.43877680721636225\n",
      "Operator 49: 0.4230334133018414\n",
      "Operator 50: -0.376944486198408\n",
      "Operator 51: -0.4120522460013713\n",
      "Operator 52: -0.16937576725914272\n",
      "Operator 53: 0.5527685668888083\n",
      "Operator 54: -0.2333415143608406\n",
      "Operator 55: 0.19850903254614746\n",
      "Operator 56: 0.5847973348406588\n",
      "Operator 57: -0.7992638512596429\n",
      "Operator 58: 0.1879341022193836\n",
      "Operator 59: -0.676605915495708\n",
      "Operator 60: -0.4685307170418011\n",
      "Operator 61: -0.21545187048405673\n",
      "Operator 62: -0.26107547259659386\n",
      "Operator 63: -0.9723627449425617\n",
      "Operator 64: -0.012023293591765658\n",
      "Operator 65: -0.18988642412966664\n",
      "Operator 66: -4.6035595746061236e-08\n",
      "Operator 67: -0.7813708722575685\n",
      "Operator 68: -0.020150248470306338\n",
      "Operator 69: -0.453527749587182\n",
      "Operator 70: -0.21006278668904077\n",
      "Operator 71: 0.5910981128377707\n",
      "Operator 72: -0.03968413274991092\n",
      "Operator 73: 0.09535725312003619\n",
      "Operator 74: 0.40953446472309346\n",
      "Operator 75: 0.024876888815618697\n",
      "Operator 76: 0.12291897035699972\n",
      "Operator 77: 0.27330139144041976\n",
      "Operator 78: -0.4938065396444307\n",
      "Operator 79: 0.0911352414832733\n",
      "Operator 80: -0.5105242380594076\n",
      "Operator 81: -0.3016714508235432\n",
      "Operator 82: -1.0956407335714788\n",
      "Operator 83: -0.20022032067456122\n",
      "Operator 84: 0.1869989555511919\n",
      "Operator 85: 0.3314079737248188\n",
      "Operator 86: 0.8586218905087614\n",
      "Operator 87: -0.08943203109136012\n",
      "Operator 88: 0.6701174052457894\n",
      "Operator 89: -0.20756366706499793\n",
      "Operator 90: 3.069336767830711e-08\n",
      "Operator 91: -0.13952810557417683\n",
      "Operator 92: -0.5592171907217622\n",
      "Operator 93: 0.1446424615684218\n",
      "Operator 94: -0.16655085404823144\n",
      "Operator 95: -0.8632391565791222\n",
      "Operator 96: 0.09815808069695685\n",
      "Operator 97: -0.687731268995358\n",
      "Operator 98: 0.3965052735981707\n",
      "Operator 99: 0.21265591453717292\n",
      "Operator 100: -0.5026613633667044\n",
      "Operator 101: 0.9699231120786032\n",
      "Operator 102: -0.21231584990377175\n",
      "Operator 103: 0.18658820118788016\n",
      "Operator 104: -0.2125562761452047\n",
      "Operator 105: 0.4382041311499179\n",
      "Operator 106: -0.4183026865940287\n",
      "Operator 107: 0.3040028319518425\n",
      "Operator 108: -0.015212701193155608\n",
      "Operator 109: 0.023416766363829517\n",
      "Operator 110: -0.2544041411824711\n",
      "Operator 111: 0.32899793319407694\n",
      "Operator 112: 0.05868596605257052\n",
      "Operator 113: -0.6711367391031742\n",
      "Operator 114: 0.41109381101273135\n",
      "Operator 115: -0.6384848814087979\n",
      "Operator 116: -0.12463801515657659\n",
      "Operator 117: 0.8060934475998357\n",
      "Operator 118: -0.5300142005050512\n",
      "Operator 119: 0.12499957031103018\n",
      "Operator 120: -0.2384969273548746\n",
      "Operator 121: -0.3026839711928425\n",
      "Operator 122: 0.6637803394546252\n",
      "Operator 123: -0.45498477911477564\n",
      "Operator 124: 0.3899694095767736\n",
      "Operator 125: -0.22108846803453233\n",
      "Operator 126: 0.5049132997231558\n",
      "Operator 127: -0.10314265616181023\n",
      "Operator 128: -0.11873217544949309\n",
      "Operator 129: 0.2080024850954378\n",
      "Operator 130: -0.09189754115594716\n",
      "Operator 131: 0.8336569154883429\n",
      "Operator 132: 0.20402321491221723\n",
      "Operator 133: 0.6702630707083567\n",
      "Operator 134: 0.5548059220202852\n",
      "Operator 135: -0.1149757639109936\n",
      "Operator 136: 0.09064445648228225\n",
      "Operator 137: 0.866329583369386\n",
      "Operator 138: -0.1979998531205285\n",
      "Operator 139: -0.1668691839719038\n",
      "Operator 140: -0.7917695073526133\n",
      "Operator 141: -0.055160471627577555\n",
      "Operator 142: -0.6345391761586372\n",
      "Operator 143: 0.22544005081793042\n",
      "Operator 144: -0.06270979541569316\n",
      "Operator 145: -0.08329905170420149\n",
      "Operator 146: 0.8167031730272227\n",
      "Operator 147: -0.22558818186659882\n",
      "Operator 148: 0.0837425953929882\n",
      "Operator 149: -0.585650303861226\n",
      "Total gradient norm: 5.451156643238743\n",
      "Operators under consideration (1):\n",
      "[82]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.0956407335714788)]\n",
      "Operator(s) added to ansatz: [82]\n",
      "Gradients: [np.float64(-1.0956407335714788)]\n",
      "Initial energy: -19.596693058197765\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82]...\n",
      "Starting point: [np.float64(0.1400681308399381), np.float64(0.7428571482850505), np.float64(0.6009388513381562), np.float64(0.2940065047853231), np.float64(0.16112187536528066), np.float64(0.4477870260261758), np.float64(0.508831346252748), np.float64(0.35303159669031947), np.float64(0.43378342000124437), np.float64(0.20134971360321077), np.float64(-0.1227737142529234), np.float64(-0.17739860185294554), np.float64(-0.13996415376451546), np.float64(0.15062462846729704), np.float64(-0.13998695104624842), np.float64(0.15641247050247706), np.float64(-0.13524895006747373), np.float64(0.0788293676502052), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.647964\n",
      "         Iterations: 28\n",
      "         Function evaluations: 49\n",
      "         Gradient evaluations: 48\n",
      "\n",
      "Current energy: -19.64796417414522\n",
      "(change of -0.05127111594745415)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.4271069388235026\n",
      "Operator 1: 0.019884914585669135\n",
      "Operator 2: 0.011571777575635683\n",
      "Operator 3: -0.4675393548651414\n",
      "Operator 4: -0.8175230469656269\n",
      "Operator 5: 0.2776172039604754\n",
      "Operator 6: -0.0002045242123323021\n",
      "Operator 7: 0.4152464227542117\n",
      "Operator 8: 0.10931572606176232\n",
      "Operator 9: 0.19376324325537453\n",
      "Operator 10: 6.039716890869206e-08\n",
      "Operator 11: 0.4097289199491774\n",
      "Operator 13: -0.35447010363789294\n",
      "Operator 14: -0.7422578568720386\n",
      "Operator 15: -0.03014466537599647\n",
      "Operator 16: -0.8828966772028457\n",
      "Operator 17: -0.16580835808377703\n",
      "Operator 18: 0.8242064529942601\n",
      "Operator 19: -0.20288829872918263\n",
      "Operator 20: 1.0492578336715828\n",
      "Operator 21: 0.010227862214977384\n",
      "Operator 22: 0.13376171871835352\n",
      "Operator 23: -0.006671396170208781\n",
      "Operator 24: 0.15918616504117078\n",
      "Operator 25: 0.10854325800860187\n",
      "Operator 26: 2.4507376839011386e-07\n",
      "Operator 27: -0.3106489392649566\n",
      "Operator 28: 0.5897755227526666\n",
      "Operator 29: -0.08480017618138243\n",
      "Operator 30: 0.7832760205410567\n",
      "Operator 31: -0.1455767512006621\n",
      "Operator 32: 0.18471144019116523\n",
      "Operator 33: 0.06577991811901335\n",
      "Operator 34: -0.22663918747665956\n",
      "Operator 35: -0.06331878027059146\n",
      "Operator 36: 0.8149566817105482\n",
      "Operator 37: 0.046190059478408124\n",
      "Operator 38: 0.788474704438449\n",
      "Operator 39: -0.009741744043061058\n",
      "Operator 40: -0.5008925348189401\n",
      "Operator 41: 0.6495284257589913\n",
      "Operator 42: -0.14258885217006748\n",
      "Operator 43: 0.2084089955012532\n",
      "Operator 44: 0.13027318269004945\n",
      "Operator 45: -0.18363994817585316\n",
      "Operator 46: 5.398414071322577e-07\n",
      "Operator 47: 0.21441219564685765\n",
      "Operator 48: -0.43013777702524597\n",
      "Operator 49: 0.44059843668356774\n",
      "Operator 50: -0.3689574893531151\n",
      "Operator 51: -0.4381842234902414\n",
      "Operator 52: -0.08032715865428115\n",
      "Operator 53: 0.5319561148867459\n",
      "Operator 54: -0.26148566816977203\n",
      "Operator 55: 0.09429806936611307\n",
      "Operator 56: 0.6024624627191542\n",
      "Operator 57: -0.8103675006448319\n",
      "Operator 58: 0.2518080990333822\n",
      "Operator 59: -0.7235306581853586\n",
      "Operator 60: -0.44163074970752914\n",
      "Operator 61: -0.2691601307447111\n",
      "Operator 62: 0.09573462410499936\n",
      "Operator 63: -0.18803903423374102\n",
      "Operator 64: -0.00032947233699082146\n",
      "Operator 65: -0.1931823511340044\n",
      "Operator 66: -1.7485987729277085e-08\n",
      "Operator 67: -0.7907340485484609\n",
      "Operator 68: -0.04303145318819172\n",
      "Operator 69: -0.4817382781429841\n",
      "Operator 70: -0.1703786337203331\n",
      "Operator 71: 0.5059432514478668\n",
      "Operator 72: -0.19556844992076353\n",
      "Operator 73: 0.029945714002033108\n",
      "Operator 74: 0.17103177228447952\n",
      "Operator 75: 0.02039109151969898\n",
      "Operator 76: 0.12675533449827237\n",
      "Operator 77: 0.3110807457750608\n",
      "Operator 78: -0.5716911007814676\n",
      "Operator 79: 0.19736785346548344\n",
      "Operator 80: -0.694611907478265\n",
      "Operator 81: -0.27041863067769095\n",
      "Operator 82: -4.686837778988817e-08\n",
      "Operator 83: 0.08568107706828201\n",
      "Operator 84: 0.2195140386780725\n",
      "Operator 85: 0.33565386147516385\n",
      "Operator 86: 0.869332846022802\n",
      "Operator 87: -0.07708342547953556\n",
      "Operator 88: 0.7073105143165732\n",
      "Operator 89: -0.27524387096159\n",
      "Operator 90: -0.029528114307877103\n",
      "Operator 91: 0.12324613458966392\n",
      "Operator 92: -0.15469298622631883\n",
      "Operator 93: 0.06042178017324082\n",
      "Operator 94: -0.07668616898008992\n",
      "Operator 95: -0.8762137702235074\n",
      "Operator 96: 0.09086646804614112\n",
      "Operator 97: -0.7095646422532491\n",
      "Operator 98: 0.4553056702026973\n",
      "Operator 99: 0.3167002416687107\n",
      "Operator 100: -0.491600451080728\n",
      "Operator 101: 0.5290978442941865\n",
      "Operator 102: -0.1934741418662002\n",
      "Operator 103: 0.15071200178998884\n",
      "Operator 104: -0.21391587241005028\n",
      "Operator 105: 0.4293809329168227\n",
      "Operator 106: -0.4362362171143717\n",
      "Operator 107: 0.29578007673546125\n",
      "Operator 108: 0.007479856679678307\n",
      "Operator 109: 0.2696816766172372\n",
      "Operator 110: -0.3011909127917418\n",
      "Operator 111: 0.365022198744939\n",
      "Operator 112: 0.1725966988291887\n",
      "Operator 113: -0.6931236239456829\n",
      "Operator 114: 0.4702145190942574\n",
      "Operator 115: -0.7099636522127237\n",
      "Operator 116: -0.037719863763203715\n",
      "Operator 117: 0.7037246455900791\n",
      "Operator 118: -0.5059002457483329\n",
      "Operator 119: 0.045051601797086116\n",
      "Operator 120: -0.23448446363654218\n",
      "Operator 121: -0.316875982231829\n",
      "Operator 122: 0.6823467452898976\n",
      "Operator 123: -0.5292371558528526\n",
      "Operator 124: 0.45190900785142407\n",
      "Operator 125: -0.2491174142416734\n",
      "Operator 126: 0.3874511216808813\n",
      "Operator 127: -0.14750917782510573\n",
      "Operator 128: -0.06758806821507146\n",
      "Operator 129: 0.12556581920810173\n",
      "Operator 130: -0.0957217188216859\n",
      "Operator 131: 0.8418574286304209\n",
      "Operator 132: 0.19532630936910564\n",
      "Operator 133: 0.7006687679469396\n",
      "Operator 134: 0.5045604180394754\n",
      "Operator 135: -0.006182240827860674\n",
      "Operator 136: -0.011807994097174111\n",
      "Operator 137: 0.05840034606097633\n",
      "Operator 138: -0.16783891377734403\n",
      "Operator 139: -0.1706607470005692\n",
      "Operator 140: -0.8077630199151483\n",
      "Operator 141: -0.03737100744833856\n",
      "Operator 142: -0.6778311668798329\n",
      "Operator 143: 0.29487582737964735\n",
      "Operator 144: 0.12280795236211584\n",
      "Operator 145: -0.06554316534038213\n",
      "Operator 146: 0.41318321888934983\n",
      "Operator 147: -0.11148162571953082\n",
      "Operator 148: 0.06039162278092369\n",
      "Operator 149: -0.6036552906095997\n",
      "Total gradient norm: 4.968712745156442\n",
      "Operators under consideration (1):\n",
      "[20]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.0492578336715828)]\n",
      "Operator(s) added to ansatz: [20]\n",
      "Gradients: [np.float64(1.0492578336715828)]\n",
      "Initial energy: -19.64796417414522\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20]...\n",
      "Starting point: [np.float64(0.1386904984069094), np.float64(0.7549201340723408), np.float64(0.6487083029880947), np.float64(0.2906741695609088), np.float64(0.15841098611949112), np.float64(0.5239524128898578), np.float64(0.5109664184455062), np.float64(0.359224675336567), np.float64(0.4472943906565014), np.float64(0.1661169596844398), np.float64(-0.1189226207113916), np.float64(-0.17666564997964873), np.float64(-0.13854473967896977), np.float64(0.15647599449151797), np.float64(-0.13478703512781148), np.float64(0.1516027029209556), np.float64(-0.13890210447015724), np.float64(0.08875807275501466), np.float64(0.08947894953276829), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.701101\n",
      "         Iterations: 27\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 42\n",
      "\n",
      "Current energy: -19.70110078288155\n",
      "(change of -0.05313660873633097)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20]\n",
      "On iteration 20.\n",
      "\n",
      "*** ADAPT-VQE Iteration 21 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.49278608523136813\n",
      "Operator 1: -0.015323784198744347\n",
      "Operator 2: -0.1034053735165924\n",
      "Operator 3: -0.2485519938548746\n",
      "Operator 4: -0.5537648786565788\n",
      "Operator 5: 0.2880121047553841\n",
      "Operator 6: 0.00186032841367334\n",
      "Operator 7: 0.3814746225879886\n",
      "Operator 8: 0.09962909526730002\n",
      "Operator 9: 0.20160219024620304\n",
      "Operator 11: 0.517698272553045\n",
      "Operator 12: -1.0965510488557609e-08\n",
      "Operator 13: -0.14726139875937447\n",
      "Operator 14: -0.4684767482274816\n",
      "Operator 15: 0.0025312554127477904\n",
      "Operator 16: -0.860603272154675\n",
      "Operator 17: -0.16951258036069206\n",
      "Operator 18: 0.7936584245540657\n",
      "Operator 19: -0.5200129150076469\n",
      "Operator 20: 1.9686000524155034e-08\n",
      "Operator 21: 0.14702960685799138\n",
      "Operator 22: 0.20647699006215386\n",
      "Operator 23: -0.11920298304694768\n",
      "Operator 24: 0.20220361398277065\n",
      "Operator 25: 0.10330047653130586\n",
      "Operator 26: -3.9452223266067676e-07\n",
      "Operator 27: -0.36063309777913083\n",
      "Operator 28: -0.17391625129106683\n",
      "Operator 29: -0.48369438703194\n",
      "Operator 30: 0.4694426986904763\n",
      "Operator 31: 0.02463329245542057\n",
      "Operator 32: 0.28300103087012857\n",
      "Operator 33: -0.004542250988610181\n",
      "Operator 34: -0.22061061147292954\n",
      "Operator 35: -0.05920050214778047\n",
      "Operator 36: 0.8118232101263112\n",
      "Operator 37: 0.027089321869898143\n",
      "Operator 38: 1.2276461414915956\n",
      "Operator 39: 0.039462213655438436\n",
      "Operator 40: -0.5020983163716306\n",
      "Operator 41: 0.7860506679085413\n",
      "Operator 42: -0.14470549505373914\n",
      "Operator 43: 0.2032837680749329\n",
      "Operator 44: 0.1446187020654882\n",
      "Operator 45: -0.169383993459362\n",
      "Operator 46: 1.2951115238024613e-06\n",
      "Operator 47: 0.21742145148950162\n",
      "Operator 48: -0.4145944796061609\n",
      "Operator 49: 0.5647077957913179\n",
      "Operator 50: -0.5467093725609314\n",
      "Operator 51: -0.14943406105255752\n",
      "Operator 52: -0.07177733921887908\n",
      "Operator 53: 0.32182304130877404\n",
      "Operator 54: -0.2570249511451521\n",
      "Operator 55: 0.08288455710983156\n",
      "Operator 56: 0.6417995467218953\n",
      "Operator 57: -0.8082256499964923\n",
      "Operator 58: 0.4104677782027662\n",
      "Operator 59: -0.06653695142560098\n",
      "Operator 60: -0.290962020285153\n",
      "Operator 61: -0.12063032137244387\n",
      "Operator 62: 0.1045827950386961\n",
      "Operator 63: -0.24759407335638717\n",
      "Operator 64: 0.003121417845580121\n",
      "Operator 65: -0.20180801477015214\n",
      "Operator 67: -0.794978930261317\n",
      "Operator 68: -0.14180143905532694\n",
      "Operator 69: -0.4155812914564001\n",
      "Operator 70: 0.21959447721583542\n",
      "Operator 71: 0.11642838321968524\n",
      "Operator 72: -0.23039312299616327\n",
      "Operator 73: 0.03943020748968752\n",
      "Operator 74: 0.1659690125417136\n",
      "Operator 75: 0.018023668080933364\n",
      "Operator 76: 0.1256300321375674\n",
      "Operator 77: 0.3604739666719283\n",
      "Operator 78: 0.12716439083596737\n",
      "Operator 79: 0.4168184262301401\n",
      "Operator 80: -0.5522928095126867\n",
      "Operator 81: -0.15514357702706694\n",
      "Operator 82: -3.59095276639132e-08\n",
      "Operator 83: 0.05821997222229162\n",
      "Operator 84: 0.21757560777923746\n",
      "Operator 85: 0.33317567782741764\n",
      "Operator 86: 0.8753930313559184\n",
      "Operator 87: -0.01566490453796783\n",
      "Operator 88: 1.0824405758277333\n",
      "Operator 89: -0.2570711431813684\n",
      "Operator 90: 0.0036448749490296216\n",
      "Operator 91: 0.21343270862300529\n",
      "Operator 92: -0.1328258470419085\n",
      "Operator 93: 0.033447535483550395\n",
      "Operator 94: -0.059560799138075014\n",
      "Operator 95: -0.8685630904425241\n",
      "Operator 96: -0.01704374523520925\n",
      "Operator 97: -1.3490108222046195\n",
      "Operator 98: 0.22898404670468736\n",
      "Operator 99: 0.4731149179700206\n",
      "Operator 100: -0.5261941217621344\n",
      "Operator 101: 0.4319037940794106\n",
      "Operator 102: -0.1939051085786504\n",
      "Operator 103: 0.13865996661273827\n",
      "Operator 104: -0.2175984516072349\n",
      "Operator 105: 0.416212007998437\n",
      "Operator 106: -0.5318905881328755\n",
      "Operator 107: 0.3158595543317713\n",
      "Operator 108: -0.054812964358266165\n",
      "Operator 109: 0.31848501832566667\n",
      "Operator 110: -0.2919869248841745\n",
      "Operator 111: 0.34779441328408034\n",
      "Operator 112: 0.17568994779804348\n",
      "Operator 113: -0.6865875234079273\n",
      "Operator 114: 0.6503032773234833\n",
      "Operator 115: -0.5084930832831568\n",
      "Operator 116: -0.09501561026556943\n",
      "Operator 117: 0.40267154681378214\n",
      "Operator 118: -0.4018348499249793\n",
      "Operator 119: 0.060752221335000973\n",
      "Operator 120: -0.22791524988546175\n",
      "Operator 121: -0.31620768181882064\n",
      "Operator 122: 0.6956818941089566\n",
      "Operator 123: -0.4779652367171925\n",
      "Operator 124: 0.6416489425076588\n",
      "Operator 125: -0.012255034298450512\n",
      "Operator 126: 0.008832892311888565\n",
      "Operator 127: -0.14763090232525447\n",
      "Operator 128: 0.023672611804296917\n",
      "Operator 129: 0.10817294971983477\n",
      "Operator 130: -0.09521318897299147\n",
      "Operator 131: 0.836828356921071\n",
      "Operator 132: 0.2127952288250624\n",
      "Operator 133: 0.007653972191415833\n",
      "Operator 134: -0.08249047929161166\n",
      "Operator 135: 0.07105685495593027\n",
      "Operator 136: -0.018597411709609345\n",
      "Operator 137: 0.05930585661327692\n",
      "Operator 138: -0.14596577897096302\n",
      "Operator 139: -0.16927694283649464\n",
      "Operator 140: -0.8187434667575338\n",
      "Operator 141: -0.052914962084396944\n",
      "Operator 142: -1.2045302201445434\n",
      "Operator 143: -0.09848323152832583\n",
      "Operator 144: 0.20860402466398456\n",
      "Operator 145: 0.010268394964913094\n",
      "Operator 146: 0.2760404952637933\n",
      "Operator 147: -0.09492880216174927\n",
      "Operator 148: 0.05473363730089387\n",
      "Operator 149: -0.6413817842382121\n",
      "Total gradient norm: 4.907090836496109\n",
      "Operators under consideration (1):\n",
      "[97]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.3490108222046195)]\n",
      "Operator(s) added to ansatz: [97]\n",
      "Gradients: [np.float64(-1.3490108222046195)]\n",
      "Initial energy: -19.70110078288155\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97]...\n",
      "Starting point: [np.float64(0.13645437138050934), np.float64(0.7573649783559021), np.float64(0.6591551923124144), np.float64(0.2978133349413172), np.float64(0.14531500986547297), np.float64(0.5449937036983796), np.float64(0.5151957292338114), np.float64(0.3732120882476478), np.float64(0.5247023179593616), np.float64(0.15943366441445378), np.float64(-0.11839363138129959), np.float64(-0.17626882069138372), np.float64(-0.13650606225988665), np.float64(0.15651705516069225), np.float64(-0.16269291507408298), np.float64(0.16948275557279116), np.float64(-0.13917726058681018), np.float64(0.11735058100586399), np.float64(0.091353462595552), np.float64(-0.09715950866886872), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.760989\n",
      "         Iterations: 28\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "Current energy: -19.760988787029415\n",
      "(change of -0.059888004147865104)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97]\n",
      "On iteration 21.\n",
      "\n",
      "*** ADAPT-VQE Iteration 22 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.5160421322364861\n",
      "Operator 1: 0.038788353515762056\n",
      "Operator 2: -0.1029151070879688\n",
      "Operator 3: 0.03621864663398236\n",
      "Operator 4: -0.4043683873316083\n",
      "Operator 5: 0.2753437454463789\n",
      "Operator 6: 0.011425164007043243\n",
      "Operator 7: 0.3595773697371194\n",
      "Operator 8: 0.09374431722004894\n",
      "Operator 9: 0.19528923265622677\n",
      "Operator 10: -0.14249915529655577\n",
      "Operator 11: 0.28158020140693363\n",
      "Operator 12: -0.08537160385625826\n",
      "Operator 13: 0.13574070216527495\n",
      "Operator 14: -0.3839956148504598\n",
      "Operator 15: 0.06673389743671176\n",
      "Operator 16: -0.8204615622407613\n",
      "Operator 17: -0.17259216221292725\n",
      "Operator 18: 0.9923146517923096\n",
      "Operator 19: 0.080643684628291\n",
      "Operator 21: 0.22430697645050068\n",
      "Operator 22: 0.34438443365098304\n",
      "Operator 23: -0.1435456723738062\n",
      "Operator 24: 0.23698245128245915\n",
      "Operator 25: 0.10235837874780541\n",
      "Operator 26: -5.109230424293898e-07\n",
      "Operator 27: -0.4725175945678698\n",
      "Operator 28: -0.015474098627771266\n",
      "Operator 29: 0.1674186477789706\n",
      "Operator 30: 0.5064927859454958\n",
      "Operator 31: 0.07694409032750077\n",
      "Operator 32: 0.35835832144923213\n",
      "Operator 33: -0.003752127217496992\n",
      "Operator 34: -0.21604978569475017\n",
      "Operator 35: -0.05663636012706784\n",
      "Operator 36: 0.9628555306369512\n",
      "Operator 37: 0.28306568924319186\n",
      "Operator 38: 0.7426453573360162\n",
      "Operator 39: 0.0680885949041384\n",
      "Operator 40: -0.52110317197902\n",
      "Operator 41: 0.8139008809381262\n",
      "Operator 42: -0.15284178090260825\n",
      "Operator 43: 0.20197770129446993\n",
      "Operator 44: 0.15258150418648692\n",
      "Operator 45: -0.15986800541219154\n",
      "Operator 46: 1.1678733775610795e-06\n",
      "Operator 47: 0.2315324596068694\n",
      "Operator 48: -0.49915262825092777\n",
      "Operator 49: 0.4128508748285976\n",
      "Operator 50: -0.26255848148480126\n",
      "Operator 51: -0.07933857426705372\n",
      "Operator 52: -0.10882537405446965\n",
      "Operator 53: 0.19636639811858383\n",
      "Operator 54: -0.24472435754408162\n",
      "Operator 55: 0.0676956864153174\n",
      "Operator 56: 0.844178071648199\n",
      "Operator 57: -0.8618900327396395\n",
      "Operator 58: 0.5163806377880892\n",
      "Operator 59: 0.1407910439308137\n",
      "Operator 60: -0.16093459740999566\n",
      "Operator 61: -0.09074821129206818\n",
      "Operator 62: 0.11979094874846151\n",
      "Operator 63: -0.2830160943779272\n",
      "Operator 64: 0.004394879333108951\n",
      "Operator 65: -0.25753498703325706\n",
      "Operator 66: -3.6075804906031105e-07\n",
      "Operator 67: -0.8914250644727297\n",
      "Operator 68: -0.133304687804858\n",
      "Operator 69: -0.6037564488181169\n",
      "Operator 70: 0.42610054441337636\n",
      "Operator 71: -0.12956761959453197\n",
      "Operator 72: -0.22524526988986276\n",
      "Operator 73: 0.020980173848755685\n",
      "Operator 74: 0.16334583159137778\n",
      "Operator 75: 0.016796765742263714\n",
      "Operator 76: 0.12502642685617904\n",
      "Operator 77: 0.5680215728175451\n",
      "Operator 78: 0.208327958583664\n",
      "Operator 79: 0.549889072069991\n",
      "Operator 80: -0.460355238815081\n",
      "Operator 81: 0.04225796274711606\n",
      "Operator 82: 2.1882173432616336e-07\n",
      "Operator 83: 0.030311495654807157\n",
      "Operator 84: 0.21715814211954104\n",
      "Operator 85: 0.33179528976537065\n",
      "Operator 86: 0.9868505843483005\n",
      "Operator 87: -0.034082942705325414\n",
      "Operator 88: 0.8456382303167775\n",
      "Operator 89: -0.41981856513678123\n",
      "Operator 90: 0.06914438284311153\n",
      "Operator 91: 0.266485324279097\n",
      "Operator 92: -0.11018796270438053\n",
      "Operator 93: 0.025404236749789668\n",
      "Operator 94: -0.04876996444220633\n",
      "Operator 95: -1.0246378679670956\n",
      "Operator 96: 0.0801144385484537\n",
      "Operator 97: -5.9032879401411595e-08\n",
      "Operator 98: 0.22765047653453882\n",
      "Operator 99: 0.573686845401901\n",
      "Operator 100: -0.464035767652935\n",
      "Operator 101: 0.40228722026764735\n",
      "Operator 102: -0.19356334049081264\n",
      "Operator 103: 0.13306492264399278\n",
      "Operator 104: -0.26977199938503205\n",
      "Operator 105: 0.4920921052249829\n",
      "Operator 106: -0.624050981380519\n",
      "Operator 107: 0.3030196938927924\n",
      "Operator 108: -0.18682370512875648\n",
      "Operator 109: 0.3861390927606967\n",
      "Operator 110: -0.32860637677983306\n",
      "Operator 111: 0.32947399082362244\n",
      "Operator 112: 0.17814501533433452\n",
      "Operator 113: -0.8371672387121493\n",
      "Operator 114: -0.2524724580416515\n",
      "Operator 115: -0.658000828466916\n",
      "Operator 116: -0.1526264563937311\n",
      "Operator 117: 0.08939013355060411\n",
      "Operator 118: -0.36740801409328844\n",
      "Operator 119: 0.06572818624435504\n",
      "Operator 120: -0.22565926674674266\n",
      "Operator 121: -0.3159542021098295\n",
      "Operator 122: 0.7501481070577616\n",
      "Operator 123: -0.7011626141663764\n",
      "Operator 124: -0.18119119135345524\n",
      "Operator 125: -0.07185695654526937\n",
      "Operator 126: -0.21924793021980005\n",
      "Operator 127: -0.1739999023991873\n",
      "Operator 128: 0.0405083296213459\n",
      "Operator 129: 0.0972169341228396\n",
      "Operator 130: -0.09496041760617235\n",
      "Operator 131: 0.8986247754747523\n",
      "Operator 132: 0.1680335546021381\n",
      "Operator 133: 0.06719390677084781\n",
      "Operator 134: -0.34635145315506305\n",
      "Operator 135: 0.17287313964041298\n",
      "Operator 136: -0.02823278693629405\n",
      "Operator 137: 0.07634645851955055\n",
      "Operator 138: -0.1320494293548448\n",
      "Operator 139: -0.1688510320625628\n",
      "Operator 140: -0.873249588037815\n",
      "Operator 141: -0.03909667265971815\n",
      "Operator 142: -0.23964035600597064\n",
      "Operator 143: 0.3907199144377966\n",
      "Operator 144: 0.31795124362219485\n",
      "Operator 145: 0.0641957279988711\n",
      "Operator 146: 0.23282738247111023\n",
      "Operator 147: -0.08290858024509612\n",
      "Operator 148: 0.0507292607975149\n",
      "Operator 149: -0.7867156312333423\n",
      "Total gradient norm: 4.680265920394913\n",
      "Operators under consideration (1):\n",
      "[95]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.0246378679670956)]\n",
      "Operator(s) added to ansatz: [95]\n",
      "Gradients: [np.float64(-1.0246378679670956)]\n",
      "Initial energy: -19.760988787029415\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95]...\n",
      "Starting point: [np.float64(0.12122125895354831), np.float64(0.7590620865668275), np.float64(0.6667005432174498), np.float64(0.284917888486966), np.float64(0.13338815313985397), np.float64(0.5575778483098718), np.float64(0.5275665838652835), np.float64(0.4218292838042357), np.float64(0.5710301622886934), np.float64(0.15687604247297335), np.float64(-0.11805855590175404), np.float64(-0.1760969112131759), np.float64(-0.13431070260214878), np.float64(0.15638106262012325), np.float64(-0.1741164839788298), np.float64(0.16817792753694652), np.float64(-0.1393745862016284), np.float64(0.12169606670519506), np.float64(0.09191756057517496), np.float64(-0.13780407123217825), np.float64(0.08925297113705627), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.794555\n",
      "         Iterations: 31\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 74\n",
      "\n",
      "Current energy: -19.7945545123739\n",
      "(change of -0.03356572534448432)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95]\n",
      "On iteration 22.\n",
      "\n",
      "*** ADAPT-VQE Iteration 23 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.38329533686956735\n",
      "Operator 1: 0.037803819458747855\n",
      "Operator 2: -0.1126969265479732\n",
      "Operator 3: 0.016116685978288255\n",
      "Operator 4: -0.3944944997040774\n",
      "Operator 5: 0.2670397941053915\n",
      "Operator 6: 0.0118007758167777\n",
      "Operator 7: 0.3478065583626069\n",
      "Operator 8: 0.09062541539037294\n",
      "Operator 9: 0.0695034551851836\n",
      "Operator 10: -0.3271132599300581\n",
      "Operator 11: 0.3142577606598126\n",
      "Operator 12: -0.1241229316217774\n",
      "Operator 13: 0.1384520242412931\n",
      "Operator 14: -0.38048812373739044\n",
      "Operator 15: 0.06620288684497931\n",
      "Operator 16: -0.8020839459431713\n",
      "Operator 17: -0.17120572264494133\n",
      "Operator 18: 1.5597603436651175\n",
      "Operator 19: 0.06932492962002705\n",
      "Operator 20: 0.04819179277618474\n",
      "Operator 21: 0.2274830605701974\n",
      "Operator 22: 0.4112891830109729\n",
      "Operator 23: -0.14914087612905094\n",
      "Operator 24: 0.23589554656054618\n",
      "Operator 25: 0.10219451835633109\n",
      "Operator 26: -4.3014400552543464e-07\n",
      "Operator 27: 0.09071396368133211\n",
      "Operator 28: 0.17156104777130082\n",
      "Operator 29: 0.16091399135228954\n",
      "Operator 30: 0.5781452203178002\n",
      "Operator 31: 0.09470589994726239\n",
      "Operator 32: 0.35526131427717056\n",
      "Operator 33: -0.0009487889767659952\n",
      "Operator 34: -0.21590900308625624\n",
      "Operator 35: -0.05553805310742907\n",
      "Operator 36: 0.8059758030858903\n",
      "Operator 37: 0.22843267096694475\n",
      "Operator 38: 0.8038031432727064\n",
      "Operator 39: 0.05047167458883699\n",
      "Operator 40: -0.5186672414686693\n",
      "Operator 41: 0.8109659303400117\n",
      "Operator 42: -0.1512311946315363\n",
      "Operator 43: 0.20258129734676417\n",
      "Operator 44: 0.1568029500372139\n",
      "Operator 45: -0.15467428792286914\n",
      "Operator 46: 1.1841703715376184e-06\n",
      "Operator 47: 0.13281951123881114\n",
      "Operator 48: -0.19991289538762663\n",
      "Operator 49: 0.24817053238808734\n",
      "Operator 50: -0.2577279511337063\n",
      "Operator 51: -0.09046149589675773\n",
      "Operator 52: -0.0973402204248404\n",
      "Operator 53: 0.19194383970351675\n",
      "Operator 54: -0.23750438967482457\n",
      "Operator 55: 0.06427201655762486\n",
      "Operator 56: 1.0388598730729897\n",
      "Operator 57: -1.103421067272782\n",
      "Operator 58: 0.6141441078034083\n",
      "Operator 59: 0.1165076981959026\n",
      "Operator 60: -0.15247477832820372\n",
      "Operator 61: -0.15197212987067102\n",
      "Operator 62: 0.11988609382268334\n",
      "Operator 63: -0.27979671202637413\n",
      "Operator 64: 0.005051575144463423\n",
      "Operator 65: -0.28737406241108016\n",
      "Operator 66: 0.14420601130844132\n",
      "Operator 67: -1.205530044370293\n",
      "Operator 68: -0.12471479803104811\n",
      "Operator 69: -0.6834179284248868\n",
      "Operator 70: 0.4643223625249449\n",
      "Operator 71: -0.15172874419644294\n",
      "Operator 72: -0.20441978767459054\n",
      "Operator 73: 0.020072717685258628\n",
      "Operator 74: 0.15766290791781543\n",
      "Operator 75: 0.016276727692179374\n",
      "Operator 76: 0.12536947102411505\n",
      "Operator 77: 0.7883968858117897\n",
      "Operator 78: 0.22945836063264788\n",
      "Operator 79: 0.6544903125860895\n",
      "Operator 80: -0.5045024036018053\n",
      "Operator 81: 0.05787742076414109\n",
      "Operator 82: 6.334201661187949e-08\n",
      "Operator 83: 0.021826352312051232\n",
      "Operator 84: 0.21926389868019455\n",
      "Operator 85: 0.33257802135214526\n",
      "Operator 86: 0.8740985165015018\n",
      "Operator 87: -0.09699296194267942\n",
      "Operator 88: 0.9475939408452677\n",
      "Operator 89: -0.4958860826927079\n",
      "Operator 90: 0.0842998122677714\n",
      "Operator 91: 0.26827463034822646\n",
      "Operator 92: -0.10353760573776095\n",
      "Operator 93: 0.02249881920964689\n",
      "Operator 94: -0.04320962979044101\n",
      "Operator 95: 7.463398263337896e-08\n",
      "Operator 96: 0.24916981812330213\n",
      "Operator 97: 0.018117261710848116\n",
      "Operator 98: 0.2804002235311392\n",
      "Operator 99: 0.5722625850192578\n",
      "Operator 100: -0.4660726863281218\n",
      "Operator 101: 0.385118209337673\n",
      "Operator 102: -0.19386394475263635\n",
      "Operator 103: 0.1310970415982392\n",
      "Operator 104: -0.31947141654952416\n",
      "Operator 105: 0.5821736914774279\n",
      "Operator 106: -0.6416301716297057\n",
      "Operator 107: 0.34420530990797477\n",
      "Operator 108: -0.18600004382481872\n",
      "Operator 109: 0.38220462684166034\n",
      "Operator 110: -0.3200746291350665\n",
      "Operator 111: 0.32068048770059165\n",
      "Operator 112: 0.17640061096634438\n",
      "Operator 113: -1.2108521435632027\n",
      "Operator 114: -0.26437681504376276\n",
      "Operator 115: -0.7751863438967643\n",
      "Operator 116: -0.14435380194521802\n",
      "Operator 117: 0.0506979267301678\n",
      "Operator 118: -0.34797092096265747\n",
      "Operator 119: 0.06825877333678564\n",
      "Operator 120: -0.22600855104737225\n",
      "Operator 121: -0.31729280817982963\n",
      "Operator 122: 0.037795357390991435\n",
      "Operator 123: -0.9090531181550096\n",
      "Operator 124: -0.1821059819856665\n",
      "Operator 125: -0.12011538881139476\n",
      "Operator 126: -0.22591667923122785\n",
      "Operator 127: -0.17037944288040358\n",
      "Operator 128: 0.04010858814917834\n",
      "Operator 129: 0.09209110139496005\n",
      "Operator 130: -0.09532153960881268\n",
      "Operator 131: 1.3790587541600785\n",
      "Operator 132: 0.20682805737238627\n",
      "Operator 133: 0.07666649156452118\n",
      "Operator 134: -0.37126832556282163\n",
      "Operator 135: 0.22847217690559948\n",
      "Operator 136: -0.026586477110713852\n",
      "Operator 137: 0.07077608523461507\n",
      "Operator 138: -0.12613163536878855\n",
      "Operator 139: -0.16938850116134418\n",
      "Operator 140: 0.005418746593760216\n",
      "Operator 141: 0.49592437213712415\n",
      "Operator 142: -0.2141830491540965\n",
      "Operator 143: 0.49069201331850654\n",
      "Operator 144: 0.34169496039337255\n",
      "Operator 145: 0.06445747335614782\n",
      "Operator 146: 0.21374404937369532\n",
      "Operator 147: -0.07626895516267217\n",
      "Operator 148: 0.049133938088284645\n",
      "Operator 149: -0.23653430000651307\n",
      "Total gradient norm: 4.996698032362319\n",
      "Operators under consideration (1):\n",
      "[18]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.5597603436651175)]\n",
      "Operator(s) added to ansatz: [18]\n",
      "Gradients: [np.float64(1.5597603436651175)]\n",
      "Initial energy: -19.7945545123739\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18]...\n",
      "Starting point: [np.float64(0.10508607711262503), np.float64(0.7599881361957403), np.float64(0.6706640968981231), np.float64(0.28260298982658993), np.float64(0.13268741101046733), np.float64(0.565358549819412), np.float64(0.5587777258502539), np.float64(0.4604510185252699), np.float64(0.5767779391698896), np.float64(0.15561159712496547), np.float64(-0.11782273793194005), np.float64(-0.1760634977201223), np.float64(-0.12566603714101665), np.float64(0.15679041940185945), np.float64(-0.17328844382959593), np.float64(0.15535677360785452), np.float64(-0.13956506863293608), np.float64(0.12174257001760651), np.float64(0.09351487504042141), np.float64(-0.135067144418902), np.float64(0.09779939707466012), np.float64(0.06582962095161939), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -19.910204\n",
      "         Iterations: 32\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "Current energy: -19.910204481966883\n",
      "(change of -0.11564996959298313)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18]\n",
      "On iteration 23.\n",
      "\n",
      "*** ADAPT-VQE Iteration 24 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.47343109865749045\n",
      "Operator 1: 0.04128341085424658\n",
      "Operator 2: -0.1277342701594755\n",
      "Operator 3: -0.08564985343285557\n",
      "Operator 4: -0.38520475242975954\n",
      "Operator 5: 0.24289595990578192\n",
      "Operator 6: 0.011144317767129874\n",
      "Operator 7: 0.314370435006037\n",
      "Operator 8: 0.08181049764299786\n",
      "Operator 9: 0.00285294444128573\n",
      "Operator 10: -0.25160826251173146\n",
      "Operator 11: 0.4189535650840559\n",
      "Operator 12: -0.2944315443029448\n",
      "Operator 13: 0.06988785643870732\n",
      "Operator 14: -0.3774096184181478\n",
      "Operator 15: 0.05526362801464239\n",
      "Operator 16: -0.746651006874477\n",
      "Operator 17: -0.164445009717403\n",
      "Operator 18: -1.2124211002420256e-07\n",
      "Operator 19: -0.04279246244976506\n",
      "Operator 20: 0.2782725562522847\n",
      "Operator 21: 0.23652621439227545\n",
      "Operator 22: 0.5862611282153833\n",
      "Operator 23: -0.16094824314742595\n",
      "Operator 24: 0.22872726261285536\n",
      "Operator 25: 0.10203713092648238\n",
      "Operator 26: -3.681735072369463e-07\n",
      "Operator 27: 0.1930254303302893\n",
      "Operator 28: 0.20402482224024865\n",
      "Operator 29: 0.07426658008719776\n",
      "Operator 30: 0.7829935302921764\n",
      "Operator 31: 0.14437195234864095\n",
      "Operator 32: 0.3362213423240394\n",
      "Operator 33: 0.007788171678549774\n",
      "Operator 34: -0.2163643772971483\n",
      "Operator 35: -0.05272680637877947\n",
      "Operator 36: 1.3399127735893572\n",
      "Operator 37: 0.22939164966154912\n",
      "Operator 38: 0.9795130412649888\n",
      "Operator 39: -0.009739397906460555\n",
      "Operator 40: -0.5119133007991787\n",
      "Operator 41: 0.7995615597552503\n",
      "Operator 42: -0.14647543637691446\n",
      "Operator 43: 0.20466241484215325\n",
      "Operator 44: 0.16818852534601614\n",
      "Operator 45: -0.1398740779403182\n",
      "Operator 46: 1.159065868172604e-06\n",
      "Operator 47: 0.1340439418698029\n",
      "Operator 48: -0.10086937592592841\n",
      "Operator 49: 0.31738075100755053\n",
      "Operator 50: -0.2750294919885234\n",
      "Operator 51: -0.14108557509251074\n",
      "Operator 52: -0.056951265896099515\n",
      "Operator 53: 0.19688126097077532\n",
      "Operator 54: -0.21651101306152615\n",
      "Operator 55: 0.05649884389655818\n",
      "Operator 56: 1.3615642305683928\n",
      "Operator 57: 0.400693013068955\n",
      "Operator 58: 0.923332702814115\n",
      "Operator 59: -0.05299883721667134\n",
      "Operator 60: -0.1643442947357524\n",
      "Operator 61: -0.333720851761645\n",
      "Operator 62: 0.11495987369087285\n",
      "Operator 63: -0.26628018923715885\n",
      "Operator 64: 0.006751582693489008\n",
      "Operator 65: -0.2686044403647844\n",
      "Operator 66: -0.020187981179986422\n",
      "Operator 67: -1.2928314223525539\n",
      "Operator 68: 0.04107633839164941\n",
      "Operator 69: -0.9341469224904886\n",
      "Operator 70: 0.517729141955245\n",
      "Operator 71: -0.17442355187504188\n",
      "Operator 72: -0.14475072309371287\n",
      "Operator 73: 0.022196179136759028\n",
      "Operator 74: 0.1414996908516278\n",
      "Operator 75: 0.014947753683717868\n",
      "Operator 76: 0.12657871632625395\n",
      "Operator 77: 1.1691318029073958\n",
      "Operator 78: 0.2815576943677668\n",
      "Operator 79: 0.9639662529319263\n",
      "Operator 80: -0.66455552888621\n",
      "Operator 81: 0.06422754346011403\n",
      "Operator 83: -0.0008713694526742075\n",
      "Operator 84: 0.22582269324144666\n",
      "Operator 85: 0.33536816496262856\n",
      "Operator 86: 1.2987564126791022\n",
      "Operator 87: -0.092604756020293\n",
      "Operator 88: 1.1010659606883197\n",
      "Operator 89: -0.690246953401665\n",
      "Operator 90: 0.11680921313215717\n",
      "Operator 91: 0.2684411153335643\n",
      "Operator 92: -0.08915278260584603\n",
      "Operator 93: 0.015032351418218955\n",
      "Operator 94: -0.028479361110905415\n",
      "Operator 95: -5.04777887188861e-07\n",
      "Operator 96: 0.2147112154438245\n",
      "Operator 97: 0.00368853965075209\n",
      "Operator 98: 0.4319833537861851\n",
      "Operator 99: 0.5618628077406118\n",
      "Operator 100: -0.48412240528308637\n",
      "Operator 101: 0.3385358385839903\n",
      "Operator 102: -0.19500291304908923\n",
      "Operator 103: 0.12628050169542349\n",
      "Operator 104: -0.37286400547953213\n",
      "Operator 105: 0.5829451317817602\n",
      "Operator 106: -0.7069562562693199\n",
      "Operator 107: 0.484312856636456\n",
      "Operator 108: -0.13502571254912304\n",
      "Operator 109: 0.35295549052748476\n",
      "Operator 110: -0.28773090833011705\n",
      "Operator 111: 0.2949555361703622\n",
      "Operator 112: 0.1686778140489648\n",
      "Operator 113: -1.2963936362999833\n",
      "Operator 114: -0.23579833850282522\n",
      "Operator 115: -1.1165955705772397\n",
      "Operator 116: -0.1099453907351593\n",
      "Operator 117: -0.009407391728440871\n",
      "Operator 118: -0.29511912911319027\n",
      "Operator 119: 0.07573955187205639\n",
      "Operator 120: -0.22761329281826442\n",
      "Operator 121: -0.3215379004293701\n",
      "Operator 122: -0.5142247580916683\n",
      "Operator 123: -1.0977180285316146\n",
      "Operator 124: -0.08087626899129907\n",
      "Operator 125: -0.24154781703376726\n",
      "Operator 126: -0.22137577098803068\n",
      "Operator 127: -0.15371361689656549\n",
      "Operator 128: 0.03650441400108143\n",
      "Operator 129: 0.07871696586240115\n",
      "Operator 130: -0.09651089888239062\n",
      "Operator 131: 0.05128419393270689\n",
      "Operator 132: -0.36484233916012243\n",
      "Operator 133: 0.19409508014993254\n",
      "Operator 134: -0.3557963770358753\n",
      "Operator 135: 0.38279726494028177\n",
      "Operator 136: -0.018552469037430276\n",
      "Operator 137: 0.05138507910395612\n",
      "Operator 138: -0.1104268859677676\n",
      "Operator 139: -0.1711693663597913\n",
      "Operator 140: 0.058045413212993904\n",
      "Operator 141: 0.5763923947501124\n",
      "Operator 142: -0.2557173710506178\n",
      "Operator 143: 0.7239486966722988\n",
      "Operator 144: 0.3945533485169428\n",
      "Operator 145: 0.05882147881432515\n",
      "Operator 146: 0.16280347240958798\n",
      "Operator 147: -0.05837749950032388\n",
      "Operator 148: 0.045091909346585274\n",
      "Operator 149: -0.09504975439640971\n",
      "Total gradient norm: 5.359769369969305\n",
      "Operators under consideration (1):\n",
      "[56]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.3615642305683928)]\n",
      "Operator(s) added to ansatz: [56]\n",
      "Gradients: [np.float64(1.3615642305683928)]\n",
      "Initial energy: -19.910204481966883\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56]...\n",
      "Starting point: [np.float64(0.1027454020609361), np.float64(0.7625915707073735), np.float64(0.6818597491081884), np.float64(0.27858756763826503), np.float64(0.13311975229923204), np.float64(0.5876728022015264), np.float64(0.6211969860033872), np.float64(0.5689175931432747), np.float64(0.5885064865341355), np.float64(0.15237569818900107), np.float64(-0.1171711171731426), np.float64(-0.17599848242270433), np.float64(-0.1038676394319773), np.float64(0.15809468384503256), np.float64(-0.17021574922751423), np.float64(0.1142163733103335), np.float64(-0.1401014611093027), np.float64(0.12146731475558939), np.float64(0.09802779174306951), np.float64(-0.1190278972979581), np.float64(0.11205713469727334), np.float64(0.12726510201795768), np.float64(-0.13745938966194904), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -20.004838\n",
      "         Iterations: 32\n",
      "         Function evaluations: 48\n",
      "         Gradient evaluations: 48\n",
      "\n",
      "Current energy: -20.004837690701578\n",
      "(change of -0.09463320873469527)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56]\n",
      "On iteration 24.\n",
      "\n",
      "*** ADAPT-VQE Iteration 25 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -0.3524830087836671\n",
      "Operator 1: 0.026168861548724665\n",
      "Operator 2: -0.1034442850828291\n",
      "Operator 3: -0.11482698953841067\n",
      "Operator 4: -0.2537614394968519\n",
      "Operator 5: 0.195357638689279\n",
      "Operator 6: 0.00784622989998951\n",
      "Operator 7: 0.23731118461896983\n",
      "Operator 8: 0.06141862385209684\n",
      "Operator 9: -0.038722455188861485\n",
      "Operator 10: -0.0883127562171159\n",
      "Operator 11: 0.12974657623902758\n",
      "Operator 12: -0.21559020380462485\n",
      "Operator 13: 0.12034298408264843\n",
      "Operator 14: -0.2419979064361846\n",
      "Operator 15: 0.037076974628875575\n",
      "Operator 16: -0.6010467904439226\n",
      "Operator 17: -0.13781091129339962\n",
      "Operator 18: -0.006747653293246415\n",
      "Operator 19: -0.09282620982945303\n",
      "Operator 20: 0.3979944422590628\n",
      "Operator 21: 0.30590492620695986\n",
      "Operator 22: 0.851990910461999\n",
      "Operator 23: -0.24185535219083687\n",
      "Operator 24: 0.22905446547281436\n",
      "Operator 25: 0.09942070295238403\n",
      "Operator 26: -1.889841896307587e-07\n",
      "Operator 27: -0.026494350061729007\n",
      "Operator 28: 0.35376852924414354\n",
      "Operator 29: 0.08293505295224707\n",
      "Operator 30: 0.9801009909696291\n",
      "Operator 31: 0.23805864191172216\n",
      "Operator 32: 0.32517791048702993\n",
      "Operator 33: -0.01616927511723328\n",
      "Operator 34: -0.21568541998328472\n",
      "Operator 35: -0.04713816022527889\n",
      "Operator 36: -0.06440633992595547\n",
      "Operator 37: -0.004474174685775111\n",
      "Operator 38: 1.4389665147558142\n",
      "Operator 39: 0.0011037456519536076\n",
      "Operator 40: -0.4725940480683823\n",
      "Operator 41: 0.8375169156421152\n",
      "Operator 42: -0.14110858535634213\n",
      "Operator 43: 0.20630915852418735\n",
      "Operator 44: 0.19055963232302273\n",
      "Operator 45: -0.10584320796361213\n",
      "Operator 46: 9.227585078264156e-07\n",
      "Operator 47: 0.05282860764711607\n",
      "Operator 48: -0.09750454191311883\n",
      "Operator 49: 0.23226263626221227\n",
      "Operator 50: -0.21366620465101482\n",
      "Operator 51: -0.12481178004025564\n",
      "Operator 52: -0.018909437561155158\n",
      "Operator 53: 0.11811039115767613\n",
      "Operator 54: -0.1705585246032421\n",
      "Operator 55: 0.0415525332395108\n",
      "Operator 56: -3.9923693877513116e-07\n",
      "Operator 57: -0.04238267366571016\n",
      "Operator 58: 1.356285454127962\n",
      "Operator 59: -0.06712943419700436\n",
      "Operator 60: -0.14853492085634643\n",
      "Operator 61: -0.5463732969744115\n",
      "Operator 62: 0.10804514323487918\n",
      "Operator 63: -0.2661111254808083\n",
      "Operator 64: 0.010443501067961755\n",
      "Operator 65: 0.043078971909723665\n",
      "Operator 66: 0.060820461897619325\n",
      "Operator 67: 0.08460312193379976\n",
      "Operator 68: 0.06292471903114999\n",
      "Operator 69: -1.3558978320338309\n",
      "Operator 70: 0.6605220701412954\n",
      "Operator 71: -0.3316387811252951\n",
      "Operator 72: -0.0681153704312838\n",
      "Operator 73: 0.0362431947213328\n",
      "Operator 74: 0.11398906916556986\n",
      "Operator 75: 0.012143011344497912\n",
      "Operator 76: 0.12797031494614108\n",
      "Operator 77: -0.09558652219345462\n",
      "Operator 78: 0.2094323432771034\n",
      "Operator 79: 1.404498426679297\n",
      "Operator 80: -0.8010669029584838\n",
      "Operator 81: 0.1265193829312572\n",
      "Operator 83: -0.05073583620807817\n",
      "Operator 84: 0.2357465702486402\n",
      "Operator 85: 0.33869660698817217\n",
      "Operator 86: -0.09149965939326371\n",
      "Operator 87: -0.07656062077027373\n",
      "Operator 88: 1.384846796327104\n",
      "Operator 89: -0.7808922664931811\n",
      "Operator 90: 0.165728989265709\n",
      "Operator 91: 0.29682384087904723\n",
      "Operator 92: -0.05804209861534297\n",
      "Operator 93: -0.009043491139392297\n",
      "Operator 94: -0.0009207289381437788\n",
      "Operator 95: 1.9304275624360348e-07\n",
      "Operator 96: 0.024630492568079077\n",
      "Operator 97: 0.01955138081302814\n",
      "Operator 98: 0.5596781093154922\n",
      "Operator 99: 0.5753774241850667\n",
      "Operator 100: -0.5482218120120532\n",
      "Operator 101: 0.21483570280230557\n",
      "Operator 102: -0.1985782759923976\n",
      "Operator 103: 0.11434572732853535\n",
      "Operator 104: -0.0702133323350032\n",
      "Operator 105: -0.017169015524755228\n",
      "Operator 106: -0.4195907837150611\n",
      "Operator 107: 0.4899339185746643\n",
      "Operator 108: -0.15226952900990992\n",
      "Operator 109: 0.30242693107414687\n",
      "Operator 110: -0.21206369678665055\n",
      "Operator 111: 0.2314806636773739\n",
      "Operator 112: 0.1405130930111118\n",
      "Operator 113: 0.07481472737664871\n",
      "Operator 114: -0.2099847367379345\n",
      "Operator 115: -1.6331400447103488\n",
      "Operator 116: -0.09341109507609376\n",
      "Operator 117: -0.19390861609143645\n",
      "Operator 118: -0.1579484207756468\n",
      "Operator 119: 0.09937135516648463\n",
      "Operator 120: -0.22736675628034317\n",
      "Operator 121: -0.32763783941349267\n",
      "Operator 122: 0.0005624584672973557\n",
      "Operator 123: -1.4730147254809451\n",
      "Operator 124: -0.2153889021166695\n",
      "Operator 125: -0.3024784870793474\n",
      "Operator 126: -0.32474280925976434\n",
      "Operator 127: -0.11569426583659678\n",
      "Operator 128: 0.07707414194238957\n",
      "Operator 129: 0.05323938372580089\n",
      "Operator 130: -0.09811164123977406\n",
      "Operator 131: 0.019905468355590955\n",
      "Operator 132: -0.405493809617805\n",
      "Operator 133: 0.2788435306089419\n",
      "Operator 134: -0.48238182720892964\n",
      "Operator 135: 0.6419163226384546\n",
      "Operator 136: -0.006095521982297493\n",
      "Operator 137: 0.013431816889538789\n",
      "Operator 138: -0.07711839723173262\n",
      "Operator 139: -0.1729564910364394\n",
      "Operator 140: 0.04077401853456502\n",
      "Operator 141: 0.5788889620229882\n",
      "Operator 142: -0.13418675801129268\n",
      "Operator 143: 0.8882245011828227\n",
      "Operator 144: 0.4741721648620577\n",
      "Operator 145: 0.0681337433301314\n",
      "Operator 146: 0.008417824619997297\n",
      "Operator 147: -0.025742544099094815\n",
      "Operator 148: 0.03750698683386047\n",
      "Operator 149: -0.21666601062382435\n",
      "Total gradient norm: 5.116873032665352\n",
      "Operators under consideration (1):\n",
      "[115]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-1.6331400447103488)]\n",
      "Operator(s) added to ansatz: [115]\n",
      "Gradients: [np.float64(-1.6331400447103488)]\n",
      "Initial energy: -20.004837690701578\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115]...\n",
      "Starting point: [np.float64(0.03569980084307572), np.float64(0.7683282998405698), np.float64(0.7071167657224665), np.float64(0.2807487753407975), np.float64(0.09844778783900089), np.float64(0.6370686760055417), np.float64(0.683719576083328), np.float64(0.6435191133383683), np.float64(0.639662220519992), np.float64(0.14459849595128774), np.float64(-0.11603759830732975), np.float64(-0.17575280205207292), np.float64(-0.04851724881362236), np.float64(0.16000142843232484), np.float64(-0.17402211024238734), np.float64(0.08623129861117373), np.float64(-0.140977214669019), np.float64(0.1343430825293309), np.float64(0.10567838694995041), np.float64(-0.12108283923885658), np.float64(0.12757908527563075), np.float64(0.114675391417699), np.float64(-0.14771313018260881), np.float64(-0.12636149113899514), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.133826\n",
      "         Iterations: 22\n",
      "         Function evaluations: 78\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "Current energy: -20.13382622554317\n",
      "(change of -0.1289885348415929)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115]\n",
      "On iteration 25.\n",
      "\n",
      "*** ADAPT-VQE Iteration 26 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.0002682881346921656\n",
      "Operator 1: -0.0005709456173360675\n",
      "Operator 2: -7.092259409621077e-05\n",
      "Operator 3: 2.1417588301573393e-07\n",
      "Operator 4: -9.653986853028033e-05\n",
      "Operator 5: 3.5957672862726834e-05\n",
      "Operator 6: 0.00011927891999859223\n",
      "Operator 7: -4.082482654756448e-05\n",
      "Operator 8: 0.00015636132974940277\n",
      "Operator 9: 0.0002965427906211904\n",
      "Operator 10: 5.143752909532451e-06\n",
      "Operator 11: 3.461726465080195e-05\n",
      "Operator 12: 0.0004174787732218804\n",
      "Operator 13: 1.123357821473936e-05\n",
      "Operator 14: -1.926730596694666e-05\n",
      "Operator 15: 0.0001183847975914391\n",
      "Operator 16: -0.00012525677913709346\n",
      "Operator 17: -0.00018171666831054358\n",
      "Operator 18: -0.08406373634776453\n",
      "Operator 19: -0.10500322117669737\n",
      "Operator 20: 0.4857704687655997\n",
      "Operator 21: 0.3231794261601604\n",
      "Operator 22: 1.2151680201561923\n",
      "Operator 23: -0.41942302615829435\n",
      "Operator 24: 0.20227700649019534\n",
      "Operator 25: 0.09265991645755214\n",
      "Operator 26: 0.00011490868034853507\n",
      "Operator 27: -0.01452724245884946\n",
      "Operator 28: 0.45201113737811466\n",
      "Operator 29: 0.142685859396734\n",
      "Operator 30: 1.0773741468056388\n",
      "Operator 31: 0.2571017675056401\n",
      "Operator 32: 0.26529343163564517\n",
      "Operator 33: -0.09769306158077447\n",
      "Operator 34: -0.21550589392566677\n",
      "Operator 35: -0.039500633559570966\n",
      "Operator 36: -0.09535904105987744\n",
      "Operator 37: -0.14919449508795227\n",
      "Operator 38: -0.21688422739039379\n",
      "Operator 39: -0.01838375755025829\n",
      "Operator 40: -0.24844060089717546\n",
      "Operator 41: 0.896791515365468\n",
      "Operator 42: -0.13401515599885142\n",
      "Operator 43: 0.20800872254614644\n",
      "Operator 44: 0.22129559130719037\n",
      "Operator 45: -5.001195202935272e-06\n",
      "Operator 46: -0.00017144873753114693\n",
      "Operator 47: 0.00038208095633662786\n",
      "Operator 48: -0.00024884235672895105\n",
      "Operator 49: -0.00023210177243346286\n",
      "Operator 50: 7.630339986464639e-05\n",
      "Operator 51: 3.393021995285253e-05\n",
      "Operator 52: -4.002089482036796e-05\n",
      "Operator 53: 7.489696551332605e-05\n",
      "Operator 54: -5.3037316302862395e-05\n",
      "Operator 55: -0.0001354683299076415\n",
      "Operator 56: 0.00042993882561704916\n",
      "Operator 57: 0.03403771629570267\n",
      "Operator 58: -0.0010889392022591631\n",
      "Operator 59: -0.5954779078133937\n",
      "Operator 60: -0.18466508191963144\n",
      "Operator 61: -0.8375362204460707\n",
      "Operator 62: 0.10652302095372054\n",
      "Operator 63: -0.2536256162396498\n",
      "Operator 64: 0.015978071236003217\n",
      "Operator 65: 6.437288153238185e-05\n",
      "Operator 66: 0.000940885238573802\n",
      "Operator 67: 0.04859203594881401\n",
      "Operator 68: 0.08784015007152826\n",
      "Operator 69: 0.03865794229394967\n",
      "Operator 70: 0.6425505712956239\n",
      "Operator 71: -0.42798774233394693\n",
      "Operator 72: 0.04279443709958195\n",
      "Operator 73: 0.0652963026858094\n",
      "Operator 74: 0.07170986714469285\n",
      "Operator 75: 0.007978103239327808\n",
      "Operator 76: 0.12913158131184713\n",
      "Operator 77: -0.14450941324608088\n",
      "Operator 78: -0.28770977693897865\n",
      "Operator 79: -0.10996272868482831\n",
      "Operator 80: -1.023513677305123\n",
      "Operator 81: 0.3536288409011529\n",
      "Operator 82: 0.00016211910889447578\n",
      "Operator 83: -0.12303934979805105\n",
      "Operator 84: 0.24872910401936302\n",
      "Operator 85: 0.3418916085664852\n",
      "Operator 86: -0.06196429324282521\n",
      "Operator 87: -0.0569110894359743\n",
      "Operator 88: -0.13406397073614892\n",
      "Operator 89: -0.8494833223617181\n",
      "Operator 90: 0.22523069947599889\n",
      "Operator 91: 0.31997701014463786\n",
      "Operator 92: -0.0018362658302382695\n",
      "Operator 93: -0.05221136093668191\n",
      "Operator 94: 0.034890555816043514\n",
      "Operator 95: 0.0901498405674401\n",
      "Operator 96: 0.08023015255068944\n",
      "Operator 97: 0.187538298391524\n",
      "Operator 98: -0.03225786013512043\n",
      "Operator 99: 0.5398482027953994\n",
      "Operator 100: -0.651068622924594\n",
      "Operator 101: -0.017397299904556563\n",
      "Operator 102: -0.20858323823556196\n",
      "Operator 103: 0.09550104259991378\n",
      "Operator 104: -8.869858546378673e-05\n",
      "Operator 105: 0.00010082046374472464\n",
      "Operator 106: -5.909691946381912e-05\n",
      "Operator 107: -3.435466174716781e-05\n",
      "Operator 108: 3.2659061031878323e-06\n",
      "Operator 109: 0.0001715853887144811\n",
      "Operator 110: -0.00014677598536397317\n",
      "Operator 111: 6.264675316656276e-05\n",
      "Operator 112: 0.00014998598367108057\n",
      "Operator 113: 0.09990573813400552\n",
      "Operator 114: 0.2318474809876636\n",
      "Operator 115: 3.925790878527386e-05\n",
      "Operator 116: -0.03328614010467905\n",
      "Operator 117: -0.5394945375807615\n",
      "Operator 118: 0.09687897869889611\n",
      "Operator 119: 0.15238919868493395\n",
      "Operator 120: -0.22411367257678338\n",
      "Operator 121: -0.3345527870657075\n",
      "Operator 122: -0.08175048416497262\n",
      "Operator 123: -0.08553879517147914\n",
      "Operator 124: 0.1632652573381266\n",
      "Operator 125: -0.2931445787173521\n",
      "Operator 126: -0.41556593976568423\n",
      "Operator 127: -0.04549238030227419\n",
      "Operator 128: 0.17961203390490885\n",
      "Operator 129: 0.02010500546611706\n",
      "Operator 130: -0.09990236162206567\n",
      "Operator 131: -0.025126461776963156\n",
      "Operator 132: -0.06390308015109779\n",
      "Operator 133: 0.332670668920204\n",
      "Operator 134: -0.25670307932761094\n",
      "Operator 135: 0.7932309271802124\n",
      "Operator 136: 0.01400370911872699\n",
      "Operator 137: -0.05907119723038815\n",
      "Operator 138: -0.029931681855805362\n",
      "Operator 139: -0.17323709469267207\n",
      "Operator 140: 0.09063571249918309\n",
      "Operator 141: 0.23822996167492555\n",
      "Operator 142: 0.03169841935914737\n",
      "Operator 143: 0.8869979967568523\n",
      "Operator 144: 0.526482912358711\n",
      "Operator 145: 0.07208041105599179\n",
      "Operator 146: -0.2728848650850603\n",
      "Operator 147: 0.016519700418698224\n",
      "Operator 148: 0.02862720243575471\n",
      "Operator 149: -0.14422903506120308\n",
      "Total gradient norm: 3.560085780821444\n",
      "Operators under consideration (1):\n",
      "[22]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(1.2151680201561923)]\n",
      "Operator(s) added to ansatz: [22]\n",
      "Gradients: [np.float64(1.2151680201561923)]\n",
      "Initial energy: -20.13382622554317\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22]...\n",
      "Starting point: [np.float64(1.1259333160481647e-05), np.float64(0.7854129803488726), np.float64(0.7853900362899987), np.float64(0.28888228096271673), np.float64(5.926107098420031e-05), np.float64(0.7853330706550735), np.float64(0.7853676993510745), np.float64(0.7853594131882259), np.float64(0.7853514450226743), np.float64(0.13121881763815818), np.float64(-0.11461075973284864), np.float64(-0.1753295740422467), np.float64(4.0845707658452303e-05), np.float64(0.16199893708681162), np.float64(-0.17570026113155698), np.float64(6.685924155626957e-05), np.float64(-0.14205465745982887), np.float64(0.16231569218935119), np.float64(0.11708060793455), np.float64(-0.09683437345313217), np.float64(0.15511890085318933), np.float64(0.12615624121953462), np.float64(-0.16421152545709197), np.float64(-0.13617850357235275), np.float64(0.13085443496696647), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.212003\n",
      "         Iterations: 26\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 64\n",
      "\n",
      "Current energy: -20.212003146027694\n",
      "(change of -0.0781769204845233)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22]\n",
      "On iteration 26.\n",
      "\n",
      "*** ADAPT-VQE Iteration 27 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 6.014207890802509e-05\n",
      "Operator 1: -2.9340479547648648e-05\n",
      "Operator 2: -1.4946167388165017e-05\n",
      "Operator 3: -1.5779785440057014e-05\n",
      "Operator 4: 1.7278268513035394e-05\n",
      "Operator 5: -1.2533227675803595e-05\n",
      "Operator 6: 8.486994403729891e-06\n",
      "Operator 7: -2.2447598212411167e-05\n",
      "Operator 8: 1.4464097607359104e-05\n",
      "Operator 9: 2.6524145548822986e-05\n",
      "Operator 10: 4.861918137852132e-05\n",
      "Operator 11: 2.7431631198227002e-06\n",
      "Operator 12: 4.183898811226279e-05\n",
      "Operator 13: 2.315120422504216e-05\n",
      "Operator 14: 2.2930921425205937e-05\n",
      "Operator 15: -9.16121512069585e-06\n",
      "Operator 16: 3.0397567077201254e-05\n",
      "Operator 17: -8.012977353820844e-06\n",
      "Operator 18: -0.09897614393524831\n",
      "Operator 19: 0.06576142993800446\n",
      "Operator 20: 0.3995971480082244\n",
      "Operator 21: -0.040849706986085636\n",
      "Operator 22: 2.2029507670636617e-05\n",
      "Operator 23: 0.1496108792390971\n",
      "Operator 24: 0.3811459598885067\n",
      "Operator 25: 0.10741361263136968\n",
      "Operator 26: 2.9373826569983454e-05\n",
      "Operator 27: -0.003495469508473318\n",
      "Operator 28: 0.3543141541432588\n",
      "Operator 29: 0.27117830662570785\n",
      "Operator 30: 0.23014162422366718\n",
      "Operator 31: -0.13408856976338743\n",
      "Operator 32: 0.330821613163627\n",
      "Operator 33: 0.2254002542961565\n",
      "Operator 34: -0.1935664208034203\n",
      "Operator 35: -0.036381954924382544\n",
      "Operator 36: -0.1113351612491784\n",
      "Operator 37: -0.11650375334954276\n",
      "Operator 38: -0.3053247102050895\n",
      "Operator 39: -0.0002113648233782491\n",
      "Operator 40: 0.1102864520258878\n",
      "Operator 41: 0.044249970553446344\n",
      "Operator 42: -0.11452662887723358\n",
      "Operator 43: 0.2009989614275622\n",
      "Operator 44: 0.1817383953223806\n",
      "Operator 45: 7.128426143345216e-06\n",
      "Operator 46: -2.081958527697004e-05\n",
      "Operator 47: 2.7346888165025174e-05\n",
      "Operator 48: -2.4956206027045708e-05\n",
      "Operator 49: -5.163262428843629e-05\n",
      "Operator 50: 1.0428291152224335e-05\n",
      "Operator 51: 1.8010076339848233e-05\n",
      "Operator 52: 4.480130421731694e-06\n",
      "Operator 53: -9.612285594802787e-06\n",
      "Operator 54: 5.323857700641199e-06\n",
      "Operator 55: -1.3971610378138568e-05\n",
      "Operator 56: 2.403653061333582e-05\n",
      "Operator 57: 0.04067782360837742\n",
      "Operator 58: -0.15816530294337058\n",
      "Operator 59: -0.5036020961853422\n",
      "Operator 60: 0.0860273996128637\n",
      "Operator 61: 0.01097862817116064\n",
      "Operator 62: 0.3155551270081285\n",
      "Operator 63: -0.2838209439023977\n",
      "Operator 64: 0.004560745227316072\n",
      "Operator 65: 3.653456157870494e-05\n",
      "Operator 66: 0.0001199225771767818\n",
      "Operator 67: 0.05600703484219456\n",
      "Operator 68: 0.1016020039101841\n",
      "Operator 69: 0.1209612537533018\n",
      "Operator 70: 0.5625747147855886\n",
      "Operator 71: -0.09249344343159825\n",
      "Operator 72: 0.586642128608709\n",
      "Operator 73: -0.33395700723060445\n",
      "Operator 74: 0.09726287778706796\n",
      "Operator 75: 0.012297547567710999\n",
      "Operator 76: 0.1167861537251541\n",
      "Operator 77: -0.16791314354723927\n",
      "Operator 78: -0.18356399758713238\n",
      "Operator 79: -0.23783162354646156\n",
      "Operator 80: -0.18926251515324732\n",
      "Operator 81: 0.4422550025436518\n",
      "Operator 82: -5.436128334470126e-06\n",
      "Operator 83: 0.07309818735126747\n",
      "Operator 84: 0.23882602710532894\n",
      "Operator 85: 0.3092622511637211\n",
      "Operator 86: -0.07298593371199556\n",
      "Operator 87: -0.06526199793313023\n",
      "Operator 88: -0.2216568430625796\n",
      "Operator 89: -0.7906467981215826\n",
      "Operator 90: 0.16843683941780307\n",
      "Operator 91: 0.3056856474587664\n",
      "Operator 92: -0.03016415635940932\n",
      "Operator 93: 0.014934989066627047\n",
      "Operator 94: 0.031229979452910863\n",
      "Operator 95: 0.10919991412609141\n",
      "Operator 96: 0.08394516880651953\n",
      "Operator 97: 0.2569528348380734\n",
      "Operator 98: -0.0755529231274787\n",
      "Operator 99: 0.05873218117894499\n",
      "Operator 100: 0.1523394674141675\n",
      "Operator 101: 0.24367756361293091\n",
      "Operator 102: -0.11230300493440258\n",
      "Operator 103: 0.11312334991833034\n",
      "Operator 104: 2.867544030377145e-05\n",
      "Operator 105: -1.9355498243403546e-07\n",
      "Operator 106: 3.5365696300186655e-06\n",
      "Operator 107: -1.2060712909555543e-05\n",
      "Operator 108: -1.8657622633059742e-05\n",
      "Operator 109: -9.684987631883703e-06\n",
      "Operator 110: 1.543342646671776e-05\n",
      "Operator 111: -1.139818900409827e-05\n",
      "Operator 112: 3.969331378201804e-06\n",
      "Operator 113: 0.1155246149193557\n",
      "Operator 114: 0.08150198171035385\n",
      "Operator 115: 0.0825620252442103\n",
      "Operator 116: 0.07546807343207335\n",
      "Operator 117: -0.48047408717718015\n",
      "Operator 118: -0.1906443249601297\n",
      "Operator 119: -0.16889993394399733\n",
      "Operator 120: -0.22748284223500476\n",
      "Operator 121: -0.30235837397082344\n",
      "Operator 122: -0.09875364993234964\n",
      "Operator 123: 0.05518199618572019\n",
      "Operator 124: 0.08159247601427258\n",
      "Operator 125: -0.26789872077988114\n",
      "Operator 126: -0.27761701943887507\n",
      "Operator 127: -0.1644286412045406\n",
      "Operator 128: -0.1739312967567602\n",
      "Operator 129: 0.0192605917707309\n",
      "Operator 130: -0.08977073237221186\n",
      "Operator 131: -0.02940046396466655\n",
      "Operator 132: -0.032993482986947445\n",
      "Operator 133: 0.23919272260692853\n",
      "Operator 134: -0.25634765431363254\n",
      "Operator 135: 0.008536333121503753\n",
      "Operator 136: -0.7433603387632778\n",
      "Operator 137: 0.2043491957780616\n",
      "Operator 138: -0.03697527610711361\n",
      "Operator 139: -0.16365226798178625\n",
      "Operator 140: 0.10920936220362679\n",
      "Operator 141: 0.2282220535353448\n",
      "Operator 142: 0.11776499536372077\n",
      "Operator 143: 0.7906502671631209\n",
      "Operator 144: 0.19528845371338663\n",
      "Operator 145: -0.15609173162203452\n",
      "Operator 146: 0.22354894317685123\n",
      "Operator 147: 0.04088148965812251\n",
      "Operator 148: 0.022034439609476233\n",
      "Operator 149: -0.15541288145616106\n",
      "Total gradient norm: 2.4811420255220034\n",
      "Operators under consideration (1):\n",
      "[143]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.7906502671631209)]\n",
      "Operator(s) added to ansatz: [143]\n",
      "Gradients: [np.float64(0.7906502671631209)]\n",
      "Initial energy: -20.212003146027694\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143]...\n",
      "Starting point: [np.float64(-9.189505426721408e-06), np.float64(0.7854012169512903), np.float64(0.7854029194203661), np.float64(0.11100260572798411), np.float64(2.48373090277712e-06), np.float64(0.78540148697296), np.float64(0.7854051280902452), np.float64(0.7854025794580989), np.float64(0.7854009229036218), np.float64(0.14548740012242536), np.float64(-0.11634709337238276), np.float64(-0.17570454161553278), np.float64(1.0068897103740133e-05), np.float64(0.15024719507114784), np.float64(-0.16229392284541222), np.float64(-5.059038679449971e-06), np.float64(-0.14125784288618076), np.float64(0.004862948342372359), np.float64(0.10896866595391341), np.float64(-0.10246482091187573), np.float64(0.16853525120475557), np.float64(0.12614638168291473), np.float64(-0.16402437773192427), np.float64(-0.13701228542266009), np.float64(0.14011873571625535), np.float64(-0.12094070643452355), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.226240\n",
      "         Iterations: 19\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 60\n",
      "\n",
      "Current energy: -20.22623963380697\n",
      "(change of -0.014236487779275109)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143]\n",
      "On iteration 27.\n",
      "\n",
      "*** ADAPT-VQE Iteration 28 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -1.5111663078791688e-05\n",
      "Operator 1: -2.3178361110226805e-05\n",
      "Operator 2: -4.100758076195504e-06\n",
      "Operator 3: -5.788011967889847e-06\n",
      "Operator 4: 1.1892425839619969e-05\n",
      "Operator 5: -4.810855367656103e-06\n",
      "Operator 6: 1.346279895994562e-06\n",
      "Operator 7: -1.0793936633043955e-06\n",
      "Operator 8: -4.745520792326374e-06\n",
      "Operator 9: 8.94500506318691e-05\n",
      "Operator 10: -0.0001378741302002106\n",
      "Operator 11: 6.425760434370086e-05\n",
      "Operator 12: 1.4170849514140113e-05\n",
      "Operator 13: 4.0871715261819874e-06\n",
      "Operator 14: 1.605227986514789e-05\n",
      "Operator 15: -7.44469874772169e-06\n",
      "Operator 16: 8.368564413756968e-06\n",
      "Operator 17: 1.0507373425505673e-07\n",
      "Operator 18: -0.10045488452486921\n",
      "Operator 19: 0.23848648216588977\n",
      "Operator 20: 0.3753661500790419\n",
      "Operator 21: -0.01191980851793706\n",
      "Operator 22: -3.585361704511301e-05\n",
      "Operator 23: 0.1532493007006919\n",
      "Operator 24: 0.38071247379944556\n",
      "Operator 25: 0.1081089041909751\n",
      "Operator 26: -0.00010080632971038738\n",
      "Operator 27: 0.01656265802261862\n",
      "Operator 28: 0.43814515800912696\n",
      "Operator 29: 0.28863277179784363\n",
      "Operator 30: 0.19535892836650987\n",
      "Operator 31: -0.11696422034156193\n",
      "Operator 32: 0.3263545007569865\n",
      "Operator 33: 0.22955994560786602\n",
      "Operator 34: -0.19521333173470423\n",
      "Operator 35: -0.03682603449858906\n",
      "Operator 36: -0.10958183336152345\n",
      "Operator 37: -0.07741529512034803\n",
      "Operator 38: -0.31161542164456757\n",
      "Operator 39: 0.06366755139317966\n",
      "Operator 40: 0.08287088397749044\n",
      "Operator 41: 0.06309850830788734\n",
      "Operator 42: -0.12025297915279956\n",
      "Operator 43: 0.20208608613039059\n",
      "Operator 44: 0.18405363715539116\n",
      "Operator 45: 1.173859332306848e-06\n",
      "Operator 46: 4.676576534912158e-06\n",
      "Operator 47: 8.03048369804488e-05\n",
      "Operator 48: -5.646969065087954e-05\n",
      "Operator 49: 1.0733020206450629e-05\n",
      "Operator 50: 5.616443855877571e-06\n",
      "Operator 51: 6.374232885222354e-06\n",
      "Operator 52: 2.534049178142244e-06\n",
      "Operator 53: -9.645343649875016e-06\n",
      "Operator 54: 2.9636557649613537e-06\n",
      "Operator 55: -1.2855440615078262e-06\n",
      "Operator 56: -5.874954772495474e-05\n",
      "Operator 57: 0.05057587480554147\n",
      "Operator 58: -0.15170961112650633\n",
      "Operator 59: -0.5246350861510612\n",
      "Operator 60: 0.07321691116543648\n",
      "Operator 61: -0.04971320322353428\n",
      "Operator 62: 0.29354587217097405\n",
      "Operator 63: -0.28625487201218747\n",
      "Operator 64: 0.0049008947863182035\n",
      "Operator 65: -2.718152552696153e-05\n",
      "Operator 66: 0.00030436288582456265\n",
      "Operator 67: 0.06083197428656122\n",
      "Operator 68: 0.10045272667600237\n",
      "Operator 69: 0.04388329359727137\n",
      "Operator 70: 0.632486079286881\n",
      "Operator 71: -0.10808536464223863\n",
      "Operator 72: 0.5717298801957235\n",
      "Operator 73: -0.31351516766307985\n",
      "Operator 74: 0.09751626762075201\n",
      "Operator 75: 0.01224043881422152\n",
      "Operator 76: 0.11810201430416147\n",
      "Operator 77: -0.17309502313836517\n",
      "Operator 78: -0.2096736840701199\n",
      "Operator 79: -0.24028830016349337\n",
      "Operator 80: -0.21920508519828938\n",
      "Operator 81: 0.2716223515398401\n",
      "Operator 82: 6.079283986924916e-05\n",
      "Operator 83: 0.05896454810356027\n",
      "Operator 84: 0.24048386260119692\n",
      "Operator 85: 0.31235986266449617\n",
      "Operator 86: -0.08030731268801834\n",
      "Operator 87: -0.06348947099765036\n",
      "Operator 88: -0.21560765950177768\n",
      "Operator 89: -0.0860151813984357\n",
      "Operator 90: -0.07077074635184624\n",
      "Operator 91: 0.30388066289870463\n",
      "Operator 92: -0.04726803897094421\n",
      "Operator 93: 0.015541759086996674\n",
      "Operator 94: 0.031662301873016205\n",
      "Operator 95: 0.11091125689529766\n",
      "Operator 96: 0.09419841380285846\n",
      "Operator 97: 0.22821916481303273\n",
      "Operator 98: -0.1694723837335546\n",
      "Operator 99: 0.08804921015154307\n",
      "Operator 100: 0.08783597654646706\n",
      "Operator 101: 0.2504232302976514\n",
      "Operator 102: -0.11659217925077477\n",
      "Operator 103: 0.11347580715317382\n",
      "Operator 104: -2.376039343325833e-05\n",
      "Operator 105: -8.409438259656014e-07\n",
      "Operator 106: -6.12307831043953e-05\n",
      "Operator 107: -5.382441962197698e-06\n",
      "Operator 108: 2.0009580601783317e-06\n",
      "Operator 109: -3.7554086433390366e-06\n",
      "Operator 110: 8.595732146340213e-06\n",
      "Operator 111: -3.2224933678506873e-06\n",
      "Operator 112: 4.7223686991393343e-07\n",
      "Operator 113: 0.11338758579298946\n",
      "Operator 114: -0.09169542563705177\n",
      "Operator 115: 0.10152691174772187\n",
      "Operator 116: 0.05753197035467213\n",
      "Operator 117: -0.3042894708556183\n",
      "Operator 118: -0.19597196758413357\n",
      "Operator 119: -0.15583391299133817\n",
      "Operator 120: -0.22893880800905772\n",
      "Operator 121: -0.3053310379537264\n",
      "Operator 122: -0.11433037690266941\n",
      "Operator 123: -0.02248446112121004\n",
      "Operator 124: -0.1059086697513652\n",
      "Operator 125: -0.0008183937518468185\n",
      "Operator 126: -0.23618284927493638\n",
      "Operator 127: -0.14581084622306328\n",
      "Operator 128: -0.17420679511746712\n",
      "Operator 129: 0.019319326259538493\n",
      "Operator 130: -0.09065552233384375\n",
      "Operator 131: -0.036526603897103695\n",
      "Operator 132: -0.03796349986681898\n",
      "Operator 133: 0.3282500086179445\n",
      "Operator 134: -0.49331968271158405\n",
      "Operator 135: 0.058550255686548265\n",
      "Operator 136: -0.7152448955630859\n",
      "Operator 137: 0.19934037295501145\n",
      "Operator 138: -0.036671606008747734\n",
      "Operator 139: -0.1653037331216141\n",
      "Operator 140: 0.12415243852772193\n",
      "Operator 141: 0.24759242133581127\n",
      "Operator 142: 0.299201926828588\n",
      "Operator 143: -1.96391787524292e-05\n",
      "Operator 144: 0.2067147425839261\n",
      "Operator 145: -0.1705635362761721\n",
      "Operator 146: 0.22754488336571477\n",
      "Operator 147: 0.04032927146427204\n",
      "Operator 148: 0.02208910609785981\n",
      "Operator 149: -0.14774173954595926\n",
      "Total gradient norm: 2.2356713041352863\n",
      "Operators under consideration (1):\n",
      "[136]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-0.7152448955630859)]\n",
      "Operator(s) added to ansatz: [136]\n",
      "Gradients: [np.float64(-0.7152448955630859)]\n",
      "Initial energy: -20.22623963380697\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136]...\n",
      "Starting point: [np.float64(-1.0783113688148713e-05), np.float64(0.7853979500099593), np.float64(0.7853993436957469), np.float64(0.11928529953964227), np.float64(2.924230623700555e-06), np.float64(0.7854000215852589), np.float64(0.7853977982819776), np.float64(0.7853970674311203), np.float64(0.7854005099232186), np.float64(0.1459026514788132), np.float64(-0.11619691332843185), np.float64(-0.1757362954273793), np.float64(1.689347365843033e-05), np.float64(0.1512917456909353), np.float64(-0.16378691862778744), np.float64(-7.323783521073545e-06), np.float64(-0.1413548861258265), np.float64(0.008927915399509241), np.float64(0.10894599255572855), np.float64(-0.09957821048831905), np.float64(0.1816493352611279), np.float64(0.12694370988819595), np.float64(-0.16481790525653378), np.float64(-0.13708474948322416), np.float64(0.13821872970186128), np.float64(-0.11675935822331507), np.float64(-0.036140635540153995), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.238014\n",
      "         Iterations: 25\n",
      "         Function evaluations: 70\n",
      "         Gradient evaluations: 63\n",
      "\n",
      "Current energy: -20.238013723096543\n",
      "(change of -0.011774089289573908)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136]\n",
      "On iteration 28.\n",
      "\n",
      "*** ADAPT-VQE Iteration 29 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -8.29465782769107e-05\n",
      "Operator 1: 9.966940021839926e-05\n",
      "Operator 2: -3.1624230092397454e-06\n",
      "Operator 3: 6.96227036912217e-05\n",
      "Operator 4: -4.07399084158544e-05\n",
      "Operator 5: 1.684641447076418e-05\n",
      "Operator 6: -2.696369456600678e-07\n",
      "Operator 7: 1.9249280035382732e-05\n",
      "Operator 8: 6.9451510018581075e-06\n",
      "Operator 9: 7.992190239725009e-05\n",
      "Operator 10: -0.00011505472394821438\n",
      "Operator 11: 0.00011756239509530921\n",
      "Operator 12: -2.9357217725000826e-05\n",
      "Operator 13: 1.3918985578592613e-05\n",
      "Operator 14: -9.767938942120691e-05\n",
      "Operator 15: 2.3368391600803173e-05\n",
      "Operator 16: -4.4642045977622e-05\n",
      "Operator 17: -6.823949602274837e-06\n",
      "Operator 18: -0.10216249090197324\n",
      "Operator 19: 0.24229066029434126\n",
      "Operator 20: 0.3594833718182223\n",
      "Operator 21: -0.0505346689982872\n",
      "Operator 22: 3.0043003013038166e-07\n",
      "Operator 23: 0.19110116366524696\n",
      "Operator 24: 0.40681177934396756\n",
      "Operator 25: 0.1091429826675445\n",
      "Operator 26: -2.794335336142595e-05\n",
      "Operator 27: 0.01646194325668271\n",
      "Operator 28: 0.4234349138039627\n",
      "Operator 29: 0.29294324432652746\n",
      "Operator 30: 0.07761617798090975\n",
      "Operator 31: -0.06811689920914885\n",
      "Operator 32: 0.3206681106103519\n",
      "Operator 33: 0.22271474540672948\n",
      "Operator 34: -0.19742990940191296\n",
      "Operator 35: -0.0374832180327989\n",
      "Operator 36: -0.11142692417173822\n",
      "Operator 37: -0.07734757961141685\n",
      "Operator 38: -0.3155930487368617\n",
      "Operator 39: 0.07246901548616468\n",
      "Operator 40: 0.06123370309102019\n",
      "Operator 41: -0.03299656474749759\n",
      "Operator 42: -0.10650280976309148\n",
      "Operator 43: 0.1942641216847564\n",
      "Operator 44: 0.19483823065318906\n",
      "Operator 45: -9.437729884043122e-06\n",
      "Operator 46: -2.4383331780342132e-06\n",
      "Operator 47: 7.588636063891943e-05\n",
      "Operator 48: -3.3043511144200305e-05\n",
      "Operator 49: 9.83109318988323e-05\n",
      "Operator 50: -2.5439251348284564e-05\n",
      "Operator 51: -1.142296267939158e-05\n",
      "Operator 52: -1.8126783034505545e-05\n",
      "Operator 53: 2.678650745222e-05\n",
      "Operator 54: -1.2397990386159297e-05\n",
      "Operator 55: 3.7403862762630524e-06\n",
      "Operator 56: -2.0776113179185688e-05\n",
      "Operator 57: 0.050790070058105874\n",
      "Operator 58: -0.16660544482695006\n",
      "Operator 59: -0.5105961649617283\n",
      "Operator 60: 0.11032422382339352\n",
      "Operator 61: 0.08709891814765693\n",
      "Operator 62: 0.07606987771287244\n",
      "Operator 63: -0.35743899128538603\n",
      "Operator 64: 0.00783170645908765\n",
      "Operator 65: -6.869771769322485e-05\n",
      "Operator 66: 0.00014053202590653756\n",
      "Operator 67: 0.061441228445636674\n",
      "Operator 68: 0.10217297395516764\n",
      "Operator 69: 0.054688041574645685\n",
      "Operator 70: 0.612629379662835\n",
      "Operator 71: -0.07245612290519686\n",
      "Operator 72: -0.05462522972630193\n",
      "Operator 73: -0.13128289833488774\n",
      "Operator 74: 0.10860292714735217\n",
      "Operator 75: 0.009986915241867289\n",
      "Operator 76: 0.121291116699618\n",
      "Operator 77: -0.17544537051351167\n",
      "Operator 78: -0.197710191639898\n",
      "Operator 79: -0.2506055078184434\n",
      "Operator 80: -0.1003299952444178\n",
      "Operator 81: 0.19446839407266187\n",
      "Operator 82: -0.00974546978095396\n",
      "Operator 83: -0.06951705097977372\n",
      "Operator 84: 0.24105523005484697\n",
      "Operator 85: 0.32122939444674004\n",
      "Operator 86: -0.0811465666955816\n",
      "Operator 87: -0.06458750517711724\n",
      "Operator 88: -0.22031161736248622\n",
      "Operator 89: -0.09289330766904158\n",
      "Operator 90: -0.05846024856973142\n",
      "Operator 91: 0.26586890100393445\n",
      "Operator 92: -0.031334295296965964\n",
      "Operator 93: 0.004563851602656448\n",
      "Operator 94: 0.03493586303987961\n",
      "Operator 95: 0.1128835846067515\n",
      "Operator 96: 0.09373939615084086\n",
      "Operator 97: 0.22151020956129838\n",
      "Operator 98: -0.17365600142020507\n",
      "Operator 99: 0.11094136801213325\n",
      "Operator 100: 0.1696568515124534\n",
      "Operator 101: 0.1882234204724029\n",
      "Operator 102: -0.14368888114111944\n",
      "Operator 103: 0.1102675251662086\n",
      "Operator 104: -6.864829222713783e-05\n",
      "Operator 105: 2.431923507627379e-05\n",
      "Operator 106: -7.87653302895216e-05\n",
      "Operator 107: 7.64140074377851e-06\n",
      "Operator 108: -2.1582986404006244e-05\n",
      "Operator 109: 2.8832073679013748e-05\n",
      "Operator 110: -3.292904332087565e-05\n",
      "Operator 111: 1.817145782848084e-05\n",
      "Operator 112: 7.264950177887641e-06\n",
      "Operator 113: 0.11527716095156787\n",
      "Operator 114: -0.09303064984733292\n",
      "Operator 115: 0.11174562126922824\n",
      "Operator 116: 0.04416128076190444\n",
      "Operator 117: -0.2867483100743453\n",
      "Operator 118: -0.1306798543392449\n",
      "Operator 119: -0.03422304211526558\n",
      "Operator 120: -0.22902036771112402\n",
      "Operator 121: -0.3140728724269945\n",
      "Operator 122: -0.11531728971405229\n",
      "Operator 123: -0.005504645770194709\n",
      "Operator 124: -0.09951553743685404\n",
      "Operator 125: 0.0009454947380360561\n",
      "Operator 126: -0.2305795108528407\n",
      "Operator 127: -0.15135864664364024\n",
      "Operator 128: -0.11792058117394819\n",
      "Operator 129: 0.016087581113042926\n",
      "Operator 130: -0.0934119792854265\n",
      "Operator 131: -0.036654809862056534\n",
      "Operator 132: -0.03514853477130704\n",
      "Operator 133: 0.3114107684669661\n",
      "Operator 134: -0.4834054706027217\n",
      "Operator 135: -0.06166664078242451\n",
      "Operator 136: 9.126968692962575e-07\n",
      "Operator 137: 0.2325784819802313\n",
      "Operator 138: -0.02793532093754231\n",
      "Operator 139: -0.16861270190126032\n",
      "Operator 140: 0.12535171500826298\n",
      "Operator 141: 0.24620629136603353\n",
      "Operator 142: 0.2914091041250888\n",
      "Operator 143: -2.1626347609824516e-05\n",
      "Operator 144: 0.15552830356874495\n",
      "Operator 145: 0.009131515322708856\n",
      "Operator 146: 0.1990170490381635\n",
      "Operator 147: 0.039717708456786224\n",
      "Operator 148: 0.02433755315503419\n",
      "Operator 149: -0.14979256045047457\n",
      "Total gradient norm: 1.9414393250168989\n",
      "Operators under consideration (1):\n",
      "[70]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.612629379662835)]\n",
      "Operator(s) added to ansatz: [70]\n",
      "Gradients: [np.float64(0.612629379662835)]\n",
      "Initial energy: -20.238013723096543\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136, 70]...\n",
      "Starting point: [np.float64(4.503876200244802e-06), np.float64(0.7853968721362072), np.float64(0.7853908427285567), np.float64(0.09654439971643713), np.float64(4.3502433724576744e-06), np.float64(0.7853895043811996), np.float64(0.7853776176876274), np.float64(0.7853802538248049), np.float64(0.7853854334733744), np.float64(0.1446491927487734), np.float64(-0.11579479405855346), np.float64(-0.17574468372011895), np.float64(-5.248356420208114e-06), np.float64(0.15312990880092475), np.float64(-0.16480819445710587), np.float64(1.008671745825257e-05), np.float64(-0.1415274231872506), np.float64(-0.010525416824486976), np.float64(0.10742963186480656), np.float64(-0.1004940811461115), np.float64(0.18229626030631182), np.float64(0.12688318399866985), np.float64(-0.16474323634799945), np.float64(-0.1371775297795374), np.float64(0.13933631487060116), np.float64(-0.13438827556771082), np.float64(-0.03474060109776823), np.float64(0.03301149592599866), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -20.246209\n",
      "         Iterations: 22\n",
      "         Function evaluations: 59\n",
      "         Gradient evaluations: 50\n",
      "\n",
      "Current energy: -20.246209498165022\n",
      "(change of -0.00819577506847935)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136, 70]\n",
      "On iteration 29.\n",
      "\n",
      "*** ADAPT-VQE Iteration 30 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -5.029678215926121e-05\n",
      "Operator 1: 0.00014098450697446694\n",
      "Operator 2: 4.864041111024564e-05\n",
      "Operator 3: -2.5850945994318475e-05\n",
      "Operator 4: -1.5136213698807577e-05\n",
      "Operator 5: -4.3459908471606135e-06\n",
      "Operator 6: 5.556551377239316e-06\n",
      "Operator 7: -1.0869325112516925e-05\n",
      "Operator 8: 9.047905900949615e-06\n",
      "Operator 9: -5.52844880714024e-05\n",
      "Operator 10: 3.481610353601322e-05\n",
      "Operator 11: 0.0001064173413396507\n",
      "Operator 12: -1.6250267195051012e-05\n",
      "Operator 13: -6.235990363305521e-06\n",
      "Operator 14: 2.0186282842287948e-05\n",
      "Operator 15: 1.5902035341956722e-05\n",
      "Operator 16: 1.121695367426595e-05\n",
      "Operator 17: -6.042935875472111e-06\n",
      "Operator 18: -0.10115920787904692\n",
      "Operator 19: 0.1952974215668195\n",
      "Operator 20: 0.3656464877291826\n",
      "Operator 21: -0.0631324585806002\n",
      "Operator 22: -5.990922840796824e-05\n",
      "Operator 23: 0.1953444630891281\n",
      "Operator 24: 0.4006280143864892\n",
      "Operator 25: 0.10936070860250771\n",
      "Operator 26: 1.1631074413360393e-06\n",
      "Operator 27: 0.005678109779300891\n",
      "Operator 28: 0.318883615918237\n",
      "Operator 29: 0.25310722239518485\n",
      "Operator 30: 0.07388312357480048\n",
      "Operator 31: -0.07038897246226741\n",
      "Operator 32: 0.31373811229872345\n",
      "Operator 33: 0.22300605398200704\n",
      "Operator 34: -0.19839354808207682\n",
      "Operator 35: -0.03757769059353939\n",
      "Operator 36: -0.11038627930299211\n",
      "Operator 37: -0.07753574095369148\n",
      "Operator 38: -0.30759396126365\n",
      "Operator 39: 0.17408248210451283\n",
      "Operator 40: 0.12118139213066907\n",
      "Operator 41: -0.04399458261123214\n",
      "Operator 42: -0.10712960242558431\n",
      "Operator 43: 0.1953049131889053\n",
      "Operator 44: 0.19573237539571242\n",
      "Operator 45: 3.3101556086434503e-06\n",
      "Operator 46: -1.2100974681850118e-05\n",
      "Operator 47: -5.814093990804054e-05\n",
      "Operator 48: 9.361697564450854e-06\n",
      "Operator 49: 8.87640745782686e-05\n",
      "Operator 50: -3.247688262689435e-05\n",
      "Operator 51: -4.699013074887029e-05\n",
      "Operator 52: 7.088262694687764e-06\n",
      "Operator 53: 1.6596380620230444e-05\n",
      "Operator 54: 1.103315529476373e-06\n",
      "Operator 55: -8.234699737930118e-06\n",
      "Operator 56: 8.612718474211216e-05\n",
      "Operator 57: 0.048544854610194796\n",
      "Operator 58: -0.10089823055075224\n",
      "Operator 59: -0.32403893701878944\n",
      "Operator 60: -0.020412733466787136\n",
      "Operator 61: 0.03950316851356655\n",
      "Operator 62: 0.06943521865860393\n",
      "Operator 63: -0.351468584977148\n",
      "Operator 64: 0.008032786583538837\n",
      "Operator 65: 6.703499917123327e-05\n",
      "Operator 66: -1.7526708344974396e-06\n",
      "Operator 67: 0.06292237243853406\n",
      "Operator 68: 0.09762933253099111\n",
      "Operator 69: 0.02343783909273725\n",
      "Operator 70: -9.354180483563669e-06\n",
      "Operator 71: 0.057202521233543815\n",
      "Operator 72: -0.04264963675664446\n",
      "Operator 73: -0.12492082901054508\n",
      "Operator 74: 0.10732267921619015\n",
      "Operator 75: 0.00990571793488245\n",
      "Operator 76: 0.12186980386894473\n",
      "Operator 77: -0.17208715916142125\n",
      "Operator 78: -0.08050366525079122\n",
      "Operator 79: -0.23911469907487654\n",
      "Operator 80: -0.06909172746659008\n",
      "Operator 81: 0.08488653826249068\n",
      "Operator 82: -0.009477337123334494\n",
      "Operator 83: -0.07022148082443047\n",
      "Operator 84: 0.24206731831782824\n",
      "Operator 85: 0.32265856620199684\n",
      "Operator 86: -0.08148896126877284\n",
      "Operator 87: -0.0600620829851367\n",
      "Operator 88: -0.1267683749447216\n",
      "Operator 89: 0.07981460401571563\n",
      "Operator 90: -0.06113854101780913\n",
      "Operator 91: 0.2549611391063455\n",
      "Operator 92: -0.03859482246006546\n",
      "Operator 93: 0.005132457206968609\n",
      "Operator 94: 0.03506519840523036\n",
      "Operator 95: 0.10997417331157547\n",
      "Operator 96: 0.09344191716110742\n",
      "Operator 97: 0.18647808406233626\n",
      "Operator 98: -0.19681596398361947\n",
      "Operator 99: 0.10266420405151856\n",
      "Operator 100: 0.14705724883641166\n",
      "Operator 101: 0.18844787960500972\n",
      "Operator 102: -0.145105875575189\n",
      "Operator 103: 0.1106107493706005\n",
      "Operator 104: 7.027371732214816e-05\n",
      "Operator 105: -1.0696923779156218e-05\n",
      "Operator 106: -6.25238193840885e-05\n",
      "Operator 107: 4.532196352220307e-06\n",
      "Operator 108: 4.072434107105227e-06\n",
      "Operator 109: -8.550269036525227e-06\n",
      "Operator 110: -1.1583791482992767e-05\n",
      "Operator 111: -3.685360829081691e-06\n",
      "Operator 112: 3.668037475090813e-06\n",
      "Operator 113: 0.1150150028078058\n",
      "Operator 114: -0.0908510807322242\n",
      "Operator 115: 0.1073723547612834\n",
      "Operator 116: 0.020274448157840658\n",
      "Operator 117: -0.1750459332318715\n",
      "Operator 118: -0.13051041197446567\n",
      "Operator 119: -0.0314540101825663\n",
      "Operator 120: -0.22987840128606774\n",
      "Operator 121: -0.31550437187620245\n",
      "Operator 122: -0.11088112334484065\n",
      "Operator 123: -0.032250449943325044\n",
      "Operator 124: -0.11983443751329731\n",
      "Operator 125: 0.001027287613294769\n",
      "Operator 126: -0.17778928955977605\n",
      "Operator 127: -0.1427861407643758\n",
      "Operator 128: -0.11976829796455732\n",
      "Operator 129: 0.016207922218574693\n",
      "Operator 130: -0.09393797962812445\n",
      "Operator 131: -0.035891366581854496\n",
      "Operator 132: -0.04830782360787315\n",
      "Operator 133: 0.15073069014203838\n",
      "Operator 134: 0.06569710828382863\n",
      "Operator 135: -0.058060754667648276\n",
      "Operator 136: 3.6816546694835006e-05\n",
      "Operator 137: 0.22635026061454905\n",
      "Operator 138: -0.028429321700109592\n",
      "Operator 139: -0.1692276078003362\n",
      "Operator 140: 0.12155475822572603\n",
      "Operator 141: 0.2521677153986457\n",
      "Operator 142: 0.23850121292019233\n",
      "Operator 143: -0.012835494222951936\n",
      "Operator 144: 0.1480861987397289\n",
      "Operator 145: 0.0008405331735481076\n",
      "Operator 146: 0.20453837276892112\n",
      "Operator 147: 0.039435186303283375\n",
      "Operator 148: 0.024495323884032357\n",
      "Operator 149: -0.15764980545005194\n",
      "Total gradient norm: 1.6029866332262606\n",
      "Operators under consideration (1):\n",
      "[24]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(0.4006280143864892)]\n",
      "Operator(s) added to ansatz: [24]\n",
      "Gradients: [np.float64(0.4006280143864892)]\n",
      "Initial energy: -20.246209498165022\n",
      "Optimizing energy with indices [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136, 70, 24]...\n",
      "Starting point: [np.float64(3.1657625136446998e-06), np.float64(0.7853997907176075), np.float64(0.7854003225780868), np.float64(0.10499599896928571), np.float64(-1.0200197424173462e-05), np.float64(0.7853978879855072), np.float64(0.785404181830062), np.float64(0.7853947334308508), np.float64(0.7854006023446287), np.float64(0.14472483860013144), np.float64(-0.11572089154008959), np.float64(-0.17576565145856096), np.float64(4.458949902862033e-06), np.float64(0.15365677485086585), np.float64(-0.16430668446574628), np.float64(-1.0257882987099818e-07), np.float64(-0.141575870797781), np.float64(-0.010916904145189602), np.float64(0.10773706227490593), np.float64(-0.10310999176056743), np.float64(0.1821517922430366), np.float64(0.12637610654970272), np.float64(-0.16420559574206994), np.float64(-0.13718142358537933), np.float64(0.13714412292773523), np.float64(-0.13166471515657252), np.float64(-0.03721862454457223), np.float64(0.03223665867163121), np.float64(-0.02678854465324131), np.float64(0.0)]\n",
      "         Current function value: -20.262077\n",
      "         Iterations: 27\n",
      "         Function evaluations: 75\n",
      "         Gradient evaluations: 68\n",
      "\n",
      "Current energy: -20.26207738700024\n",
      "(change of -0.01586788883521706)\n",
      "Current ansatz: [149, 139, 112, 134, 50, 110, 65, 28, 108, 33, 85, 130, 66, 119, 116, 12, 26, 90, 82, 20, 97, 95, 18, 56, 115, 22, 143, 136, 70, 24]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 3 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(30):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "23\n",
      "30\n",
      "37\n",
      "44\n",
      "49\n",
      "56\n",
      "63\n",
      "70\n",
      "77\n",
      "84\n",
      "89\n",
      "96\n",
      "103\n",
      "108\n",
      "115\n",
      "122\n",
      "129\n",
      "136\n",
      "143\n",
      "150\n",
      "157\n",
      "164\n",
      "171\n",
      "178\n",
      "185\n",
      "192\n",
      "199\n",
      "206\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 12, 'x': 5, 'cx': 4, 'rx': 2, 'barrier': 2, 'u2': 1, 'rz': 1, 'h': 1})\n",
      "On circuit 1/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 12, 'cx': 8, 'x': 5, 'rx': 4, 'h': 3, 'barrier': 3, 'rz': 2, 'u2': 1})\n",
      "On circuit 2/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 14, 'measure': 12, 'rx': 6, 'x': 5, 'h': 5, 'barrier': 4, 'rz': 3, 'u2': 1})\n",
      "On circuit 3/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 18, 'measure': 12, 'rx': 8, 'h': 7, 'x': 5, 'barrier': 5, 'rz': 4, 'u2': 1})\n",
      "On circuit 4/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 24, 'measure': 12, 'rx': 10, 'h': 9, 'barrier': 6, 'x': 5, 'rz': 5, 'u2': 1})\n",
      "On circuit 5/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 30, 'rx': 12, 'measure': 12, 'h': 11, 'barrier': 7, 'rz': 6, 'x': 5, 'u2': 1})\n",
      "On circuit 6/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 32, 'rx': 14, 'h': 13, 'measure': 12, 'barrier': 8, 'rz': 7, 'x': 5, 'u2': 1})\n",
      "On circuit 7/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'rx': 16, 'h': 15, 'measure': 12, 'barrier': 9, 'rz': 8, 'x': 5, 'u2': 1})\n",
      "On circuit 8/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 42, 'rx': 18, 'h': 17, 'measure': 12, 'barrier': 10, 'rz': 9, 'x': 5, 'u2': 1})\n",
      "On circuit 9/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 46, 'rx': 20, 'h': 19, 'measure': 12, 'barrier': 11, 'rz': 10, 'x': 5, 'u2': 1})\n",
      "On circuit 10/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 50, 'rx': 22, 'h': 21, 'barrier': 12, 'measure': 12, 'rz': 11, 'x': 5, 'u2': 1})\n",
      "On circuit 11/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 54, 'rx': 24, 'h': 23, 'barrier': 13, 'rz': 12, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 12/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 56, 'rx': 26, 'h': 25, 'barrier': 14, 'rz': 13, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 13/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 60, 'rx': 28, 'h': 27, 'barrier': 15, 'rz': 14, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 14/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 64, 'rx': 30, 'h': 29, 'barrier': 16, 'rz': 15, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 15/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 66, 'rx': 32, 'h': 31, 'barrier': 17, 'rz': 16, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 16/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 70, 'rx': 34, 'h': 33, 'barrier': 18, 'rz': 17, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 17/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 74, 'rx': 36, 'h': 35, 'barrier': 19, 'rz': 18, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 18/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 78, 'rx': 38, 'h': 37, 'barrier': 20, 'rz': 19, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 19/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 82, 'rx': 40, 'h': 39, 'barrier': 21, 'rz': 20, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 20/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 86, 'rx': 42, 'h': 41, 'barrier': 22, 'rz': 21, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 21/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 90, 'rx': 44, 'h': 43, 'barrier': 23, 'rz': 22, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 22/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 94, 'rx': 46, 'h': 45, 'barrier': 24, 'rz': 23, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 23/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 98, 'rx': 48, 'h': 47, 'barrier': 25, 'rz': 24, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 24/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 102, 'rx': 50, 'h': 49, 'barrier': 26, 'rz': 25, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 25/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 106, 'rx': 52, 'h': 51, 'barrier': 27, 'rz': 26, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 26/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 110, 'rx': 54, 'h': 53, 'barrier': 28, 'rz': 27, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 27/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 114, 'rx': 56, 'h': 55, 'barrier': 29, 'rz': 28, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 28/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 116, 'rx': 56, 'h': 55, 'barrier': 30, 'rz': 28, 'measure': 12, 'x': 5, 'u2': 1})\n",
      "On circuit 29/30\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 114, 'h': 54, 'rx': 54, 'barrier': 31, 'rz': 27, 'measure': 12, 'x': 5, 'u3': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sampler.run([to_run], shots=30_000)\n",
    "    job = sim.run(to_run)\n",
    "    # bit_array = job.result()[0].data.meas\n",
    "    # bit_array = job.result().data().meas\n",
    "    counts = job.result().data()['counts']\n",
    "    bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    all_counts = collections.Counter()\n",
    "    tuple_of_counts = tuple(counts_list[:i])\n",
    "    assert len(tuple_of_counts) == i\n",
    "    for counts in tuple_of_counts:\n",
    "        for bitstring, count in counts.items():\n",
    "            all_counts[bitstring] += count\n",
    "\n",
    "    bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x3469a9450>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"SQD\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"iSQD\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
