{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "from qiskit.qasm2 import dumps\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates, project_operator_to_subspace\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "268707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474288453)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 198]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819708\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [241, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531757\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.089492926734934)]\n",
      "Initial energy: -6.327276154819708\n",
      "Optimizing energy with indices [241, 79, 228, 198, 45]...\n",
      "Starting point: [np.float64(-0.7853981618473267), np.float64(0.7853981651745618), np.float64(0.16357019740836948), np.float64(-0.16356963668286137), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134384\n",
      "(change of -0.13682546031467613)\n",
      "Current ansatz: [241, 79, 228, 198, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.929018984664254e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531757 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474292965)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 147]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819989\n",
      "(change of -0.2041705292023419)\n",
      "Current ansatz: [241, 79, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042617831\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.08949164278134)]\n",
      "Initial energy: -6.327276154819989\n",
      "Optimizing energy with indices [241, 79, 225, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981633974467), np.float64(0.7853981633974486), np.float64(-0.16357028656039646), np.float64(-0.16356997222705624), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072785\n",
      "(change of -0.13682546025279585)\n",
      "Current ansatz: [241, 79, 225, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875632299282\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042617831 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 241]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -4.626284539634973\n",
      "(change of -0.7978574148887763)\n",
      "Current ansatz: [211, 241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273456531\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.7252880142064084)]\n",
      "Initial energy: -4.626284539634973\n",
      "Optimizing energy with indices [211, 241, 79]...\n",
      "Starting point: [np.float64(-0.2651612342265216), np.float64(-0.44546905390000224), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625614734\n",
      "(change of -1.4968210859797608)\n",
      "Current ansatz: [211, 241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917530720141803\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.485073566996777)]\n",
      "Initial energy: -6.123105625614734\n",
      "Optimizing energy with indices [211, 241, 79, 177]...\n",
      "Starting point: [np.float64(-0.122489928621197), np.float64(-0.7853981633782835), np.float64(0.7853981632897755), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920525247)\n",
      "Current ansatz: [211, 241, 79, 177]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042228472\n",
      "Operator(s) added to ansatz: [114]\n",
      "Gradients: [np.float64(2.089491643491183)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [211, 241, 79, 177, 114]...\n",
      "Starting point: [np.float64(-0.163570286606266), np.float64(-0.7853981637982552), np.float64(0.7853981645146019), np.float64(0.16356997206808754), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.46410161507278\n",
      "(change of -0.13682546025279319)\n",
      "Current ansatz: [211, 241, 79, 177, 114]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350418291715858\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042228472 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474292024)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 147]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819699\n",
      "(change of -0.20417052920205947)\n",
      "Current ansatz: [244, 79, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531783\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.089492926734874)]\n",
      "Initial energy: -6.327276154819699\n",
      "Optimizing energy with indices [244, 79, 228, 147, 135]...\n",
      "Starting point: [np.float64(0.7853981646904773), np.float64(0.7853981643651435), np.float64(0.16357019740836662), np.float64(-0.1635696366828739), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151326465\n",
      "(change of -0.13682546031294773)\n",
      "Current ansatz: [244, 79, 228, 147, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.3720329358920564e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531783 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 135]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.32727615481999\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [241, 79, 228, 135]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964043989466\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916415770357)]\n",
      "Initial energy: -6.32727615481999\n",
      "Optimizing energy with indices [241, 79, 228, 135, 198]...\n",
      "Starting point: [np.float64(-0.7853981865515778), np.float64(0.785398160349101), np.float64(0.16357028664406387), np.float64(0.16356997254179764), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614980915\n",
      "(change of -0.13682546016092534)\n",
      "Current ansatz: [241, 79, 228, 135, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00018441503393950355\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964043989466 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429297)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020577)\n",
      "Current ansatz: [241, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531791\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.089492926734878)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 79, 225, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853981607743267), np.float64(0.7853981678304174), np.float64(-0.163570197408367), np.float64(0.16356963668287403), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615008455\n",
      "(change of -0.1368254601887502)\n",
      "Current ansatz: [241, 79, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00015113996083335524\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531791 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 147]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819713\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [244, 74, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531879\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734878)]\n",
      "Initial energy: -6.327276154819713\n",
      "Optimizing energy with indices [244, 74, 228, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981767049507), np.float64(-0.7853981525341226), np.float64(0.16357019740838014), np.float64(-0.1635696366828784), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614750672\n",
      "(change of -0.13682545993095907)\n",
      "Current ansatz: [244, 74, 228, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00031096863104971493\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531879 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [177]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499794\n",
      "(change of -1.2360679774997898)\n",
      "Current ansatz: [177]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696146\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000001)]\n",
      "Initial energy: -4.236067977499794\n",
      "Optimizing energy with indices [177, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970463), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983939)\n",
      "Current ansatz: [177, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-1.9999999999944214)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [177, 31, 228]...\n",
      "Starting point: [np.float64(-0.7853985607314273), np.float64(-0.7853989420959481), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610641\n",
      "(change of -0.1231056256124532)\n",
      "Current ansatz: [177, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955644\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868894)]\n",
      "Initial energy: -6.123105625610641\n",
      "Optimizing energy with indices [177, 31, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853983869831804), np.float64(-0.7853991695302192), np.float64(0.12248869758309348), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816648\n",
      "(change of -0.20417052920600653)\n",
      "Current ansatz: [177, 31, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964056413545\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916407281743)]\n",
      "Initial energy: -6.327276154816648\n",
      "Optimizing energy with indices [177, 31, 228, 210, 57]...\n",
      "Starting point: [np.float64(-0.7853984379181601), np.float64(-0.7853991591701148), np.float64(0.16357028929925477), np.float64(0.1635699734866594), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614714411\n",
      "(change of -0.1368254598977634)\n",
      "Current ansatz: [177, 31, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0002893896144582505\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964056413545 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047430588)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 147]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154820074\n",
      "(change of -0.20417052920242362)\n",
      "Current ansatz: [244, 79, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963655680355\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.089491982574535)]\n",
      "Initial energy: -6.327276154820074\n",
      "Optimizing energy with indices [244, 79, 225, 147, 120]...\n",
      "Starting point: [np.float64(0.7853981479801436), np.float64(0.7853981834326691), np.float64(-0.16357026296685157), np.float64(-0.16356988342672032), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615066587\n",
      "(change of -0.13682546024651288)\n",
      "Current ansatz: [244, 79, 225, 147, 120]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013897882250844647\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963655680355 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071048479702)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [241, 74, 228, 198]...\n",
      "Starting point: [np.float64(-0.7853981639976342), np.float64(-0.7853981625399249), np.float64(0.12248927961411428), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197165\n",
      "(change of -0.20417052920207368)\n",
      "Current ansatz: [241, 74, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531976\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929267348803)]\n",
      "Initial energy: -6.3272761548197165\n",
      "Optimizing energy with indices [241, 74, 228, 198, 120]...\n",
      "Starting point: [np.float64(-0.7853981657328081), np.float64(-0.7853981589236129), np.float64(0.16357019740840653), np.float64(-0.16356963668288546), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150902075\n",
      "(change of -0.13682546027049103)\n",
      "Current ansatz: [241, 74, 228, 198, 120]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.355247463480744e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531976 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474292024)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 147]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819699\n",
      "(change of -0.20417052920205947)\n",
      "Current ansatz: [244, 79, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531783\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.0894929267348745)]\n",
      "Initial energy: -6.327276154819699\n",
      "Optimizing energy with indices [244, 79, 228, 147, 45]...\n",
      "Starting point: [np.float64(0.7853981646904773), np.float64(0.7853981643651435), np.float64(0.16357019740836662), np.float64(-0.1635696366828739), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615131512\n",
      "(change of -0.13682546031181353)\n",
      "Current ansatz: [244, 79, 228, 147, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.671985654963492e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531783 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297208)\n",
      "Current ansatz: [228, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-1.999999999966421)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [228, 26, 210]...\n",
      "Starting point: [np.float64(-0.7853947065773501), np.float64(0.785399377726245), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.1231056256007985\n",
      "(change of -0.12310562567128258)\n",
      "Current ansatz: [228, 26, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752996715211\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485073147248)]\n",
      "Initial energy: -6.1231056256007985\n",
      "Optimizing energy with indices [228, 26, 210, 225]...\n",
      "Starting point: [np.float64(-0.7853960819261802), np.float64(0.7853979331209127), np.float64(0.1224898204618799), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154801719\n",
      "(change of -0.20417052920092083)\n",
      "Current ansatz: [228, 26, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962581069684\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.089492926705316)]\n",
      "Initial energy: -6.327276154801719\n",
      "Optimizing energy with indices [228, 26, 210, 225, 201]...\n",
      "Starting point: [np.float64(-0.7853960819261787), np.float64(0.7853972521899164), np.float64(0.16357019748863957), np.float64(-0.1635696367050071), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615115375\n",
      "(change of -0.1368254603136556)\n",
      "Current ansatz: [228, 26, 210, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 6.249374096538525e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962581069684 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0000000000000058)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 216]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 31, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200767483\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.4850710481677654)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 31, 216, 201]...\n",
      "Starting point: [np.float64(-0.7853981626365961), np.float64(-0.7853981627961343), np.float64(-0.12248927953372957), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819704\n",
      "(change of -0.2041705292020568)\n",
      "Current ansatz: [241, 31, 216, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531914\n",
      "Operator(s) added to ansatz: [75]\n",
      "Gradients: [np.float64(2.089492926734876)]\n",
      "Initial energy: -6.327276154819704\n",
      "Optimizing energy with indices [241, 31, 216, 201, 75]...\n",
      "Starting point: [np.float64(-0.7853981586751632), np.float64(-0.7853981606230303), np.float64(-0.1635701974083947), np.float64(0.16356963668288196), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615131158\n",
      "(change of -0.13682546031145382)\n",
      "Current ansatz: [241, 31, 216, 201, 75]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 3.874599970985359e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531914 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 225]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617648\n",
      "(change of -0.12310562561763749)\n",
      "Current ansatz: [241, 31, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944177\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.4850710474288835)]\n",
      "Initial energy: -6.123105625617648\n",
      "Optimizing energy with indices [241, 31, 225, 45]...\n",
      "Starting point: [np.float64(-0.7853981605940241), np.float64(-0.7853981659075159), np.float64(-0.12248927934332374), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819719\n",
      "(change of -0.20417052920207102)\n",
      "Current ansatz: [241, 31, 225, 45]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531839\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267348754)]\n",
      "Initial energy: -6.327276154819719\n",
      "Optimizing energy with indices [241, 31, 225, 45, 198]...\n",
      "Starting point: [np.float64(-0.7853981499557928), np.float64(-0.785398177188549), np.float64(-0.1635701974083672), np.float64(0.16356963668287564), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614763143\n",
      "(change of -0.13682545994342377)\n",
      "Current ansatz: [241, 31, 225, 45, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00030817030776114597\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531839 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 244]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634967\n",
      "(change of -0.79785741488877)\n",
      "Current ansatz: [211, 244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273103219\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.725288012410362)]\n",
      "Initial energy: -4.626284539634967\n",
      "Optimizing energy with indices [211, 244, 79]...\n",
      "Starting point: [np.float64(-0.2651612351142246), np.float64(0.44546905146305454), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999417229\n",
      "(change of -1.3737154597822618)\n",
      "Current ansatz: [211, 244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958970181085\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.9999999998057452)]\n",
      "Initial energy: -5.999999999417229\n",
      "Optimizing energy with indices [211, 244, 79, 216]...\n",
      "Starting point: [np.float64(-9.855379305101342e-06), np.float64(0.7853981714334987), np.float64(0.7853981407341446), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625027778\n",
      "(change of -0.12310562561054894)\n",
      "Current ansatz: [211, 244, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526189818066\n",
      "Operator(s) added to ansatz: [99]\n",
      "Gradients: [np.float64(-2.485071042304514)]\n",
      "Initial energy: -6.123105625027778\n",
      "Optimizing energy with indices [211, 244, 79, 216, 99]...\n",
      "Starting point: [np.float64(-9.814922853752443e-06), np.float64(0.7853981254153397), np.float64(0.7853981182984304), np.float64(-0.12248927808449805), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154210948\n",
      "(change of -0.20417052918317058)\n",
      "Current ansatz: [211, 244, 79, 216, 99]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 6.240962579520348\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 8.917526189818066 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617646\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797047)]\n",
      "Initial energy: -6.123105625617646\n",
      "Optimizing energy with indices [241, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.785398163997711), np.float64(-0.7853981625399766), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206213)\n",
      "Current ansatz: [241, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531922\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.089492926734948)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [241, 74, 225, 210, 57]...\n",
      "Starting point: [np.float64(-0.7853981657335386), np.float64(-0.7853981641901568), np.float64(-0.16357019740840956), np.float64(0.16356963668286917), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615120212\n",
      "(change of -0.13682546030050347)\n",
      "Current ansatz: [241, 74, 225, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 5.841823248781745e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531922 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000018)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 225]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199444833\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474305875)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 79, 225, 198]...\n",
      "Starting point: [np.float64(0.7853981583089785), np.float64(0.785398168718093), np.float64(-0.12248927934376257), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.32727615482008\n",
      "(change of -0.20417052920242895)\n",
      "Current ansatz: [244, 79, 225, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240963610610418\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.089492022153505)]\n",
      "Initial energy: -6.32727615482008\n",
      "Optimizing energy with indices [244, 79, 225, 198, 135]...\n",
      "Starting point: [np.float64(0.7853981493735948), np.float64(0.7853981777037257), np.float64(-0.16357026021874255), np.float64(-0.16356987308330864), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615050828\n",
      "(change of -0.1368254602307486)\n",
      "Current ansatz: [244, 79, 225, 198, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00014811047136609125\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240963610610418 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 228]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617643\n",
      "(change of -0.12310562561763838)\n",
      "Current ansatz: [241, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797034)]\n",
      "Initial energy: -6.123105625617643\n",
      "Optimizing energy with indices [241, 74, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853981639976342), np.float64(-0.7853981625399249), np.float64(0.12248927961411428), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819971\n",
      "(change of -0.2041705292023286)\n",
      "Current ansatz: [241, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964057733311\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.089491629507777)]\n",
      "Initial energy: -6.327276154819971\n",
      "Optimizing energy with indices [241, 74, 228, 210, 57]...\n",
      "Starting point: [np.float64(-0.785398162447052), np.float64(-0.7853981711958521), np.float64(0.16357028748210503), np.float64(0.16356997569593643), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615067938\n",
      "(change of -0.13682546024796682)\n",
      "Current ansatz: [241, 74, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013840069558915347\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964057733311 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 31]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132743\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000018)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 31, 228]...\n",
      "Starting point: [np.float64(-0.7853981718257756), np.float64(-0.7853981815917112), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 31, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441852\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047428929)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 31, 228, 147]...\n",
      "Starting point: [np.float64(-0.7853981609748782), np.float64(-0.785398165605737), np.float64(0.12248927934333585), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819714\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [241, 31, 228, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531371\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.0894929267356055)]\n",
      "Initial energy: -6.327276154819714\n",
      "Optimizing energy with indices [241, 31, 228, 147, 135]...\n",
      "Starting point: [np.float64(-0.7853981663792285), np.float64(-0.7853981742082982), np.float64(0.16357019740840556), np.float64(-0.1635696366827097), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615040445\n",
      "(change of -0.13682546022073083)\n",
      "Current ansatz: [241, 31, 228, 147, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001273389285281687\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531371 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000011\n",
      "(change of -1.7639320225002146)\n",
      "Current ansatz: [241, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113274\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000005)]\n",
      "Initial energy: -6.000000000000011\n",
      "Optimizing energy with indices [241, 26, 216]...\n",
      "Starting point: [np.float64(-0.7853981718257758), np.float64(0.7853981815917098), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617644\n",
      "(change of -0.12310562561763305)\n",
      "Current ansatz: [241, 26, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200770151\n",
      "Operator(s) added to ansatz: [156]\n",
      "Gradients: [np.float64(-2.485071048169255)]\n",
      "Initial energy: -6.123105625617644\n",
      "Optimizing energy with indices [241, 26, 216, 156]...\n",
      "Starting point: [np.float64(-0.7853981650265022), np.float64(0.7853981646314862), np.float64(-0.12248927953411375), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.20417052920206125)\n",
      "Current ansatz: [241, 26, 216, 156]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531799\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.089492926735055)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 26, 216, 156, 225]...\n",
      "Starting point: [np.float64(-0.7853981660307093), np.float64(0.7853981691726957), np.float64(-0.16357019740840442), np.float64(0.16356963668284105), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151230035\n",
      "(change of -0.13682546030329856)\n",
      "Current ansatz: [241, 26, 216, 156, 225]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 5.269047862335745e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531799 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 26]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929516\n",
      "(change of -1.7639320224297208)\n",
      "Current ansatz: [228, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000193667)]\n",
      "Initial energy: -5.999999999929516\n",
      "Optimizing energy with indices [228, 26, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773501), np.float64(0.785399377726245), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056255624825\n",
      "(change of -0.12310562563296656)\n",
      "Current ansatz: [228, 26, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526201775818\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484682823)]\n",
      "Initial energy: -6.1231056255624825\n",
      "Optimizing energy with indices [228, 26, 228, 210]...\n",
      "Starting point: [np.float64(-0.7853947065772612), np.float64(0.7853985308794263), np.float64(0.12248927961669973), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154765017\n",
      "(change of -0.20417052920253465)\n",
      "Current ansatz: [228, 26, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042069122\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438633278)]\n",
      "Initial energy: -6.327276154765017\n",
      "Optimizing energy with indices [228, 26, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853947065771328), np.float64(0.7853982468601286), np.float64(0.16357028648716873), np.float64(0.16356997194353046), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016150180075\n",
      "(change of -0.13682546025299036)\n",
      "Current ansatz: [228, 26, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016418668716674994\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042069122 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 228]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562482\n",
      "(change of -0.1231056256329639)\n",
      "Current ansatz: [228, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201775814\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710484682805)]\n",
      "Initial energy: -6.123105625562482\n",
      "Optimizing energy with indices [228, 79, 228, 198]...\n",
      "Starting point: [np.float64(-0.7853947065772693), np.float64(0.7853985308794288), np.float64(0.1224892796166995), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154764737\n",
      "(change of -0.20417052920225576)\n",
      "Current ansatz: [228, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258122057\n",
      "Operator(s) added to ansatz: [120]\n",
      "Gradients: [np.float64(-2.0894929266902547)]\n",
      "Initial energy: -6.327276154764737\n",
      "Optimizing energy with indices [228, 79, 228, 198, 120]...\n",
      "Starting point: [np.float64(-0.7853947065772768), np.float64(0.7853982468607503), np.float64(0.16357019741060347), np.float64(-0.16356963668333527), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615075626\n",
      "(change of -0.13682546031088894)\n",
      "Current ansatz: [228, 79, 228, 198, 120]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.000106842004955926\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258122057 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 244]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634967\n",
      "(change of -0.79785741488877)\n",
      "Current ansatz: [211, 244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273103219\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(3.7252880124103616)]\n",
      "Initial energy: -4.626284539634967\n",
      "Optimizing energy with indices [211, 244, 31]...\n",
      "Starting point: [np.float64(-0.2651612351142246), np.float64(0.44546905146305454), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625614727\n",
      "(change of -1.49682108597976)\n",
      "Current ansatz: [211, 244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917530720521563\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.485073567208433)]\n",
      "Initial energy: -6.123105625614727\n",
      "Optimizing energy with indices [211, 244, 31, 201]...\n",
      "Starting point: [np.float64(-0.12248992867574016), np.float64(0.7853981549207331), np.float64(-0.7853981610270105), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819711\n",
      "(change of -0.20417052920498424)\n",
      "Current ansatz: [211, 244, 31, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580955357\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0894929267315017)]\n",
      "Initial energy: -6.327276154819711\n",
      "Optimizing energy with indices [211, 244, 31, 201, 225]...\n",
      "Starting point: [np.float64(-0.16357019750392648), np.float64(0.7853981548243515), np.float64(-0.785398159357834), np.float64(0.16356963671034297), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151349096\n",
      "(change of -0.13682546031519838)\n",
      "Current ansatz: [211, 244, 31, 201, 225]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.7167511487574445e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580955357 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.0000000000000044)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 210]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.12310562561764\n",
      "(change of -0.12310562561763483)\n",
      "Current ansatz: [241, 79, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199634684\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.4850710475364073)]\n",
      "Initial energy: -6.12310562561764\n",
      "Optimizing energy with indices [241, 79, 210, 225]...\n",
      "Starting point: [np.float64(-0.7853981634187125), np.float64(0.7853981633741678), np.float64(0.12248927937103306), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020648)\n",
      "Current ansatz: [241, 79, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531805\n",
      "Operator(s) added to ansatz: [54]\n",
      "Gradients: [np.float64(-2.0894929267348794)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 79, 210, 225, 54]...\n",
      "Starting point: [np.float64(-0.7853981633782906), np.float64(0.7853981634050643), np.float64(0.16357019740837012), np.float64(-0.16356963668287453), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135389\n",
      "(change of -0.1368254603156842)\n",
      "Current ansatz: [241, 79, 210, 225, 54]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5896041675932895e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531805 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.999999999952205)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 216]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056252069065\n",
      "(change of -0.12310562527738877)\n",
      "Current ansatz: [228, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917518430081312\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.4850667159374624)]\n",
      "Initial energy: -6.1231056252069065\n",
      "Optimizing energy with indices [228, 79, 216, 201]...\n",
      "Starting point: [np.float64(-0.7853879048615375), np.float64(0.7853901599313474), np.float64(-0.12248816317024809), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.3272761548213765\n",
      "(change of -0.20417052961447002)\n",
      "Current ansatz: [228, 79, 216, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962087341802\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.089491704704269)]\n",
      "Initial energy: -6.3272761548213765\n",
      "Optimizing energy with indices [228, 79, 216, 201, 177]...\n",
      "Starting point: [np.float64(-0.785398172787821), np.float64(0.7853981748078923), np.float64(-0.1635698366550077), np.float64(0.16356985419052347), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614959846\n",
      "(change of -0.13682546013846952)\n",
      "Current ansatz: [228, 79, 216, 201, 177]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00021445472702253258\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962087341802 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000004\n",
      "(change of -1.7639320225002084)\n",
      "Current ansatz: [241, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132728\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000004\n",
      "Optimizing energy with indices [241, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853981634264042), np.float64(-0.7853981633494633), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617646\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327169\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710484797047)]\n",
      "Initial energy: -6.123105625617646\n",
      "Optimizing energy with indices [241, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.785398163997711), np.float64(-0.7853981625399766), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197085\n",
      "(change of -0.20417052920206213)\n",
      "Current ansatz: [241, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531922\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.089492926734948)]\n",
      "Initial energy: -6.3272761548197085\n",
      "Optimizing energy with indices [241, 74, 225, 210, 57]...\n",
      "Starting point: [np.float64(-0.7853981657335386), np.float64(-0.7853981641901568), np.float64(-0.16357019740840956), np.float64(0.16356963668286917), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615120212\n",
      "(change of -0.13682546030050347)\n",
      "Current ansatz: [241, 74, 225, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 5.841823248781745e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531922 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072685\n",
      "(change of -0.13682546025269726)\n",
      "Current ansatz: [244, 79, 228, 210, 147]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605784767053\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000004)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 216]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617655\n",
      "(change of -0.12310562561764904)\n",
      "Current ansatz: [244, 31, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620132718\n",
      "Operator(s) added to ansatz: [75]\n",
      "Gradients: [np.float64(2.4850710484797074)]\n",
      "Initial energy: -6.123105625617655\n",
      "Optimizing energy with indices [244, 31, 216, 75]...\n",
      "Starting point: [np.float64(0.7853981646238432), np.float64(-0.7853981621539974), np.float64(-0.1224892796141143), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819986\n",
      "(change of -0.20417052920233036)\n",
      "Current ansatz: [244, 31, 216, 75]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640447491155\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.0894916409098707)]\n",
      "Initial energy: -6.327276154819986\n",
      "Optimizing energy with indices [244, 31, 216, 75, 201]...\n",
      "Starting point: [np.float64(0.7853981649668343), np.float64(-0.7853981676916625), np.float64(-0.16357028669037835), np.float64(-0.16356997271614918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072744\n",
      "(change of -0.13682546025275855)\n",
      "Current ansatz: [244, 31, 216, 75, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013506586546068505\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640447491155 > 1e-05)\n",
      "Pool will be tiled from 24 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 20\n",
      "No pre-computed energy for given parameters.\n",
      "Solving by DMRG with quimb.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/.venv/adapt/lib/python3.13/site-packages/cotengra/hyperoptimizers/hyper.py:55: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization. It is recommended to install one of these libraries for higher quality contraction paths.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -3.47299e+01-4.61853e-14j\n",
      "Tiled pool has 297 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -19.00000000000003\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 3.9999999999999996\n",
      "Operator 1: -4.000000000000045\n",
      "Operator 2: 4.000000000000009\n",
      "Operator 3: -3.99999999999999\n",
      "Operator 4: 4.000000000000025\n",
      "Operator 5: -4.000000000000014\n",
      "Operator 6: 4.00000000000002\n",
      "Operator 7: -4.0000000000000115\n",
      "Operator 8: 4.000000000000016\n",
      "Operator 9: -4.000000000000035\n",
      "Operator 10: 4.000000000000042\n",
      "Operator 11: -4.00000000000003\n",
      "Operator 12: 4.0000000000000515\n",
      "Operator 13: -4.00000000000003\n",
      "Operator 14: 4.000000000000028\n",
      "Operator 15: -4.000000000000019\n",
      "Operator 16: 4.000000000000017\n",
      "Operator 17: -4.000000000000009\n",
      "Operator 18: 4.000000000000006\n",
      "Operator 19: -3.9999999999999996\n",
      "Operator 20: 4.000000000000045\n",
      "Operator 21: -4.000000000000009\n",
      "Operator 22: 3.99999999999999\n",
      "Operator 23: -4.000000000000025\n",
      "Operator 24: 4.000000000000014\n",
      "Operator 25: -4.00000000000002\n",
      "Operator 26: 4.0000000000000115\n",
      "Operator 27: -4.000000000000016\n",
      "Operator 28: 4.000000000000035\n",
      "Operator 29: -4.000000000000042\n",
      "Operator 30: 4.00000000000003\n",
      "Operator 31: -4.0000000000000515\n",
      "Operator 32: 4.00000000000003\n",
      "Operator 33: -4.000000000000028\n",
      "Operator 34: 3.9999999999999996\n",
      "Operator 35: 4.000000000000045\n",
      "Operator 36: 4.000000000000009\n",
      "Operator 37: 3.99999999999999\n",
      "Operator 38: 4.000000000000025\n",
      "Operator 39: 4.000000000000014\n",
      "Operator 40: 4.00000000000002\n",
      "Operator 41: 4.0000000000000115\n",
      "Operator 42: 4.000000000000016\n",
      "Operator 43: 4.000000000000035\n",
      "Operator 44: 4.000000000000042\n",
      "Operator 45: 4.00000000000003\n",
      "Operator 46: 4.0000000000000515\n",
      "Operator 47: 4.00000000000003\n",
      "Operator 48: 4.000000000000028\n",
      "Operator 49: 4.000000000000019\n",
      "Operator 50: 4.000000000000017\n",
      "Operator 51: 4.000000000000006\n",
      "Operator 52: 3.9999999999999996\n",
      "Operator 53: 4.000000000000045\n",
      "Operator 54: 4.000000000000009\n",
      "Operator 55: 3.99999999999999\n",
      "Operator 56: 4.000000000000025\n",
      "Operator 57: 4.000000000000014\n",
      "Operator 58: 4.00000000000002\n",
      "Operator 59: 4.0000000000000115\n",
      "Operator 60: 4.000000000000016\n",
      "Operator 61: 4.000000000000035\n",
      "Operator 62: 4.000000000000042\n",
      "Operator 63: 4.00000000000003\n",
      "Operator 64: 4.0000000000000515\n",
      "Operator 65: 4.00000000000003\n",
      "Operator 66: 4.000000000000028\n",
      "Operator 67: 4.000000000000019\n",
      "Operator 85: -3.9999999999999996\n",
      "Operator 86: -4.000000000000045\n",
      "Operator 87: -4.000000000000009\n",
      "Operator 88: -3.99999999999999\n",
      "Operator 89: -4.000000000000025\n",
      "Operator 90: -4.000000000000014\n",
      "Operator 91: -4.00000000000002\n",
      "Operator 92: -4.0000000000000115\n",
      "Operator 93: -4.000000000000016\n",
      "Operator 94: -4.000000000000035\n",
      "Operator 95: -4.000000000000042\n",
      "Operator 96: -4.00000000000003\n",
      "Operator 97: -4.0000000000000515\n",
      "Operator 98: -4.00000000000003\n",
      "Operator 99: -4.000000000000028\n",
      "Operator 100: -4.000000000000019\n",
      "Operator 101: -4.000000000000017\n",
      "Operator 102: 4.000000000000006\n",
      "Operator 103: -4.000000000000006\n",
      "Operator 104: -3.9999999999999996\n",
      "Operator 105: -4.000000000000045\n",
      "Operator 106: -4.000000000000009\n",
      "Operator 107: -3.99999999999999\n",
      "Operator 108: -4.000000000000025\n",
      "Operator 109: -4.000000000000014\n",
      "Operator 110: -4.00000000000002\n",
      "Operator 111: -4.0000000000000115\n",
      "Operator 112: -4.000000000000016\n",
      "Operator 113: -4.000000000000035\n",
      "Operator 114: -4.000000000000042\n",
      "Operator 115: -4.00000000000003\n",
      "Operator 116: -4.0000000000000515\n",
      "Operator 117: -4.00000000000003\n",
      "Operator 118: -4.000000000000028\n",
      "Operator 119: -4.000000000000019\n",
      "Operator 120: 4.000000000000019\n",
      "Operator 121: 3.9999999999999996\n",
      "Operator 122: 4.000000000000045\n",
      "Operator 123: 4.000000000000009\n",
      "Operator 124: 3.99999999999999\n",
      "Operator 125: 4.000000000000025\n",
      "Operator 126: 4.000000000000014\n",
      "Operator 127: 4.00000000000002\n",
      "Operator 128: 4.0000000000000115\n",
      "Operator 129: 4.000000000000016\n",
      "Operator 130: 4.000000000000035\n",
      "Operator 131: 4.000000000000042\n",
      "Operator 132: 4.00000000000003\n",
      "Operator 133: 4.0000000000000515\n",
      "Operator 134: 4.00000000000003\n",
      "Operator 135: 4.000000000000028\n",
      "Operator 136: 4.000000000000019\n",
      "Operator 137: 4.000000000000017\n",
      "Operator 155: -4.000000000000017\n",
      "Operator 156: -4.000000000000006\n",
      "Operator 157: 4.000000000000009\n",
      "Operator 158: -4.000000000000006\n",
      "Operator 159: 3.9999999999999996\n",
      "Operator 160: -4.000000000000045\n",
      "Operator 161: 4.000000000000009\n",
      "Operator 162: -3.99999999999999\n",
      "Operator 163: 4.000000000000025\n",
      "Operator 164: -4.000000000000014\n",
      "Operator 165: 4.00000000000002\n",
      "Operator 166: -4.0000000000000115\n",
      "Operator 167: 4.000000000000016\n",
      "Operator 168: -4.000000000000035\n",
      "Operator 169: 4.000000000000042\n",
      "Operator 170: -4.00000000000003\n",
      "Operator 171: 4.0000000000000515\n",
      "Operator 172: -4.00000000000003\n",
      "Operator 173: 4.000000000000028\n",
      "Operator 174: 4.000000000000009\n",
      "Operator 175: -4.000000000000006\n",
      "Operator 210: -4.000000000000009\n",
      "Operator 211: 4.000000000000006\n",
      "Operator 212: -3.9999999999999996\n",
      "Operator 213: 4.000000000000045\n",
      "Operator 214: -4.000000000000009\n",
      "Operator 215: 3.99999999999999\n",
      "Operator 216: -4.000000000000025\n",
      "Operator 217: 4.000000000000014\n",
      "Operator 218: -4.00000000000002\n",
      "Operator 219: 4.0000000000000115\n",
      "Operator 220: -4.000000000000016\n",
      "Operator 221: 4.000000000000035\n",
      "Operator 222: -4.000000000000042\n",
      "Operator 223: 4.00000000000003\n",
      "Operator 224: -4.0000000000000515\n",
      "Operator 225: 4.00000000000003\n",
      "Operator 226: -4.000000000000028\n",
      "Operator 261: -4.000000000000009\n",
      "Total gradient norm: 50.59644256269434\n",
      "Operators under consideration (1):\n",
      "[226]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000028)]\n",
      "Operator(s) added to ansatz: [226]\n",
      "Gradients: [np.float64(-4.000000000000028)]\n",
      "Initial energy: -19.00000000000003\n",
      "Optimizing energy with indices [226]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19.828427\n",
      "         Iterations: 4\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "\n",
      "Current energy: -19.828427124746213\n",
      "(change of -0.8284271247461845)\n",
      "Current ansatz: [226]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.0\n",
      "Operator 1: -4.000000000000045\n",
      "Operator 2: 4.000000000000011\n",
      "Operator 3: -3.9999999999999902\n",
      "Operator 4: 4.000000000000025\n",
      "Operator 5: -4.000000000000012\n",
      "Operator 6: 4.0000000000000195\n",
      "Operator 7: -4.00000000000001\n",
      "Operator 8: 4.000000000000014\n",
      "Operator 9: -4.000000000000034\n",
      "Operator 10: 4.000000000000041\n",
      "Operator 11: -4.00000000000003\n",
      "Operator 12: 4.00000000000005\n",
      "Operator 13: -3.414213562369187\n",
      "Operator 15: -3.4142135623691794\n",
      "Operator 16: 4.000000000000012\n",
      "Operator 17: -4.000000000000009\n",
      "Operator 18: 4.000000000000006\n",
      "Operator 19: -4.0\n",
      "Operator 20: 4.000000000000045\n",
      "Operator 21: -4.000000000000011\n",
      "Operator 22: 3.9999999999999902\n",
      "Operator 23: -4.000000000000025\n",
      "Operator 24: 4.000000000000012\n",
      "Operator 25: -4.0000000000000195\n",
      "Operator 26: 4.00000000000001\n",
      "Operator 27: -4.000000000000014\n",
      "Operator 28: 4.000000000000034\n",
      "Operator 29: -4.000000000000041\n",
      "Operator 30: 4.00000000000003\n",
      "Operator 31: -4.00000000000005\n",
      "Operator 32: 3.4142135623691883\n",
      "Operator 34: 4.0\n",
      "Operator 35: 4.000000000000045\n",
      "Operator 36: 4.000000000000011\n",
      "Operator 37: 3.9999999999999902\n",
      "Operator 38: 4.000000000000025\n",
      "Operator 39: 4.000000000000012\n",
      "Operator 40: 4.0000000000000195\n",
      "Operator 41: 4.00000000000001\n",
      "Operator 42: 4.000000000000014\n",
      "Operator 43: 4.000000000000034\n",
      "Operator 44: 4.000000000000041\n",
      "Operator 45: 4.00000000000003\n",
      "Operator 46: 4.00000000000005\n",
      "Operator 47: 3.4142135623691883\n",
      "Operator 49: 3.4142135623691794\n",
      "Operator 50: 2.828427124738335\n",
      "Operator 51: 4.000000000000006\n",
      "Operator 52: 4.0\n",
      "Operator 53: 4.000000000000045\n",
      "Operator 54: 4.000000000000011\n",
      "Operator 55: 3.9999999999999902\n",
      "Operator 56: 4.000000000000025\n",
      "Operator 57: 4.000000000000012\n",
      "Operator 58: 4.0000000000000195\n",
      "Operator 59: 4.00000000000001\n",
      "Operator 60: 4.000000000000014\n",
      "Operator 61: 4.000000000000034\n",
      "Operator 62: 4.000000000000041\n",
      "Operator 63: 4.00000000000003\n",
      "Operator 64: 2.8284271247383597\n",
      "Operator 65: 3.4142135623691883\n",
      "Operator 67: 3.4142135623691794\n",
      "Operator 85: -4.0\n",
      "Operator 86: -4.000000000000045\n",
      "Operator 87: -4.000000000000011\n",
      "Operator 88: -3.9999999999999902\n",
      "Operator 89: -4.000000000000025\n",
      "Operator 90: -4.000000000000012\n",
      "Operator 91: -4.0000000000000195\n",
      "Operator 92: -4.00000000000001\n",
      "Operator 93: -4.000000000000014\n",
      "Operator 94: -4.000000000000034\n",
      "Operator 95: -4.000000000000041\n",
      "Operator 96: -4.00000000000003\n",
      "Operator 97: -4.00000000000005\n",
      "Operator 98: -3.414213562369187\n",
      "Operator 100: -3.4142135623691767\n",
      "Operator 101: -2.828427124738335\n",
      "Operator 102: 4.000000000000006\n",
      "Operator 103: -4.000000000000006\n",
      "Operator 104: -4.0\n",
      "Operator 105: -4.000000000000045\n",
      "Operator 106: -4.000000000000011\n",
      "Operator 107: -3.9999999999999902\n",
      "Operator 108: -4.000000000000025\n",
      "Operator 109: -4.000000000000012\n",
      "Operator 110: -4.0000000000000195\n",
      "Operator 111: -4.00000000000001\n",
      "Operator 112: -4.000000000000014\n",
      "Operator 113: -4.000000000000034\n",
      "Operator 114: -4.000000000000041\n",
      "Operator 115: -4.00000000000003\n",
      "Operator 116: -2.8284271247383597\n",
      "Operator 117: -3.414213562369187\n",
      "Operator 119: -3.4142135623691767\n",
      "Operator 120: 3.4142135623691767\n",
      "Operator 121: 4.0\n",
      "Operator 122: 4.000000000000045\n",
      "Operator 123: 4.000000000000011\n",
      "Operator 124: 3.9999999999999902\n",
      "Operator 125: 4.000000000000025\n",
      "Operator 126: 4.000000000000012\n",
      "Operator 127: 4.0000000000000195\n",
      "Operator 128: 4.00000000000001\n",
      "Operator 129: 4.000000000000014\n",
      "Operator 130: 4.000000000000034\n",
      "Operator 131: 4.000000000000041\n",
      "Operator 132: 4.00000000000003\n",
      "Operator 133: 4.00000000000005\n",
      "Operator 134: 3.414213562369187\n",
      "Operator 136: 3.4142135623691794\n",
      "Operator 137: 2.828427124738335\n",
      "Operator 153: -1.4142135623770378\n",
      "Operator 154: 1.4142135623770302\n",
      "Operator 155: -4.000000000000012\n",
      "Operator 156: -4.000000000000006\n",
      "Operator 157: 4.000000000000009\n",
      "Operator 158: -4.000000000000006\n",
      "Operator 159: 4.0\n",
      "Operator 160: -4.000000000000045\n",
      "Operator 161: 4.000000000000011\n",
      "Operator 162: -3.9999999999999902\n",
      "Operator 163: 4.000000000000025\n",
      "Operator 164: -4.000000000000012\n",
      "Operator 165: 4.0000000000000195\n",
      "Operator 166: -4.00000000000001\n",
      "Operator 167: 4.000000000000014\n",
      "Operator 168: -4.000000000000034\n",
      "Operator 169: 4.000000000000041\n",
      "Operator 170: -2.8284271247383455\n",
      "Operator 171: 4.00000000000005\n",
      "Operator 172: -3.414213562369187\n",
      "Operator 174: 4.000000000000009\n",
      "Operator 175: -4.000000000000006\n",
      "Operator 210: -4.000000000000009\n",
      "Operator 211: 4.000000000000006\n",
      "Operator 212: -4.0\n",
      "Operator 213: 4.000000000000045\n",
      "Operator 214: -4.000000000000011\n",
      "Operator 215: 3.9999999999999902\n",
      "Operator 216: -4.000000000000025\n",
      "Operator 217: 4.000000000000012\n",
      "Operator 218: -4.0000000000000195\n",
      "Operator 219: 4.00000000000001\n",
      "Operator 220: -4.000000000000014\n",
      "Operator 221: 4.000000000000034\n",
      "Operator 222: -4.000000000000041\n",
      "Operator 223: 2.8284271247383455\n",
      "Operator 224: -4.00000000000005\n",
      "Operator 225: 3.4142135623691883\n",
      "Operator 241: -1.4142135623770378\n",
      "Operator 242: 1.4142135623770302\n",
      "Operator 258: 1.4142135623770355\n",
      "Operator 259: -1.4142135623770358\n",
      "Operator 261: -4.000000000000009\n",
      "Operator 277: -1.4142135623770378\n",
      "Operator 278: 1.4142135623770302\n",
      "Operator 295: 1.4142135623770355\n",
      "Operator 296: -1.4142135623770358\n",
      "Total gradient norm: 48.06776121259726\n",
      "Operators under consideration (1):\n",
      "[261]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000009)]\n",
      "Operator(s) added to ansatz: [261]\n",
      "Gradients: [np.float64(-4.000000000000009)]\n",
      "Initial energy: -19.828427124746213\n",
      "Optimizing energy with indices [226, 261]...\n",
      "Starting point: [np.float64(0.39269908170011425), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -21.064495\n",
      "         Iterations: 8\n",
      "         Function evaluations: 45\n",
      "         Gradient evaluations: 33\n",
      "\n",
      "Current energy: -21.064495102246088\n",
      "(change of -1.236067977499875)\n",
      "Current ansatz: [226, 261]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000025\n",
      "Operator 1: -4.000000000000067\n",
      "Operator 2: 4.000000000000029\n",
      "Operator 3: -4.000000000000008\n",
      "Operator 4: 4.000000000000046\n",
      "Operator 5: -4.0000000000000355\n",
      "Operator 6: 4.000000000000042\n",
      "Operator 7: -4.000000000000034\n",
      "Operator 8: 4.000000000000038\n",
      "Operator 9: -4.000000000000058\n",
      "Operator 10: 4.000000000000066\n",
      "Operator 11: -4.000000000000055\n",
      "Operator 12: 4.000000000000073\n",
      "Operator 13: -3.4142136858175043\n",
      "Operator 14: 4.93777490007119e-07\n",
      "Operator 15: -3.414213685817496\n",
      "Operator 16: 4.0000000000000355\n",
      "Operator 17: -4.3315740310487983e-07\n",
      "Operator 18: 2.8944273642628655\n",
      "Operator 19: -4.000000000000025\n",
      "Operator 20: 4.000000000000067\n",
      "Operator 21: -4.000000000000029\n",
      "Operator 22: 4.000000000000008\n",
      "Operator 23: -4.000000000000046\n",
      "Operator 24: 4.0000000000000355\n",
      "Operator 25: -4.000000000000042\n",
      "Operator 26: 4.000000000000034\n",
      "Operator 27: -4.000000000000038\n",
      "Operator 28: 4.000000000000058\n",
      "Operator 29: -4.000000000000066\n",
      "Operator 30: 4.000000000000055\n",
      "Operator 31: -4.000000000000073\n",
      "Operator 32: 3.4142136858175056\n",
      "Operator 33: -4.93777490007119e-07\n",
      "Operator 34: 1.7888547285257048\n",
      "Operator 35: 4.000000000000067\n",
      "Operator 36: 4.000000000000029\n",
      "Operator 37: 4.000000000000008\n",
      "Operator 38: 4.000000000000046\n",
      "Operator 39: 4.0000000000000355\n",
      "Operator 40: 4.000000000000042\n",
      "Operator 41: 4.000000000000034\n",
      "Operator 42: 4.000000000000038\n",
      "Operator 43: 4.000000000000058\n",
      "Operator 44: 4.000000000000066\n",
      "Operator 45: 4.000000000000055\n",
      "Operator 46: 4.000000000000073\n",
      "Operator 47: 3.4142136858175056\n",
      "Operator 48: 4.93777490007119e-07\n",
      "Operator 49: 3.414213685817496\n",
      "Operator 50: 2.8284273716349464\n",
      "Operator 51: 2.894427364262874\n",
      "Operator 52: 4.000000000000025\n",
      "Operator 53: 4.000000000000067\n",
      "Operator 54: 4.000000000000029\n",
      "Operator 55: 4.000000000000008\n",
      "Operator 56: 4.000000000000046\n",
      "Operator 57: 4.0000000000000355\n",
      "Operator 58: 4.000000000000042\n",
      "Operator 59: 4.000000000000034\n",
      "Operator 60: 4.000000000000038\n",
      "Operator 61: 4.000000000000058\n",
      "Operator 62: 4.000000000000066\n",
      "Operator 63: 4.000000000000055\n",
      "Operator 64: 2.8284273716349713\n",
      "Operator 65: 3.4142136858175056\n",
      "Operator 66: 4.93777490007119e-07\n",
      "Operator 67: 3.414213685817496\n",
      "Operator 85: -1.7888547285257048\n",
      "Operator 86: -4.000000000000067\n",
      "Operator 87: -4.000000000000029\n",
      "Operator 88: -4.000000000000008\n",
      "Operator 89: -4.000000000000046\n",
      "Operator 90: -4.0000000000000355\n",
      "Operator 91: -4.000000000000042\n",
      "Operator 92: -4.000000000000034\n",
      "Operator 93: -4.000000000000038\n",
      "Operator 94: -4.000000000000058\n",
      "Operator 95: -4.000000000000066\n",
      "Operator 96: -4.000000000000055\n",
      "Operator 97: -4.000000000000073\n",
      "Operator 98: -3.4142136858175043\n",
      "Operator 99: -4.93777490007119e-07\n",
      "Operator 100: -3.414213685817493\n",
      "Operator 101: -2.8284273716349464\n",
      "Operator 102: 2.894427364262874\n",
      "Operator 103: -2.8944273642628655\n",
      "Operator 104: -4.000000000000025\n",
      "Operator 105: -4.000000000000067\n",
      "Operator 106: -4.000000000000029\n",
      "Operator 107: -4.000000000000008\n",
      "Operator 108: -4.000000000000046\n",
      "Operator 109: -4.0000000000000355\n",
      "Operator 110: -4.000000000000042\n",
      "Operator 111: -4.000000000000034\n",
      "Operator 112: -4.000000000000038\n",
      "Operator 113: -4.000000000000058\n",
      "Operator 114: -4.000000000000066\n",
      "Operator 115: -4.000000000000055\n",
      "Operator 116: -2.8284273716349713\n",
      "Operator 117: -3.4142136858175043\n",
      "Operator 118: -4.93777490007119e-07\n",
      "Operator 119: -3.414213685817493\n",
      "Operator 120: 3.414213685817493\n",
      "Operator 121: 1.7888547285257048\n",
      "Operator 122: 1.7888547285257288\n",
      "Operator 123: 4.000000000000029\n",
      "Operator 124: 4.000000000000008\n",
      "Operator 125: 4.000000000000046\n",
      "Operator 126: 4.0000000000000355\n",
      "Operator 127: 4.000000000000042\n",
      "Operator 128: 4.000000000000034\n",
      "Operator 129: 4.000000000000038\n",
      "Operator 130: 4.000000000000058\n",
      "Operator 131: 4.000000000000066\n",
      "Operator 132: 4.000000000000055\n",
      "Operator 133: 4.000000000000073\n",
      "Operator 134: 3.4142136858175043\n",
      "Operator 135: 4.93777490007119e-07\n",
      "Operator 136: 3.414213685817496\n",
      "Operator 137: 2.8284273716349464\n",
      "Operator 138: 1.7888542953683564\n",
      "Operator 153: -1.4142134389287382\n",
      "Operator 154: 1.41421343892873\n",
      "Operator 155: -4.0000000000000355\n",
      "Operator 156: -2.894427364262865\n",
      "Operator 157: 4.3315740310487983e-07\n",
      "Operator 158: -2.8944273642628655\n",
      "Operator 159: 4.000000000000025\n",
      "Operator 160: -4.000000000000067\n",
      "Operator 161: 4.000000000000029\n",
      "Operator 162: -4.000000000000008\n",
      "Operator 163: 4.000000000000046\n",
      "Operator 164: -4.0000000000000355\n",
      "Operator 165: 4.000000000000042\n",
      "Operator 166: -4.000000000000034\n",
      "Operator 167: 4.000000000000038\n",
      "Operator 168: -4.000000000000058\n",
      "Operator 169: 4.000000000000066\n",
      "Operator 170: -2.828427371634959\n",
      "Operator 171: 4.000000000000073\n",
      "Operator 172: -3.4142136858175043\n",
      "Operator 173: 4.93777490007119e-07\n",
      "Operator 174: 4.3315740310487983e-07\n",
      "Operator 175: -2.894427364262874\n",
      "Operator 210: -4.3315740310487983e-07\n",
      "Operator 211: 2.894427364262874\n",
      "Operator 212: -4.000000000000025\n",
      "Operator 213: 4.000000000000067\n",
      "Operator 214: -4.000000000000029\n",
      "Operator 215: 4.000000000000008\n",
      "Operator 216: -4.000000000000046\n",
      "Operator 217: 4.0000000000000355\n",
      "Operator 218: -4.000000000000042\n",
      "Operator 219: 4.000000000000034\n",
      "Operator 220: -4.000000000000038\n",
      "Operator 221: 4.000000000000058\n",
      "Operator 222: -4.000000000000066\n",
      "Operator 223: 2.828427371634959\n",
      "Operator 224: -4.000000000000073\n",
      "Operator 225: 3.4142136858175056\n",
      "Operator 226: -4.93777490007119e-07\n",
      "Operator 241: -1.4142134389287382\n",
      "Operator 242: 1.41421343892873\n",
      "Operator 258: 1.4142134389287355\n",
      "Operator 259: -1.4142134389287357\n",
      "Operator 261: -4.3315740310487983e-07\n",
      "Operator 262: -1.7888542953683713\n",
      "Operator 277: -1.4142134389287382\n",
      "Operator 278: 1.41421343892873\n",
      "Operator 280: -1.7888542953683713\n",
      "Operator 295: 1.4142134389287355\n",
      "Operator 296: -1.4142134389287357\n",
      "Total gradient norm: 46.12950649994837\n",
      "Operators under consideration (1):\n",
      "[224]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000073)]\n",
      "Operator(s) added to ansatz: [224]\n",
      "Gradients: [np.float64(-4.000000000000073)]\n",
      "Initial energy: -21.064495102246088\n",
      "Optimizing energy with indices [226, 261, 224]...\n",
      "Starting point: [np.float64(0.3926990380545486), np.float64(0.5535743104685809), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21.999366\n",
      "         Iterations: 7\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "\n",
      "Current energy: -21.999365806054517\n",
      "(change of -0.9348707038084285)\n",
      "Current ansatz: [226, 261, 224]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.0000000000000275\n",
      "Operator 1: -4.000000000000068\n",
      "Operator 2: 4.000000000000033\n",
      "Operator 3: -4.00000000000001\n",
      "Operator 4: 4.000000000000048\n",
      "Operator 5: -4.000000000000037\n",
      "Operator 6: 4.0000000000000435\n",
      "Operator 7: -4.000000000000035\n",
      "Operator 8: 4.000000000000039\n",
      "Operator 9: -4.0000000000000595\n",
      "Operator 10: 4.000000000000066\n",
      "Operator 11: -3.2645854460254595\n",
      "Operator 13: -2.529170893086169\n",
      "Operator 15: -3.2645854470607545\n",
      "Operator 16: 4.000000000000032\n",
      "Operator 18: 2.8944271908762325\n",
      "Operator 19: -4.0000000000000275\n",
      "Operator 20: 4.000000000000068\n",
      "Operator 21: -4.000000000000033\n",
      "Operator 22: 4.00000000000001\n",
      "Operator 23: -4.000000000000048\n",
      "Operator 24: 4.000000000000037\n",
      "Operator 25: -4.0000000000000435\n",
      "Operator 26: 4.000000000000035\n",
      "Operator 27: -4.000000000000039\n",
      "Operator 28: 4.0000000000000595\n",
      "Operator 29: -4.000000000000066\n",
      "Operator 30: 3.2645854460254626\n",
      "Operator 32: 2.5291708930861705\n",
      "Operator 34: 1.7888543817524396\n",
      "Operator 35: 4.000000000000068\n",
      "Operator 36: 4.000000000000033\n",
      "Operator 37: 4.00000000000001\n",
      "Operator 38: 4.000000000000048\n",
      "Operator 39: 4.000000000000037\n",
      "Operator 40: 4.0000000000000435\n",
      "Operator 41: 4.000000000000035\n",
      "Operator 42: 4.000000000000039\n",
      "Operator 43: 4.0000000000000595\n",
      "Operator 44: 4.000000000000066\n",
      "Operator 45: 3.2645854460254626\n",
      "Operator 47: 2.7995881758043017\n",
      "Operator 48: -0.929994538044709\n",
      "Operator 49: 3.2645854470607545\n",
      "Operator 50: 2.5291708941214632\n",
      "Operator 51: 2.8944271908762422\n",
      "Operator 52: 4.0000000000000275\n",
      "Operator 53: 4.000000000000068\n",
      "Operator 54: 4.000000000000033\n",
      "Operator 55: 4.00000000000001\n",
      "Operator 56: 4.000000000000048\n",
      "Operator 57: 4.000000000000037\n",
      "Operator 58: 4.0000000000000435\n",
      "Operator 59: 4.000000000000035\n",
      "Operator 60: 4.000000000000039\n",
      "Operator 61: 4.0000000000000595\n",
      "Operator 62: 2.5291708920508724\n",
      "Operator 63: 3.2645854460254626\n",
      "Operator 64: -0.9299945402260151\n",
      "Operator 65: 2.7995881758043017\n",
      "Operator 67: 3.2645854470607545\n",
      "Operator 82: -1.2004118241957462\n",
      "Operator 85: -1.7888543817524396\n",
      "Operator 86: -4.000000000000068\n",
      "Operator 87: -4.000000000000033\n",
      "Operator 88: -4.00000000000001\n",
      "Operator 89: -4.000000000000048\n",
      "Operator 90: -4.000000000000037\n",
      "Operator 91: -4.0000000000000435\n",
      "Operator 92: -4.000000000000035\n",
      "Operator 93: -4.000000000000039\n",
      "Operator 94: -4.0000000000000595\n",
      "Operator 95: -4.000000000000066\n",
      "Operator 96: -3.2645854460254595\n",
      "Operator 98: -2.7995881758043\n",
      "Operator 99: 0.929994538044709\n",
      "Operator 100: -3.264585447060751\n",
      "Operator 101: -2.5291708941214632\n",
      "Operator 102: 2.8944271908762422\n",
      "Operator 103: -2.8944271908762325\n",
      "Operator 104: -4.0000000000000275\n",
      "Operator 105: -4.000000000000068\n",
      "Operator 106: -4.000000000000033\n",
      "Operator 107: -4.00000000000001\n",
      "Operator 108: -4.000000000000048\n",
      "Operator 109: -4.000000000000037\n",
      "Operator 110: -4.0000000000000435\n",
      "Operator 111: -4.000000000000035\n",
      "Operator 112: -4.000000000000039\n",
      "Operator 113: -4.0000000000000595\n",
      "Operator 114: -2.5291708920508724\n",
      "Operator 115: -3.264585446025459\n",
      "Operator 116: 0.9299945402260151\n",
      "Operator 117: -2.7995881758043004\n",
      "Operator 119: -3.264585447060751\n",
      "Operator 120: 3.264585447060751\n",
      "Operator 121: 1.7888543817524396\n",
      "Operator 122: 1.7888543817524631\n",
      "Operator 123: 4.000000000000033\n",
      "Operator 124: 4.00000000000001\n",
      "Operator 125: 4.000000000000048\n",
      "Operator 126: 4.000000000000037\n",
      "Operator 127: 4.0000000000000435\n",
      "Operator 128: 4.000000000000035\n",
      "Operator 129: 4.000000000000039\n",
      "Operator 130: 4.0000000000000595\n",
      "Operator 131: 4.000000000000066\n",
      "Operator 132: 3.2645854460254595\n",
      "Operator 134: 2.529170893086169\n",
      "Operator 135: -0.929994538044709\n",
      "Operator 136: 2.0641736218297027\n",
      "Operator 137: 2.5291708941214632\n",
      "Operator 138: 1.7888543820616838\n",
      "Operator 151: -0.9797117443573911\n",
      "Operator 152: 1.5494591474771955\n",
      "Operator 153: -0.979711743021044\n",
      "Operator 154: 1.5494591466322265\n",
      "Operator 155: -4.000000000000032\n",
      "Operator 156: -2.8944271908762325\n",
      "Operator 158: -2.8944271908762325\n",
      "Operator 159: 4.0000000000000275\n",
      "Operator 160: -4.000000000000068\n",
      "Operator 161: 4.000000000000033\n",
      "Operator 162: -4.00000000000001\n",
      "Operator 163: 4.000000000000048\n",
      "Operator 164: -4.000000000000037\n",
      "Operator 165: 4.0000000000000435\n",
      "Operator 166: -4.000000000000035\n",
      "Operator 167: 4.000000000000039\n",
      "Operator 168: -2.5291708920508684\n",
      "Operator 169: 4.000000000000066\n",
      "Operator 170: -2.0641736228650163\n",
      "Operator 172: -2.7995881758043004\n",
      "Operator 175: -2.8944271908762422\n",
      "Operator 190: 1.2004118241957473\n",
      "Operator 207: -1.2004118241957462\n",
      "Operator 211: 2.8944271908762422\n",
      "Operator 212: -4.0000000000000275\n",
      "Operator 213: 4.000000000000068\n",
      "Operator 214: -4.000000000000033\n",
      "Operator 215: 4.00000000000001\n",
      "Operator 216: -4.000000000000048\n",
      "Operator 217: 4.000000000000037\n",
      "Operator 218: -4.0000000000000435\n",
      "Operator 219: 4.000000000000035\n",
      "Operator 220: -4.000000000000039\n",
      "Operator 221: 2.5291708920508684\n",
      "Operator 222: -4.000000000000066\n",
      "Operator 223: 2.0641736228650185\n",
      "Operator 225: 2.7995881758043017\n",
      "Operator 239: -1.5494591474772013\n",
      "Operator 240: 1.5494591474771955\n",
      "Operator 241: -1.5494591466322347\n",
      "Operator 242: 1.5494591466322265\n",
      "Operator 256: 1.5494591474771948\n",
      "Operator 257: -1.549459147477196\n",
      "Operator 258: 1.5494591466322327\n",
      "Operator 259: -1.5494591466322327\n",
      "Operator 262: -1.7888543820616993\n",
      "Operator 275: -1.5494591474772013\n",
      "Operator 276: 0.9797117443573891\n",
      "Operator 277: -1.5494591466322345\n",
      "Operator 278: 0.979711743021037\n",
      "Operator 280: -1.7888543820616993\n",
      "Operator 293: 0.9797117443573868\n",
      "Operator 294: -1.5494591474771964\n",
      "Operator 295: 0.9797117430210416\n",
      "Operator 296: -1.5494591466322327\n",
      "Total gradient norm: 43.44839488915033\n",
      "Operators under consideration (1):\n",
      "[222]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000066)]\n",
      "Operator(s) added to ansatz: [222]\n",
      "Gradients: [np.float64(-4.000000000000066)]\n",
      "Initial energy: -21.999365806054517\n",
      "Optimizing energy with indices [226, 261, 224, 222]...\n",
      "Starting point: [np.float64(0.4431436453178893), np.float64(0.55357435893162), np.float64(0.44314364565197617), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -22.968513\n",
      "         Iterations: 9\n",
      "         Function evaluations: 87\n",
      "         Gradient evaluations: 80\n",
      "\n",
      "Current energy: -22.968512542244305\n",
      "(change of -0.9691467361897885)\n",
      "Current ansatz: [226, 261, 224, 222]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000036\n",
      "Operator 1: -4.000000000000076\n",
      "Operator 2: 4.000000000000041\n",
      "Operator 3: -4.000000000000018\n",
      "Operator 4: 4.000000000000055\n",
      "Operator 5: -4.000000000000043\n",
      "Operator 6: 4.000000000000051\n",
      "Operator 7: -4.000000000000042\n",
      "Operator 8: 4.000000000000046\n",
      "Operator 9: -3.2087802172651108\n",
      "Operator 11: -2.2432916652169874\n",
      "Operator 13: -2.2432916613417473\n",
      "Operator 15: -3.2087802133898564\n",
      "Operator 16: 4.000000000000036\n",
      "Operator 18: 2.8944271933178585\n",
      "Operator 19: -4.000000000000036\n",
      "Operator 20: 4.000000000000076\n",
      "Operator 21: -4.000000000000041\n",
      "Operator 22: 4.000000000000018\n",
      "Operator 23: -4.000000000000055\n",
      "Operator 24: 4.000000000000043\n",
      "Operator 25: -4.000000000000051\n",
      "Operator 26: 4.000000000000042\n",
      "Operator 27: -4.000000000000046\n",
      "Operator 28: 3.208780217265114\n",
      "Operator 30: 2.2432916652169936\n",
      "Operator 32: 2.2432916613417486\n",
      "Operator 34: 1.7888543866356859\n",
      "Operator 35: 4.000000000000076\n",
      "Operator 36: 4.000000000000041\n",
      "Operator 37: 4.000000000000018\n",
      "Operator 38: 4.000000000000055\n",
      "Operator 39: 4.000000000000043\n",
      "Operator 40: 4.000000000000051\n",
      "Operator 41: 4.000000000000042\n",
      "Operator 42: 4.000000000000046\n",
      "Operator 43: 3.208780217265114\n",
      "Operator 45: 2.625248486409281\n",
      "Operator 46: -1.086413298274064\n",
      "Operator 47: 2.6252484844047883\n",
      "Operator 48: -1.1670634675430316\n",
      "Operator 49: 3.2087802133898564\n",
      "Operator 50: 2.4175604267796653\n",
      "Operator 51: 2.8944271933178682\n",
      "Operator 52: 4.000000000000036\n",
      "Operator 53: 4.000000000000076\n",
      "Operator 54: 4.000000000000041\n",
      "Operator 55: 4.000000000000018\n",
      "Operator 56: 4.000000000000055\n",
      "Operator 57: 4.000000000000043\n",
      "Operator 58: 4.000000000000051\n",
      "Operator 59: 4.000000000000042\n",
      "Operator 60: 2.4175604345301496\n",
      "Operator 61: 3.208780217265114\n",
      "Operator 62: -1.167063459073523\n",
      "Operator 63: 2.625248486409281\n",
      "Operator 64: -1.086413302283046\n",
      "Operator 65: 2.6252484844047883\n",
      "Operator 67: 3.2087802133898564\n",
      "Operator 80: -1.3636612923164628\n",
      "Operator 82: -1.3636612948324864\n",
      "Operator 85: -1.7888543866356859\n",
      "Operator 86: -4.000000000000076\n",
      "Operator 87: -4.000000000000041\n",
      "Operator 88: -4.000000000000018\n",
      "Operator 89: -4.000000000000055\n",
      "Operator 90: -4.000000000000043\n",
      "Operator 91: -4.000000000000051\n",
      "Operator 92: -4.000000000000042\n",
      "Operator 93: -4.000000000000046\n",
      "Operator 94: -3.2087802172651108\n",
      "Operator 96: -2.6252484864092813\n",
      "Operator 97: 1.086413298274064\n",
      "Operator 98: -2.6252484844047865\n",
      "Operator 99: 1.1670634675430316\n",
      "Operator 100: -3.208780213389853\n",
      "Operator 101: -2.4175604267796653\n",
      "Operator 102: 2.894427193317868\n",
      "Operator 103: -2.8944271933178585\n",
      "Operator 104: -4.000000000000036\n",
      "Operator 105: -4.000000000000076\n",
      "Operator 106: -4.000000000000041\n",
      "Operator 107: -4.000000000000018\n",
      "Operator 108: -4.000000000000055\n",
      "Operator 109: -4.000000000000043\n",
      "Operator 110: -4.000000000000051\n",
      "Operator 111: -4.000000000000042\n",
      "Operator 112: -2.4175604345301496\n",
      "Operator 113: -3.2087802172651108\n",
      "Operator 114: 1.167063459073523\n",
      "Operator 115: -2.6252484864092813\n",
      "Operator 116: 1.086413302283046\n",
      "Operator 117: -2.6252484844047865\n",
      "Operator 119: -3.208780213389853\n",
      "Operator 120: 3.208780213389853\n",
      "Operator 121: 1.7888543866356859\n",
      "Operator 122: 1.7888543866357096\n",
      "Operator 123: 4.000000000000041\n",
      "Operator 124: 4.000000000000018\n",
      "Operator 125: 4.000000000000055\n",
      "Operator 126: 4.000000000000043\n",
      "Operator 127: 4.000000000000051\n",
      "Operator 128: 4.000000000000042\n",
      "Operator 129: 4.000000000000046\n",
      "Operator 130: 3.2087802172651108\n",
      "Operator 132: 2.2432916652169874\n",
      "Operator 133: -1.0864132982740644\n",
      "Operator 134: 1.3558232908927892\n",
      "Operator 135: -1.1670634675430342\n",
      "Operator 136: 1.659759932356667\n",
      "Operator 137: 2.4175604267796653\n",
      "Operator 138: 1.7888543808408766\n",
      "Operator 149: -0.8241833935396307\n",
      "Operator 150: 1.593377038352414\n",
      "Operator 151: -0.6252484867272997\n",
      "Operator 152: 1.7116617843652833\n",
      "Operator 153: -0.824183395060292\n",
      "Operator 154: 1.5933770412922672\n",
      "Operator 155: -4.000000000000036\n",
      "Operator 156: -2.8944271933178585\n",
      "Operator 158: -2.8944271933178585\n",
      "Operator 159: 4.000000000000036\n",
      "Operator 160: -4.000000000000076\n",
      "Operator 161: 4.000000000000041\n",
      "Operator 162: -4.000000000000018\n",
      "Operator 163: 4.000000000000055\n",
      "Operator 164: -4.000000000000043\n",
      "Operator 165: 4.000000000000051\n",
      "Operator 166: -2.417560434530148\n",
      "Operator 167: 4.000000000000046\n",
      "Operator 168: -1.6597599343611649\n",
      "Operator 170: -1.5866742128015585\n",
      "Operator 172: -2.6252484844047865\n",
      "Operator 175: -2.8944271933178682\n",
      "Operator 188: 1.3636612923164622\n",
      "Operator 190: 1.3636612948324873\n",
      "Operator 205: -1.3636612923164628\n",
      "Operator 207: -1.3636612948324862\n",
      "Operator 211: 2.8944271933178682\n",
      "Operator 212: -4.000000000000036\n",
      "Operator 213: 4.000000000000076\n",
      "Operator 214: -4.000000000000041\n",
      "Operator 215: 4.000000000000018\n",
      "Operator 216: -4.000000000000055\n",
      "Operator 217: 4.000000000000043\n",
      "Operator 218: -4.000000000000051\n",
      "Operator 219: 2.417560434530148\n",
      "Operator 220: -4.000000000000046\n",
      "Operator 221: 1.6597599343611669\n",
      "Operator 223: 1.5866742128015576\n",
      "Operator 225: 2.6252484844047883\n",
      "Operator 237: -1.593377038352416\n",
      "Operator 238: 1.593377038352414\n",
      "Operator 239: -1.7116617843652884\n",
      "Operator 240: 1.7116617843652833\n",
      "Operator 241: -1.5933770412922752\n",
      "Operator 242: 1.593377041292267\n",
      "Operator 254: 1.5933770383524093\n",
      "Operator 255: -1.5933770383524086\n",
      "Operator 256: 1.711661784365284\n",
      "Operator 257: -1.7116617843652837\n",
      "Operator 258: 1.5933770412922728\n",
      "Operator 259: -1.5933770412922736\n",
      "Operator 262: -1.7888543808408928\n",
      "Operator 273: -1.5933770383524157\n",
      "Operator 274: 0.8241833935396321\n",
      "Operator 275: -1.7116617843652884\n",
      "Operator 276: 0.6252484867272983\n",
      "Operator 277: -1.5933770412922752\n",
      "Operator 278: 0.824183395060286\n",
      "Operator 280: -1.7888543808408928\n",
      "Operator 291: 0.8241833935396273\n",
      "Operator 292: -1.5933770383524082\n",
      "Operator 293: 0.6252484867272958\n",
      "Operator 294: -1.7116617843652837\n",
      "Operator 295: 0.8241833950602894\n",
      "Operator 296: -1.5933770412922736\n",
      "Total gradient norm: 40.72559597160192\n",
      "Operators under consideration (1):\n",
      "[220]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000046)]\n",
      "Operator(s) added to ansatz: [220]\n",
      "Gradients: [np.float64(-4.000000000000046)]\n",
      "Initial energy: -22.968512542244305\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220]...\n",
      "Starting point: [np.float64(0.46089810865967334), np.float64(0.5535743582491652), np.float64(0.5135775291243945), np.float64(0.4608981074436275), np.float64(0.0)]\n",
      "         Current function value: -23.951870\n",
      "         Iterations: 8\n",
      "         Function evaluations: 21\n",
      "         Gradient evaluations: 18\n",
      "\n",
      "Current energy: -23.951870198185443\n",
      "(change of -0.9833576559411377)\n",
      "Current ansatz: [226, 261, 224, 222, 220]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000016\n",
      "Operator 1: -4.000000000000055\n",
      "Operator 2: 4.00000000000002\n",
      "Operator 3: -3.9999999999999973\n",
      "Operator 4: 4.000000000000033\n",
      "Operator 5: -4.00000000000002\n",
      "Operator 6: 4.000000000000028\n",
      "Operator 7: -3.1836067506928614\n",
      "Operator 8: -2.886350009562946e-08\n",
      "Operator 9: -2.120296174707436\n",
      "Operator 11: -1.8733788564011737\n",
      "Operator 12: 1.9613637158855113e-08\n",
      "Operator 13: -2.12029619429214\n",
      "Operator 15: -3.183606761905546\n",
      "Operator 16: 4.000000000000008\n",
      "Operator 17: -1.372991173179047e-08\n",
      "Operator 18: 2.894427196491859\n",
      "Operator 19: -4.000000000000016\n",
      "Operator 20: 4.000000000000055\n",
      "Operator 21: -4.000000000000019\n",
      "Operator 22: 3.9999999999999973\n",
      "Operator 23: -4.000000000000033\n",
      "Operator 24: 4.00000000000002\n",
      "Operator 25: -4.000000000000028\n",
      "Operator 26: 3.1836067506928596\n",
      "Operator 27: 2.886350009562946e-08\n",
      "Operator 28: 2.12029617470744\n",
      "Operator 30: 1.8733788564011813\n",
      "Operator 31: -1.9613637158855113e-08\n",
      "Operator 32: 2.1202961942921417\n",
      "Operator 34: 1.7888543929837089\n",
      "Operator 35: 4.000000000000055\n",
      "Operator 36: 4.000000000000019\n",
      "Operator 37: 3.9999999999999973\n",
      "Operator 38: 4.000000000000033\n",
      "Operator 39: 4.00000000000002\n",
      "Operator 40: 4.000000000000028\n",
      "Operator 41: 3.1836067506928596\n",
      "Operator 42: -2.886350009562946e-08\n",
      "Operator 43: 2.5543359627831457\n",
      "Operator 44: -1.1482009628462149\n",
      "Operator 45: 2.4386935424513854\n",
      "Operator 46: -1.3794857981041733\n",
      "Operator 47: 2.5543359729891306\n",
      "Operator 48: -1.2585415789320207\n",
      "Operator 49: 3.183606761905546\n",
      "Operator 50: 2.3672135238110696\n",
      "Operator 51: 2.8944271964918684\n",
      "Operator 52: 4.000000000000016\n",
      "Operator 53: 4.000000000000055\n",
      "Operator 54: 4.00000000000002\n",
      "Operator 55: 3.9999999999999973\n",
      "Operator 56: 4.000000000000033\n",
      "Operator 57: 4.00000000000002\n",
      "Operator 58: 2.367213501385713\n",
      "Operator 59: 3.1836067506928596\n",
      "Operator 60: -1.2585416046829616\n",
      "Operator 61: 2.5543359627831457\n",
      "Operator 62: -1.3794858035097284\n",
      "Operator 63: 2.4386935424513854\n",
      "Operator 64: -1.1482009370286805\n",
      "Operator 65: 2.554335972989131\n",
      "Operator 67: 3.183606761905546\n",
      "Operator 78: -1.424421406146474\n",
      "Operator 80: -1.561306457548647\n",
      "Operator 82: -1.4244213952958544\n",
      "Operator 85: -1.7888543929837089\n",
      "Operator 86: -4.000000000000055\n",
      "Operator 87: -4.00000000000002\n",
      "Operator 88: -3.9999999999999973\n",
      "Operator 89: -4.000000000000033\n",
      "Operator 90: -4.00000000000002\n",
      "Operator 91: -4.000000000000028\n",
      "Operator 92: -3.1836067506928614\n",
      "Operator 93: 2.886350009562946e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operator 94: -2.5543359627831435\n",
      "Operator 95: 1.148200962846216\n",
      "Operator 96: -2.4386935424513854\n",
      "Operator 97: 1.3794857981041733\n",
      "Operator 98: -2.554335972989129\n",
      "Operator 99: 1.2585415789320207\n",
      "Operator 100: -3.1836067619055424\n",
      "Operator 101: -2.3672135238110696\n",
      "Operator 102: 2.8944271964918684\n",
      "Operator 103: -2.894427196491859\n",
      "Operator 104: -4.000000000000016\n",
      "Operator 105: -4.000000000000055\n",
      "Operator 106: -4.000000000000019\n",
      "Operator 107: -3.9999999999999973\n",
      "Operator 108: -4.000000000000033\n",
      "Operator 109: -4.00000000000002\n",
      "Operator 110: -2.367213501385713\n",
      "Operator 111: -3.1836067506928614\n",
      "Operator 112: 1.2585416046829616\n",
      "Operator 113: -2.5543359627831435\n",
      "Operator 114: 1.379485803509728\n",
      "Operator 115: -2.4386935424513854\n",
      "Operator 116: 1.1482009370286805\n",
      "Operator 117: -2.5543359729891293\n",
      "Operator 119: -3.1836067619055433\n",
      "Operator 120: 3.1836067619055433\n",
      "Operator 121: 1.7888543929837089\n",
      "Operator 122: 1.788854392983732\n",
      "Operator 123: 4.00000000000002\n",
      "Operator 124: 3.9999999999999973\n",
      "Operator 125: 4.000000000000033\n",
      "Operator 126: 4.00000000000002\n",
      "Operator 127: 4.000000000000028\n",
      "Operator 128: 3.1836067506928614\n",
      "Operator 129: -2.886350009562946e-08\n",
      "Operator 130: 2.120296174707436\n",
      "Operator 131: -1.148200962846216\n",
      "Operator 132: 1.1086719305208408\n",
      "Operator 133: -1.3794857981041733\n",
      "Operator 134: 0.9930295104858948\n",
      "Operator 135: -1.2585415789320207\n",
      "Operator 136: 1.4910254053757124\n",
      "Operator 137: 2.3672135238110696\n",
      "Operator 138: 1.7888543792538587\n",
      "Operator 147: -0.7550488179091869\n",
      "Operator 148: 1.6121647123400167\n",
      "Operator 149: -0.48978123178023786\n",
      "Operator 150: 1.7670916566322499\n",
      "Operator 151: -0.48978123081247016\n",
      "Operator 152: 1.7670916521944529\n",
      "Operator 153: -0.7550488208022983\n",
      "Operator 154: 1.61216470410796\n",
      "Operator 155: -4.000000000000008\n",
      "Operator 156: -2.894427196491859\n",
      "Operator 157: 1.372991173179047e-08\n",
      "Operator 158: -2.894427196491859\n",
      "Operator 159: 4.000000000000016\n",
      "Operator 160: -4.000000000000055\n",
      "Operator 161: 4.000000000000019\n",
      "Operator 162: -3.9999999999999973\n",
      "Operator 163: 4.000000000000033\n",
      "Operator 164: -2.3672135013857094\n",
      "Operator 165: 4.000000000000028\n",
      "Operator 166: -1.4910253867976966\n",
      "Operator 167: -2.886350009562946e-08\n",
      "Operator 168: -1.1963097515520014\n",
      "Operator 170: -1.4432270835304082\n",
      "Operator 171: 1.9613637158855113e-08\n",
      "Operator 172: -2.5543359729891293\n",
      "Operator 174: 1.3729910970448264e-08\n",
      "Operator 175: -2.8944271964918684\n",
      "Operator 186: 1.4244214061464746\n",
      "Operator 188: 1.5613064575486457\n",
      "Operator 190: 1.4244213952958558\n",
      "Operator 203: -1.424421406146474\n",
      "Operator 205: -1.5613064575486473\n",
      "Operator 207: -1.4244213952958549\n",
      "Operator 210: -1.3729910970448264e-08\n",
      "Operator 211: 2.8944271964918684\n",
      "Operator 212: -4.000000000000016\n",
      "Operator 213: 4.000000000000055\n",
      "Operator 214: -4.00000000000002\n",
      "Operator 215: 3.9999999999999973\n",
      "Operator 216: -4.000000000000033\n",
      "Operator 217: 2.3672135013857094\n",
      "Operator 218: -4.000000000000028\n",
      "Operator 219: 1.4910253867976957\n",
      "Operator 220: 2.886350009562946e-08\n",
      "Operator 221: 1.196309751552002\n",
      "Operator 223: 1.4432270835304075\n",
      "Operator 224: -1.9613637158855113e-08\n",
      "Operator 225: 2.554335972989131\n",
      "Operator 235: -1.6121647123400047\n",
      "Operator 236: 1.6121647123400167\n",
      "Operator 237: -1.767091656632252\n",
      "Operator 238: 1.7670916566322499\n",
      "Operator 239: -1.767091652194457\n",
      "Operator 240: 1.7670916521944526\n",
      "Operator 241: -1.6121647041079685\n",
      "Operator 242: 1.61216470410796\n",
      "Operator 252: 1.6121647123400078\n",
      "Operator 253: -1.6121647123400147\n",
      "Operator 254: 1.767091656632248\n",
      "Operator 255: -1.7670916566322443\n",
      "Operator 256: 1.7670916521944549\n",
      "Operator 257: -1.767091652194453\n",
      "Operator 258: 1.6121647041079665\n",
      "Operator 259: -1.6121647041079668\n",
      "Operator 261: -1.372991173179047e-08\n",
      "Operator 262: -1.7888543792538754\n",
      "Operator 271: -1.6121647123400047\n",
      "Operator 272: 0.7550488179091935\n",
      "Operator 273: -1.767091656632252\n",
      "Operator 274: 0.4897812317802394\n",
      "Operator 275: -1.767091652194457\n",
      "Operator 276: 0.48978123081246866\n",
      "Operator 277: -1.6121647041079687\n",
      "Operator 278: 0.7550488208022921\n",
      "Operator 280: -1.7888543792538747\n",
      "Operator 289: 0.7550488179091885\n",
      "Operator 290: -1.6121647123400147\n",
      "Operator 291: 0.48978123178023536\n",
      "Operator 292: -1.7670916566322445\n",
      "Operator 293: 0.48978123081246605\n",
      "Operator 294: -1.767091652194453\n",
      "Operator 295: 0.7550488208022955\n",
      "Operator 296: -1.6121647041079665\n",
      "Total gradient norm: 37.885741364047746\n",
      "Operators under consideration (1):\n",
      "[218]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000028)]\n",
      "Operator(s) added to ansatz: [218]\n",
      "Gradients: [np.float64(-4.000000000000028)]\n",
      "Initial energy: -23.951870198185443\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218]...\n",
      "Starting point: [np.float64(0.46875105084859725), np.float64(0.5535743573620011), np.float64(0.5416899682811634), np.float64(0.5416899706500331), np.float64(0.4687510543261212), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -24.942063\n",
      "         Iterations: 11\n",
      "         Function evaluations: 52\n",
      "         Gradient evaluations: 51\n",
      "\n",
      "Current energy: -24.94206301992796\n",
      "(change of -0.990192821742518)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000043\n",
      "Operator 1: -4.000000000000081\n",
      "Operator 2: 4.000000000000046\n",
      "Operator 3: -4.000000000000023\n",
      "Operator 4: 4.000000000000059\n",
      "Operator 5: -3.1707837271135353\n",
      "Operator 7: -2.0589317728015852\n",
      "Operator 9: -1.6998595428770267\n",
      "Operator 11: -1.6998595472283906\n",
      "Operator 13: -2.0589317800124385\n",
      "Operator 15: -3.1707837299730146\n",
      "Operator 16: 4.000000000000028\n",
      "Operator 18: 2.8944271908260806\n",
      "Operator 19: -4.000000000000043\n",
      "Operator 20: 4.000000000000081\n",
      "Operator 21: -4.000000000000046\n",
      "Operator 22: 4.000000000000023\n",
      "Operator 23: -4.000000000000059\n",
      "Operator 24: 3.170783727113535\n",
      "Operator 26: 2.058931772801581\n",
      "Operator 28: 1.699859542877033\n",
      "Operator 30: 1.6998595472283993\n",
      "Operator 32: 2.05893178001244\n",
      "Operator 34: 1.7888543816521234\n",
      "Operator 35: 4.000000000000081\n",
      "Operator 36: 4.000000000000046\n",
      "Operator 37: 4.000000000000023\n",
      "Operator 38: 4.000000000000059\n",
      "Operator 39: 3.170783727113535\n",
      "Operator 41: 2.5199146395796443\n",
      "Operator 42: -1.1778979189063727\n",
      "Operator 43: 2.360459989945482\n",
      "Operator 44: -1.4674143065604222\n",
      "Operator 45: 2.3604599917115032\n",
      "Operator 46: -1.4968072130134622\n",
      "Operator 47: 2.5199146433967283\n",
      "Operator 48: -1.3017381731000173\n",
      "Operator 49: 3.1707837299730146\n",
      "Operator 50: 2.3415674599459866\n",
      "Operator 51: 2.894427190826089\n",
      "Operator 52: 4.000000000000043\n",
      "Operator 53: 4.000000000000081\n",
      "Operator 54: 4.000000000000046\n",
      "Operator 55: 4.000000000000023\n",
      "Operator 56: 2.3415674542270346\n",
      "Operator 57: 3.1707837271135335\n",
      "Operator 58: -1.3017381801878245\n",
      "Operator 59: 2.5199146395796443\n",
      "Operator 60: -1.4968072181747298\n",
      "Operator 61: 2.360459989945482\n",
      "Operator 62: -1.467414303028357\n",
      "Operator 63: 2.3604599917115032\n",
      "Operator 64: -1.1778979096430042\n",
      "Operator 65: 2.5199146433967283\n",
      "Operator 67: 3.1707837299730146\n",
      "Operator 76: -1.4528488378754123\n",
      "Operator 78: -1.6377572770784186\n",
      "Operator 80: -1.637757275107381\n",
      "Operator 82: -1.452848834277009\n",
      "Operator 85: -1.7888543816521234\n",
      "Operator 86: -4.000000000000081\n",
      "Operator 87: -4.000000000000046\n",
      "Operator 88: -4.000000000000023\n",
      "Operator 89: -4.000000000000059\n",
      "Operator 90: -3.1707837271135353\n",
      "Operator 92: -2.519914639579643\n",
      "Operator 93: 1.1778979189063716\n",
      "Operator 94: -2.3604599899454812\n",
      "Operator 95: 1.4674143065604224\n",
      "Operator 96: -2.3604599917115054\n",
      "Operator 97: 1.4968072130134622\n",
      "Operator 98: -2.519914643396727\n",
      "Operator 99: 1.3017381731000173\n",
      "Operator 100: -3.170783729973011\n",
      "Operator 101: -2.3415674599459866\n",
      "Operator 102: 2.894427190826089\n",
      "Operator 103: -2.8944271908260806\n",
      "Operator 104: -4.000000000000043\n",
      "Operator 105: -4.000000000000081\n",
      "Operator 106: -4.000000000000046\n",
      "Operator 107: -4.000000000000023\n",
      "Operator 108: -2.3415674542270355\n",
      "Operator 109: -3.1707837271135353\n",
      "Operator 110: 1.301738180187824\n",
      "Operator 111: -2.519914639579643\n",
      "Operator 112: 1.4968072181747298\n",
      "Operator 113: -2.3604599899454812\n",
      "Operator 114: 1.4674143030283568\n",
      "Operator 115: -2.3604599917115054\n",
      "Operator 116: 1.1778979096430011\n",
      "Operator 117: -2.519914643396727\n",
      "Operator 119: -3.1707837299730115\n",
      "Operator 120: 3.1707837299730115\n",
      "Operator 121: 1.7888543816521234\n",
      "Operator 122: 1.7888543816521456\n",
      "Operator 123: 4.000000000000046\n",
      "Operator 124: 4.000000000000023\n",
      "Operator 125: 4.000000000000059\n",
      "Operator 126: 3.1707837271135353\n",
      "Operator 128: 2.0589317728015852\n",
      "Operator 129: -1.1778979189063727\n",
      "Operator 130: 0.9950839455895104\n",
      "Operator 131: -1.4674143065604215\n",
      "Operator 132: 0.754863467407547\n",
      "Operator 133: -1.4968072130134629\n",
      "Operator 134: 0.8356292988818962\n",
      "Operator 135: -1.3017381731000173\n",
      "Operator 136: 1.4080626934361398\n",
      "Operator 137: 2.3415674599459866\n",
      "Operator 138: 1.78885438208677\n",
      "Operator 145: -0.7200664681891034\n",
      "Operator 146: 1.6215009911579634\n",
      "Operator 147: -0.4257469947875392\n",
      "Operator 148: 1.7919802032780887\n",
      "Operator 149: -0.3604599895339047\n",
      "Operator 150: 1.827874296917428\n",
      "Operator 151: -0.4257469953149887\n",
      "Operator 152: 1.7919802011214387\n",
      "Operator 153: -0.7200664708001201\n",
      "Operator 154: 1.6215009890933094\n",
      "Operator 155: -4.000000000000028\n",
      "Operator 156: -2.8944271908260806\n",
      "Operator 158: -2.8944271908260806\n",
      "Operator 159: 4.000000000000043\n",
      "Operator 160: -4.000000000000081\n",
      "Operator 161: 4.000000000000046\n",
      "Operator 162: -2.3415674542270137\n",
      "Operator 163: 4.000000000000059\n",
      "Operator 164: -1.4080626852676987\n",
      "Operator 166: -1.0227218424407634\n",
      "Operator 168: -1.0482189686330845\n",
      "Operator 170: -1.3817940767739973\n",
      "Operator 172: -2.519914643396727\n",
      "Operator 175: -2.894427190826089\n",
      "Operator 184: 1.4528488378754143\n",
      "Operator 186: 1.63775727707842\n",
      "Operator 188: 1.6377572751073797\n",
      "Operator 190: 1.45284883427701\n",
      "Operator 201: -1.4528488378754123\n",
      "Operator 203: -1.637757277078419\n",
      "Operator 205: -1.6377572751073808\n",
      "Operator 207: -1.4528488342770092\n",
      "Operator 211: 2.894427190826089\n",
      "Operator 212: -4.000000000000043\n",
      "Operator 213: 4.000000000000081\n",
      "Operator 214: -4.000000000000046\n",
      "Operator 215: 2.3415674542270137\n",
      "Operator 216: -4.000000000000059\n",
      "Operator 217: 1.4080626852676978\n",
      "Operator 219: 1.0227218424407643\n",
      "Operator 221: 1.048218968633085\n",
      "Operator 223: 1.3817940767739962\n",
      "Operator 225: 2.5199146433967283\n",
      "Operator 233: -1.621500991157968\n",
      "Operator 234: 1.6215009911579634\n",
      "Operator 235: -1.7919802032780752\n",
      "Operator 236: 1.7919802032780887\n",
      "Operator 237: -1.8278742969174306\n",
      "Operator 238: 1.827874296917428\n",
      "Operator 239: -1.7919802011214436\n",
      "Operator 240: 1.7919802011214387\n",
      "Operator 241: -1.6215009890933179\n",
      "Operator 242: 1.6215009890933094\n",
      "Operator 250: 1.6215009911579694\n",
      "Operator 251: -1.6215009911579683\n",
      "Operator 252: 1.7919802032780763\n",
      "Operator 253: -1.7919802032780872\n",
      "Operator 254: 1.8278742969174273\n",
      "Operator 255: -1.8278742969174224\n",
      "Operator 256: 1.7919802011214414\n",
      "Operator 257: -1.7919802011214392\n",
      "Operator 258: 1.6215009890933159\n",
      "Operator 259: -1.6215009890933159\n",
      "Operator 262: -1.7888543820867855\n",
      "Operator 269: -1.621500991157968\n",
      "Operator 270: 0.7200664681891001\n",
      "Operator 271: -1.7919802032780752\n",
      "Operator 272: 0.42574699478754363\n",
      "Operator 273: -1.8278742969174306\n",
      "Operator 274: 0.3604599895339063\n",
      "Operator 275: -1.7919802011214432\n",
      "Operator 276: 0.42574699531498694\n",
      "Operator 277: -1.6215009890933176\n",
      "Operator 278: 0.7200664708001142\n",
      "Operator 280: -1.7888543820867855\n",
      "Operator 287: 0.7200664681891034\n",
      "Operator 288: -1.6215009911579683\n",
      "Operator 289: 0.4257469947875409\n",
      "Operator 290: -1.7919802032780874\n",
      "Operator 291: 0.36045998953390174\n",
      "Operator 292: -1.8278742969174224\n",
      "Operator 293: 0.42574699531498406\n",
      "Operator 294: -1.7919802011214392\n",
      "Operator 295: 0.7200664708001179\n",
      "Operator 296: -1.6215009890933154\n",
      "Total gradient norm: 34.86436732690668\n",
      "Operators under consideration (1):\n",
      "[213]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000081)]\n",
      "Operator(s) added to ansatz: [213]\n",
      "Gradients: [np.float64(4.000000000000081)]\n",
      "Initial energy: -24.94206301992796\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213]...\n",
      "Starting point: [np.float64(0.47271650860942815), np.float64(0.5535743589456409), np.float64(0.5553278982965393), np.float64(0.5764406793126658), np.float64(0.5553278995106626), np.float64(0.4727165094911681), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.770490\n",
      "         Iterations: 7\n",
      "         Function evaluations: 53\n",
      "         Gradient evaluations: 47\n",
      "\n",
      "Current energy: -25.770490144674127\n",
      "(change of -0.8284271247461668)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 3.4142135745441164\n",
      "Operator 1: -4.868401239071595e-08\n",
      "Operator 2: 3.414213574544119\n",
      "Operator 3: -4.000000000000012\n",
      "Operator 4: 4.000000000000052\n",
      "Operator 5: -3.1707837916530988\n",
      "Operator 6: 2.4372938227293583e-07\n",
      "Operator 7: -2.058931772615301\n",
      "Operator 8: -1.804128650780062e-07\n",
      "Operator 9: -1.6998594281393875\n",
      "Operator 10: -1.420615501128677e-07\n",
      "Operator 11: -1.699859592736211\n",
      "Operator 12: 2.7455638906825897e-07\n",
      "Operator 13: -2.058931887087165\n",
      "Operator 14: -4.2231523167401974e-08\n",
      "Operator 15: -3.17078374152813\n",
      "Operator 16: 4.000000000000025\n",
      "Operator 17: -1.618500044251994e-08\n",
      "Operator 18: 2.8944271974739086\n",
      "Operator 19: -3.4142135745441173\n",
      "Operator 20: 4.868400960767554e-08\n",
      "Operator 21: -3.414213574544121\n",
      "Operator 22: 4.000000000000012\n",
      "Operator 23: -4.000000000000052\n",
      "Operator 24: 3.170783791653098\n",
      "Operator 25: -2.4372938227293583e-07\n",
      "Operator 26: 2.058931772615296\n",
      "Operator 27: 1.804128650780062e-07\n",
      "Operator 28: 1.6998594281393937\n",
      "Operator 29: 1.4206155149672588e-07\n",
      "Operator 30: 1.6998595927362192\n",
      "Operator 31: -2.7455638818008055e-07\n",
      "Operator 32: 2.0589318870871667\n",
      "Operator 33: 4.2231523167401974e-08\n",
      "Operator 34: 1.5268827395283946\n",
      "Operator 35: 4.868400960767554e-08\n",
      "Operator 36: 3.4142135745441196\n",
      "Operator 37: 2.8284271490881885\n",
      "Operator 38: 4.000000000000052\n",
      "Operator 39: 3.170783791653098\n",
      "Operator 40: 2.4372938227293583e-07\n",
      "Operator 41: 2.5199146303499997\n",
      "Operator 42: -1.177897974304859\n",
      "Operator 43: 2.3604599414671874\n",
      "Operator 44: -1.4674144426033984\n",
      "Operator 45: 2.3604600082697456\n",
      "Operator 46: -1.4968070992779812\n",
      "Operator 47: 2.5199147044444574\n",
      "Operator 48: -1.3017381163988935\n",
      "Operator 49: 3.17078374152813\n",
      "Operator 50: 2.3415674830562216\n",
      "Operator 51: 2.0466691165986064\n",
      "Operator 52: 3.414213574544119\n",
      "Operator 53: 4.868401060538735e-08\n",
      "Operator 54: 3.414213574544119\n",
      "Operator 55: 4.000000000000012\n",
      "Operator 56: 2.341567583306169\n",
      "Operator 57: 3.170783791653098\n",
      "Operator 58: -1.3017380788768071\n",
      "Operator 59: 2.5199146303499997\n",
      "Operator 60: -1.4968073520705172\n",
      "Operator 61: 2.3604599414671874\n",
      "Operator 62: -1.4674143089982625\n",
      "Operator 63: 2.3604600082697456\n",
      "Operator 64: -1.177897706928544\n",
      "Operator 65: 2.5199147044444574\n",
      "Operator 66: -4.2231523167401974e-08\n",
      "Operator 67: 3.17078374152813\n",
      "Operator 76: -1.4528488221309164\n",
      "Operator 78: -1.6377573262962069\n",
      "Operator 80: -1.6377572517389396\n",
      "Operator 82: -1.4528487884191776\n",
      "Operator 85: -1.5268827395283946\n",
      "Operator 86: -4.868401239071595e-08\n",
      "Operator 87: -3.414213574544121\n",
      "Operator 88: -2.8284271490881885\n",
      "Operator 89: -4.000000000000052\n",
      "Operator 90: -3.1707837916530988\n",
      "Operator 91: -2.4372938227293583e-07\n",
      "Operator 92: -2.519914630349997\n",
      "Operator 93: 1.177897974304859\n",
      "Operator 94: -2.360459941467187\n",
      "Operator 95: 1.4674144426033995\n",
      "Operator 96: -2.360460008269748\n",
      "Operator 97: 1.4968070992779794\n",
      "Operator 98: -2.519914704444456\n",
      "Operator 99: 1.3017381163988953\n",
      "Operator 100: -3.1707837415281266\n",
      "Operator 101: -2.3415674830562216\n",
      "Operator 102: 2.8944271974739184\n",
      "Operator 103: -2.0466691165985993\n",
      "Operator 104: -3.4142135745441156\n",
      "Operator 105: -4.868400960767554e-08\n",
      "Operator 106: -3.414213574544121\n",
      "Operator 107: -4.000000000000012\n",
      "Operator 108: -2.341567583306169\n",
      "Operator 109: -3.1707837916530988\n",
      "Operator 110: 1.3017380788768071\n",
      "Operator 111: -2.5199146303499975\n",
      "Operator 112: 1.4968073520705165\n",
      "Operator 113: -2.360459941467187\n",
      "Operator 114: 1.4674143089982625\n",
      "Operator 115: -2.360460008269748\n",
      "Operator 116: 1.1778977069285455\n",
      "Operator 117: -2.519914704444455\n",
      "Operator 118: 4.2231523167401974e-08\n",
      "Operator 119: -3.1707837415281266\n",
      "Operator 120: 3.1707837415281266\n",
      "Operator 121: 1.5268827395283946\n",
      "Operator 122: 2.1772154142752743e-08\n",
      "Operator 123: 3.414213574544119\n",
      "Operator 124: 2.8284271490881885\n",
      "Operator 125: 2.828427149088215\n",
      "Operator 126: 3.1707837916530988\n",
      "Operator 127: 2.4372938227293583e-07\n",
      "Operator 128: 2.058931772615301\n",
      "Operator 129: -1.177897974304859\n",
      "Operator 130: 0.9950839332771263\n",
      "Operator 131: -1.4674144426034\n",
      "Operator 132: 0.7548634326039592\n",
      "Operator 133: -1.4968070992779796\n",
      "Operator 134: 0.8356292908533568\n",
      "Operator 135: -1.3017381163988953\n",
      "Operator 136: 1.4080628500034793\n",
      "Operator 137: 2.3415674830562216\n",
      "Operator 138: 1.264911072664527\n",
      "Operator 140: -1.414213550202116\n",
      "Operator 141: 1.4142135502021174\n",
      "Operator 145: -0.7200663950187574\n",
      "Operator 146: 1.6215009445580022\n",
      "Operator 147: -0.4257469996470706\n",
      "Operator 148: 1.7919802353577579\n",
      "Operator 149: -0.3604600064113665\n",
      "Operator 150: 1.8278743191263622\n",
      "Operator 151: -0.42574696203779117\n",
      "Operator 152: 1.7919801537796414\n",
      "Operator 153: -0.7200665445376804\n",
      "Operator 154: 1.6215009807500833\n",
      "Operator 155: -4.000000000000025\n",
      "Operator 156: -2.8944271974739086\n",
      "Operator 157: 1.1444523221485826e-08\n",
      "Operator 158: -2.8944271974739086\n",
      "Operator 159: 3.4142135745441156\n",
      "Operator 160: -4.868400960767554e-08\n",
      "Operator 161: 3.414213574544121\n",
      "Operator 162: -2.3415675833061464\n",
      "Operator 163: 4.000000000000052\n",
      "Operator 164: -1.4080626113122041\n",
      "Operator 165: 2.4372938227293583e-07\n",
      "Operator 166: -1.0227217756821403\n",
      "Operator 167: -1.8041286351396003e-07\n",
      "Operator 168: -1.048219059840239\n",
      "Operator 169: -1.4206155149672588e-07\n",
      "Operator 170: -1.3817941001047562\n",
      "Operator 171: 2.7455638818008055e-07\n",
      "Operator 172: -2.519914704444455\n",
      "Operator 173: -4.2231523167401974e-08\n",
      "Operator 174: 1.6184999594022113e-08\n",
      "Operator 175: -2.8944271974739184\n",
      "Operator 184: 1.4528488221309188\n",
      "Operator 186: 1.637757326296207\n",
      "Operator 188: 1.6377572517389387\n",
      "Operator 190: 1.4528487884191787\n",
      "Operator 201: -1.4528488221309162\n",
      "Operator 203: -1.6377573262962069\n",
      "Operator 205: -1.6377572517389396\n",
      "Operator 207: -1.4528487884191776\n",
      "Operator 210: -1.1444522900685117e-08\n",
      "Operator 211: 2.8944271974739184\n",
      "Operator 212: -3.414213574544119\n",
      "Operator 213: 4.868401060538735e-08\n",
      "Operator 214: -3.414213574544119\n",
      "Operator 215: 2.3415675833061464\n",
      "Operator 216: -4.000000000000052\n",
      "Operator 217: 1.4080626113122041\n",
      "Operator 218: -2.4372938227293583e-07\n",
      "Operator 219: 1.0227217756821416\n",
      "Operator 220: 1.8041286351396003e-07\n",
      "Operator 221: 1.0482190598402392\n",
      "Operator 222: 1.4206155149672588e-07\n",
      "Operator 223: 1.3817941001047551\n",
      "Operator 224: -2.7455638818008055e-07\n",
      "Operator 225: 2.5199147044444574\n",
      "Operator 226: 4.2231523167401974e-08\n",
      "Operator 228: -1.4142135502021156\n",
      "Operator 229: 1.4142135502021174\n",
      "Operator 233: -1.6215009445580073\n",
      "Operator 234: 1.6215009445580022\n",
      "Operator 235: -1.7919802353577439\n",
      "Operator 236: 1.7919802353577576\n",
      "Operator 237: -1.8278743191263649\n",
      "Operator 238: 1.8278743191263622\n",
      "Operator 239: -1.7919801537796463\n",
      "Operator 240: 1.7919801537796411\n",
      "Operator 241: -1.621500980750093\n",
      "Operator 242: 1.6215009807500833\n",
      "Operator 245: 1.4142135502021116\n",
      "Operator 246: -1.4142135502021136\n",
      "Operator 250: 1.6215009445580082\n",
      "Operator 251: -1.621500944558008\n",
      "Operator 252: 1.7919802353577443\n",
      "Operator 253: -1.791980235357756\n",
      "Operator 254: 1.827874319126362\n",
      "Operator 255: -1.8278743191263562\n",
      "Operator 256: 1.7919801537796445\n",
      "Operator 257: -1.7919801537796416\n",
      "Operator 258: 1.6215009807500902\n",
      "Operator 259: -1.62150098075009\n",
      "Operator 261: -1.618500044251994e-08\n",
      "Operator 262: -1.788854378762867\n",
      "Operator 264: -0.6324555311684357\n",
      "Operator 265: 1.4142135502021174\n",
      "Operator 269: -1.6215009445580073\n",
      "Operator 270: 0.7200663950187541\n",
      "Operator 271: -1.7919802353577436\n",
      "Operator 272: 0.4257469996470752\n",
      "Operator 273: -1.8278743191263649\n",
      "Operator 274: 0.3604600064113683\n",
      "Operator 275: -1.7919801537796463\n",
      "Operator 276: 0.4257469620377896\n",
      "Operator 277: -1.6215009807500924\n",
      "Operator 278: 0.7200665445376746\n",
      "Operator 280: -1.264911072664538\n",
      "Operator 282: 1.4142135502021118\n",
      "Operator 283: -1.4142135502021138\n",
      "Operator 287: 0.7200663950187577\n",
      "Operator 288: -1.621500944558008\n",
      "Operator 289: 0.4257469996470723\n",
      "Operator 290: -1.7919802353577559\n",
      "Operator 291: 0.3604600064113636\n",
      "Operator 292: -1.8278743191263565\n",
      "Operator 293: 0.42574696203778634\n",
      "Operator 294: -1.7919801537796418\n",
      "Operator 295: 0.7200665445376778\n",
      "Operator 296: -1.62150098075009\n",
      "Total gradient norm: 31.48639919882635\n",
      "Operators under consideration (1):\n",
      "[216]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000052)]\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(-4.000000000000052)]\n",
      "Initial energy: -25.770490144674127\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216]...\n",
      "Starting point: [np.float64(0.47271650504633494), np.float64(0.5535743570875149), np.float64(0.5553278716445686), np.float64(0.5764406929929802), np.float64(0.5553279175705292), np.float64(0.4727164895899872), np.float64(-0.39269907739562815), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -26.764322\n",
      "         Iterations: 12\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 66\n",
      "\n",
      "Current energy: -26.76432245574464\n",
      "(change of -0.9938323110705127)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 3.414213566329976\n",
      "Operator 1: -1.5827420106847523e-08\n",
      "Operator 2: 3.4142135663299813\n",
      "Operator 3: -3.1636615186664416\n",
      "Operator 5: -2.0252067805934835\n",
      "Operator 6: -6.502008502086716e-08\n",
      "Operator 7: -1.6072089750143237\n",
      "Operator 8: -1.0921113121300833e-07\n",
      "Operator 9: -1.4913274934035277\n",
      "Operator 10: 3.768566207286098e-08\n",
      "Operator 11: -1.6072091194612348\n",
      "Operator 12: 3.6942640058157394e-08\n",
      "Operator 13: -2.0252068877176335\n",
      "Operator 14: 1.9771562165260548e-08\n",
      "Operator 15: -3.1636615485726627\n",
      "Operator 16: 4.000000000000033\n",
      "Operator 18: 2.894427191037312\n",
      "Operator 19: -3.4142135663299786\n",
      "Operator 20: 1.5827420501005173e-08\n",
      "Operator 21: -3.414213566329983\n",
      "Operator 22: 3.163661518666443\n",
      "Operator 24: 2.025206780593483\n",
      "Operator 25: 6.502008502086716e-08\n",
      "Operator 26: 1.6072089750143186\n",
      "Operator 27: 1.0921113019181128e-07\n",
      "Operator 28: 1.4913274934035332\n",
      "Operator 29: -3.768566207286098e-08\n",
      "Operator 30: 1.6072091194612437\n",
      "Operator 31: -3.6942640058157394e-08\n",
      "Operator 32: 2.0252068877176366\n",
      "Operator 33: -1.9771562165260548e-08\n",
      "Operator 34: 1.5268827248669485\n",
      "Operator 35: 1.5827420501005173e-08\n",
      "Operator 36: 3.4142135663299813\n",
      "Operator 37: 2.2370465194870324\n",
      "Operator 39: 2.5012735339469687\n",
      "Operator 40: -1.1939064777893253\n",
      "Operator 41: 2.321211519500654\n",
      "Operator 42: -1.511426651169407\n",
      "Operator 43: 2.2780072115726875\n",
      "Operator 44: -1.597835235515444\n",
      "Operator 45: 2.321211577250374\n",
      "Operator 46: -1.554030391691389\n",
      "Operator 47: 2.5012735917575215\n",
      "Operator 48: -1.3247758938587477\n",
      "Operator 49: 3.1636615485726627\n",
      "Operator 50: 2.3273230971452796\n",
      "Operator 51: 2.046669100159616\n",
      "Operator 52: 3.4142135663299786\n",
      "Operator 53: 1.5827420106847523e-08\n",
      "Operator 54: 1.9864944718235307\n",
      "Operator 55: 3.163661518666443\n",
      "Operator 56: -1.3247759772884085\n",
      "Operator 57: 2.5012735339469687\n",
      "Operator 58: -1.5540305066819455\n",
      "Operator 59: 2.321211519500654\n",
      "Operator 60: -1.5978352670253722\n",
      "Operator 61: 2.2780072115726875\n",
      "Operator 62: -1.511426504160052\n",
      "Operator 63: 2.321211577250374\n",
      "Operator 64: -1.1939063626770894\n",
      "Operator 65: 2.501273591757522\n",
      "Operator 66: 1.9771562165260548e-08\n",
      "Operator 67: 3.1636615485726627\n",
      "Operator 74: -1.467960076930476\n",
      "Operator 76: -1.674784233534559\n",
      "Operator 78: -1.7219927884273822\n",
      "Operator 80: -1.6747841749556291\n",
      "Operator 82: -1.467960027645378\n",
      "Operator 85: -1.5268827248669476\n",
      "Operator 86: -1.5827420106847523e-08\n",
      "Operator 87: -3.414213566329983\n",
      "Operator 88: -2.2370465194870315\n",
      "Operator 90: -2.5012735339469665\n",
      "Operator 91: 1.1939064777893236\n",
      "Operator 92: -2.3212115195006513\n",
      "Operator 93: 1.511426651169406\n",
      "Operator 94: -2.278007211572687\n",
      "Operator 95: 1.5978352355154442\n",
      "Operator 96: -2.321211577250376\n",
      "Operator 97: 1.554030391691389\n",
      "Operator 98: -2.501273591757521\n",
      "Operator 99: 1.324775893858746\n",
      "Operator 100: -3.163661548572659\n",
      "Operator 101: -2.3273230971452796\n",
      "Operator 102: 2.8944271910373214\n",
      "Operator 103: -2.046669100159609\n",
      "Operator 104: -3.414213566329976\n",
      "Operator 105: -1.5827420106529277e-08\n",
      "Operator 106: -1.9864944718235307\n",
      "Operator 107: -3.163661518666441\n",
      "Operator 108: 1.3247759772884096\n",
      "Operator 109: -2.5012735339469665\n",
      "Operator 110: 1.5540305066819455\n",
      "Operator 111: -2.3212115195006513\n",
      "Operator 112: 1.5978352670253706\n",
      "Operator 113: -2.2780072115726875\n",
      "Operator 114: 1.5114265041600525\n",
      "Operator 115: -2.3212115772503763\n",
      "Operator 116: 1.1939063626770894\n",
      "Operator 117: -2.501273591757521\n",
      "Operator 118: -1.9771562165260548e-08\n",
      "Operator 119: -3.163661548572659\n",
      "Operator 120: 3.163661548572659\n",
      "Operator 121: 1.5268827248669476\n",
      "Operator 123: 3.4142135663299813\n",
      "Operator 124: 2.2370465194870315\n",
      "Operator 126: 2.0252067805934835\n",
      "Operator 127: -1.1939064777893236\n",
      "Operator 128: 0.935123618339739\n",
      "Operator 129: -1.511426651169406\n",
      "Operator 130: 0.6424230679616665\n",
      "Operator 131: -1.5978352355154442\n",
      "Operator 132: 0.5992187598625945\n",
      "Operator 133: -1.554030391691389\n",
      "Operator 134: 0.7550617119090065\n",
      "Operator 135: -1.3247758938587486\n",
      "Operator 136: 1.3628189309024779\n",
      "Operator 137: 2.3273230971452796\n",
      "Operator 138: 1.2649110675932553\n",
      "Operator 140: -1.4142135584162696\n",
      "Operator 141: 0.8228329485526635\n",
      "Operator 143: -0.700703277081874\n",
      "Operator 144: 1.626619768100404\n",
      "Operator 145: -0.391532560805709\n",
      "Operator 146: 1.8049209848774925\n",
      "Operator 147: -0.2980518490831723\n",
      "Operator 148: 1.8557978410875018\n",
      "Operator 149: -0.2980518445860153\n",
      "Operator 150: 1.855797814074748\n",
      "Operator 151: -0.3915325981731311\n",
      "Operator 152: 1.804920948018957\n",
      "Operator 153: -0.700703330667835\n",
      "Operator 154: 1.6266197467059065\n",
      "Operator 155: -4.000000000000033\n",
      "Operator 156: -2.894427191037312\n",
      "Operator 158: -2.894427191037312\n",
      "Operator 159: 3.414213566329976\n",
      "Operator 161: 3.414213566329983\n",
      "Operator 162: -1.36281879587398\n",
      "Operator 164: -0.9325544553849061\n",
      "Operator 165: -6.502008502086716e-08\n",
      "Operator 166: -0.8654216782722265\n",
      "Operator 167: -1.0921113021814871e-07\n",
      "Operator 168: -0.9813032478345465\n",
      "Operator 169: 3.768566207286098e-08\n",
      "Operator 170: -1.3505523292739439\n",
      "Operator 171: 3.6942640058157394e-08\n",
      "Operator 172: -2.501273591757521\n",
      "Operator 173: 1.9771562165260548e-08\n",
      "Operator 175: -2.8944271910373214\n",
      "Operator 182: 1.4679600769304773\n",
      "Operator 184: 1.6747842335345622\n",
      "Operator 186: 1.7219927884273827\n",
      "Operator 188: 1.6747841749556271\n",
      "Operator 190: 1.4679600276453797\n",
      "Operator 199: -1.467960076930476\n",
      "Operator 201: -1.6747842335345593\n",
      "Operator 203: -1.7219927884273825\n",
      "Operator 205: -1.6747841749556291\n",
      "Operator 207: -1.4679600276453781\n",
      "Operator 211: 2.8944271910373214\n",
      "Operator 212: -3.4142135663299786\n",
      "Operator 214: -3.4142135663299813\n",
      "Operator 215: 1.3628187958739804\n",
      "Operator 217: 0.9325544553849064\n",
      "Operator 218: 6.502008502086716e-08\n",
      "Operator 219: 0.8654216782722274\n",
      "Operator 220: 1.0921113123934574e-07\n",
      "Operator 221: 0.9813032478345469\n",
      "Operator 222: -3.768566207286098e-08\n",
      "Operator 223: 1.3505523292739425\n",
      "Operator 224: -3.6942640058157394e-08\n",
      "Operator 225: 2.501273591757522\n",
      "Operator 226: -1.9771562165260548e-08\n",
      "Operator 228: -1.4142135584162696\n",
      "Operator 229: 1.4142135584162705\n",
      "Operator 231: -1.6266197681003958\n",
      "Operator 232: 1.626619768100404\n",
      "Operator 233: -1.8049209848774979\n",
      "Operator 234: 1.8049209848774928\n",
      "Operator 235: -1.8557978410874878\n",
      "Operator 236: 1.8557978410875018\n",
      "Operator 237: -1.855797814074751\n",
      "Operator 238: 1.8557978140747484\n",
      "Operator 239: -1.804920948018963\n",
      "Operator 240: 1.8049209480189572\n",
      "Operator 241: -1.626619746705917\n",
      "Operator 242: 1.6266197467059065\n",
      "Operator 245: 1.414213558416265\n",
      "Operator 246: -1.4142135584162676\n",
      "Operator 248: 1.6266197681003933\n",
      "Operator 249: -1.6266197681004053\n",
      "Operator 250: 1.8049209848774972\n",
      "Operator 251: -1.804920984877498\n",
      "Operator 252: 1.8557978410874876\n",
      "Operator 253: -1.8557978410875002\n",
      "Operator 254: 1.8557978140747484\n",
      "Operator 255: -1.8557978140747426\n",
      "Operator 256: 1.804920948018961\n",
      "Operator 257: -1.8049209480189576\n",
      "Operator 258: 1.6266197467059142\n",
      "Operator 259: -1.6266197467059134\n",
      "Operator 262: -1.788854381981174\n",
      "Operator 264: -0.6324555302905501\n",
      "Operator 265: 1.4142135584162703\n",
      "Operator 267: -1.1501938716540212\n",
      "Operator 268: 0.7007032770818769\n",
      "Operator 269: -1.8049209848774979\n",
      "Operator 270: 0.3915325608057072\n",
      "Operator 271: -1.8557978410874878\n",
      "Operator 272: 0.2980518490831753\n",
      "Operator 273: -1.8557978140747509\n",
      "Operator 274: 0.29805184458601736\n",
      "Operator 275: -1.8049209480189627\n",
      "Operator 276: 0.3915325981731292\n",
      "Operator 277: -1.6266197467059171\n",
      "Operator 278: 0.700703330667828\n",
      "Operator 280: -1.2649110675932662\n",
      "Operator 282: 1.4142135584162654\n",
      "Operator 283: -0.8228329485526621\n",
      "Operator 285: 0.7007032770818726\n",
      "Operator 286: -1.6266197681004058\n",
      "Operator 287: 0.3915325608057092\n",
      "Operator 288: -1.804920984877498\n",
      "Operator 289: 0.2980518490831739\n",
      "Operator 290: -1.8557978410874998\n",
      "Operator 291: 0.29805184458601264\n",
      "Operator 292: -1.8557978140747422\n",
      "Operator 293: 0.39153259817312625\n",
      "Operator 294: -1.8049209480189579\n",
      "Operator 295: 0.700703330667832\n",
      "Operator 296: -1.6266197467059134\n",
      "Total gradient norm: 28.524510039322582\n",
      "Operators under consideration (1):\n",
      "[16]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000033)]\n",
      "Operator(s) added to ansatz: [16]\n",
      "Gradients: [np.float64(4.000000000000033)]\n",
      "Initial energy: -26.76432245574464\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16]...\n",
      "Starting point: [np.float64(0.4749092134271646), np.float64(0.5535743588866002), np.float64(0.5627237717840016), np.float64(0.5943686160504732), np.float64(0.5943686341636998), np.float64(0.5627237931749488), np.float64(-0.39269908029976835), np.float64(0.47490922261991175), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -28.401685\n",
      "         Iterations: 17\n",
      "         Function evaluations: 101\n",
      "         Gradient evaluations: 88\n",
      "\n",
      "Current energy: -28.40168533038331\n",
      "(change of -1.6373628746386686)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 3.414213562071446\n",
      "Operator 2: 3.41421356207145\n",
      "Operator 3: -3.1467050062332875\n",
      "Operator 5: -1.9459060032436346\n",
      "Operator 7: -1.3961640240662225\n",
      "Operator 9: -1.048712117704073\n",
      "Operator 11: -0.7822533488368469\n",
      "Operator 13: -0.5489783435531328\n",
      "Operator 15: -0.32754855471735467\n",
      "Operator 17: 9.149101099059769e-08\n",
      "Operator 18: 2.894427154403535\n",
      "Operator 19: -3.4142135620714473\n",
      "Operator 21: -3.4142135620714513\n",
      "Operator 22: 3.1467050062332893\n",
      "Operator 24: 1.9459060032436337\n",
      "Operator 26: 1.3961640240662176\n",
      "Operator 28: 1.0487121177040792\n",
      "Operator 30: 0.7822533488368559\n",
      "Operator 32: 0.5489783435531355\n",
      "Operator 34: 1.5268826604246666\n",
      "Operator 36: 3.41421356207145\n",
      "Operator 37: 2.225056447826513\n",
      "Operator 39: 2.458223892129234\n",
      "Operator 40: -1.2306865035333168\n",
      "Operator 41: 2.2385467232007157\n",
      "Operator 42: -1.60402765627811\n",
      "Operator 43: 2.1348387523115937\n",
      "Operator 44: -1.7747352486330437\n",
      "Operator 45: 2.074652499046082\n",
      "Operator 46: -1.8718669806544175\n",
      "Operator 47: 2.0361033077584567\n",
      "Operator 48: -1.9337417645473935\n",
      "Operator 49: 2.011914972464276\n",
      "Operator 50: -1.9731935301802963\n",
      "Operator 51: 2.0466690680925796\n",
      "Operator 52: 3.4142135620714473\n",
      "Operator 54: 1.9575478919883889\n",
      "Operator 55: 3.1467050062332893\n",
      "Operator 56: -1.376962221140812\n",
      "Operator 57: 2.458223892129234\n",
      "Operator 58: -1.6700408413903534\n",
      "Operator 59: 2.2385467232007152\n",
      "Operator 60: -1.8114435980563852\n",
      "Operator 61: 2.134838752311593\n",
      "Operator 62: -1.8951077551640534\n",
      "Operator 63: 2.074652499046082\n",
      "Operator 64: -1.948965363229651\n",
      "Operator 65: 2.0361033077584567\n",
      "Operator 66: -1.9821184351357448\n",
      "Operator 67: 2.011914972464276\n",
      "Operator 74: -1.5021033687384429\n",
      "Operator 76: -1.7498049748060116\n",
      "Operator 78: -1.8594997694412196\n",
      "Operator 80: -1.921526135661884\n",
      "Operator 82: -1.960698716828921\n",
      "Operator 84: -1.9850727444425031\n",
      "Operator 85: -1.5268826604246661\n",
      "Operator 87: -3.4142135620714513\n",
      "Operator 88: -2.225056447826512\n",
      "Operator 90: -2.4582238921292325\n",
      "Operator 91: 1.230686503533318\n",
      "Operator 92: -2.238546723200712\n",
      "Operator 93: 1.6040276562781082\n",
      "Operator 94: -2.134838752311592\n",
      "Operator 95: 1.7747352486330437\n",
      "Operator 96: -2.074652499046084\n",
      "Operator 97: 1.8718669806544184\n",
      "Operator 98: -2.036103307758455\n",
      "Operator 99: 1.9337417645473933\n",
      "Operator 100: -2.0119149724642735\n",
      "Operator 101: 1.9731935301802963\n",
      "Operator 102: 2.8944271544035445\n",
      "Operator 103: -2.046669068092573\n",
      "Operator 104: -3.414213562071445\n",
      "Operator 106: -1.9575478919883895\n",
      "Operator 107: -3.1467050062332884\n",
      "Operator 108: 1.376962221140812\n",
      "Operator 109: -2.4582238921292325\n",
      "Operator 110: 1.670040841390355\n",
      "Operator 111: -2.238546723200712\n",
      "Operator 112: 1.8114435980563866\n",
      "Operator 113: -2.1348387523115924\n",
      "Operator 114: 1.8951077551640538\n",
      "Operator 115: -2.074652499046084\n",
      "Operator 116: 1.9489653632296533\n",
      "Operator 117: -2.036103307758455\n",
      "Operator 118: 1.9821184351357448\n",
      "Operator 119: -2.0119149724642735\n",
      "Operator 120: 0.3275485547173492\n",
      "Operator 121: 1.5268826604246661\n",
      "Operator 123: 3.41421356207145\n",
      "Operator 124: 2.225056447826512\n",
      "Operator 126: 1.9459060032436346\n",
      "Operator 127: -1.2306865035333177\n",
      "Operator 128: 0.8004941379597467\n",
      "Operator 129: -1.6040276562781075\n",
      "Operator 130: 0.4190658850229674\n",
      "Operator 131: -1.7747352486330423\n",
      "Operator 132: 0.2334881635231088\n",
      "Operator 133: -1.8718669806544193\n",
      "Operator 134: 0.12400023374284433\n",
      "Operator 135: -1.9337417645473938\n",
      "Operator 136: 0.05412809604880764\n",
      "Operator 137: -1.9731935301802963\n",
      "Operator 138: 1.264911076736298\n",
      "Operator 140: -1.4142135626748389\n",
      "Operator 141: 0.8108428861010971\n",
      "Operator 143: -0.6547924085573487\n",
      "Operator 144: 1.6386175968418364\n",
      "Operator 145: -0.3137555035853984\n",
      "Operator 146: 1.8333787842062994\n",
      "Operator 147: -0.1722902709061276\n",
      "Operator 148: 1.9088308317733422\n",
      "Operator 149: -0.0960999551924678\n",
      "Operator 150: 1.9483127980636719\n",
      "Operator 151: -0.048669270175642744\n",
      "Operator 152: 1.9725027085708489\n",
      "Operator 153: -0.017916923063537454\n",
      "Operator 154: 1.988031457000687\n",
      "Operator 156: -2.894427154403534\n",
      "Operator 157: -6.469391376701452e-08\n",
      "Operator 158: -2.894427154403535\n",
      "Operator 159: 3.414213562071445\n",
      "Operator 161: 3.4142135620714513\n",
      "Operator 162: -1.2574248891395543\n",
      "Operator 164: -0.7337343879132321\n",
      "Operator 166: -0.5056307232897248\n",
      "Operator 168: -0.3527866490925352\n",
      "Operator 170: -0.22662890358912968\n",
      "Operator 172: -0.11104344392066778\n",
      "Operator 174: -9.149101087108734e-08\n",
      "Operator 175: -2.8944271544035445\n",
      "Operator 182: 1.5021033687384442\n",
      "Operator 184: 1.7498049748060147\n",
      "Operator 186: 1.85949976944122\n",
      "Operator 188: 1.9215261356618818\n",
      "Operator 190: 1.9606987168289223\n",
      "Operator 192: 1.985072744442506\n",
      "Operator 199: -1.5021033687384429\n",
      "Operator 201: -1.7498049748060107\n",
      "Operator 203: -1.8594997694412196\n",
      "Operator 205: -1.9215261356618836\n",
      "Operator 207: -1.960698716828921\n",
      "Operator 209: -1.9850727444425031\n",
      "Operator 210: 6.469391368588797e-08\n",
      "Operator 211: 2.8944271544035445\n",
      "Operator 212: -3.4142135620714473\n",
      "Operator 214: -3.41421356207145\n",
      "Operator 215: 1.257424889139555\n",
      "Operator 217: 0.7337343879132321\n",
      "Operator 219: 0.5056307232897254\n",
      "Operator 221: 0.3527866490925353\n",
      "Operator 223: 0.22662890358912952\n",
      "Operator 225: 0.11104344392066767\n",
      "Operator 228: -1.414213562674839\n",
      "Operator 229: 1.4142135626748404\n",
      "Operator 231: -1.6386175968418302\n",
      "Operator 232: 1.6386175968418364\n",
      "Operator 233: -1.8333787842063056\n",
      "Operator 234: 1.8333787842062994\n",
      "Operator 235: -1.9088308317733287\n",
      "Operator 236: 1.9088308317733422\n",
      "Operator 237: -1.948312798063676\n",
      "Operator 238: 1.9483127980636716\n",
      "Operator 239: -1.9725027085708564\n",
      "Operator 240: 1.972502708570849\n",
      "Operator 241: -1.9880314570006945\n",
      "Operator 242: 1.9880314570006865\n",
      "Operator 243: -1.997023475108776\n",
      "Operator 245: 1.414213562674835\n",
      "Operator 246: -1.414213562674837\n",
      "Operator 248: 1.6386175968418266\n",
      "Operator 249: -1.638617596841838\n",
      "Operator 250: 1.833378784206305\n",
      "Operator 251: -1.833378784206305\n",
      "Operator 252: 1.9088308317733276\n",
      "Operator 253: -1.9088308317733411\n",
      "Operator 254: 1.9483127980636732\n",
      "Operator 255: -1.948312798063668\n",
      "Operator 256: 1.9725027085708557\n",
      "Operator 257: -1.9725027085708509\n",
      "Operator 258: 1.988031457000693\n",
      "Operator 259: -1.98803145700069\n",
      "Operator 260: 1.997023475108774\n",
      "Operator 261: 9.149101099059769e-08\n",
      "Operator 262: -1.788854400298088\n",
      "Operator 264: -0.6324555062910315\n",
      "Operator 265: 1.4142135626748398\n",
      "Operator 267: -1.158677614251265\n",
      "Operator 268: 0.6547924085573504\n",
      "Operator 269: -1.8333787842063056\n",
      "Operator 270: 0.31375550358539694\n",
      "Operator 271: -1.9088308317733287\n",
      "Operator 272: 0.17229027090612964\n",
      "Operator 273: -1.9483127980636756\n",
      "Operator 274: 0.09609995519246937\n",
      "Operator 275: -1.9725027085708557\n",
      "Operator 276: 0.048669270175643334\n",
      "Operator 277: -1.9880314570006945\n",
      "Operator 278: 0.017916923063536844\n",
      "Operator 279: -1.997023475108776\n",
      "Operator 280: -1.2649110767363099\n",
      "Operator 282: 1.414213562674835\n",
      "Operator 283: -0.8108428861010955\n",
      "Operator 285: 0.6547924085573471\n",
      "Operator 286: -1.6386175968418386\n",
      "Operator 287: 0.3137555035853987\n",
      "Operator 288: -1.8333787842063056\n",
      "Operator 289: 0.17229027090612858\n",
      "Operator 290: -1.908830831773342\n",
      "Operator 291: 0.09609995519246659\n",
      "Operator 292: -1.948312798063668\n",
      "Operator 293: 0.048669270175641724\n",
      "Operator 294: -1.9725027085708504\n",
      "Operator 295: 0.0179169230635369\n",
      "Operator 296: -1.98803145700069\n",
      "Total gradient norm: 27.48543125666603\n",
      "Operators under consideration (1):\n",
      "[212]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.4142135620714473)]\n",
      "Operator(s) added to ansatz: [212]\n",
      "Gradients: [np.float64(-3.4142135620714473)]\n",
      "Initial energy: -28.40168533038331\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212]...\n",
      "Starting point: [np.float64(0.7306704299026111), np.float64(0.5535743691260615), np.float64(0.7023913385532571), np.float64(0.6714778187954679), np.float64(0.6338475407823382), np.float64(0.5798576655770256), np.float64(-0.3926990818053962), np.float64(0.4801022172217067), np.float64(-0.7581160103813274), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -29.073188\n",
      "         Iterations: 9\n",
      "         Function evaluations: 42\n",
      "         Gradient evaluations: 31\n",
      "\n",
      "Current energy: -29.073188084483625\n",
      "(change of -0.6715027541003167)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.0427784538801357\n",
      "Operator 1: 7.637349956076382e-08\n",
      "Operator 2: 3.6894586735725414\n",
      "Operator 3: -3.1467050259006486\n",
      "Operator 4: 6.700497604393167e-08\n",
      "Operator 5: -1.9459060212756512\n",
      "Operator 7: -1.3961640134693656\n",
      "Operator 8: -4.417267738981879e-08\n",
      "Operator 9: -1.0487121422742376\n",
      "Operator 10: 1.1188828191606207e-07\n",
      "Operator 11: -0.7822533392474171\n",
      "Operator 12: -1.4287609049503737e-07\n",
      "Operator 13: -0.5489783232490189\n",
      "Operator 14: 1.0059546129426167e-07\n",
      "Operator 15: -0.3275485675659444\n",
      "Operator 16: -4.266148323495145e-08\n",
      "Operator 17: 3.884030818166733e-07\n",
      "Operator 18: 2.094788886790817\n",
      "Operator 19: 7.221974909135025e-08\n",
      "Operator 20: 1.3681172621574007\n",
      "Operator 21: -3.6894586735725423\n",
      "Operator 22: 3.1467050259006504\n",
      "Operator 23: -6.700497604393167e-08\n",
      "Operator 24: 1.9459060212756487\n",
      "Operator 26: 1.39616401346936\n",
      "Operator 27: 4.417267841337395e-08\n",
      "Operator 28: 1.0487121422742445\n",
      "Operator 29: -1.1188828191606207e-07\n",
      "Operator 30: 0.7822533392474262\n",
      "Operator 31: 1.4287609334585018e-07\n",
      "Operator 32: 0.5489783232490211\n",
      "Operator 33: -1.0059546629026528e-07\n",
      "Operator 34: -1.246375442414253\n",
      "Operator 35: -7.637350044443451e-08\n",
      "Operator 36: 2.626928857460803\n",
      "Operator 37: 2.658114049591008\n",
      "Operator 38: 6.700497604393167e-08\n",
      "Operator 39: 2.4582238990506777\n",
      "Operator 40: -1.2306864911965143\n",
      "Operator 41: 2.238546719131559\n",
      "Operator 42: -1.6040276799078605\n",
      "Operator 43: 2.1348387602959864\n",
      "Operator 44: -1.7747352105249856\n",
      "Operator 45: 2.0746524948472884\n",
      "Operator 46: -1.8718670051508743\n",
      "Operator 47: 2.0361033068185743\n",
      "Operator 48: -1.9337417599219786\n",
      "Operator 49: 2.011914972619718\n",
      "Operator 50: -1.9731935304138253\n",
      "Operator 51: 2.092896918043158\n",
      "Operator 52: -7.221974957512674e-08\n",
      "Operator 53: -7.637349965441302e-08\n",
      "Operator 54: 2.115360401919155\n",
      "Operator 55: 3.1467050259006504\n",
      "Operator 56: -1.3769621866949793\n",
      "Operator 57: 2.458223899050677\n",
      "Operator 58: -1.6700408510347495\n",
      "Operator 59: 2.238546719131559\n",
      "Operator 60: -1.8114435975790295\n",
      "Operator 61: 2.134838760295987\n",
      "Operator 62: -1.895107741422367\n",
      "Operator 63: 2.0746524948472884\n",
      "Operator 64: -1.9489653812082874\n",
      "Operator 65: 2.0361033068185748\n",
      "Operator 66: -1.9821184283196742\n",
      "Operator 67: 2.011914972619718\n",
      "Operator 68: -1.1175788153687123\n",
      "Operator 74: -1.502103356705883\n",
      "Operator 76: -1.7498049780554912\n",
      "Operator 78: -1.8594997647509053\n",
      "Operator 80: -1.921526135032346\n",
      "Operator 82: -1.9606987215378653\n",
      "Operator 84: -1.9850727424799546\n",
      "Operator 85: 0.7031174706083454\n",
      "Operator 86: -1.3681172621574007\n",
      "Operator 87: -2.626928857460803\n",
      "Operator 88: -2.658114049591006\n",
      "Operator 89: -6.700497604393167e-08\n",
      "Operator 90: -2.4582238990506773\n",
      "Operator 91: 1.230686491196515\n",
      "Operator 92: -2.238546719131556\n",
      "Operator 93: 1.6040276799078574\n",
      "Operator 94: -2.1348387602959855\n",
      "Operator 95: 1.7747352105249856\n",
      "Operator 96: -2.0746524948472898\n",
      "Operator 97: 1.871867005150877\n",
      "Operator 98: -2.036103306818573\n",
      "Operator 99: 1.9337417599219764\n",
      "Operator 100: -2.011914972619715\n",
      "Operator 101: 1.9731935304138253\n",
      "Operator 102: 2.4775946885016884\n",
      "Operator 103: -2.0928969180431527\n",
      "Operator 104: -1.0427784538801355\n",
      "Operator 105: -1.3681172621574005\n",
      "Operator 106: -2.1153604019191556\n",
      "Operator 107: -3.1467050259006486\n",
      "Operator 108: 1.3769621866949793\n",
      "Operator 109: -2.458223899050676\n",
      "Operator 110: 1.67004085103475\n",
      "Operator 111: -2.2385467191315556\n",
      "Operator 112: 1.8114435975790286\n",
      "Operator 113: -2.134838760295986\n",
      "Operator 114: 1.8951077414223656\n",
      "Operator 115: -2.0746524948472893\n",
      "Operator 116: 1.9489653812082866\n",
      "Operator 117: -2.036103306818573\n",
      "Operator 118: 1.9821184283196753\n",
      "Operator 119: -2.011914972619715\n",
      "Operator 120: 0.3275485675659392\n",
      "Operator 121: -0.7031174706083453\n",
      "Operator 122: -2.5614537510158113e-08\n",
      "Operator 123: 2.6269288574608023\n",
      "Operator 124: 1.8926019020918345\n",
      "Operator 125: 5.660106554231945e-08\n",
      "Operator 126: 1.9459060212756512\n",
      "Operator 127: -1.230686491196515\n",
      "Operator 128: 0.8004941456134596\n",
      "Operator 129: -1.6040276799078579\n",
      "Operator 130: 0.4190658939837093\n",
      "Operator 131: -1.774735210524986\n",
      "Operator 132: 0.2334881571557622\n",
      "Operator 133: -1.8718670051508743\n",
      "Operator 134: 0.1240002383607406\n",
      "Operator 135: -1.9337417599219782\n",
      "Operator 136: 0.0541280911099391\n",
      "Operator 137: -1.9731935304138253\n",
      "Operator 138: 1.591607086627831\n",
      "Operator 139: -0.39786313526624273\n",
      "Operator 140: 1.645881753614197\n",
      "Operator 141: 0.4369667670352069\n",
      "Operator 143: -0.6547924017176852\n",
      "Operator 144: 1.6386175830785834\n",
      "Operator 145: -0.3137555043786466\n",
      "Operator 146: 1.8333787849191663\n",
      "Operator 147: -0.17229028359501058\n",
      "Operator 148: 1.9088308345759235\n",
      "Operator 149: -0.09609994082812112\n",
      "Operator 150: 1.9483127902887762\n",
      "Operator 151: -0.04866927904937353\n",
      "Operator 152: 1.9725027157960213\n",
      "Operator 153: -0.017916919065868453\n",
      "Operator 154: 1.9880314544932058\n",
      "Operator 155: 4.266148323495145e-08\n",
      "Operator 156: -2.4775946885016813\n",
      "Operator 157: -3.280954711498293e-07\n",
      "Operator 158: -2.4775946885016813\n",
      "Operator 159: 1.0427784538801355\n",
      "Operator 160: -0.7844134702686514\n",
      "Operator 161: 3.6894586735725423\n",
      "Operator 162: -1.2574248944256519\n",
      "Operator 163: 6.700497533363638e-08\n",
      "Operator 164: -0.7337343789644772\n",
      "Operator 166: -0.5056307599017058\n",
      "Operator 167: -4.417267840035607e-08\n",
      "Operator 168: -0.35278660438368736\n",
      "Operator 169: 1.1188828191606207e-07\n",
      "Operator 170: -0.22662892679911745\n",
      "Operator 171: -1.4287609049503737e-07\n",
      "Operator 172: -0.11104343372107695\n",
      "Operator 173: 1.0059546629026528e-07\n",
      "Operator 174: -3.884030810199569e-07\n",
      "Operator 175: -2.0947888867908318\n",
      "Operator 176: 1.1175788153687132\n",
      "Operator 177: -0.252073478640206\n",
      "Operator 178: -0.7515940329341739\n",
      "Operator 182: 1.5021033567058844\n",
      "Operator 184: 1.7498049780554936\n",
      "Operator 186: 1.8594997647509068\n",
      "Operator 188: 1.9215261350323436\n",
      "Operator 190: 1.9606987215378673\n",
      "Operator 192: 1.9850727424799577\n",
      "Operator 193: -1.3230022525564409\n",
      "Operator 199: -1.502103356705883\n",
      "Operator 201: -1.7498049780554914\n",
      "Operator 203: -1.8594997647509057\n",
      "Operator 205: -1.9215261350323458\n",
      "Operator 207: -1.9606987215378657\n",
      "Operator 209: -1.9850727424799546\n",
      "Operator 210: 3.28095470938372e-07\n",
      "Operator 211: 2.477594688501689\n",
      "Operator 212: 7.221974959909628e-08\n",
      "Operator 213: -4.378893924432738e-08\n",
      "Operator 214: -3.6894586735725414\n",
      "Operator 215: 1.257424894425652\n",
      "Operator 216: -6.700497335796019e-08\n",
      "Operator 217: 0.7337343789644772\n",
      "Operator 219: 0.5056307599017063\n",
      "Operator 220: 4.417267729205765e-08\n",
      "Operator 221: 0.35278660438368764\n",
      "Operator 222: -1.1188828191606207e-07\n",
      "Operator 223: 0.22662892679911723\n",
      "Operator 224: 1.4287609146244036e-07\n",
      "Operator 225: 0.11104343372107695\n",
      "Operator 226: -1.0059546129426167e-07\n",
      "Operator 227: -1.1862872683143908\n",
      "Operator 228: 1.645881753614197\n",
      "Operator 229: 1.0703874953916\n",
      "Operator 231: -1.638617583078577\n",
      "Operator 232: 1.638617583078584\n",
      "Operator 233: -1.8333787849191712\n",
      "Operator 234: 1.8333787849191665\n",
      "Operator 235: -1.9088308345759102\n",
      "Operator 236: 1.9088308345759235\n",
      "Operator 237: -1.9483127902887811\n",
      "Operator 238: 1.9483127902887765\n",
      "Operator 239: -1.972502715796029\n",
      "Operator 240: 1.9725027157960215\n",
      "Operator 241: -1.9880314544932145\n",
      "Operator 242: 1.9880314544932058\n",
      "Operator 243: -1.9970234756532188\n",
      "Operator 244: 1.4043400846330225\n",
      "Operator 245: 1.3872882929455315\n",
      "Operator 246: -0.7621258425932307\n",
      "Operator 248: 1.6386175830785734\n",
      "Operator 249: -1.6386175830785858\n",
      "Operator 250: 1.8333787849191707\n",
      "Operator 251: -1.8333787849191714\n",
      "Operator 252: 1.9088308345759093\n",
      "Operator 253: -1.9088308345759226\n",
      "Operator 254: 1.9483127902887785\n",
      "Operator 255: -1.9483127902887731\n",
      "Operator 256: 1.972502715796029\n",
      "Operator 257: -1.9725027157960233\n",
      "Operator 258: 1.9880314544932123\n",
      "Operator 259: -1.9880314544932092\n",
      "Operator 260: 1.9970234756532168\n",
      "Operator 261: 0.9289728242089886\n",
      "Operator 262: -1.8841622012123729\n",
      "Operator 263: -1.4043400846330196\n",
      "Operator 264: 1.2190566324688974\n",
      "Operator 265: 1.0703874953916\n",
      "Operator 267: -1.3841883442002492\n",
      "Operator 268: 0.6547924017176867\n",
      "Operator 269: -1.8333787849191712\n",
      "Operator 270: 0.3137555043786452\n",
      "Operator 271: -1.9088308345759097\n",
      "Operator 272: 0.17229028359501286\n",
      "Operator 273: -1.9483127902887811\n",
      "Operator 274: 0.09609994082812245\n",
      "Operator 275: -1.972502715796029\n",
      "Operator 276: 0.04866927904937411\n",
      "Operator 277: -1.9880314544932145\n",
      "Operator 278: 0.01791691906586773\n",
      "Operator 279: -1.997023475653219\n",
      "Operator 280: -1.5916070866278411\n",
      "Operator 281: 0.47099481211332606\n",
      "Operator 282: 1.3872882929455315\n",
      "Operator 283: -0.6137093603133665\n",
      "Operator 285: 0.6547924017176838\n",
      "Operator 286: -1.6386175830785858\n",
      "Operator 287: 0.31375550437864663\n",
      "Operator 288: -1.8333787849191707\n",
      "Operator 289: 0.17229028359501175\n",
      "Operator 290: -1.9088308345759226\n",
      "Operator 291: 0.09609994082811985\n",
      "Operator 292: -1.9483127902887731\n",
      "Operator 293: 0.048669279049372544\n",
      "Operator 294: -1.9725027157960233\n",
      "Operator 295: 0.017916919065867898\n",
      "Operator 296: -1.9880314544932094\n",
      "Total gradient norm: 26.16167748708298\n",
      "Operators under consideration (1):\n",
      "[2]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.6894586735725414)]\n",
      "Operator(s) added to ansatz: [2]\n",
      "Gradients: [np.float64(3.6894586735725414)]\n",
      "Initial energy: -29.073188084483625\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2]...\n",
      "Starting point: [np.float64(0.7306704241640253), np.float64(0.6143911479309777), np.float64(0.7023913494838104), np.float64(0.6714778101901587), np.float64(0.6338475431297165), np.float64(0.5798576660230201), np.float64(-0.2823685455773487), np.float64(0.48010221122049457), np.float64(-0.7581160128771389), np.float64(0.38922035360185975), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.123106\n",
      "         Iterations: 25\n",
      "         Function evaluations: 117\n",
      "         Gradient evaluations: 103\n",
      "\n",
      "Current energy: -30.123105625617836\n",
      "(change of -1.04991754113421)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 1: -1.096673322370451e-08\n",
      "Operator 6: -1.1588998840393574e-08\n",
      "Operator 9: -2.142501685147586e-08\n",
      "Operator 10: 1.1316130171801216e-08\n",
      "Operator 11: -2.695629577886826e-08\n",
      "Operator 12: 1.0137026486769916e-08\n",
      "Operator 13: -1.1034460719350618e-08\n",
      "Operator 14: -1.2352670775150898e-08\n",
      "Operator 18: 1.899752360158145e-08\n",
      "Operator 25: 1.1588998840393574e-08\n",
      "Operator 28: 2.142502258584703e-08\n",
      "Operator 29: -1.13161339465595e-08\n",
      "Operator 30: 2.6956305160252818e-08\n",
      "Operator 31: -1.0137026486769916e-08\n",
      "Operator 32: 1.1034463859535536e-08\n",
      "Operator 33: 1.2352670775150898e-08\n",
      "Operator 34: -2.0000000000000187\n",
      "Operator 35: 2.6810089865965403e-08\n",
      "Operator 36: -2.4850712437644535\n",
      "Operator 37: 1.940285001867736\n",
      "Operator 38: -2.0000000000000107\n",
      "Operator 39: 2.0000000000000213\n",
      "Operator 40: -2.000000000000017\n",
      "Operator 41: 2.00000000000002\n",
      "Operator 42: -2.000000000000023\n",
      "Operator 43: 2.0000000000000346\n",
      "Operator 44: -2.00000000000003\n",
      "Operator 45: 2.0000000000000258\n",
      "Operator 46: -2.000000000000022\n",
      "Operator 47: 2.00000000000002\n",
      "Operator 48: -2.0000000000000133\n",
      "Operator 49: 2.000000000000015\n",
      "Operator 50: -2.000000000000016\n",
      "Operator 51: 1.940285001867748\n",
      "Operator 52: -2.485071243764456\n",
      "Operator 53: 2.681008966731937e-08\n",
      "Operator 54: -2.000000000000013\n",
      "Operator 55: 2.0000000000000084\n",
      "Operator 56: -2.0000000000000178\n",
      "Operator 57: 2.0000000000000213\n",
      "Operator 58: -2.000000000000019\n",
      "Operator 59: 2.00000000000002\n",
      "Operator 60: -2.0000000000000275\n",
      "Operator 61: 2.0000000000000338\n",
      "Operator 62: -2.0000000000000195\n",
      "Operator 63: 2.0000000000000253\n",
      "Operator 64: -2.000000000000013\n",
      "Operator 65: 2.0000000000000204\n",
      "Operator 66: -2.0000000000000133\n",
      "Operator 67: 2.000000000000015\n",
      "Operator 68: -1.9402850018677453\n",
      "Operator 70: -1.9402850018677602\n",
      "Operator 72: -2.0000000000000093\n",
      "Operator 74: -2.0000000000000195\n",
      "Operator 76: -2.000000000000016\n",
      "Operator 78: -2.0000000000000333\n",
      "Operator 80: -2.000000000000028\n",
      "Operator 82: -2.000000000000018\n",
      "Operator 84: -2.000000000000012\n",
      "Operator 85: 1.9402850018677453\n",
      "Operator 86: -1.9402850018677604\n",
      "Operator 87: 1.9402850018677622\n",
      "Operator 88: -1.9402850018677364\n",
      "Operator 89: 2.0000000000000107\n",
      "Operator 90: -2.0000000000000195\n",
      "Operator 91: 2.000000000000017\n",
      "Operator 92: -2.000000000000016\n",
      "Operator 93: 2.000000000000023\n",
      "Operator 94: -2.0000000000000338\n",
      "Operator 95: 2.000000000000029\n",
      "Operator 96: -2.0000000000000284\n",
      "Operator 97: 2.0000000000000218\n",
      "Operator 98: -2.0000000000000187\n",
      "Operator 99: 2.0000000000000133\n",
      "Operator 100: -2.0000000000000124\n",
      "Operator 101: 2.000000000000016\n",
      "Operator 102: 2.0000000000000213\n",
      "Operator 103: -1.9402850018677453\n",
      "Operator 104: 1.9402850018677629\n",
      "Operator 105: -1.9402850018677602\n",
      "Operator 106: 1.9402850018677373\n",
      "Operator 107: -2.0000000000000098\n",
      "Operator 108: 2.0000000000000178\n",
      "Operator 109: -2.0000000000000195\n",
      "Operator 110: 2.000000000000019\n",
      "Operator 111: -2.000000000000016\n",
      "Operator 112: 2.0000000000000275\n",
      "Operator 113: -2.0000000000000338\n",
      "Operator 114: 2.000000000000017\n",
      "Operator 115: -2.000000000000028\n",
      "Operator 116: 2.000000000000013\n",
      "Operator 117: -2.0000000000000187\n",
      "Operator 118: 2.0000000000000133\n",
      "Operator 119: -2.0000000000000124\n",
      "Operator 121: -1.9402850018677453\n",
      "Operator 123: -2.4850712437644535\n",
      "Operator 125: -1.9402850018677382\n",
      "Operator 127: -2.000000000000017\n",
      "Operator 129: -2.000000000000023\n",
      "Operator 131: -2.000000000000029\n",
      "Operator 133: -2.0000000000000218\n",
      "Operator 135: -2.0000000000000133\n",
      "Operator 137: -2.000000000000016\n",
      "Operator 138: 1.9402850018677453\n",
      "Operator 140: 2.4850712437644544\n",
      "Operator 142: 1.940285001867737\n",
      "Operator 144: 2.0000000000000195\n",
      "Operator 146: 2.000000000000015\n",
      "Operator 148: 2.0000000000000338\n",
      "Operator 150: 2.000000000000028\n",
      "Operator 152: 2.0000000000000187\n",
      "Operator 154: 2.000000000000012\n",
      "Operator 156: -2.000000000000018\n",
      "Operator 165: -1.1588998840393574e-08\n",
      "Operator 166: -1.4924354025937795e-08\n",
      "Operator 168: -1.20319459950835e-08\n",
      "Operator 169: 1.1316132697558597e-08\n",
      "Operator 171: 1.0137026486769916e-08\n",
      "Operator 173: -1.2352670775150898e-08\n",
      "Operator 175: -1.8997540235339642e-08\n",
      "Operator 176: 1.940285001867748\n",
      "Operator 178: 2.681009082812655e-08\n",
      "Operator 179: -0.48507124376441324\n",
      "Operator 180: 2.0000000000000084\n",
      "Operator 182: 2.0000000000000213\n",
      "Operator 184: 2.00000000000002\n",
      "Operator 186: 2.0000000000000346\n",
      "Operator 188: 2.0000000000000258\n",
      "Operator 190: 2.0000000000000204\n",
      "Operator 192: 2.000000000000015\n",
      "Operator 193: -2.0000000000000187\n",
      "Operator 195: -1.9402850018677609\n",
      "Operator 197: -1.9402850018677364\n",
      "Operator 199: -2.0000000000000195\n",
      "Operator 201: -2.000000000000016\n",
      "Operator 203: -2.0000000000000333\n",
      "Operator 205: -2.0000000000000275\n",
      "Operator 207: -2.000000000000018\n",
      "Operator 209: -2.0000000000000124\n",
      "Operator 218: 1.1588998840393574e-08\n",
      "Operator 219: 1.492435394743317e-08\n",
      "Operator 221: 1.2031945412216413e-08\n",
      "Operator 222: -1.1316128922800315e-08\n",
      "Operator 224: -1.0137026486769916e-08\n",
      "Operator 226: 1.2352670775150898e-08\n",
      "Operator 227: -1.940285001867748\n",
      "Operator 228: 2.4850712437644544\n",
      "Operator 229: -2.68100902033825e-08\n",
      "Operator 230: 2.0000000000000093\n",
      "Operator 231: -2.0000000000000084\n",
      "Operator 232: 2.0000000000000195\n",
      "Operator 233: -2.0000000000000213\n",
      "Operator 234: 2.000000000000015\n",
      "Operator 235: -2.00000000000002\n",
      "Operator 236: 2.0000000000000338\n",
      "Operator 237: -2.0000000000000346\n",
      "Operator 238: 2.000000000000028\n",
      "Operator 239: -2.0000000000000258\n",
      "Operator 240: 2.0000000000000187\n",
      "Operator 241: -2.0000000000000204\n",
      "Operator 242: 2.000000000000012\n",
      "Operator 243: -2.000000000000015\n",
      "Operator 244: 2.0000000000000187\n",
      "Operator 245: -2.681009077220887e-08\n",
      "Operator 246: 2.4850712437644544\n",
      "Operator 247: -1.9402850018677356\n",
      "Operator 248: 2.0000000000000093\n",
      "Operator 249: -2.0000000000000213\n",
      "Operator 250: 2.0000000000000195\n",
      "Operator 251: -2.00000000000002\n",
      "Operator 252: 2.000000000000015\n",
      "Operator 253: -2.0000000000000338\n",
      "Operator 254: 2.0000000000000333\n",
      "Operator 255: -2.000000000000025\n",
      "Operator 256: 2.0000000000000275\n",
      "Operator 257: -2.00000000000002\n",
      "Operator 258: 2.0000000000000187\n",
      "Operator 259: -2.000000000000015\n",
      "Operator 260: 2.000000000000012\n",
      "Operator 261: 2.0000000000000155\n",
      "Operator 262: -2.0000000000000213\n",
      "Operator 263: -2.0000000000000213\n",
      "Operator 264: 0.48507124376441557\n",
      "Operator 265: -2.6810090596538076e-08\n",
      "Operator 267: -1.9402850018677356\n",
      "Operator 269: -2.0000000000000213\n",
      "Operator 271: -2.00000000000002\n",
      "Operator 273: -2.0000000000000346\n",
      "Operator 275: -2.0000000000000258\n",
      "Operator 277: -2.00000000000002\n",
      "Operator 279: -2.000000000000015\n",
      "Operator 280: -1.940285001867748\n",
      "Operator 282: -2.681009079459959e-08\n",
      "Operator 283: 0.48507124376441346\n",
      "Operator 284: -2.0000000000000084\n",
      "Operator 286: -2.0000000000000213\n",
      "Operator 288: -2.00000000000002\n",
      "Operator 290: -2.0000000000000338\n",
      "Operator 292: -2.0000000000000258\n",
      "Operator 294: -2.00000000000002\n",
      "Operator 296: -2.000000000000015\n",
      "Total gradient norm: 25.61928165052603\n",
      "Operators under consideration (1):\n",
      "[228]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.4850712437644544)]\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(2.4850712437644544)]\n",
      "Initial energy: -30.123105625617836\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228]...\n",
      "Starting point: [np.float64(0.7853981636468196), np.float64(0.7853981611459632), np.float64(0.7853981603894613), np.float64(0.7853981596663586), np.float64(0.7853981617722813), np.float64(0.7853981639648691), np.float64(-0.12248932993783217), np.float64(0.7853981632602128), np.float64(-0.7853981638160075), np.float64(0.7853981608995515), np.float64(-0.7853981620847993), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.327276\n",
      "         Iterations: 5\n",
      "         Function evaluations: 55\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "Current energy: -30.32727615482132\n",
      "(change of -0.2041705292034841)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -7.058832742447974e-07\n",
      "Operator 1: 4.214351424835907e-07\n",
      "Operator 2: 4.8962155756256e-07\n",
      "Operator 3: -2.677156362977057e-07\n",
      "Operator 4: 1.496502260880919e-07\n",
      "Operator 5: -2.1775430677734408e-07\n",
      "Operator 6: -2.105406966573699e-07\n",
      "Operator 7: -2.2868334355546934e-07\n",
      "Operator 8: 7.164816493288355e-07\n",
      "Operator 9: 2.663286188123403e-07\n",
      "Operator 10: -1.0970691662337578e-06\n",
      "Operator 11: 3.807530256916279e-07\n",
      "Operator 12: 5.160731740768753e-07\n",
      "Operator 13: -8.581860645876076e-08\n",
      "Operator 14: -2.7371022470745743e-07\n",
      "Operator 15: -3.100273066047876e-07\n",
      "Operator 16: 5.284528441507064e-07\n",
      "Operator 17: -2.1415711829743147e-07\n",
      "Operator 18: -3.419127648902735e-07\n",
      "Operator 19: 8.451366795499526e-07\n",
      "Operator 20: -1.1060187035230614e-08\n",
      "Operator 21: -3.5036815374880566e-07\n",
      "Operator 22: 2.6771564123217997e-07\n",
      "Operator 23: -1.496502234974393e-07\n",
      "Operator 24: 2.1775430467756267e-07\n",
      "Operator 25: 2.105406966573699e-07\n",
      "Operator 26: 2.2868333679766632e-07\n",
      "Operator 27: -7.164816493288355e-07\n",
      "Operator 28: -2.6632861260694143e-07\n",
      "Operator 29: 1.0970691662337578e-06\n",
      "Operator 30: -3.807530155330872e-07\n",
      "Operator 31: -5.160731758824819e-07\n",
      "Operator 32: 8.581860912791796e-08\n",
      "Operator 33: 2.737102273719927e-07\n",
      "Operator 34: -1.8939306553646\n",
      "Operator 35: -0.7090316520838255\n",
      "Operator 36: -2.089491622819905\n",
      "Operator 37: 1.793486662686808\n",
      "Operator 38: -1.9999999999999838\n",
      "Operator 39: 2.0000000000000195\n",
      "Operator 40: -2.0000000000000178\n",
      "Operator 41: 2.0000000000000195\n",
      "Operator 42: -1.9999999999999836\n",
      "Operator 43: 1.9999999999999916\n",
      "Operator 44: -2.0000000000000524\n",
      "Operator 45: 2.000000000000008\n",
      "Operator 46: -2.0000000000000315\n",
      "Operator 47: 2.000000000000015\n",
      "Operator 48: -2.000000000000012\n",
      "Operator 49: 2.000000000000013\n",
      "Operator 50: -1.9999999999999822\n",
      "Operator 51: 1.793486662686818\n",
      "Operator 52: -2.0894916228199056\n",
      "Operator 53: -0.7090316520838255\n",
      "Operator 54: -1.8939306553645843\n",
      "Operator 55: 2.00000000000001\n",
      "Operator 56: -2.000000000000001\n",
      "Operator 57: 2.00000000000002\n",
      "Operator 58: -2.0000000000000213\n",
      "Operator 59: 2.0000000000000195\n",
      "Operator 60: -2.0000000000000733\n",
      "Operator 61: 1.9999999999999916\n",
      "Operator 62: -1.9999999999999973\n",
      "Operator 63: 2.000000000000008\n",
      "Operator 64: -1.9999999999999973\n",
      "Operator 65: 2.000000000000015\n",
      "Operator 66: -2.0000000000000084\n",
      "Operator 67: 2.0000000000000138\n",
      "Operator 68: -1.8939306543316796\n",
      "Operator 69: 0.6085876388671221\n",
      "Operator 70: 0.709031639566726\n",
      "Operator 72: -1.893930655364628\n",
      "Operator 74: -2.000000000000006\n",
      "Operator 76: -2.0000000000000018\n",
      "Operator 78: -1.9999999999999731\n",
      "Operator 80: -1.9999999999999738\n",
      "Operator 82: -2.0000000000000115\n",
      "Operator 84: -1.9999999999999862\n",
      "Operator 85: 1.8939306543316645\n",
      "Operator 86: 0.7090316395666881\n",
      "Operator 87: 2.089491618572443\n",
      "Operator 88: -1.7934866626868091\n",
      "Operator 89: 1.9999999999999838\n",
      "Operator 90: -2.0000000000000178\n",
      "Operator 91: 2.0000000000000178\n",
      "Operator 92: -2.0000000000000147\n",
      "Operator 93: 1.9999999999999838\n",
      "Operator 94: -1.9999999999999907\n",
      "Operator 95: 2.0000000000000515\n",
      "Operator 96: -2.00000000000001\n",
      "Operator 97: 2.000000000000031\n",
      "Operator 98: -2.0000000000000133\n",
      "Operator 99: 2.000000000000011\n",
      "Operator 100: -2.00000000000001\n",
      "Operator 101: 1.9999999999999822\n",
      "Operator 102: 2.0000000000000213\n",
      "Operator 103: -1.7934866626868156\n",
      "Operator 104: 2.0894916185724473\n",
      "Operator 105: 0.7090316395666882\n",
      "Operator 106: 1.8939306543316565\n",
      "Operator 107: -2.0000000000000115\n",
      "Operator 108: 2.0\n",
      "Operator 109: -2.000000000000018\n",
      "Operator 110: 2.0000000000000213\n",
      "Operator 111: -2.0000000000000147\n",
      "Operator 112: 2.0000000000000724\n",
      "Operator 113: -1.9999999999999907\n",
      "Operator 114: 1.9999999999999964\n",
      "Operator 115: -2.0000000000000098\n",
      "Operator 116: 1.9999999999999973\n",
      "Operator 117: -2.0000000000000133\n",
      "Operator 118: 2.0000000000000084\n",
      "Operator 119: -2.00000000000001\n",
      "Operator 120: 3.100273007206056e-07\n",
      "Operator 121: -1.8939306543316645\n",
      "Operator 123: -2.089491622819905\n",
      "Operator 125: -1.7934866626867836\n",
      "Operator 127: -2.0000000000000178\n",
      "Operator 129: -1.9999999999999853\n",
      "Operator 131: -2.0000000000000515\n",
      "Operator 133: -2.000000000000031\n",
      "Operator 135: -2.000000000000011\n",
      "Operator 137: -1.9999999999999822\n",
      "Operator 138: 1.793486662686808\n",
      "Operator 141: -0.6085876388671079\n",
      "Operator 142: 1.8939306543316952\n",
      "Operator 144: 2.000000000000007\n",
      "Operator 146: 2.000000000000009\n",
      "Operator 148: 2.00000000000002\n",
      "Operator 150: 1.9999999999999745\n",
      "Operator 152: 2.0000000000000115\n",
      "Operator 154: 2.0000000000000058\n",
      "Operator 155: -5.284528441507064e-07\n",
      "Operator 156: -2.0000000000000178\n",
      "Operator 157: 2.3419985654466942e-07\n",
      "Operator 158: -1.045383588151557e-07\n",
      "Operator 159: -7.05883273899539e-07\n",
      "Operator 160: 5.6290046052504176e-08\n",
      "Operator 161: 3.5036815374880566e-07\n",
      "Operator 162: -5.897424290230726e-08\n",
      "Operator 163: 1.496502234974393e-07\n",
      "Operator 164: -1.697090970311879e-07\n",
      "Operator 165: -2.1054069657886535e-07\n",
      "Operator 166: 4.360377099795773e-07\n",
      "Operator 167: 7.164816493288355e-07\n",
      "Operator 168: -5.5284688660339886e-08\n",
      "Operator 169: -1.0970691662337578e-06\n",
      "Operator 170: -3.053391658587043e-08\n",
      "Operator 171: 5.160731732133244e-07\n",
      "Operator 172: -2.794933865457594e-07\n",
      "Operator 173: -2.737102273719927e-07\n",
      "Operator 174: 2.1415711799769393e-07\n",
      "Operator 175: 3.4191274872504574e-07\n",
      "Operator 176: 1.8939306543316827\n",
      "Operator 178: -0.7090316520838083\n",
      "Operator 179: -0.608587642081633\n",
      "Operator 180: 1.8939306553646267\n",
      "Operator 182: 2.000000000000008\n",
      "Operator 184: 2.000000000000006\n",
      "Operator 186: 1.9999999999999731\n",
      "Operator 188: 1.9999999999999716\n",
      "Operator 190: 2.000000000000013\n",
      "Operator 192: 1.999999999999989\n",
      "Operator 193: -1.8939306553646207\n",
      "Operator 195: 0.7090316395667255\n",
      "Operator 196: 0.6085876388671085\n",
      "Operator 197: -1.8939306543316894\n",
      "Operator 199: -2.000000000000006\n",
      "Operator 201: -2.000000000000002\n",
      "Operator 203: -1.9999999999999725\n",
      "Operator 205: -1.9999999999999734\n",
      "Operator 207: -2.0000000000000115\n",
      "Operator 209: -1.9999999999999862\n",
      "Operator 210: -2.3419985654466942e-07\n",
      "Operator 211: 1.0453836049990933e-07\n",
      "Operator 212: 8.45136679194197e-07\n",
      "Operator 213: -5.6290046719793555e-08\n",
      "Operator 214: -4.896215576410645e-07\n",
      "Operator 215: 5.897424298081191e-08\n",
      "Operator 216: -1.496502260880919e-07\n",
      "Operator 217: 1.6970909710969252e-07\n",
      "Operator 218: 2.1054069540129595e-07\n",
      "Operator 219: -4.3603771013658653e-07\n",
      "Operator 220: -7.164816493288355e-07\n",
      "Operator 221: 5.528468871585104e-08\n",
      "Operator 222: 1.0970691662337578e-06\n",
      "Operator 223: 3.053391611484269e-08\n",
      "Operator 224: -5.160731732133244e-07\n",
      "Operator 225: 2.794933866567817e-07\n",
      "Operator 226: 2.7371022470745743e-07\n",
      "Operator 227: -1.8939306543316832\n",
      "Operator 229: 2.1571787988752297e-08\n",
      "Operator 230: 1.893930655364634\n",
      "Operator 231: -1.9999999999999951\n",
      "Operator 232: 2.000000000000008\n",
      "Operator 233: -2.000000000000014\n",
      "Operator 234: 2.000000000000009\n",
      "Operator 235: -2.000000000000007\n",
      "Operator 236: 2.00000000000002\n",
      "Operator 237: -1.9999999999999813\n",
      "Operator 238: 1.9999999999999747\n",
      "Operator 239: -2.0000000000000187\n",
      "Operator 240: 2.0000000000000115\n",
      "Operator 241: -2.0000000000000133\n",
      "Operator 242: 2.0000000000000058\n",
      "Operator 243: -1.9999999999999893\n",
      "Operator 244: 1.8939306553646214\n",
      "Operator 245: 2.1571799235629903e-08\n",
      "Operator 247: -1.8939306543316938\n",
      "Operator 248: 1.9999999999999964\n",
      "Operator 249: -2.000000000000009\n",
      "Operator 250: 2.0000000000000124\n",
      "Operator 251: -2.0000000000000138\n",
      "Operator 252: 2.000000000000003\n",
      "Operator 253: -2.0000000000000213\n",
      "Operator 254: 1.9999999999999805\n",
      "Operator 255: -1.999999999999972\n",
      "Operator 256: 2.000000000000021\n",
      "Operator 257: -2.0000000000000133\n",
      "Operator 258: 2.0000000000000124\n",
      "Operator 259: -2.000000000000009\n",
      "Operator 260: 1.9999999999999862\n",
      "Operator 261: 1.9999999999999956\n",
      "Operator 262: -2.000000000000014\n",
      "Operator 263: -1.893930655364624\n",
      "Operator 264: 0.6085876420816281\n",
      "Operator 265: 2.157178803660766e-08\n",
      "Operator 267: -1.7934866626867945\n",
      "Operator 269: -2.0000000000000133\n",
      "Operator 271: -2.000000000000007\n",
      "Operator 273: -1.9999999999999813\n",
      "Operator 275: -2.000000000000019\n",
      "Operator 277: -2.0000000000000133\n",
      "Operator 279: -1.9999999999999893\n",
      "Operator 280: -1.7934866626868102\n",
      "Operator 282: 2.1571799202781062e-08\n",
      "Operator 283: 0.6085876420815887\n",
      "Operator 284: -1.8939306553646325\n",
      "Operator 286: -2.000000000000009\n",
      "Operator 288: -2.0000000000000138\n",
      "Operator 290: -2.0000000000000204\n",
      "Operator 292: -1.999999999999972\n",
      "Operator 294: -2.0000000000000133\n",
      "Operator 296: -2.0000000000000084\n",
      "Total gradient norm: 24.77639009048176\n",
      "Operators under consideration (1):\n",
      "[104]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.0894916185724473)]\n",
      "Operator(s) added to ansatz: [104]\n",
      "Gradients: [np.float64(2.0894916185724473)]\n",
      "Initial energy: -30.32727615482132\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104]...\n",
      "Starting point: [np.float64(0.785398155763969), np.float64(0.7853981740437577), np.float64(0.7853981495762763), np.float64(0.7853982724068751), np.float64(0.785398120970174), np.float64(0.7853981486538875), np.float64(-0.16356985846427802), np.float64(0.7853981237024318), np.float64(-0.7853980935241017), np.float64(0.7853982453898488), np.float64(-0.7853981361635562), np.float64(-0.16356985766064863), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.464102\n",
      "         Iterations: 12\n",
      "         Function evaluations: 77\n",
      "         Gradient evaluations: 65\n",
      "\n",
      "Current energy: -30.46410161513806\n",
      "(change of -0.13682546031673937)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -3.2660286471219815e-08\n",
      "Operator 1: 2.698632011530405e-08\n",
      "Operator 2: 2.0042682356977627e-07\n",
      "Operator 3: -2.3176561684854654e-07\n",
      "Operator 4: 2.683569260527788e-07\n",
      "Operator 5: 6.943393615845839e-08\n",
      "Operator 6: -5.123762429083913e-07\n",
      "Operator 7: 1.2661426382013687e-07\n",
      "Operator 8: 2.9707446941636594e-07\n",
      "Operator 9: -3.1507131631137395e-08\n",
      "Operator 10: -1.120228400597335e-07\n",
      "Operator 12: -2.9266241048736872e-08\n",
      "Operator 13: -1.18935844133228e-07\n",
      "Operator 14: 2.899135425060706e-07\n",
      "Operator 15: 2.7352519582635182e-08\n",
      "Operator 16: -3.4082769317933526e-07\n",
      "Operator 17: 2.938652343713632e-07\n",
      "Operator 18: -1.4776970783887827e-07\n",
      "Operator 19: 5.211659186959919e-08\n",
      "Operator 20: 2.6169261173079365e-08\n",
      "Operator 21: -1.8097051614650094e-07\n",
      "Operator 22: 2.317656220970394e-07\n",
      "Operator 23: -2.683569260527788e-07\n",
      "Operator 24: -6.94339383367444e-08\n",
      "Operator 25: 5.123762429083913e-07\n",
      "Operator 26: -1.2661427049943527e-07\n",
      "Operator 27: -2.9707446941636594e-07\n",
      "Operator 28: 3.1507138464573275e-08\n",
      "Operator 29: 1.120228400597335e-07\n",
      "Operator 31: 2.926624725060206e-08\n",
      "Operator 32: 1.1893584664537592e-07\n",
      "Operator 33: -2.899135425060706e-07\n",
      "Operator 34: -1.8213671276121284\n",
      "Operator 35: -1.5217081205571587e-07\n",
      "Operator 36: 1.8971854906943331e-07\n",
      "Operator 37: 1.821367129738257\n",
      "Operator 38: -1.9999999999999953\n",
      "Operator 39: 2.000000000000024\n",
      "Operator 40: -2.0000000000000453\n",
      "Operator 41: 2.000000000000028\n",
      "Operator 42: -2.000000000000044\n",
      "Operator 43: 2.000000000000046\n",
      "Operator 44: -2.000000000000044\n",
      "Operator 45: 2.000000000000038\n",
      "Operator 46: -2.0000000000000333\n",
      "Operator 47: 2.000000000000034\n",
      "Operator 48: -2.0000000000000204\n",
      "Operator 49: 2.0000000000000213\n",
      "Operator 50: -2.0000000000000377\n",
      "Operator 51: 1.8213671297382643\n",
      "Operator 52: 1.8971855631641073e-07\n",
      "Operator 53: -1.5217081187857597e-07\n",
      "Operator 54: -1.8213671276121188\n",
      "Operator 55: 2.0000000000000266\n",
      "Operator 56: -2.000000000000038\n",
      "Operator 57: 2.000000000000024\n",
      "Operator 58: -2.000000000000035\n",
      "Operator 59: 2.000000000000028\n",
      "Operator 60: -2.000000000000044\n",
      "Operator 61: 2.000000000000047\n",
      "Operator 62: -2.000000000000031\n",
      "Operator 63: 2.000000000000038\n",
      "Operator 64: -2.0000000000000244\n",
      "Operator 65: 2.000000000000034\n",
      "Operator 66: -2.0000000000000338\n",
      "Operator 67: 2.0000000000000213\n",
      "Operator 68: -1.8213671893845542\n",
      "Operator 69: 0.48803407889377515\n",
      "Operator 70: 9.507769653901924e-07\n",
      "Operator 72: -1.8213671276121306\n",
      "Operator 74: -2.0000000000000213\n",
      "Operator 76: -2.0000000000000195\n",
      "Operator 78: -2.000000000000046\n",
      "Operator 80: -2.00000000000004\n",
      "Operator 82: -2.0000000000000284\n",
      "Operator 84: -2.0000000000000187\n",
      "Operator 85: 1.8213671893845442\n",
      "Operator 86: 9.507769620322016e-07\n",
      "Operator 87: 1.0259160607189274e-07\n",
      "Operator 88: -1.8213671297382585\n",
      "Operator 89: 1.9999999999999953\n",
      "Operator 90: -2.000000000000022\n",
      "Operator 91: 2.0000000000000453\n",
      "Operator 92: -2.000000000000024\n",
      "Operator 93: 2.000000000000044\n",
      "Operator 94: -2.000000000000046\n",
      "Operator 95: 2.000000000000043\n",
      "Operator 96: -2.000000000000041\n",
      "Operator 97: 2.000000000000032\n",
      "Operator 98: -2.000000000000032\n",
      "Operator 99: 2.0000000000000204\n",
      "Operator 100: -2.000000000000018\n",
      "Operator 101: 2.0000000000000377\n",
      "Operator 102: 2.0000000000000338\n",
      "Operator 103: -1.8213671297382605\n",
      "Operator 104: 1.0259160312056393e-07\n",
      "Operator 105: 9.507769623552818e-07\n",
      "Operator 106: 1.8213671893845338\n",
      "Operator 107: -2.0000000000000284\n",
      "Operator 108: 2.000000000000038\n",
      "Operator 109: -2.000000000000023\n",
      "Operator 110: 2.000000000000035\n",
      "Operator 111: -2.000000000000024\n",
      "Operator 112: 2.000000000000044\n",
      "Operator 113: -2.000000000000046\n",
      "Operator 114: 2.0000000000000333\n",
      "Operator 115: -2.000000000000041\n",
      "Operator 116: 2.0000000000000266\n",
      "Operator 117: -2.0000000000000324\n",
      "Operator 118: 2.0000000000000338\n",
      "Operator 119: -2.000000000000018\n",
      "Operator 120: -2.7352524134549583e-08\n",
      "Operator 121: -1.8213671893845451\n",
      "Operator 123: 1.8971854922003683e-07\n",
      "Operator 124: 0.48803385629080065\n",
      "Operator 125: -1.821367129738229\n",
      "Operator 127: -2.000000000000045\n",
      "Operator 129: -2.000000000000044\n",
      "Operator 131: -2.000000000000043\n",
      "Operator 133: -2.0000000000000338\n",
      "Operator 135: -2.000000000000019\n",
      "Operator 137: -2.0000000000000377\n",
      "Operator 138: 1.8213671297382548\n",
      "Operator 140: -9.608378522637555e-07\n",
      "Operator 141: -0.48803407889376915\n",
      "Operator 142: 1.8213671893845476\n",
      "Operator 144: 2.000000000000029\n",
      "Operator 146: 2.0000000000000204\n",
      "Operator 148: 2.0000000000000453\n",
      "Operator 150: 2.0000000000000404\n",
      "Operator 152: 2.000000000000031\n",
      "Operator 154: 2.000000000000022\n",
      "Operator 155: 3.4082769317933526e-07\n",
      "Operator 156: -2.000000000000031\n",
      "Operator 157: -2.5648480339099267e-07\n",
      "Operator 158: -7.502287822990113e-08\n",
      "Operator 159: -3.266028712128184e-08\n",
      "Operator 161: 1.8097051591098709e-07\n",
      "Operator 162: 1.771061079584239e-07\n",
      "Operator 163: 2.683569260527788e-07\n",
      "Operator 164: -5.049184083032996e-08\n",
      "Operator 165: -5.123762429083913e-07\n",
      "Operator 166: 1.8984703099728986e-08\n",
      "Operator 167: 2.9707446918085203e-07\n",
      "Operator 168: -2.3561630341228954e-08\n",
      "Operator 169: -1.1202284125322324e-07\n",
      "Operator 170: -9.53742125566877e-08\n",
      "Operator 171: -2.926624725060206e-08\n",
      "Operator 172: 1.227267351078254e-07\n",
      "Operator 173: 2.899135425060706e-07\n",
      "Operator 174: -2.9386523497316965e-07\n",
      "Operator 175: 1.4776969187417945e-07\n",
      "Operator 176: 1.8213671893845573\n",
      "Operator 178: -1.5217081286496225e-07\n",
      "Operator 179: -0.48803384835603053\n",
      "Operator 180: 1.8213671276121295\n",
      "Operator 182: 2.000000000000023\n",
      "Operator 184: 2.0000000000000244\n",
      "Operator 186: 2.000000000000046\n",
      "Operator 188: 2.000000000000038\n",
      "Operator 190: 2.000000000000031\n",
      "Operator 192: 2.0000000000000213\n",
      "Operator 193: -1.8213671276121401\n",
      "Operator 195: 9.50776965695707e-07\n",
      "Operator 196: 0.48803407889377065\n",
      "Operator 197: -1.8213671893845451\n",
      "Operator 199: -2.0000000000000213\n",
      "Operator 201: -2.0000000000000195\n",
      "Operator 203: -2.000000000000046\n",
      "Operator 205: -2.00000000000004\n",
      "Operator 207: -2.0000000000000284\n",
      "Operator 209: -2.000000000000018\n",
      "Operator 210: 2.564848034807973e-07\n",
      "Operator 211: 7.502288308943411e-08\n",
      "Operator 212: 5.2116590469538657e-08\n",
      "Operator 214: -2.0042682356977627e-07\n",
      "Operator 215: -1.7710610795842402e-07\n",
      "Operator 216: -2.683569260527788e-07\n",
      "Operator 217: 5.0491841144348455e-08\n",
      "Operator 218: 5.123762429083913e-07\n",
      "Operator 219: -1.8984703335242855e-08\n",
      "Operator 220: -2.9707446918085203e-07\n",
      "Operator 221: 2.3561629952650895e-08\n",
      "Operator 222: 1.1202284003197793e-07\n",
      "Operator 223: 9.537421239967844e-08\n",
      "Operator 224: 2.9266241048736872e-08\n",
      "Operator 225: -1.227267351078254e-07\n",
      "Operator 226: -2.899135425060706e-07\n",
      "Operator 227: -1.821367189384562\n",
      "Operator 228: -9.608378521518794e-07\n",
      "Operator 229: -1.300784543025562e-07\n",
      "Operator 230: 1.8213671276121333\n",
      "Operator 231: -2.0000000000000178\n",
      "Operator 232: 2.000000000000029\n",
      "Operator 233: -2.0000000000000266\n",
      "Operator 234: 2.0000000000000204\n",
      "Operator 235: -2.000000000000032\n",
      "Operator 236: 2.0000000000000457\n",
      "Operator 237: -2.000000000000047\n",
      "Operator 238: 2.0000000000000404\n",
      "Operator 239: -2.0000000000000386\n",
      "Operator 240: 2.000000000000031\n",
      "Operator 241: -2.000000000000031\n",
      "Operator 242: 2.000000000000022\n",
      "Operator 243: -2.000000000000024\n",
      "Operator 244: 1.8213671276121448\n",
      "Operator 245: -1.300784532941106e-07\n",
      "Operator 246: -9.60837851892242e-07\n",
      "Operator 247: -1.8213671893845458\n",
      "Operator 248: 2.000000000000018\n",
      "Operator 249: -2.00000000000003\n",
      "Operator 250: 2.000000000000024\n",
      "Operator 251: -2.000000000000025\n",
      "Operator 252: 2.000000000000027\n",
      "Operator 253: -2.000000000000046\n",
      "Operator 254: 2.000000000000046\n",
      "Operator 255: -2.000000000000038\n",
      "Operator 256: 2.0000000000000404\n",
      "Operator 257: -2.000000000000033\n",
      "Operator 258: 2.000000000000029\n",
      "Operator 259: -2.000000000000025\n",
      "Operator 260: 2.0000000000000204\n",
      "Operator 261: 2.000000000000023\n",
      "Operator 262: -2.0000000000000284\n",
      "Operator 263: -1.821367127612148\n",
      "Operator 264: 0.4880338483560287\n",
      "Operator 265: -1.300784544590775e-07\n",
      "Operator 267: -1.821367129738249\n",
      "Operator 269: -2.0000000000000266\n",
      "Operator 271: -2.000000000000032\n",
      "Operator 273: -2.000000000000047\n",
      "Operator 275: -2.0000000000000386\n",
      "Operator 277: -2.0000000000000306\n",
      "Operator 279: -2.0000000000000244\n",
      "Operator 280: -1.8213671297382585\n",
      "Operator 282: -1.3007845350406037e-07\n",
      "Operator 283: 0.488033848356025\n",
      "Operator 284: -1.8213671276121315\n",
      "Operator 286: -2.00000000000003\n",
      "Operator 288: -2.000000000000025\n",
      "Operator 290: -2.0000000000000466\n",
      "Operator 292: -2.000000000000038\n",
      "Operator 294: -2.000000000000033\n",
      "Operator 296: -2.000000000000025\n",
      "Total gradient norm: 24.164417551063824\n",
      "Operators under consideration (1):\n",
      "[290]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.0000000000000466)]\n",
      "Operator(s) added to ansatz: [290]\n",
      "Gradients: [np.float64(-2.0000000000000466)]\n",
      "Initial energy: -30.46410161513806\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290]...\n",
      "Starting point: [np.float64(0.7853981395538948), np.float64(0.7853982002003667), np.float64(0.7853981575070407), np.float64(0.785398168143625), np.float64(0.7853981507744877), np.float64(0.7853982076739756), np.float64(-0.13089969308561422), np.float64(0.7853981364794045), np.float64(-0.7853981940791326), np.float64(0.7853981749059774), np.float64(-0.7853981323740864), np.float64(-0.1699185138171217), np.float64(-0.13089969526386), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.587207\n",
      "         Iterations: 7\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 67\n",
      "\n",
      "Current energy: -30.58720724075527\n",
      "(change of -0.12310562561721028)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.1693208426857226e-08\n",
      "Operator 1: 2.0685565557182906e-08\n",
      "Operator 2: -1.0882634310247575e-08\n",
      "Operator 3: -2.8215986902110783e-08\n",
      "Operator 4: 6.354661927543373e-08\n",
      "Operator 5: -3.0647475313191384e-08\n",
      "Operator 6: -1.4195013097593478e-08\n",
      "Operator 7: 1.627274790583758e-08\n",
      "Operator 8: -1.2226351347206664e-08\n",
      "Operator 9: 2.3078483032851324e-08\n",
      "Operator 10: -3.572168749688133e-08\n",
      "Operator 11: 2.460425119199528e-08\n",
      "Operator 13: -3.841431229949092e-08\n",
      "Operator 14: 8.434215137409495e-08\n",
      "Operator 15: -2.6098958549880535e-08\n",
      "Operator 16: -5.944366066046314e-08\n",
      "Operator 18: -2.682822969627771e-08\n",
      "Operator 19: 2.8523192860128717e-08\n",
      "Operator 21: 1.7712619846109015e-08\n",
      "Operator 22: 2.821599216081938e-08\n",
      "Operator 23: -6.354661927543373e-08\n",
      "Operator 24: 3.064747331490793e-08\n",
      "Operator 25: 1.4195013097593478e-08\n",
      "Operator 26: -1.6272754781218925e-08\n",
      "Operator 28: -4.593354154914675e-08\n",
      "Operator 29: 3.5183645996639263e-08\n",
      "Operator 30: -2.4604240922432297e-08\n",
      "Operator 32: 3.8414314654629615e-08\n",
      "Operator 33: -8.434215137409495e-08\n",
      "Operator 34: -1.8213671996345995\n",
      "Operator 35: -7.223323274211171e-08\n",
      "Operator 36: 7.823893268380068e-08\n",
      "Operator 37: 1.8213671999746612\n",
      "Operator 38: -2.0000000000000124\n",
      "Operator 39: 2.0000000000000235\n",
      "Operator 40: -2.00000000000002\n",
      "Operator 41: 2.000000000000022\n",
      "Operator 42: -2.000000000000024\n",
      "Operator 43: 3.3945403376423326e-06\n",
      "Operator 44: -2.485070451357292\n",
      "Operator 45: 1.9402851999693689\n",
      "Operator 46: -2.000000000000023\n",
      "Operator 47: 2.000000000000022\n",
      "Operator 48: -2.000000000000015\n",
      "Operator 49: 2.000000000000017\n",
      "Operator 50: -2.0000000000000164\n",
      "Operator 51: 1.8213671999746732\n",
      "Operator 52: 7.823892772168613e-08\n",
      "Operator 53: -7.223323295945768e-08\n",
      "Operator 54: -1.8213671996345937\n",
      "Operator 55: 2.0000000000000107\n",
      "Operator 56: -2.000000000000016\n",
      "Operator 57: 2.0000000000000235\n",
      "Operator 58: -2.0000000000000235\n",
      "Operator 59: 1.940285199969364\n",
      "Operator 60: -2.4850704513572897\n",
      "Operator 61: 3.3945403373273447e-06\n",
      "Operator 62: -2.0000000000000253\n",
      "Operator 63: 2.000000000000028\n",
      "Operator 64: -2.000000000000015\n",
      "Operator 65: 2.000000000000022\n",
      "Operator 66: -2.000000000000013\n",
      "Operator 67: 2.0000000000000164\n",
      "Operator 68: -1.8213671905510809\n",
      "Operator 69: 0.48803385701525104\n",
      "Operator 70: -4.5200366468993407e-08\n",
      "Operator 72: -1.8213671996345928\n",
      "Operator 74: -2.0000000000000213\n",
      "Operator 76: -1.9402851999693596\n",
      "Operator 78: -1.940285199969376\n",
      "Operator 80: -2.0000000000000293\n",
      "Operator 82: -2.00000000000002\n",
      "Operator 84: -2.0000000000000133\n",
      "Operator 85: 1.8213671905510784\n",
      "Operator 86: -4.5200364477811866e-08\n",
      "Operator 87: -1.2122260994801347e-07\n",
      "Operator 88: -1.8213671999746621\n",
      "Operator 89: 2.0000000000000124\n",
      "Operator 90: -2.0000000000000213\n",
      "Operator 91: 2.00000000000002\n",
      "Operator 92: -2.0000000000000178\n",
      "Operator 93: 1.940285199969367\n",
      "Operator 94: -1.9402851999693762\n",
      "Operator 95: 1.9402851999693709\n",
      "Operator 96: -1.9402851999693718\n",
      "Operator 97: 2.000000000000023\n",
      "Operator 98: -2.000000000000021\n",
      "Operator 99: 2.0000000000000147\n",
      "Operator 100: -2.0000000000000138\n",
      "Operator 101: 2.0000000000000164\n",
      "Operator 102: 2.000000000000023\n",
      "Operator 103: -1.8213671999746701\n",
      "Operator 104: -1.2122260721732314e-07\n",
      "Operator 105: -4.52003647725672e-08\n",
      "Operator 106: 1.821367190551074\n",
      "Operator 107: -2.000000000000011\n",
      "Operator 108: 2.000000000000017\n",
      "Operator 109: -2.0000000000000218\n",
      "Operator 110: 2.0000000000000235\n",
      "Operator 111: -1.94028519996936\n",
      "Operator 112: 1.9402851999693689\n",
      "Operator 113: -1.9402851999693764\n",
      "Operator 114: 1.9402851999693604\n",
      "Operator 115: -2.0000000000000298\n",
      "Operator 116: 2.000000000000016\n",
      "Operator 117: -2.000000000000021\n",
      "Operator 118: 2.000000000000013\n",
      "Operator 119: -2.0000000000000138\n",
      "Operator 120: 2.609895333183232e-08\n",
      "Operator 121: -1.8213671905510784\n",
      "Operator 123: 7.8238933423016e-08\n",
      "Operator 124: 0.4880338921845623\n",
      "Operator 125: -1.8213671999746635\n",
      "Operator 127: -2.00000000000002\n",
      "Operator 129: -1.940285199969367\n",
      "Operator 131: -2.485070451357295\n",
      "Operator 133: -1.9402851999693627\n",
      "Operator 135: -2.000000000000015\n",
      "Operator 137: -2.0000000000000164\n",
      "Operator 138: 1.8213671999746701\n",
      "Operator 140: 4.3591142825080357e-08\n",
      "Operator 141: -0.48803385701524854\n",
      "Operator 142: 1.8213671905510724\n",
      "Operator 144: 2.0000000000000213\n",
      "Operator 146: 1.9402851999693598\n",
      "Operator 148: 2.485070451357296\n",
      "Operator 150: 1.9402851999693715\n",
      "Operator 152: 2.000000000000021\n",
      "Operator 154: 2.0000000000000133\n",
      "Operator 155: 5.944366066046314e-08\n",
      "Operator 156: -2.00000000000002\n",
      "Operator 159: -2.1693208091694438e-08\n",
      "Operator 161: -1.7712620003118265e-08\n",
      "Operator 163: 6.354661927543373e-08\n",
      "Operator 164: 1.6317685440383685e-08\n",
      "Operator 165: -1.2781784812105423e-08\n",
      "Operator 166: 2.5804705130582047e-08\n",
      "Operator 167: -1.2226351818234405e-08\n",
      "Operator 169: -3.5183646079906e-08\n",
      "Operator 170: -3.721385257752294e-08\n",
      "Operator 172: 1.1114897557895631e-08\n",
      "Operator 173: 8.434215137409495e-08\n",
      "Operator 175: 2.6828213369282945e-08\n",
      "Operator 176: 1.8213671905510838\n",
      "Operator 178: -7.223322893074581e-08\n",
      "Operator 179: -0.4880338909154074\n",
      "Operator 180: 1.8213671996345921\n",
      "Operator 182: 2.000000000000022\n",
      "Operator 184: 1.9402851999693644\n",
      "Operator 186: 3.394540336624531e-06\n",
      "Operator 187: -0.48507045135726035\n",
      "Operator 188: 2.0000000000000275\n",
      "Operator 190: 2.0000000000000218\n",
      "Operator 192: 2.000000000000017\n",
      "Operator 193: -1.821367199634601\n",
      "Operator 195: -4.5200366219153495e-08\n",
      "Operator 196: 0.4880338570152478\n",
      "Operator 197: -1.8213671905510722\n",
      "Operator 199: -2.0000000000000213\n",
      "Operator 201: -2.0000000000000178\n",
      "Operator 203: -1.940285199969376\n",
      "Operator 205: -1.9402851999693715\n",
      "Operator 207: -2.00000000000002\n",
      "Operator 209: -2.0000000000000138\n",
      "Operator 212: 2.8523192094968505e-08\n",
      "Operator 214: 1.0882634310247575e-08\n",
      "Operator 216: -6.354661927543373e-08\n",
      "Operator 217: -1.631768551031224e-08\n",
      "Operator 218: 1.2781784812105423e-08\n",
      "Operator 219: -2.5804705130582226e-08\n",
      "Operator 222: 3.572168763565921e-08\n",
      "Operator 223: 3.721385257752294e-08\n",
      "Operator 225: -1.1114897557895631e-08\n",
      "Operator 226: -8.434215137409495e-08\n",
      "Operator 227: -1.8213671905510838\n",
      "Operator 228: 4.359114305616157e-08\n",
      "Operator 229: 1.1682613517803982e-07\n",
      "Operator 230: 1.8213671996345935\n",
      "Operator 231: -2.0000000000000107\n",
      "Operator 232: 2.0000000000000218\n",
      "Operator 233: -2.000000000000023\n",
      "Operator 234: 2.0000000000000178\n",
      "Operator 235: -1.9402851999693635\n",
      "Operator 236: 2.485070451357295\n",
      "Operator 237: -3.394540336905829e-06\n",
      "Operator 238: 2.0000000000000293\n",
      "Operator 239: -2.000000000000028\n",
      "Operator 240: 2.000000000000021\n",
      "Operator 241: -2.0000000000000213\n",
      "Operator 242: 2.0000000000000138\n",
      "Operator 243: -2.000000000000017\n",
      "Operator 244: 1.8213671996346008\n",
      "Operator 245: 1.168261345661759e-07\n",
      "Operator 246: 4.359114302513513e-08\n",
      "Operator 247: -1.8213671905510709\n",
      "Operator 248: 2.0000000000000107\n",
      "Operator 249: -2.000000000000023\n",
      "Operator 250: 2.0000000000000218\n",
      "Operator 251: -2.000000000000022\n",
      "Operator 252: 2.0000000000000178\n",
      "Operator 253: -3.3945403369940246e-06\n",
      "Operator 254: 2.485070451357294\n",
      "Operator 255: -1.9402851999693689\n",
      "Operator 256: 2.00000000000003\n",
      "Operator 257: -2.0000000000000226\n",
      "Operator 258: 2.0000000000000204\n",
      "Operator 259: -2.000000000000017\n",
      "Operator 260: 2.000000000000014\n",
      "Operator 261: 2.000000000000016\n",
      "Operator 262: -2.000000000000023\n",
      "Operator 263: -1.8213671996346035\n",
      "Operator 264: 0.4880338909154088\n",
      "Operator 265: 1.1682613511559384e-07\n",
      "Operator 267: -1.8213671999746612\n",
      "Operator 269: -2.000000000000023\n",
      "Operator 271: -2.000000000000022\n",
      "Operator 272: 0.4850704513572589\n",
      "Operator 273: -3.3945403365926576e-06\n",
      "Operator 275: -1.9402851999693689\n",
      "Operator 277: -2.0000000000000218\n",
      "Operator 279: -2.000000000000017\n",
      "Operator 280: -1.8213671999746728\n",
      "Operator 282: 1.1682613478632612e-07\n",
      "Operator 283: 0.48803389091540766\n",
      "Operator 284: -1.8213671996345921\n",
      "Operator 286: -2.000000000000023\n",
      "Operator 288: -1.9402851999693644\n",
      "Operator 290: -3.39454033730763e-06\n",
      "Operator 291: 0.48507045135725746\n",
      "Operator 292: -2.0000000000000275\n",
      "Operator 294: -2.0000000000000226\n",
      "Operator 296: -2.000000000000017\n",
      "Total gradient norm: 23.754297791602422\n",
      "Operators under consideration (1):\n",
      "[254]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.485070451357294)]\n",
      "Operator(s) added to ansatz: [254]\n",
      "Gradients: [np.float64(2.485070451357294)]\n",
      "Initial energy: -30.58720724075527\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254]...\n",
      "Starting point: [np.float64(0.7853981540939855), np.float64(0.7853981662264785), np.float64(0.7853981630973339), np.float64(0.7853981698486254), np.float64(0.7853981676024194), np.float64(0.7853981633862147), np.float64(-0.1308996991660288), np.float64(0.7853981557468128), np.float64(-0.7853981661761729), np.float64(0.7853981674374296), np.float64(-0.7853981639940874), np.float64(-0.1699184546733698), np.float64(-0.13089969951443567), np.float64(0.12248912573919207), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.791378\n",
      "         Iterations: 8\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 29\n",
      "\n",
      "Current energy: -30.791377769959126\n",
      "(change of -0.20417052920385714)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 7.236460874001148e-08\n",
      "Operator 1: -7.360482307416768e-08\n",
      "Operator 2: -8.160713107642389e-08\n",
      "Operator 3: 2.2930574391884678e-07\n",
      "Operator 4: -4.36921174098822e-07\n",
      "Operator 5: -2.3475857761516397e-08\n",
      "Operator 6: 5.93416625631414e-07\n",
      "Operator 7: -1.1976189110442962e-07\n",
      "Operator 8: -3.1202388199959656e-07\n",
      "Operator 10: -2.3460217102311318e-07\n",
      "Operator 11: -2.3621998976319823e-07\n",
      "Operator 12: 6.661542516293404e-07\n",
      "Operator 13: -1.2720572498962127e-07\n",
      "Operator 14: -3.455354168346858e-07\n",
      "Operator 15: 3.0242735815644295e-07\n",
      "Operator 16: -2.1428693131042564e-07\n",
      "Operator 17: -2.1424028788950892e-07\n",
      "Operator 18: 1.7202093440563974e-07\n",
      "Operator 19: -1.0306023224532879e-07\n",
      "Operator 20: -1.0257186631024701e-08\n",
      "Operator 21: 5.09115119717426e-08\n",
      "Operator 22: -2.2930573869722898e-07\n",
      "Operator 23: 4.36921174098822e-07\n",
      "Operator 24: 2.347585533779481e-08\n",
      "Operator 25: -5.93416625631414e-07\n",
      "Operator 26: 1.1976188466278027e-07\n",
      "Operator 27: 2.736537096019526e-07\n",
      "Operator 28: -1.1461562632400203e-07\n",
      "Operator 29: 1.9623200916973927e-07\n",
      "Operator 30: 2.3621999870049357e-07\n",
      "Operator 31: -6.661542525713959e-07\n",
      "Operator 32: 1.2720572742326456e-07\n",
      "Operator 33: 3.455354169457081e-07\n",
      "Operator 34: -1.8213672501598164\n",
      "Operator 35: 2.1980599377822095e-07\n",
      "Operator 36: -2.347125343381042e-07\n",
      "Operator 37: 1.8213672493157578\n",
      "Operator 38: -1.99999999999998\n",
      "Operator 39: 2.000000000000001\n",
      "Operator 40: -2.000000000000041\n",
      "Operator 41: 2.0000000000000093\n",
      "Operator 42: -1.893930610024042\n",
      "Operator 43: -0.7090307922868004\n",
      "Operator 44: -2.089491413974317\n",
      "Operator 45: 1.7934866863461165\n",
      "Operator 46: -2.0000000000000058\n",
      "Operator 47: 1.9999999999999982\n",
      "Operator 48: -2.000000000000013\n",
      "Operator 49: 2.0000000000000213\n",
      "Operator 50: -1.9999999999999802\n",
      "Operator 51: 1.8213672493157684\n",
      "Operator 52: -2.3471254227578908e-07\n",
      "Operator 53: 2.1980599355211802e-07\n",
      "Operator 54: -1.8213672501598082\n",
      "Operator 55: 2.0000000000000075\n",
      "Operator 56: -2.000000000000029\n",
      "Operator 57: 2.0000000000000013\n",
      "Operator 58: -2.000000000000025\n",
      "Operator 59: 1.793486686346109\n",
      "Operator 60: -2.089491413974325\n",
      "Operator 61: -0.7090307922868007\n",
      "Operator 62: -1.8939306100240323\n",
      "Operator 63: 2.0000000000000173\n",
      "Operator 64: -2.000000000000026\n",
      "Operator 65: 1.9999999999999982\n",
      "Operator 66: -1.9999999999999605\n",
      "Operator 67: 2.000000000000021\n",
      "Operator 68: -1.8213672410963042\n",
      "Operator 69: 0.4880338084008884\n",
      "Operator 70: -3.3698112472307327e-07\n",
      "Operator 72: -1.82136725015982\n",
      "Operator 74: -1.999999999999999\n",
      "Operator 76: -1.8939307246566512\n",
      "Operator 77: 0.608587787995908\n",
      "Operator 78: 0.7090321813993884\n",
      "Operator 80: -1.893930610024025\n",
      "Operator 82: -1.9999999999999927\n",
      "Operator 84: -1.9999999999999951\n",
      "Operator 85: 1.8213672410962927\n",
      "Operator 86: -3.369811276043987e-07\n",
      "Operator 87: 1.9182346064180564e-07\n",
      "Operator 88: -1.8213672493157593\n",
      "Operator 89: 1.9999999999999807\n",
      "Operator 90: -1.9999999999999991\n",
      "Operator 91: 2.000000000000041\n",
      "Operator 92: -2.0000000000000044\n",
      "Operator 93: 1.8939307246566774\n",
      "Operator 94: 0.709032181399389\n",
      "Operator 95: 2.0894918853448368\n",
      "Operator 96: -1.7934866863461187\n",
      "Operator 97: 2.0000000000000058\n",
      "Operator 98: -1.9999999999999967\n",
      "Operator 99: 2.000000000000014\n",
      "Operator 100: -2.0000000000000178\n",
      "Operator 101: 1.9999999999999802\n",
      "Operator 102: 2.0000000000000187\n",
      "Operator 103: -1.8213672493157647\n",
      "Operator 104: 1.9182346959500998e-07\n",
      "Operator 105: -3.3698112728876185e-07\n",
      "Operator 106: 1.8213672410962847\n",
      "Operator 107: -2.000000000000008\n",
      "Operator 108: 2.000000000000029\n",
      "Operator 109: -1.999999999999999\n",
      "Operator 110: 2.000000000000025\n",
      "Operator 111: -1.7934866863461045\n",
      "Operator 112: 2.0894918853448416\n",
      "Operator 113: 0.7090321813993887\n",
      "Operator 114: 1.8939307246566588\n",
      "Operator 115: -2.0000000000000204\n",
      "Operator 116: 2.0000000000000258\n",
      "Operator 117: -1.9999999999999964\n",
      "Operator 118: 1.9999999999999591\n",
      "Operator 119: -2.0000000000000178\n",
      "Operator 120: -3.0242736326346886e-07\n",
      "Operator 121: -1.8213672410962927\n",
      "Operator 123: -2.3471253480991294e-07\n",
      "Operator 124: 0.4880338390763014\n",
      "Operator 125: -1.8213672493157338\n",
      "Operator 127: -2.0000000000000395\n",
      "Operator 129: -1.8939307246566774\n",
      "Operator 131: -2.0894914139743173\n",
      "Operator 133: -1.7934866863461063\n",
      "Operator 135: -2.000000000000013\n",
      "Operator 137: -1.9999999999999802\n",
      "Operator 138: 1.8213672493157582\n",
      "Operator 140: 3.409753213188076e-07\n",
      "Operator 141: -0.48803380840088334\n",
      "Operator 142: 1.8213672410962998\n",
      "Operator 144: 2.000000000000007\n",
      "Operator 146: 1.7934866863461028\n",
      "Operator 148: -8.264634468511608e-07\n",
      "Operator 149: -0.6085877879958914\n",
      "Operator 150: 1.8939307246566728\n",
      "Operator 152: 1.999999999999997\n",
      "Operator 154: 2.0000000000000018\n",
      "Operator 155: 2.1428693131042564e-07\n",
      "Operator 156: -2.0000000000000147\n",
      "Operator 157: 1.8511194639587182e-07\n",
      "Operator 158: 2.5700107114332106e-08\n",
      "Operator 159: 7.236460788363878e-08\n",
      "Operator 161: -5.09115119717426e-08\n",
      "Operator 162: -1.841635995443714e-07\n",
      "Operator 163: -4.36921174098822e-07\n",
      "Operator 164: 6.440171169490089e-08\n",
      "Operator 165: 5.387923195485333e-07\n",
      "Operator 167: -3.120238826276336e-07\n",
      "Operator 168: 9.125066671722431e-08\n",
      "Operator 169: -1.96232009058717e-07\n",
      "Operator 170: 1.3018926837677425e-07\n",
      "Operator 171: 6.661542525713959e-07\n",
      "Operator 172: 1.722380930102574e-07\n",
      "Operator 173: -3.455354169457081e-07\n",
      "Operator 174: 2.1424028998132622e-07\n",
      "Operator 175: -1.7202095114045118e-07\n",
      "Operator 176: 1.821367241096307\n",
      "Operator 178: 2.198059945014671e-07\n",
      "Operator 179: -0.4880338422264311\n",
      "Operator 180: 1.8213672501598186\n",
      "Operator 182: 2.0000000000000004\n",
      "Operator 184: 1.8939307246566552\n",
      "Operator 186: -0.7090307922868025\n",
      "Operator 187: -0.6085874312580257\n",
      "Operator 188: 1.8939306100240225\n",
      "Operator 190: 1.9999999999999938\n",
      "Operator 192: 1.9999999999999987\n",
      "Operator 193: -1.8213672501598313\n",
      "Operator 195: -3.36981124262217e-07\n",
      "Operator 196: 0.48803380840088445\n",
      "Operator 197: -1.8213672410962936\n",
      "Operator 199: -1.999999999999999\n",
      "Operator 201: -1.8939306100240194\n",
      "Operator 203: 0.7090321813993887\n",
      "Operator 204: 0.6085877879959098\n",
      "Operator 205: -1.8939307246566566\n",
      "Operator 207: -1.9999999999999927\n",
      "Operator 209: -1.9999999999999951\n",
      "Operator 210: -1.8511194566118075e-07\n",
      "Operator 211: -2.5700101999011044e-08\n",
      "Operator 212: -1.030602325436534e-07\n",
      "Operator 214: 8.160713107642389e-08\n",
      "Operator 215: 1.8416359938736215e-07\n",
      "Operator 216: 4.36921174098822e-07\n",
      "Operator 217: -6.440171170963868e-08\n",
      "Operator 218: -5.387923195485333e-07\n",
      "Operator 220: 2.7365370850288793e-07\n",
      "Operator 221: -9.125048784641725e-08\n",
      "Operator 222: 2.3460217102311318e-07\n",
      "Operator 223: -1.301892681412604e-07\n",
      "Operator 224: -6.661542516293404e-07\n",
      "Operator 225: -1.72238093232302e-07\n",
      "Operator 226: 3.455354168346858e-07\n",
      "Operator 227: -1.8213672410963109\n",
      "Operator 228: 3.4097532059740545e-07\n",
      "Operator 229: -1.8091111699165453e-07\n",
      "Operator 230: 1.8213672501598261\n",
      "Operator 231: -1.9999999999999956\n",
      "Operator 232: 2.000000000000007\n",
      "Operator 233: -2.0000000000000075\n",
      "Operator 234: 2.0000000000000018\n",
      "Operator 235: -1.8939307246566632\n",
      "Operator 236: -8.264634465390472e-07\n",
      "Operator 237: -1.0340196563866014e-06\n",
      "Operator 238: 1.8939306100240407\n",
      "Operator 239: -2.000000000000004\n",
      "Operator 240: 1.9999999999999973\n",
      "Operator 241: -2.0000000000000107\n",
      "Operator 242: 2.0000000000000027\n",
      "Operator 243: -2.0000000000000027\n",
      "Operator 244: 1.8213672501598344\n",
      "Operator 245: -1.8091111685214407e-07\n",
      "Operator 246: 3.4097532098988753e-07\n",
      "Operator 247: -1.8213672410962982\n",
      "Operator 248: 1.9999999999999958\n",
      "Operator 249: -2.0000000000000084\n",
      "Operator 250: 2.0000000000000053\n",
      "Operator 251: -2.000000000000006\n",
      "Operator 252: 1.8939306100240274\n",
      "Operator 253: -1.0340196588259358e-06\n",
      "Operator 254: -8.264634452228439e-07\n",
      "Operator 255: -1.89393072465667\n",
      "Operator 256: 2.000000000000006\n",
      "Operator 257: -1.9999999999999987\n",
      "Operator 258: 2.000000000000009\n",
      "Operator 259: -2.0000000000000053\n",
      "Operator 260: 1.9999999999999991\n",
      "Operator 261: 1.9999999999999982\n",
      "Operator 262: -2.000000000000011\n",
      "Operator 263: -1.8213672501598381\n",
      "Operator 264: 0.48803384222642765\n",
      "Operator 265: -1.8091111670340763e-07\n",
      "Operator 267: -1.8213672493157465\n",
      "Operator 269: -2.000000000000007\n",
      "Operator 271: -1.8939306100240316\n",
      "Operator 272: 0.6085874312580151\n",
      "Operator 273: -1.0340196569562232e-06\n",
      "Operator 275: -1.7934866863461045\n",
      "Operator 277: -2.0000000000000107\n",
      "Operator 279: -2.0000000000000027\n",
      "Operator 280: -1.8213672493157609\n",
      "Operator 282: -1.8091111695637137e-07\n",
      "Operator 283: 0.4880338422264207\n",
      "Operator 284: -1.8213672501598244\n",
      "Operator 286: -2.0000000000000084\n",
      "Operator 288: -1.7934866863461076\n",
      "Operator 290: -1.034019659221843e-06\n",
      "Operator 291: 0.608587431258026\n",
      "Operator 292: -1.893930610024038\n",
      "Operator 294: -1.9999999999999987\n",
      "Operator 296: -2.0000000000000053\n",
      "Total gradient norm: 22.842692271298308\n",
      "Operators under consideration (1):\n",
      "[112]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.0894918853448416)]\n",
      "Operator(s) added to ansatz: [112]\n",
      "Gradients: [np.float64(2.0894918853448416)]\n",
      "Initial energy: -30.791377769959126\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112]...\n",
      "Starting point: [np.float64(0.7853981959447661), np.float64(0.7853981312090176), np.float64(0.7853980990487003), np.float64(0.7853981689876733), np.float64(0.7853981803995783), np.float64(0.785398117356548), np.float64(-0.13089968330906532), np.float64(0.785398203569385), np.float64(-0.7853982064569718), np.float64(0.7853981452407875), np.float64(-0.7853981805519472), np.float64(-0.169918428379711), np.float64(-0.13089968244429634), np.float64(0.16356980375128258), np.float64(-0.16356989293575364), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.928203\n",
      "         Iterations: 13\n",
      "         Function evaluations: 84\n",
      "         Gradient evaluations: 72\n",
      "\n",
      "Current energy: -30.928203230275912\n",
      "(change of -0.13682546031678555)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -5.3280194866206756e-08\n",
      "Operator 1: 2.656367376894323e-08\n",
      "Operator 2: 1.5798873434765865e-07\n",
      "Operator 3: -1.09092785613776e-07\n",
      "Operator 4: 8.106234291793105e-08\n",
      "Operator 5: 5.073383895657669e-08\n",
      "Operator 6: -2.383550962300116e-07\n",
      "Operator 7: 5.32678241767988e-08\n",
      "Operator 8: 1.654161055383946e-07\n",
      "Operator 10: 1.535727081614091e-08\n",
      "Operator 11: 4.912773043930231e-08\n",
      "Operator 12: -1.7958263401031808e-07\n",
      "Operator 13: -2.895056877861339e-08\n",
      "Operator 14: 2.575677564564671e-07\n",
      "Operator 15: -2.9043740412681984e-08\n",
      "Operator 16: -1.785840879620082e-07\n",
      "Operator 17: 1.3206329586186295e-07\n",
      "Operator 18: -7.797379979832268e-08\n",
      "Operator 19: 6.845966208970879e-08\n",
      "Operator 20: 1.4907406716255517e-08\n",
      "Operator 21: -1.4280926670618416e-07\n",
      "Operator 22: 1.0909279091389835e-07\n",
      "Operator 23: -8.106234291793105e-08\n",
      "Operator 24: -5.0733841458802915e-08\n",
      "Operator 25: 2.383550962300116e-07\n",
      "Operator 26: -5.3267831246485125e-08\n",
      "Operator 27: -1.5015825018783874e-07\n",
      "Operator 28: 4.526129977867557e-08\n",
      "Operator 30: -4.912772039178394e-08\n",
      "Operator 31: 1.7958263401031808e-07\n",
      "Operator 32: 2.8950571290761322e-08\n",
      "Operator 33: -2.575677564564671e-07\n",
      "Operator 34: -1.8213672233277356\n",
      "Operator 35: 1.2297084893428626e-07\n",
      "Operator 36: -1.245088726959565e-07\n",
      "Operator 37: 1.8213672232406464\n",
      "Operator 38: -2.0000000000000204\n",
      "Operator 39: 2.0000000000000346\n",
      "Operator 40: -2.0000000000000373\n",
      "Operator 41: 2.0000000000000338\n",
      "Operator 42: -1.8213671769283963\n",
      "Operator 43: -6.133182566231766e-08\n",
      "Operator 44: 5.596946245439793e-08\n",
      "Operator 45: 1.821367176624757\n",
      "Operator 46: -2.000000000000035\n",
      "Operator 47: 2.000000000000033\n",
      "Operator 48: -2.000000000000033\n",
      "Operator 49: 2.000000000000028\n",
      "Operator 50: -2.000000000000027\n",
      "Operator 51: 1.8213672232406575\n",
      "Operator 52: -1.2450886861055407e-07\n",
      "Operator 53: 1.2297084876408134e-07\n",
      "Operator 54: -1.8213672233277316\n",
      "Operator 55: 2.000000000000024\n",
      "Operator 56: -2.0000000000000338\n",
      "Operator 57: 2.0000000000000346\n",
      "Operator 58: -2.000000000000038\n",
      "Operator 59: 1.821367176624751\n",
      "Operator 60: 5.596945890805342e-08\n",
      "Operator 61: -6.133182627493091e-08\n",
      "Operator 62: -1.821367176928392\n",
      "Operator 63: 2.000000000000041\n",
      "Operator 64: -2.00000000000003\n",
      "Operator 65: 2.0000000000000324\n",
      "Operator 66: -2.00000000000003\n",
      "Operator 67: 2.000000000000028\n",
      "Operator 68: -1.8213672263083285\n",
      "Operator 69: 0.48803385999043986\n",
      "Operator 70: -8.443721877704731e-08\n",
      "Operator 72: -1.821367223327734\n",
      "Operator 74: -2.0000000000000324\n",
      "Operator 76: -1.8213672002263612\n",
      "Operator 77: 0.488033950509251\n",
      "Operator 78: 3.625327519248177e-07\n",
      "Operator 80: -1.821367176928399\n",
      "Operator 82: -2.000000000000031\n",
      "Operator 84: -2.000000000000025\n",
      "Operator 85: 1.8213672263083247\n",
      "Operator 86: -8.443721782674771e-08\n",
      "Operator 87: 1.38613159393545e-07\n",
      "Operator 88: -1.821367223240648\n",
      "Operator 89: 2.0000000000000204\n",
      "Operator 90: -2.0000000000000338\n",
      "Operator 91: 2.0000000000000373\n",
      "Operator 92: -2.0000000000000293\n",
      "Operator 93: 1.8213672002263719\n",
      "Operator 94: 3.625327523962555e-07\n",
      "Operator 95: 5.427772835258969e-08\n",
      "Operator 96: -1.8213671766247597\n",
      "Operator 97: 2.000000000000035\n",
      "Operator 98: -2.0000000000000315\n",
      "Operator 99: 2.0000000000000338\n",
      "Operator 100: -2.0000000000000244\n",
      "Operator 101: 2.000000000000027\n",
      "Operator 102: 2.000000000000037\n",
      "Operator 103: -1.8213672232406546\n",
      "Operator 104: 1.3861315420900416e-07\n",
      "Operator 105: -8.443721777540718e-08\n",
      "Operator 106: 1.8213672263083192\n",
      "Operator 107: -2.0000000000000253\n",
      "Operator 108: 2.0000000000000346\n",
      "Operator 109: -2.0000000000000333\n",
      "Operator 110: 2.000000000000038\n",
      "Operator 111: -1.8213671766247468\n",
      "Operator 112: 5.427773191551408e-08\n",
      "Operator 113: 3.6253275237586773e-07\n",
      "Operator 114: 1.8213672002263666\n",
      "Operator 115: -2.0000000000000435\n",
      "Operator 116: 2.0000000000000275\n",
      "Operator 117: -2.0000000000000315\n",
      "Operator 118: 2.00000000000003\n",
      "Operator 119: -2.000000000000025\n",
      "Operator 120: 2.904373563872298e-08\n",
      "Operator 121: -1.8213672263083254\n",
      "Operator 123: -1.245088725725601e-07\n",
      "Operator 124: 0.48803384854171306\n",
      "Operator 125: -1.8213672232406428\n",
      "Operator 127: -2.0000000000000364\n",
      "Operator 129: -1.8213672002263719\n",
      "Operator 131: 5.596946212059367e-08\n",
      "Operator 132: 0.4880338624268312\n",
      "Operator 133: -1.821367176624752\n",
      "Operator 135: -2.000000000000032\n",
      "Operator 137: -2.000000000000027\n",
      "Operator 138: 1.8213672232406535\n",
      "Operator 140: 8.484933034699826e-08\n",
      "Operator 141: -0.48803385999043647\n",
      "Operator 142: 1.821367226308321\n",
      "Operator 144: 2.0000000000000346\n",
      "Operator 146: 1.8213671766247468\n",
      "Operator 148: -3.6109590953561944e-07\n",
      "Operator 149: -0.48803395050924975\n",
      "Operator 150: 1.8213672002263748\n",
      "Operator 152: 2.000000000000033\n",
      "Operator 154: 2.000000000000025\n",
      "Operator 155: 1.785840879620082e-07\n",
      "Operator 156: -2.0000000000000333\n",
      "Operator 157: -1.1292617434139866e-07\n",
      "Operator 158: -5.089997477017377e-08\n",
      "Operator 159: -5.328019409794241e-08\n",
      "Operator 161: 1.4280926662767956e-07\n",
      "Operator 162: 8.558918341879047e-08\n",
      "Operator 163: 8.106234291793105e-08\n",
      "Operator 164: -3.232135541585243e-08\n",
      "Operator 165: -2.1997281170157818e-07\n",
      "Operator 166: 2.7717509455869535e-08\n",
      "Operator 167: 1.6541610483185296e-07\n",
      "Operator 170: -7.889051397831422e-08\n",
      "Operator 171: -1.7958263401031808e-07\n",
      "Operator 172: 4.9846776373208e-08\n",
      "Operator 173: 2.575677564564671e-07\n",
      "Operator 174: -1.3206329604382318e-07\n",
      "Operator 175: 7.797378305756691e-08\n",
      "Operator 176: 1.8213672263083314\n",
      "Operator 178: 1.2297085077101593e-07\n",
      "Operator 179: -0.4880338488667351\n",
      "Operator 180: 1.8213672233277327\n",
      "Operator 182: 2.000000000000034\n",
      "Operator 184: 1.8213672002263661\n",
      "Operator 186: -6.133182536840296e-08\n",
      "Operator 187: -0.4880338635600351\n",
      "Operator 188: 1.8213671769283977\n",
      "Operator 190: 2.000000000000033\n",
      "Operator 192: 2.0000000000000275\n",
      "Operator 193: -1.8213672233277423\n",
      "Operator 195: -8.443721878266223e-08\n",
      "Operator 196: 0.48803385999043614\n",
      "Operator 197: -1.821367226308321\n",
      "Operator 199: -2.0000000000000324\n",
      "Operator 201: -1.8213671769283868\n",
      "Operator 203: 3.6253275214814377e-07\n",
      "Operator 204: 0.48803395050925236\n",
      "Operator 205: -1.821367200226374\n",
      "Operator 207: -2.000000000000031\n",
      "Operator 209: -2.000000000000025\n",
      "Operator 210: 1.1292617434139866e-07\n",
      "Operator 211: 5.089998014812305e-08\n",
      "Operator 212: 6.845966388390923e-08\n",
      "Operator 214: -1.5798873434765865e-07\n",
      "Operator 215: -8.558918326178125e-08\n",
      "Operator 216: -8.106234291793105e-08\n",
      "Operator 217: 3.232135540111475e-08\n",
      "Operator 218: 2.1997281170157818e-07\n",
      "Operator 219: -2.771750709930743e-08\n",
      "Operator 220: -1.501582494027925e-07\n",
      "Operator 222: -1.5357270483074007e-08\n",
      "Operator 223: 7.889051397831422e-08\n",
      "Operator 224: 1.7958263401031808e-07\n",
      "Operator 225: -4.984677681729721e-08\n",
      "Operator 226: -2.575677564564671e-07\n",
      "Operator 227: -1.821367226308333\n",
      "Operator 228: 8.484933063793119e-08\n",
      "Operator 229: -1.374872477561334e-07\n",
      "Operator 230: 1.821367223327734\n",
      "Operator 231: -2.0000000000000235\n",
      "Operator 232: 2.0000000000000346\n",
      "Operator 233: -2.0000000000000346\n",
      "Operator 234: 2.0000000000000293\n",
      "Operator 235: -1.8213672002263674\n",
      "Operator 236: -3.610959093073706e-07\n",
      "Operator 237: -5.035220796700025e-08\n",
      "Operator 238: 1.8213671769284\n",
      "Operator 239: -2.0000000000000404\n",
      "Operator 240: 2.000000000000033\n",
      "Operator 241: -2.0000000000000333\n",
      "Operator 242: 2.000000000000025\n",
      "Operator 243: -2.0000000000000293\n",
      "Operator 244: 1.8213672233277434\n",
      "Operator 245: -1.3748724768991264e-07\n",
      "Operator 246: 8.48493301422289e-08\n",
      "Operator 247: -1.8213672263083196\n",
      "Operator 248: 2.000000000000024\n",
      "Operator 249: -2.0000000000000364\n",
      "Operator 250: 2.0000000000000333\n",
      "Operator 251: -2.0000000000000333\n",
      "Operator 252: 1.8213671769283883\n",
      "Operator 253: -5.035220894304585e-08\n",
      "Operator 254: -3.610959093787532e-07\n",
      "Operator 255: -1.8213672002263726\n",
      "Operator 256: 2.0000000000000426\n",
      "Operator 257: -2.000000000000034\n",
      "Operator 258: 2.000000000000032\n",
      "Operator 259: -2.0000000000000284\n",
      "Operator 260: 2.000000000000026\n",
      "Operator 261: 2.0000000000000298\n",
      "Operator 262: -2.000000000000036\n",
      "Operator 263: -1.8213672233277465\n",
      "Operator 264: 0.4880338488667353\n",
      "Operator 265: -1.3748724779526278e-07\n",
      "Operator 267: -1.821367223240645\n",
      "Operator 269: -2.0000000000000346\n",
      "Operator 271: -1.8213671769283923\n",
      "Operator 272: 0.4880338635600326\n",
      "Operator 273: -5.035220764272804e-08\n",
      "Operator 275: -1.8213671766247566\n",
      "Operator 277: -2.000000000000033\n",
      "Operator 279: -2.0000000000000293\n",
      "Operator 280: -1.8213672232406561\n",
      "Operator 282: -1.374872486518746e-07\n",
      "Operator 283: 0.4880338488667343\n",
      "Operator 284: -1.8213672233277327\n",
      "Operator 286: -2.0000000000000364\n",
      "Operator 288: -1.8213671766247503\n",
      "Operator 290: -5.03522088379e-08\n",
      "Operator 291: 0.4880338635600333\n",
      "Operator 292: -1.8213671769283981\n",
      "Operator 294: -2.000000000000034\n",
      "Operator 296: -2.000000000000028\n",
      "Total gradient norm: 22.177424571031263\n",
      "Operators under consideration (1):\n",
      "[294]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.000000000000034)]\n",
      "Operator(s) added to ansatz: [294]\n",
      "Gradients: [np.float64(-2.000000000000034)]\n",
      "Initial energy: -30.928203230275912\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294]...\n",
      "Starting point: [np.float64(0.7853981436748196), np.float64(0.7853981809005451), np.float64(0.7853981758824355), np.float64(0.7853981631820754), np.float64(0.7853981545803633), np.float64(0.785398184794744), np.float64(-0.1308996867993734), np.float64(0.7853981546836131), np.float64(-0.7853981758591428), np.float64(0.7853981723762016), np.float64(-0.7853981448380865), np.float64(-0.16991844598071548), np.float64(-0.13089968671014854), np.float64(0.13089969376200172), np.float64(-0.16991847689916506), np.float64(-0.13089969345091562), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -31.051309\n",
      "         Iterations: 7\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 33\n",
      "\n",
      "Current energy: -31.051308855893467\n",
      "(change of -0.12310562561755489)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: -2.2486529176378286e-08\n",
      "Operator 1: 2.6270069758493025e-08\n",
      "Operator 2: -4.504764813120843e-08\n",
      "Operator 3: -4.219785573116515e-08\n",
      "Operator 4: 1.3003123873182275e-07\n",
      "Operator 5: -4.1681964321564376e-08\n",
      "Operator 6: -3.056670534221932e-08\n",
      "Operator 7: 5.829856198814132e-08\n",
      "Operator 8: -1.0907014552259788e-07\n",
      "Operator 10: 5.757494986057392e-08\n",
      "Operator 11: 4.0217148642850483e-08\n",
      "Operator 12: -8.549455272107412e-08\n",
      "Operator 13: 2.9993403582207635e-08\n",
      "Operator 14: 4.764380256983003e-08\n",
      "Operator 15: -3.672229942353056e-08\n",
      "Operator 16: 2.1718527420944156e-08\n",
      "Operator 18: -3.200757328851479e-08\n",
      "Operator 19: 3.047653820860186e-08\n",
      "Operator 21: 5.303765498368725e-08\n",
      "Operator 22: 4.219786088449404e-08\n",
      "Operator 23: -1.3003123873182275e-07\n",
      "Operator 24: 4.168196220582624e-08\n",
      "Operator 25: 3.056670534221932e-08\n",
      "Operator 26: -5.8298568493286815e-08\n",
      "Operator 27: 8.524948775811052e-08\n",
      "Operator 28: -7.31712097858124e-08\n",
      "Operator 29: -8.139560954903579e-08\n",
      "Operator 30: -4.021713946545357e-08\n",
      "Operator 31: 8.16915857731871e-08\n",
      "Operator 32: -2.573671794925082e-08\n",
      "Operator 33: -4.92458214207403e-08\n",
      "Operator 34: -1.8213672098226668\n",
      "Operator 35: 1.7696911498499183e-08\n",
      "Operator 36: -1.7486423121882573e-08\n",
      "Operator 37: 1.8213672098345808\n",
      "Operator 38: -2.0000000000000218\n",
      "Operator 39: 2.0000000000000324\n",
      "Operator 40: -2.00000000000003\n",
      "Operator 41: 2.0000000000000324\n",
      "Operator 42: -1.821367196819164\n",
      "Operator 43: -4.2246660749080884e-08\n",
      "Operator 44: 4.298003704686966e-08\n",
      "Operator 45: 1.8213671968606975\n",
      "Operator 46: -2.0000000000000324\n",
      "Operator 47: 1.4356622517007775e-06\n",
      "Operator 48: -2.485070912269812\n",
      "Operator 49: 1.9402850847413817\n",
      "Operator 50: -2.000000000000019\n",
      "Operator 51: 1.8213672098345919\n",
      "Operator 52: -1.7486423940503173e-08\n",
      "Operator 53: 1.7696911224860497e-08\n",
      "Operator 54: -1.8213672098226639\n",
      "Operator 55: 2.00000000000002\n",
      "Operator 56: -2.0000000000000298\n",
      "Operator 57: 2.000000000000033\n",
      "Operator 58: -2.0000000000000315\n",
      "Operator 59: 1.8213671968606926\n",
      "Operator 60: 4.298004251948507e-08\n",
      "Operator 61: -4.224666094953548e-08\n",
      "Operator 62: -1.8213671968191663\n",
      "Operator 63: 1.9402850847413924\n",
      "Operator 64: -2.48507091226981\n",
      "Operator 65: 1.435662251700164e-06\n",
      "Operator 66: -2.0000000000000195\n",
      "Operator 67: 2.0000000000000275\n",
      "Operator 68: -1.8213672074819836\n",
      "Operator 69: 0.48803386187358855\n",
      "Operator 70: -4.795778772872314e-08\n",
      "Operator 72: -1.8213672098226619\n",
      "Operator 74: -2.000000000000031\n",
      "Operator 76: -1.8213671982394781\n",
      "Operator 77: 0.488033882934231\n",
      "Operator 78: 6.060881194652714e-08\n",
      "Operator 80: -1.7669858029127061\n",
      "Operator 82: -1.9402850847413848\n",
      "Operator 84: -2.000000000000024\n",
      "Operator 85: 1.821367207481979\n",
      "Operator 86: -4.7957786740560366e-08\n",
      "Operator 88: -1.8213672098345817\n",
      "Operator 89: 2.0000000000000218\n",
      "Operator 90: -2.0000000000000315\n",
      "Operator 91: 2.0000000000000284\n",
      "Operator 92: -2.0000000000000284\n",
      "Operator 93: 1.8213671982394821\n",
      "Operator 94: 6.060881162713926e-08\n",
      "Operator 95: -3.625902295614094e-08\n",
      "Operator 96: -1.8213671968607001\n",
      "Operator 97: 1.9402850847413866\n",
      "Operator 98: -1.9402850847413844\n",
      "Operator 99: 1.940285084741381\n",
      "Operator 100: -1.9402850847413788\n",
      "Operator 101: 2.000000000000019\n",
      "Operator 102: 2.0000000000000338\n",
      "Operator 103: -1.821367209834589\n",
      "Operator 105: -4.795778655555439e-08\n",
      "Operator 106: 1.821367207481977\n",
      "Operator 107: -2.000000000000021\n",
      "Operator 108: 2.0000000000000298\n",
      "Operator 109: -2.0000000000000315\n",
      "Operator 110: 2.000000000000032\n",
      "Operator 111: -1.8213671968606882\n",
      "Operator 112: -3.6259027005577394e-08\n",
      "Operator 113: 6.060881139047037e-08\n",
      "Operator 114: 1.8213671982394837\n",
      "Operator 115: -1.940285084741395\n",
      "Operator 116: 1.9402850847413817\n",
      "Operator 117: -1.940285084741385\n",
      "Operator 118: 1.9402850847413768\n",
      "Operator 119: -2.000000000000024\n",
      "Operator 120: 3.6722294316504644e-08\n",
      "Operator 121: -1.8213672074819791\n",
      "Operator 123: -1.748642395262716e-08\n",
      "Operator 124: 0.4880338706536289\n",
      "Operator 125: -1.8213672098345826\n",
      "Operator 127: -2.0000000000000284\n",
      "Operator 129: -1.8213671982394821\n",
      "Operator 131: 4.298003696883933e-08\n",
      "Operator 132: 0.4880338777885186\n",
      "Operator 133: -1.7669858029529864\n",
      "Operator 135: -2.485070912269812\n",
      "Operator 137: -1.9402850847413753\n",
      "Operator 138: 1.821367209834589\n",
      "Operator 140: 4.7901387417062044e-08\n",
      "Operator 141: -0.4880338618735861\n",
      "Operator 142: 1.8213672074819762\n",
      "Operator 144: 2.0000000000000315\n",
      "Operator 146: 1.8213671968606877\n",
      "Operator 148: -6.080531854418366e-08\n",
      "Operator 149: -0.4880338829342328\n",
      "Operator 150: 1.7669858042906164\n",
      "Operator 152: 2.485070912269813\n",
      "Operator 154: 1.9402850847413782\n",
      "Operator 155: -2.1718527420944156e-08\n",
      "Operator 156: -2.0000000000000306\n",
      "Operator 158: 1.4957104420406916e-08\n",
      "Operator 159: -2.2486529882568256e-08\n",
      "Operator 161: -5.303765498368725e-08\n",
      "Operator 162: 1.1795821368709458e-08\n",
      "Operator 163: 1.3003123873182275e-07\n",
      "Operator 164: 4.650274431154032e-08\n",
      "Operator 165: -2.3509978966540882e-08\n",
      "Operator 166: 1.0994241202772168e-08\n",
      "Operator 167: -1.0907014589659764e-07\n",
      "Operator 169: 8.079609483891345e-08\n",
      "Operator 170: -1.724201539668944e-08\n",
      "Operator 171: -8.54945509939724e-08\n",
      "Operator 172: -1.8898649534193623e-08\n",
      "Operator 173: 4.92458214207403e-08\n",
      "Operator 175: 3.200755676884076e-08\n",
      "Operator 176: 1.8213672074819867\n",
      "Operator 178: 1.7696910476463524e-08\n",
      "Operator 179: -0.4880338706091477\n",
      "Operator 180: 1.8213672098226608\n",
      "Operator 182: 2.000000000000033\n",
      "Operator 184: 1.8213671982394821\n",
      "Operator 186: -4.2246661212436157e-08\n",
      "Operator 187: -0.4880338776335367\n",
      "Operator 188: 1.766985802912704\n",
      "Operator 190: 1.4356622517268433e-06\n",
      "Operator 191: -0.4850709122697808\n",
      "Operator 192: 2.0000000000000266\n",
      "Operator 193: -1.8213672098226705\n",
      "Operator 195: -4.795778745151378e-08\n",
      "Operator 196: 0.48803386187358533\n",
      "Operator 197: -1.821367207481976\n",
      "Operator 199: -2.000000000000031\n",
      "Operator 201: -1.821367196819161\n",
      "Operator 203: 6.060881202705937e-08\n",
      "Operator 204: 0.48803388293423233\n",
      "Operator 205: -1.8213671982394892\n",
      "Operator 207: -1.9402850847413848\n",
      "Operator 209: -1.9402850847413782\n",
      "Operator 211: -1.4957099494224747e-08\n",
      "Operator 212: 3.047653855331357e-08\n",
      "Operator 214: 4.504764813120843e-08\n",
      "Operator 215: -1.1795821113682566e-08\n",
      "Operator 216: -1.3003123873182275e-07\n",
      "Operator 217: -4.650274432627806e-08\n",
      "Operator 218: 2.3509978966540882e-08\n",
      "Operator 219: -1.0994239314341013e-08\n",
      "Operator 220: 8.524948726184063e-08\n",
      "Operator 222: -5.7686659503173635e-08\n",
      "Operator 223: 1.7242015475194064e-08\n",
      "Operator 224: 8.169158757879341e-08\n",
      "Operator 226: -4.764380256983003e-08\n",
      "Operator 227: -1.8213672074819862\n",
      "Operator 228: 4.790138801831508e-08\n",
      "Operator 230: 1.8213672098226632\n",
      "Operator 231: -2.0000000000000195\n",
      "Operator 232: 2.000000000000031\n",
      "Operator 233: -2.000000000000033\n",
      "Operator 234: 2.0000000000000275\n",
      "Operator 235: -1.8213671982394821\n",
      "Operator 236: -6.08053181057242e-08\n",
      "Operator 237: 3.5722154321661883e-08\n",
      "Operator 238: 1.8213671968191725\n",
      "Operator 239: -1.9402850847413924\n",
      "Operator 240: 2.485070912269813\n",
      "Operator 241: -1.435662251834787e-06\n",
      "Operator 242: 2.000000000000024\n",
      "Operator 243: -2.0000000000000266\n",
      "Operator 244: 1.8213672098226705\n",
      "Operator 246: 4.790138805085642e-08\n",
      "Operator 247: -1.821367207481975\n",
      "Operator 248: 2.0000000000000204\n",
      "Operator 249: -2.0000000000000324\n",
      "Operator 250: 2.000000000000032\n",
      "Operator 251: -2.000000000000032\n",
      "Operator 252: 1.8213671968191605\n",
      "Operator 253: 3.572215395958627e-08\n",
      "Operator 254: -6.08053181979706e-08\n",
      "Operator 255: -1.8213671982394877\n",
      "Operator 256: 2.000000000000041\n",
      "Operator 257: -1.43566225121135e-06\n",
      "Operator 258: 2.4850709122698125\n",
      "Operator 259: -1.9402850847413817\n",
      "Operator 260: 2.000000000000024\n",
      "Operator 261: 2.000000000000028\n",
      "Operator 262: -2.0000000000000338\n",
      "Operator 263: -1.821367209822673\n",
      "Operator 264: 0.48803387060914916\n",
      "Operator 267: -1.8213672098345801\n",
      "Operator 269: -2.000000000000033\n",
      "Operator 271: -1.8213671968191645\n",
      "Operator 272: 0.48803387763353545\n",
      "Operator 273: 3.5722154260414076e-08\n",
      "Operator 275: -1.8213671968606968\n",
      "Operator 276: 0.48507091226978327\n",
      "Operator 277: -1.4356622518369338e-06\n",
      "Operator 279: -1.940285084741381\n",
      "Operator 280: -1.8213672098345919\n",
      "Operator 283: 0.4880338706091475\n",
      "Operator 284: -1.8213672098226619\n",
      "Operator 286: -2.0000000000000324\n",
      "Operator 288: -1.8213671968606926\n",
      "Operator 290: 3.572215409685095e-08\n",
      "Operator 291: 0.4880338776335348\n",
      "Operator 292: -1.7669858029127037\n",
      "Operator 294: -1.4356622515275152e-06\n",
      "Operator 295: 0.48507091226978033\n",
      "Operator 296: -2.000000000000027\n",
      "Total gradient norm: 21.73445467164844\n",
      "Operators under consideration (1):\n",
      "[152]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.485070912269813)]\n",
      "Operator(s) added to ansatz: [152]\n",
      "Gradients: [np.float64(2.485070912269813)]\n",
      "Initial energy: -31.051308855893467\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152]...\n",
      "Starting point: [np.float64(0.785398159086944), np.float64(0.7853981661553273), np.float64(0.7853981719412562), np.float64(0.7853981652701205), np.float64(0.7853981756786539), np.float64(0.7853981663464036), np.float64(-0.13089969328830772), np.float64(0.7853981500280024), np.float64(-0.7853981585273777), np.float64(0.7853981681235903), np.float64(-0.7853981662174315), np.float64(-0.16991845147214935), np.float64(-0.13089969330051845), np.float64(0.13089969597785664), np.float64(-0.16991845950748397), np.float64(-0.13089969602040127), np.float64(0.12248924451362142), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -31.255479\n",
      "         Iterations: 8\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 30\n",
      "\n",
      "Current energy: -31.255479385096848\n",
      "(change of -0.20417052920338108)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.8065918405408748e-07\n",
      "Operator 1: -1.2523336412550322e-07\n",
      "Operator 2: -2.7252581350378303e-07\n",
      "Operator 3: 2.3131394520274447e-07\n",
      "Operator 4: -1.3454090499789919e-07\n",
      "Operator 5: 1.0663969283368257e-07\n",
      "Operator 6: -6.25355466525912e-08\n",
      "Operator 7: -2.151108989914475e-07\n",
      "Operator 8: 3.056329253689416e-07\n",
      "Operator 9: -1.3721039175956429e-07\n",
      "Operator 10: 4.060780941959886e-07\n",
      "Operator 11: -2.621000791128887e-07\n",
      "Operator 12: -1.510701450720162e-07\n",
      "Operator 13: -4.383624341409328e-08\n",
      "Operator 14: 1.4813455218032345e-07\n",
      "Operator 15: 1.909155822321651e-07\n",
      "Operator 16: -3.750287582704459e-07\n",
      "Operator 17: -2.2992870041827174e-07\n",
      "Operator 18: 2.2621416476197372e-07\n",
      "Operator 19: -2.325906170781399e-07\n",
      "Operator 20: -1.664596759590409e-08\n",
      "Operator 21: 2.2059438499675599e-07\n",
      "Operator 22: -2.3131393956768568e-07\n",
      "Operator 23: 1.345409062539731e-07\n",
      "Operator 24: -1.06639695659849e-07\n",
      "Operator 25: 6.25355466525912e-08\n",
      "Operator 26: 2.151108927110777e-07\n",
      "Operator 27: -1.3680242464644177e-07\n",
      "Operator 28: 5.984638254734868e-07\n",
      "Operator 29: -2.3724763516991204e-07\n",
      "Operator 30: 2.6210008872853007e-07\n",
      "Operator 31: 1.510701712897735e-07\n",
      "Operator 32: 4.3836250823045196e-08\n",
      "Operator 33: -1.4813453363959894e-07\n",
      "Operator 34: -1.8213672262925382\n",
      "Operator 35: -7.498286237399825e-08\n",
      "Operator 36: 7.505401994607723e-08\n",
      "Operator 37: 1.8213672262966\n",
      "Operator 38: -1.9999999999999885\n",
      "Operator 39: 2.000000000000016\n",
      "Operator 40: -2.0000000000000053\n",
      "Operator 41: 2.000000000000017\n",
      "Operator 42: -1.8213671604057984\n",
      "Operator 43: -4.700086306777422e-07\n",
      "Operator 44: 4.702455829671265e-07\n",
      "Operator 45: 1.8213671604192287\n",
      "Operator 46: -1.8939307127483356\n",
      "Operator 47: -0.7090327215077181\n",
      "Operator 48: -2.0894918857844518\n",
      "Operator 49: 1.7934866341723388\n",
      "Operator 50: -1.9999999999999982\n",
      "Operator 51: 1.821367226296612\n",
      "Operator 52: 7.505401804255673e-08\n",
      "Operator 53: -7.498286199301302e-08\n",
      "Operator 54: -1.8213672262925407\n",
      "Operator 55: 2.00000000000001\n",
      "Operator 56: -2.0000000000000115\n",
      "Operator 57: 2.0000000000000164\n",
      "Operator 58: -2.0000000000000027\n",
      "Operator 59: 1.8213671604192219\n",
      "Operator 60: 4.702455616034366e-07\n",
      "Operator 61: -4.700086307102234e-07\n",
      "Operator 62: -1.8213671604057893\n",
      "Operator 63: 1.793486634172352\n",
      "Operator 64: -2.0894918857844536\n",
      "Operator 65: -0.7090327215077172\n",
      "Operator 66: -1.893930712748334\n",
      "Operator 67: 2.00000000000001\n",
      "Operator 68: -1.8213671834990968\n",
      "Operator 69: 0.4880337582524703\n",
      "Operator 70: -4.782598323456239e-07\n",
      "Operator 72: -1.821367226292559\n",
      "Operator 74: -2.0000000000000115\n",
      "Operator 76: -1.8213671176558472\n",
      "Operator 77: 0.48803382421821195\n",
      "Operator 78: -8.267125281352283e-08\n",
      "Operator 80: -1.7247714692624498\n",
      "Operator 81: 0.6085874506126131\n",
      "Operator 82: 0.7090309533566002\n",
      "Operator 84: -1.8939307127483278\n",
      "Operator 85: 1.8213671834990721\n",
      "Operator 86: -4.782598387323804e-07\n",
      "Operator 87: -2.775548986930403e-07\n",
      "Operator 88: -1.8213672262966012\n",
      "Operator 89: 1.9999999999999885\n",
      "Operator 90: -2.0000000000000147\n",
      "Operator 91: 2.000000000000004\n",
      "Operator 92: -2.0000000000000124\n",
      "Operator 93: 1.8213671176558504\n",
      "Operator 94: -8.267129887896946e-08\n",
      "Operator 95: -6.725404714113362e-07\n",
      "Operator 96: -1.821367160419231\n",
      "Operator 97: 1.8939305668365898\n",
      "Operator 98: 0.7090309533565984\n",
      "Operator 99: 2.089491285793966\n",
      "Operator 100: -1.793486634172336\n",
      "Operator 101: 1.9999999999999987\n",
      "Operator 102: 2.000000000000022\n",
      "Operator 103: -1.8213672262966099\n",
      "Operator 104: -2.775548988863502e-07\n",
      "Operator 105: -4.78259838299138e-07\n",
      "Operator 106: 1.8213671834990754\n",
      "Operator 107: -2.0000000000000115\n",
      "Operator 108: 2.0000000000000115\n",
      "Operator 109: -2.0000000000000147\n",
      "Operator 110: 2.0000000000000027\n",
      "Operator 111: -1.8213671604192179\n",
      "Operator 112: -6.725404802580248e-07\n",
      "Operator 113: -8.267129917698146e-08\n",
      "Operator 114: 1.8213671176558444\n",
      "Operator 115: -1.793486634172354\n",
      "Operator 116: 2.0894912857939687\n",
      "Operator 117: 0.709030953356599\n",
      "Operator 118: 1.893930566836585\n",
      "Operator 119: -2.0000000000000075\n",
      "Operator 120: -1.909155871171464e-07\n",
      "Operator 121: -1.821367183499072\n",
      "Operator 123: 7.50540188348053e-08\n",
      "Operator 124: 0.48803391797490575\n",
      "Operator 125: -1.8213672262965803\n",
      "Operator 127: -2.000000000000004\n",
      "Operator 129: -1.8213671176558506\n",
      "Operator 131: 4.7024558248443813e-07\n",
      "Operator 132: 0.48803398381325674\n",
      "Operator 133: -1.7247714692751481\n",
      "Operator 135: -2.089491885784452\n",
      "Operator 137: -1.79348663417233\n",
      "Operator 138: 1.8213672262965979\n",
      "Operator 140: 4.782407652401208e-07\n",
      "Operator 141: -0.48803375825246376\n",
      "Operator 142: 1.8213671834990934\n",
      "Operator 144: 2.0000000000000115\n",
      "Operator 146: 1.8213671604192159\n",
      "Operator 148: 8.260781396627012e-08\n",
      "Operator 149: -0.48803382421823094\n",
      "Operator 150: 1.6332987907183774\n",
      "Operator 152: 1.029910752905223e-06\n",
      "Operator 153: -0.608587450612605\n",
      "Operator 154: 1.8939305668365902\n",
      "Operator 155: 3.750287582704459e-07\n",
      "Operator 156: -2.000000000000019\n",
      "Operator 157: 1.926582636212601e-07\n",
      "Operator 158: 6.250780281906546e-08\n",
      "Operator 159: 1.8065918360717982e-07\n",
      "Operator 161: -2.2059438499675599e-07\n",
      "Operator 162: -1.1483920646383182e-08\n",
      "Operator 163: -1.345409062539731e-07\n",
      "Operator 164: -2.0362697538169254e-07\n",
      "Operator 165: -8.088653107059813e-08\n",
      "Operator 166: -2.3830690562965593e-07\n",
      "Operator 167: 3.0563292560445546e-07\n",
      "Operator 169: 2.0701872273852322e-07\n",
      "Operator 170: -2.1944043777600576e-08\n",
      "Operator 171: -1.5107014511769654e-07\n",
      "Operator 172: -6.6878674931381e-08\n",
      "Operator 173: 1.4813453408368815e-07\n",
      "Operator 174: 2.2992870032846703e-07\n",
      "Operator 175: -2.262141815071258e-07\n",
      "Operator 176: 1.8213671834991003\n",
      "Operator 178: -7.498285630020395e-08\n",
      "Operator 179: -0.48803391795987616\n",
      "Operator 180: 1.8213672262925582\n",
      "Operator 182: 2.000000000000013\n",
      "Operator 184: 1.8213671176558515\n",
      "Operator 186: -4.700086711601252e-07\n",
      "Operator 187: -0.4880339837631723\n",
      "Operator 188: 1.7247714692624478\n",
      "Operator 190: -0.7090327215077191\n",
      "Operator 191: -0.6085879046913492\n",
      "Operator 192: 1.8939307127483307\n",
      "Operator 193: -1.8213672262925664\n",
      "Operator 195: -4.7825983233391e-07\n",
      "Operator 196: 0.4880337582524653\n",
      "Operator 197: -1.8213671834990905\n",
      "Operator 199: -2.0000000000000115\n",
      "Operator 201: -1.821367160405794\n",
      "Operator 203: -8.267125282300244e-08\n",
      "Operator 204: 0.4880338242182255\n",
      "Operator 205: -1.7247715616591615\n",
      "Operator 207: 0.7090309533566\n",
      "Operator 208: 0.6085874506126097\n",
      "Operator 209: -1.8939305668365816\n",
      "Operator 210: -1.9265826422306648e-07\n",
      "Operator 211: -6.250779779366955e-08\n",
      "Operator 212: -2.32590616882786e-07\n",
      "Operator 214: 2.725258132587449e-07\n",
      "Operator 215: 1.1483920175355444e-08\n",
      "Operator 216: 1.345409062539731e-07\n",
      "Operator 217: 2.0362697536695496e-07\n",
      "Operator 218: 8.088653107059813e-08\n",
      "Operator 219: 2.3830690778853306e-07\n",
      "Operator 220: -1.3680242480345102e-07\n",
      "Operator 222: -3.5841630840641456e-07\n",
      "Operator 223: 2.19440426024492e-08\n",
      "Operator 224: 1.5107017013777687e-07\n",
      "Operator 225: 6.687884113176779e-08\n",
      "Operator 226: -1.4813455218032345e-07\n",
      "Operator 227: -1.8213671834991052\n",
      "Operator 228: 4.782407650862768e-07\n",
      "Operator 229: 2.775028051758359e-07\n",
      "Operator 230: 1.821367226292562\n",
      "Operator 231: -2.0\n",
      "Operator 232: 2.0000000000000115\n",
      "Operator 233: -2.000000000000017\n",
      "Operator 234: 2.000000000000011\n",
      "Operator 235: -1.8213671176558515\n",
      "Operator 236: 8.260781378792813e-08\n",
      "Operator 237: 6.723670201781506e-07\n",
      "Operator 238: 1.8213671604058026\n",
      "Operator 239: -1.893930566836603\n",
      "Operator 240: 1.029910753082189e-06\n",
      "Operator 241: 1.3382308514236867e-06\n",
      "Operator 242: 1.893930712748336\n",
      "Operator 243: -2.000000000000001\n",
      "Operator 244: 1.8213672262925713\n",
      "Operator 245: 2.7750280829575146e-07\n",
      "Operator 246: 4.782407693832142e-07\n",
      "Operator 247: -1.8213671834990928\n",
      "Operator 248: 2.000000000000001\n",
      "Operator 249: -2.000000000000013\n",
      "Operator 250: 2.000000000000015\n",
      "Operator 251: -2.0000000000000155\n",
      "Operator 252: 1.821367160405794\n",
      "Operator 253: 6.723670541988787e-07\n",
      "Operator 254: 8.260777448770433e-08\n",
      "Operator 255: -1.8213671176558526\n",
      "Operator 256: 1.8939307127483518\n",
      "Operator 257: 1.3382308526851724e-06\n",
      "Operator 258: 1.029910753582701e-06\n",
      "Operator 259: -1.893930566836593\n",
      "Operator 260: 1.9999999999999978\n",
      "Operator 261: 1.999999999999993\n",
      "Operator 262: -2.0000000000000115\n",
      "Operator 263: -1.8213672262925749\n",
      "Operator 264: 0.48803391795986883\n",
      "Operator 265: 2.775028050105052e-07\n",
      "Operator 267: -1.8213672262965912\n",
      "Operator 269: -2.0000000000000173\n",
      "Operator 271: -1.821367160405798\n",
      "Operator 272: 0.48803398376318524\n",
      "Operator 273: 6.72367020604476e-07\n",
      "Operator 275: -1.724771602154592\n",
      "Operator 276: 0.6085879046913544\n",
      "Operator 277: 1.3382308516412344e-06\n",
      "Operator 279: -1.7934866341723312\n",
      "Operator 280: -1.8213672262966005\n",
      "Operator 282: 2.7750280848101e-07\n",
      "Operator 283: 0.48803391795986306\n",
      "Operator 284: -1.8213672262925609\n",
      "Operator 286: -2.000000000000013\n",
      "Operator 288: -1.82136716041922\n",
      "Operator 290: 6.723670542601645e-07\n",
      "Operator 291: 0.48803398376316837\n",
      "Operator 292: -1.6332988290541062\n",
      "Operator 294: 1.3382308526911211e-06\n",
      "Operator 295: 0.6085879046913449\n",
      "Operator 296: -1.8939307127483391\n",
      "Total gradient norm: 20.74433591230022\n",
      "Operators under consideration (1):\n",
      "[64]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.0894918857844536)]\n",
      "Operator(s) added to ansatz: [64]\n",
      "Gradients: [np.float64(-2.0894918857844536)]\n",
      "Initial energy: -31.255479385096848\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152, 64]...\n",
      "Starting point: [np.float64(0.7853981639643153), np.float64(0.7853981253855499), np.float64(0.7853981556317693), np.float64(0.7853981012218297), np.float64(0.7853981235901987), np.float64(0.7853981605264692), np.float64(-0.13089970428645373), np.float64(0.7853981929283521), np.float64(-0.7853982105594763), np.float64(0.785398132679676), np.float64(-0.7853981916950292), np.float64(-0.1699184303548682), np.float64(-0.1308997042905822), np.float64(0.13089972566178704), np.float64(-0.16991846530600735), np.float64(-0.1308997256755279), np.float64(0.1635699265356728), np.float64(-0.1635698130159878), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -31.392305\n",
      "         Iterations: 15\n",
      "         Function evaluations: 102\n",
      "         Gradient evaluations: 90\n",
      "\n",
      "Current energy: -31.392304845413513\n",
      "(change of -0.13682546031666476)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152, 64]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 1: 6.910708226754803e-08\n",
      "Operator 2: 3.013187967293331e-08\n",
      "Operator 3: -1.5397724673158717e-07\n",
      "Operator 4: 3.192743688627644e-07\n",
      "Operator 5: -1.772612051288758e-08\n",
      "Operator 6: -3.756884049664332e-07\n",
      "Operator 7: 6.211108456011658e-08\n",
      "Operator 8: 1.670801496383517e-07\n",
      "Operator 9: -2.6551874594738074e-08\n",
      "Operator 10: 3.355762363898293e-08\n",
      "Operator 11: -9.358228509595483e-08\n",
      "Operator 12: 1.46119116509344e-07\n",
      "Operator 13: -7.412582151712698e-08\n",
      "Operator 14: -5.3637747243762135e-08\n",
      "Operator 15: 1.1327056226306809e-07\n",
      "Operator 16: -1.2242462688050182e-07\n",
      "Operator 17: 2.6895408480097e-07\n",
      "Operator 18: -2.0311438315220747e-07\n",
      "Operator 19: 2.818745972812984e-08\n",
      "Operator 22: 1.5397725198282987e-07\n",
      "Operator 23: -3.192743688627644e-07\n",
      "Operator 24: 1.7726118236253514e-08\n",
      "Operator 25: 3.756884049664332e-07\n",
      "Operator 26: -6.211109044796329e-08\n",
      "Operator 27: -1.330014757383918e-07\n",
      "Operator 28: 1.196565471134842e-07\n",
      "Operator 30: 9.35822948634147e-08\n",
      "Operator 31: -1.3860014737997505e-07\n",
      "Operator 32: 9.466801414556495e-08\n",
      "Operator 33: 6.115671913331511e-08\n",
      "Operator 34: -1.8213672380001196\n",
      "Operator 35: 1.5199555609004094e-07\n",
      "Operator 36: -1.5198446927481664e-07\n",
      "Operator 37: 1.82136723800076\n",
      "Operator 38: -2.0000000000000107\n",
      "Operator 39: 2.000000000000025\n",
      "Operator 40: -2.00000000000004\n",
      "Operator 41: 2.0000000000000275\n",
      "Operator 42: -1.8213671500912336\n",
      "Operator 43: -2.4147262031885616e-07\n",
      "Operator 44: 2.415005474149948e-07\n",
      "Operator 45: 1.8213671500928177\n",
      "Operator 46: -1.8213671769550386\n",
      "Operator 47: -1.1267502532867637e-06\n",
      "Operator 48: -8.790891362332662e-08\n",
      "Operator 49: 1.8213671081757634\n",
      "Operator 50: -2.000000000000012\n",
      "Operator 51: 1.8213672380007733\n",
      "Operator 52: -1.519844747001577e-07\n",
      "Operator 53: 1.5199555617956885e-07\n",
      "Operator 54: -1.8213672380001231\n",
      "Operator 55: 2.0000000000000195\n",
      "Operator 56: -2.000000000000038\n",
      "Operator 57: 2.000000000000025\n",
      "Operator 58: -2.000000000000035\n",
      "Operator 59: 1.8213671500928093\n",
      "Operator 60: 2.415005487894291e-07\n",
      "Operator 61: -2.4147261965919924e-07\n",
      "Operator 62: -1.8213671500912296\n",
      "Operator 63: 1.8213671081757736\n",
      "Operator 64: -8.790892079443759e-08\n",
      "Operator 65: -1.1267502541292523e-06\n",
      "Operator 66: -1.8213671769550321\n",
      "Operator 67: 2.0000000000000244\n",
      "Operator 68: -1.8213672283350686\n",
      "Operator 69: 0.4880338179320845\n",
      "Operator 70: -2.7694747415363293e-07\n",
      "Operator 72: -1.8213672380001271\n",
      "Operator 74: -2.0000000000000235\n",
      "Operator 76: -1.8213671687386754\n",
      "Operator 77: 0.48803396684447226\n",
      "Operator 78: 4.825506216076297e-07\n",
      "Operator 80: -1.6586891130458032\n",
      "Operator 81: 0.48803386837916063\n",
      "Operator 82: 2.8727041027028473e-07\n",
      "Operator 84: -1.8213671769550337\n",
      "Operator 85: 1.8213672283350566\n",
      "Operator 86: -2.7694747681080424e-07\n",
      "Operator 87: 1.0624889792107436e-07\n",
      "Operator 88: -1.8213672380007608\n",
      "Operator 89: 2.0000000000000107\n",
      "Operator 90: -2.0000000000000235\n",
      "Operator 91: 2.00000000000004\n",
      "Operator 92: -2.000000000000023\n",
      "Operator 93: 1.8213671687386865\n",
      "Operator 94: 4.825506185434985e-07\n",
      "Operator 95: -1.5325987008326264e-07\n",
      "Operator 96: -1.8213671500928201\n",
      "Operator 97: 1.8213671120210275\n",
      "Operator 98: 2.87270411105147e-07\n",
      "Operator 99: -2.1936205397577884e-07\n",
      "Operator 100: -1.8213671081757608\n",
      "Operator 101: 2.0000000000000115\n",
      "Operator 102: 2.000000000000035\n",
      "Operator 103: -1.8213672380007702\n",
      "Operator 104: 1.0624890189145768e-07\n",
      "Operator 105: -2.769474766023627e-07\n",
      "Operator 106: 1.821367228335061\n",
      "Operator 107: -2.000000000000021\n",
      "Operator 108: 2.000000000000038\n",
      "Operator 109: -2.0000000000000235\n",
      "Operator 110: 2.000000000000035\n",
      "Operator 111: -1.8213671500928053\n",
      "Operator 112: -1.5325987490239918e-07\n",
      "Operator 113: 4.825506191526963e-07\n",
      "Operator 114: 1.8213671687386812\n",
      "Operator 115: -1.8213671081757763\n",
      "Operator 116: -2.1936205455007186e-07\n",
      "Operator 117: 2.872704113639763e-07\n",
      "Operator 118: 1.82136711202102\n",
      "Operator 119: -2.0000000000000213\n",
      "Operator 120: -1.132705674811163e-07\n",
      "Operator 121: -1.8213672283350566\n",
      "Operator 123: -1.5198446924046336e-07\n",
      "Operator 124: 0.48803385400493227\n",
      "Operator 125: -1.8213672380007515\n",
      "Operator 127: -2.00000000000004\n",
      "Operator 129: -1.8213671687386865\n",
      "Operator 131: 2.415005473229987e-07\n",
      "Operator 132: 0.488033897257151\n",
      "Operator 133: -1.6586891130472328\n",
      "Operator 135: -8.790891370517728e-08\n",
      "Operator 136: 0.4880338540284285\n",
      "Operator 137: -1.8213671081757543\n",
      "Operator 138: 1.8213672380007608\n",
      "Operator 140: 2.7694450217621956e-07\n",
      "Operator 141: -0.488033817932081\n",
      "Operator 142: 1.821367228335067\n",
      "Operator 144: 2.000000000000026\n",
      "Operator 146: 1.8213671500928048\n",
      "Operator 148: -4.82558100835154e-07\n",
      "Operator 149: -0.4880339668444752\n",
      "Operator 150: 1.6586891265259116\n",
      "Operator 152: 3.819654971380286e-08\n",
      "Operator 153: -0.4880338683791525\n",
      "Operator 154: 1.8213671120210249\n",
      "Operator 155: 1.2242462688050182e-07\n",
      "Operator 156: -2.0000000000000315\n",
      "Operator 157: -2.3805553101804698e-07\n",
      "Operator 158: 2.1219274516486913e-08\n",
      "Operator 162: 1.0501833795528744e-07\n",
      "Operator 163: 3.192743688627644e-07\n",
      "Operator 164: -4.2907250213791005e-08\n",
      "Operator 165: -3.4706127342870227e-07\n",
      "Operator 167: 1.670801495598471e-07\n",
      "Operator 169: -1.26748638311729e-08\n",
      "Operator 170: 1.9809697417836283e-08\n",
      "Operator 171: 1.4611911770798107e-07\n",
      "Operator 173: -6.11567190222928e-08\n",
      "Operator 174: -2.6895408572413443e-07\n",
      "Operator 175: 2.0311436659586463e-07\n",
      "Operator 176: 1.8213672283350717\n",
      "Operator 178: 1.5199555988574568e-07\n",
      "Operator 179: -0.4880338540025927\n",
      "Operator 180: 1.8213672380001262\n",
      "Operator 182: 2.0000000000000253\n",
      "Operator 184: 1.8213671687386794\n",
      "Operator 186: -2.4147262132390935e-07\n",
      "Operator 187: -0.48803389725124946\n",
      "Operator 188: 1.658689113045801\n",
      "Operator 190: -1.1267502555977118e-06\n",
      "Operator 191: -0.48803411071611624\n",
      "Operator 192: 1.8213671769550368\n",
      "Operator 193: -1.821367238000132\n",
      "Operator 195: -2.769474742282619e-07\n",
      "Operator 196: 0.4880338179320809\n",
      "Operator 197: -1.8213672283350637\n",
      "Operator 199: -2.000000000000024\n",
      "Operator 201: -1.8213671500912239\n",
      "Operator 203: 4.825506219119059e-07\n",
      "Operator 204: 0.488033966844474\n",
      "Operator 205: -1.658689189162068\n",
      "Operator 207: 2.872704103225503e-07\n",
      "Operator 208: 0.4880338683791571\n",
      "Operator 209: -1.8213671120210233\n",
      "Operator 210: 2.380555322661065e-07\n",
      "Operator 211: -2.12192693170644e-08\n",
      "Operator 212: 2.8187459889227897e-08\n",
      "Operator 214: -3.013187967293331e-08\n",
      "Operator 215: -1.0501833803379206e-07\n",
      "Operator 216: -3.192743688627644e-07\n",
      "Operator 217: 4.290725019905331e-08\n",
      "Operator 218: 3.4706127342870227e-07\n",
      "Operator 220: -1.3300147581689643e-07\n",
      "Operator 222: -1.8360028441000884e-08\n",
      "Operator 223: -1.980970091726156e-08\n",
      "Operator 224: -1.3860014673495943e-07\n",
      "Operator 226: 5.363774757682904e-08\n",
      "Operator 227: -1.8213672283350775\n",
      "Operator 228: 2.769445017944317e-07\n",
      "Operator 229: -1.062570121563305e-07\n",
      "Operator 230: 1.821367238000131\n",
      "Operator 231: -2.0000000000000138\n",
      "Operator 232: 2.0000000000000266\n",
      "Operator 233: -2.0000000000000293\n",
      "Operator 234: 2.000000000000023\n",
      "Operator 235: -1.8213671687386817\n",
      "Operator 236: -4.825581009613694e-07\n",
      "Operator 237: 1.532394270750633e-07\n",
      "Operator 238: 1.821367150091238\n",
      "Operator 239: -1.8213671120210368\n",
      "Operator 240: 3.8196549671509694e-08\n",
      "Operator 241: 1.10855426298208e-06\n",
      "Operator 242: 1.821367176955035\n",
      "Operator 243: -2.0000000000000226\n",
      "Operator 244: 1.8213672380001378\n",
      "Operator 245: -1.062570093978919e-07\n",
      "Operator 246: 2.76944505968283e-07\n",
      "Operator 247: -1.821367228335066\n",
      "Operator 248: 2.000000000000015\n",
      "Operator 249: -2.0000000000000284\n",
      "Operator 250: 2.000000000000027\n",
      "Operator 251: -2.000000000000027\n",
      "Operator 252: 1.8213671500912265\n",
      "Operator 253: 1.5323942753901625e-07\n",
      "Operator 254: -4.825581024309612e-07\n",
      "Operator 255: -1.821367168738687\n",
      "Operator 256: 1.8213671769550497\n",
      "Operator 257: 1.1085542653588204e-06\n",
      "Operator 258: 3.819655185174691e-08\n",
      "Operator 259: -1.8213671120210273\n",
      "Operator 260: 2.000000000000019\n",
      "Operator 261: 2.00000000000001\n",
      "Operator 262: -2.000000000000025\n",
      "Operator 263: -1.8213672380001413\n",
      "Operator 264: 0.4880338540025889\n",
      "Operator 265: -1.0625701180565286e-07\n",
      "Operator 267: -1.8213672380007544\n",
      "Operator 269: -2.000000000000029\n",
      "Operator 271: -1.82136715009123\n",
      "Operator 272: 0.4880338972512428\n",
      "Operator 273: 1.5323942711075318e-07\n",
      "Operator 275: -1.6586891721815782\n",
      "Operator 276: 0.48803411071612\n",
      "Operator 277: 1.1085542632548075e-06\n",
      "Operator 279: -1.821367108175761\n",
      "Operator 280: -1.8213672380007637\n",
      "Operator 282: -1.0625700929807513e-07\n",
      "Operator 283: 0.4880338540025866\n",
      "Operator 284: -1.8213672380001302\n",
      "Operator 286: -2.000000000000028\n",
      "Operator 288: -1.821367150092809\n",
      "Operator 290: 1.532394274861792e-07\n",
      "Operator 291: 0.4880338972512469\n",
      "Operator 292: -1.6586891095439817\n",
      "Operator 294: 1.1085542647518681e-06\n",
      "Operator 295: 0.4880341107161131\n",
      "Operator 296: -1.8213671769550386\n",
      "Total gradient norm: 20.014312001920338\n",
      "Operators under consideration (1):\n",
      "[286]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.000000000000028)]\n",
      "Operator(s) added to ansatz: [286]\n",
      "Gradients: [np.float64(-2.000000000000028)]\n",
      "Initial energy: -31.392304845413513\n",
      "Optimizing energy with indices [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152, 64, 286]...\n",
      "Starting point: [np.float64(0.7853981720738253), np.float64(0.785398202736495), np.float64(0.7853981462135713), np.float64(0.7853981515479085), np.float64(0.7853981546854759), np.float64(0.7853981896520319), np.float64(-0.13089968710183622), np.float64(0.7853981327113339), np.float64(-0.7853981830387126), np.float64(0.7853981788169403), np.float64(-0.7853981555892513), np.float64(-0.1699184342908875), np.float64(-0.13089968710247965), np.float64(0.13089970421220545), np.float64(-0.1699184895801274), np.float64(-0.13089970421382432), np.float64(0.16991852555116838), np.float64(-0.13089969969396087), np.float64(0.13089969575441335), np.float64(0.0)]\n",
      "         Current function value: -31.515410\n",
      "         Iterations: 10\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 56\n",
      "\n",
      "Current energy: -31.515410471031412\n",
      "(change of -0.1231056256178995)\n",
      "Current ansatz: [226, 261, 224, 222, 220, 218, 213, 216, 16, 212, 2, 228, 104, 290, 254, 112, 294, 152, 64, 286]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 5 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(20):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "23\n",
      "30\n",
      "37\n",
      "44\n",
      "51\n",
      "58\n",
      "63\n",
      "70\n",
      "75\n",
      "82\n",
      "89\n",
      "96\n",
      "103\n",
      "110\n",
      "117\n",
      "124\n",
      "131\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a9b18ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_per_circuit = 10_000\n",
    "num_shots = len(circuits) * shots_per_circuit\n",
    "sampler = Sampler(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'x': 9, 'cx': 6, 'rx': 2, 'h': 2, 'barrier': 2, 'rz': 1})\n",
      "On circuit 1/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'cx': 10, 'x': 9, 'rx': 4, 'h': 4, 'barrier': 3, 'rz': 2})\n",
      "On circuit 2/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'cx': 16, 'x': 9, 'rx': 6, 'h': 6, 'barrier': 4, 'rz': 3})\n",
      "On circuit 3/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 22, 'measure': 20, 'x': 9, 'rx': 8, 'h': 8, 'barrier': 5, 'rz': 4})\n",
      "On circuit 4/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 28, 'measure': 20, 'rx': 10, 'h': 10, 'x': 9, 'barrier': 6, 'rz': 5})\n",
      "On circuit 5/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 34, 'measure': 20, 'rx': 12, 'h': 12, 'x': 9, 'barrier': 7, 'rz': 6})\n",
      "On circuit 6/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 40, 'measure': 20, 'rx': 14, 'h': 14, 'x': 9, 'barrier': 8, 'rz': 7})\n",
      "On circuit 7/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 46, 'measure': 20, 'rx': 16, 'h': 16, 'x': 9, 'barrier': 9, 'rz': 8})\n",
      "On circuit 8/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 48, 'measure': 20, 'rx': 18, 'h': 18, 'barrier': 10, 'x': 9, 'rz': 9})\n",
      "On circuit 9/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 54, 'rx': 20, 'h': 20, 'measure': 20, 'barrier': 11, 'rz': 10, 'x': 9})\n",
      "On circuit 10/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 36, 'measure': 20, 'rx': 16, 'barrier': 12, 'unitary': 10, 'x': 9, 'h': 4, 'rz': 1})\n",
      "On circuit 11/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 58, 'rx': 24, 'h': 22, 'measure': 20, 'barrier': 13, 'rz': 11, 'x': 9, 'unitary': 1})\n",
      "On circuit 12/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 60, 'rx': 26, 'h': 22, 'measure': 20, 'barrier': 14, 'rz': 11, 'x': 9, 'unitary': 2})\n",
      "On circuit 13/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 48, 'rx': 22, 'measure': 20, 'barrier': 15, 'unitary': 10, 'h': 10, 'x': 9, 'rz': 4})\n",
      "On circuit 14/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 70, 'rx': 30, 'h': 28, 'measure': 20, 'barrier': 16, 'rz': 14, 'x': 9, 'unitary': 1})\n",
      "On circuit 15/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 68, 'rx': 32, 'h': 24, 'measure': 20, 'barrier': 17, 'rz': 12, 'x': 9, 'unitary': 4})\n",
      "On circuit 16/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 64, 'rx': 28, 'h': 20, 'measure': 20, 'barrier': 18, 'x': 9, 'rz': 9, 'unitary': 8})\n",
      "On circuit 17/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 78, 'rx': 36, 'h': 30, 'measure': 20, 'barrier': 19, 'rz': 15, 'x': 9, 'unitary': 3})\n",
      "On circuit 18/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 82, 'rx': 36, 'h': 32, 'barrier': 20, 'measure': 20, 'rz': 16, 'x': 9, 'unitary': 3})\n",
      "On circuit 19/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 78, 'rx': 36, 'h': 26, 'barrier': 21, 'measure': 20, 'rz': 13, 'x': 9, 'unitary': 7})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sim.run(to_run)\n",
    "    # counts = job.result().data()['counts']\n",
    "    # bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    # counts1 = bit_array.get_counts()\n",
    "    # num_shots = (i+1) * shots_per_circuit\n",
    "    job = sampler.run((circuit,), shots=num_shots)\n",
    "    data = job.result()[0].data\n",
    "    bit_array = data['meas']\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3bae56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_combine_bit_arrays(bit_arrays: List[BitArray], total_shots: int) -> BitArray:\n",
    "    \"\"\"Combine several bit arrays by choosing the same number of shots from each of them.\n",
    "    Choose the shots randomly from each one.\"\"\"\n",
    "\n",
    "    shots_per_circuit = [total_shots // len(bit_arrays)] * len(bit_arrays)\n",
    "    difference = total_shots - sum(shots_per_circuit)\n",
    "    i = 0\n",
    "    while i < difference:\n",
    "        shots_per_circuit[i] += 1\n",
    "        i += 1\n",
    "    assert sum(shots_per_circuit) == total_shots\n",
    "\n",
    "    random_bit_matrices: List[np.ndarray] = []\n",
    "    num_bits = bit_arrays[0].num_bits\n",
    "    for i, bit_array in enumerate(bit_arrays):\n",
    "        assert bit_array.num_shots >= shots_per_circuit[i]\n",
    "        assert bit_array.num_bits == num_bits\n",
    "        bit_matrix = bit_array.to_bool_array()\n",
    "        random_inds = random.sample(list(range(bit_matrix.shape[0])), shots_per_circuit[i])\n",
    "        random_bit_matrix = bit_matrix[random_inds, :]\n",
    "        random_bit_matrices.append(random_bit_matrix.copy())\n",
    "    total_random_bits = np.vstack(random_bit_matrices)\n",
    "    assert total_random_bits.shape[0] == total_shots\n",
    "    return BitArray.from_bool_array(total_random_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ac0f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitArray(<shape=(), num_shots=10, num_bits=20>)\n"
     ]
    }
   ],
   "source": [
    "print(randomly_combine_bit_arrays([bit_arrays[0], bit_arrays[1]], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    # all_counts = collections.Counter()\n",
    "    # tuple_of_counts = tuple(counts_list[:i])\n",
    "    # assert len(tuple_of_counts) == i\n",
    "    # for counts in tuple_of_counts:\n",
    "    #     for bitstring, count in counts.items():\n",
    "    #         all_counts[bitstring] += count\n",
    "\n",
    "    # bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_array = randomly_combine_bit_arrays(bit_arrays[:i], num_shots)\n",
    "\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQo9JREFUeJzt3Qd4FNXex/H/piekUQMhCQERUFroTWlSLFhBBFQQFMu1YEewIFcv2AuiohcLIopguQq8KNIUBFFaqKEGCCRAAqT3ZN/nnCUxgQRSdrM7u9/P88yzM7uzmzMzKb+cOcVkNpvNAgAAYFBu9i4AAABAdRBmAACAoRFmAACAoRFmAACAoRFmAACAoRFmAACAoRFmAACAoXmIkyssLJT4+HgJCAgQk8lk7+IAAIAKUMPgpaWlSWhoqLi5ubl2mFFBJjw83N7FAAAAVRAXFydhYWGuHWZUjUzRyQgMDLR3cQAAQAWkpqbqyoiiv+MOHWZyc3PlhRdekDfeeEP2798vkZGRZe735JNPyptvvimxsbHl7lOWoltLKsgQZgAAMJaKNBGxawPgQ4cOSZ8+fSQhIUEKCgrK3W/r1q0yZ86cGi0bAAAwBruGmfT0dJk7d66MHTv2gg14H3zwQZkyZUqNlg0AABiDXW8ztWnTRj8ePXq03H1mzpwpV155ZfG+AAAADtVm5kKOHTsmn3zyiaxfv17++uuvCr0nJydHLyUbEAEAAOfl0IPmPfzwwzJ9+nTx8/Or8HvU/kFBQcUL3bIBAHBuDhtmfvrpJ/Hw8JBrr722Uu+bNGmSpKSkFC+qSzYAAHBeDnubacmSJbq3U9++ffV2cnKyfhwxYoT4+PjI4sWLxd/f/7z3eXt76wUAALgGhw0zH330Uant1atXS79+/WT+/PmVGmcGAAA4N4e9zQQAAODwNTNq9N9BgwaVuoWkGuwuXLiw1H7q+ZiYmOL17t27yzvvvGOXMgMAAMdiMqtpKZ2Y6pqtejWpxsBMZwAAgPP9/eY2EwAAMDTCTDWsijkp+QWF1rsaAACg0ggzVfTWsj0y9vO/5d+Ld1X1IwAAgBUQZqqodeMgUbOSf7H+sHyx/pA1rgUAAKgCwkwVDW7dUJ4e3EqvT120S37bm1jVjwIAANVAmKmG+/s0k6Edw6Sg0CwPzdss+06kVefjAABAFRBmqsFkMsm0W9pI18g6kpaTL3fP2SinM3L5RgQAoAYRZqrJ28NdZt3ZSSLq+MmR05ly/9xNkpNfYJ2rAwAALoowYwV1annJJ2M6S4C3h/x16LQ8+8MOcfKxCAEAcBiEGSu5NCRAZt7eUdxMIt9uOiof/X7QWh8NAAAugDBjRX1a1Jcp17fW66/+HCO/7DxuzY8HAABlIMxY2ZiekXJn9yai7jI9On+r7DiWYu0vAQAASiDM2MCU6y+XKy+tJ1l5BTL+i41yMjXbFl8GAAAQZmzDw91NZo7qKJfUryUJKdk60GTn0cMJAABboGbGRoJ8PeWTMV0k2M9Too+myBMLo6WwkB5OAABYG2HGhiLr1ZJZd3QST3eTLNmWIO+s2GfLLwcAgEsizNhY92Z15T83t9XrM1bskx+3HrP1lwQAwKUQZmrA8M7hcl/vZnr9qW+3yeYjZ2riywIA4BIIMzXk6atbyYDLQiQ3v1Du/WKjHD2TWVNfGgAAp0aYqSHubiZ5d0SUXNYoUJLSc+WeORslPSe/pr48AABOizBTg2p5e8jsMZ2lnr+3xBxPkwlfb5ECejgBAFAthJka1jjYV/47upN4ebjJipiT8srS3TVdBAAAnAphxg46RNSWN25tr9f/uyZWvvn7iD2KAQCAUyDM2MkN7UNlwlWX6vVnf9gh6w+csldRAAAwNMKMHT064FK5vn2o5Bea5YF5myQ2KcOexQEAwJAIM3ZkMpnk9WHtJCo8WJIz8+Tuz/+WlMw8exYJAADDIczYmY+nu3w8upOEBvnIwaQM+ddXmySvoNDexQIAwDAIMw6gQYCPzB7TRfy83OWP/afkxZ92itnMpJQAAFQEYcZBXB4aKO+O6CAmk8i8DUfk83WH7F0kAAAMgTDjQAZeHiLPXN1Kr7+0eJes2nPS3kUCAMDhEWYczL29m8mtncJEDQz88FdbZO+JNHsXCQAAh0aYccAeTv+5ua10bVpHz9007vO/5VR6jr2LBQCAwyLMOCA11cGsOzpJRB0/OXomSx6Yt5keTgAAlIMw46Dq1PKST+/qLP7eHvJX7Gl5dWmMvYsEAIBDIsw4sOYNAuSNW9vp9dlrY2VRdLy9iwQAgMMhzDi4q9s0kvv7XKLXJ363jQbBAACcgzBjAE8OaiE9L6krmbkFct/cTZKazZQHAAAUIcwYgIe7m7w3soOe8kBNRvnEgmgpVH23AQAAYcYo6vp7y4d3dBIvdzf5ddcJ+fC3A/YuEgAADoGaGQNpHx4sU29srdffXLZH1uxLtHeRAACwO8KMwYzsGiG3dQ7XIwQ/8vUWOXom095FAgDArggzBqRqZ9o2DpIzmXnywJebJTuvwN5FAgDAbggzBuTj6S4f3tFRavt5yvZjKfLCjzvEbKZBMADANRFmDCqstp/MGNlB3EwiCzYela//irN3kQAAsAvCjIFdeWl9eWJQS73+4k87ZWtcsr2LBABAjSPMGNy/+l4igy4PkdyCQvnXl5uYYRsA4HIIMwZnMpnkjeHtpVm9WhKfki0Pf71F8gsK7V0sAABqDGHGCQT6eMqsOzuJn5e7rDtwSl5ftsfeRQIAoMYQZpxEi5AAeW2YZYbtj347KEu3J9i7SAAAuEaYyc3NlWeeeUY8PDzk0KFDxc/n5+fL7NmzpV+/ftK/f3/p1KmT3HPPPZKUlGTX8jqyIe1C5Z4rmur1JxdGy/6T6fYuEgAAzh1mVHjp06ePJCQkSEFB6YHfjh8/Lg8//LC8++67snLlSlm3bp3ExsbKsGHD7FZeI3jmmlbSrWkdydAzbG+U9Jx8excJAADnDTPp6ekyd+5cGTt27HmveXl5ybhx46RdO8utE29vb3nggQfkt99+0+EH5c+wPXNUR2kY6CMHEjPkqYXRDKgHAHBqdg0zbdq0kebNm5f5WoMGDeT9998v9ZyPj49+zMnJqZHyGVX9AG95//aO4ulukqU7jsvHvx+0d5EAAHDeNjOVsX79eunSpYtERkaWu48KOqmpqaUWV9SpSW15Ycjlev3Vn2Nk3X7aGgEAnJNhwoxq+PvJJ5/IzJkzL7jf9OnTJSgoqHgJDw8XV3VH9yZyS8fGeobth77eIvHJWfYuEgAArhlmVM+mkSNHyssvvyxdu3a94L6TJk2SlJSU4iUuLs6lB9SbdnNbubxRoJzOyJUH5m2WnHxm2AYAOBeHDzOFhYUyZswYGTBggO6afTGqoXBgYGCpxdVn2P7ozk4S5Osp0XHJMnXRLnsXCQAA1wozDz74oERERMjEiRP19vLly+XgQRq0VkZ4HT95Z0SUmEwiX204Igs2um5tFQDA+Th0mFGD6cXExMjQoUNl48aNelmwYIEcOXLE3kUznH4tG8ijV7XQ68/9b4dsP5pi7yIBAGAVJrPZbBY7jv47aNAgSU5OlujoaOnWrZtusLtw4ULZuXOn7rpdllWrVknfvn0r9DVUbybVEFi1n3H1W06FhWYZ/8VGWRFzUhoH+8rih6+Q2rW87F0sAACq9ffbrmGmJhBmSkvJypMbZq6Vw6cy5cpL68nnY7uKu5vJTlcHAIDq//126NtMsD7VEHjWHZ3Ex9NN1uxLkrd/3ctpBgAYGmHGBV3WKFBeucUyTcTMVftl2c7j9i4SAABVRphxUTd1aCx39bSMpPzEgmg5mMgM2wAAYyLMuLDJ114mnZvUlrScfHngy82SlcuAegAA4yHMuDAvDzf54PaOUs/fW/acSJMpP+2wd5EAAKg0woyLaxDoIzPODqi3YONRWciAegAAgyHMQHo2ryePDbAMqPf8jztkz/E0zgoAwDAIM9Ae6tdcjzuTnVcoD8zbJBk5+ZwZAIAhEGZg+UZwM8k7t0VJw0AfOZiYIZN/2C5OPp4iAMBJEGZQrK6/t7w3qoMeEfjHrfHy1V/MgQUAcHyEGZTSJbKOPD24pV6fumiX7DjGhJQAAMdGmMF5xl/ZTAZc1kBy8wvlwa82S2p2HmcJAOCwCDM4/5vCzSRv3Npez6ytJqSc+O022s8AABwWYQZlCvbzkvdv7yie7iZZuuO4fL7uEGcKAOCQCDMoV1R4sJ7yQJn2f7tly5EznC0AgMMhzOCC1GSU17RpKHkFZnnoqy2SnJnLGQMAOBTCDC7IZDLJq8PaSZO6fnIsOUvPsF1YyPgzAADHQZjBRQX6eMr7ozrqiSlXxJyUj9cc5KwBABwGYQYV0qZxkLx4fWu9/vove+Sv2NOcOQCAQyDMoMJGdg2Xm6JCpaDQLA9/vVmS0nM4ewAAuyPMoFLtZ/5zc1u5pH4tOZGaI4/O36qDDQAA9kSYQaXU8vaQD+/oJD6ebrJ2f5LMXLmfMwgAsCvCDCqtRUiAvHxTW73+zoq98sf+JM4iAMBuCDOokmGdwuS2zuFiNotMmL9FTqZmcyYBAHZBmEGVTb2xtbRqGCBJ6bny8NdbJL+gkLMJAKhxhBlUmY+nu3xwe0ep5eUuG2JPy9vL93I2AQA1jjCDamlW319eGdpOr7+/6oCs2nOSMwoAqFGEGVTb9e1D5c7uTfT6Y99slfjkLM4qAKDGEGZgFc8NuUzaNg6S5Mw8efCrzZKbT/sZAEDNIMzAKrw93PX8TQE+HrLlSLK89nMMZxYAUCMIM7CaiLp+8sat7fX67LWx8vOO45xdAIDNEWZgVYNbN5R7rmiq15/6NlqOnMrkDAMAbIowA6ubeE0r6RgRLGnZ+fKvrzZJdl4BZxkAYDOEGVidp7ubzBzVUWr7ecqOY6nynyW7OcsAAJshzMAmQoN95a3bovT63D8Py6LoeM40AMAmCDOwmX4tG8iD/S7R6898t00OJqZztgEAVkeYgU09NqCFdGtaRzJyC+Rf8zbTfgYAYHWEGdiUh7ubvDeyg9Tz95KY42ny0uJdnHEAgFURZmBzDQJ95K3hUWIyiczbcEQWb6P9DADAeggzqBG9W9SXf/Utaj+zXQ6fyuDMAwCsgjCDGm0/0yWytqTn5Ov5m3LyGX8GAFB9hBnUaPuZGSM7FI8/M/3/mL8JAFB9hBnUqEZBvvLmcMv8TZ+vOyS/7GT+JgBA9RBmUOP6twqR8Veenb9pYbTEnWb+JgBA1RFmYBdPDW4lUeHBkpqdLw9/vUXyCgq5EgCAKiHMwC68PCzjzwT6eMjWuGR5/Zc9XAkAQJUQZmA34XX85LVhlvYzH/9+UFbGnOBqAAAqjTADu7q6TUO5q2ekXn9iQbQkpGRxRQAAlUKYgd1NuraVtGkcKGcy8+SRr7dIPu1nAABGCjO5ubnyzDPPiIeHhxw6dOi81z/66CPp1KmT9OrVS6677jo5duyYXcoJ2/H2cJeZIzuKv7eH/H3ojLyzfB+nGwBgjDCjwkufPn0kISFBCgrOHw32+++/l6lTp8ovv/wif/zxh3Tr1k2GDBkihYX0fHE2kfVqybRb2ur191fvlzX7Eu1dJACAQdg1zKSnp8vcuXNl7NixZb7+8ssvy5gxY6RevXp6e8KECbJjxw5ZsmRJDZcUNeGG9qEysmuEmM0ij32zVU6mZnPiAQCOHWbatGkjzZs3L/O106dPy5YtW6Rz587FzwUFBUmLFi1k+fLlNVhK1KQp118urRoGSFJ6rjz6zVYpKDRzAQAAjt1mpjyxsbH6MSQkpNTzDRs2LH6tLDk5OZKamlpqgXH4eLrLzFEdxc/LXdYdOCUzV+63d5EAAA7OYcNMZqZliHtvb+9Sz6vtotfKMn36dF2DU7SEh4fbvKywruYN/OXlm9ro9XdX7JX1B05xigEAxgszfn5+xTUtJantotfKMmnSJElJSSle4uLibF5WWN8tHcNkWKcwUXeZJszfIknppb8PAABw+DDTrFkz/XjiROlRYY8fP178WllUzU1gYGCpBcb07xtb61qak2k58viCaCmk/QwAwEhhpnbt2tKhQwfZtGlT8XOq/cvevXtlwIABdi0baoafl4e8P6qjeHu4ye97E+Wj3w9y6gEAxgkzynPPPSdz5syRU6csbSZmzJihe0Bde+219i4aakjLhgEy9YbWev2NZXtk46HTnHsAQCkeYufRfwcNGiTJycl6e8SIEbrB7sKFC/X2LbfcIidPnpSBAweKj4+Prq1ZtGiRuLk5dAaDld3WJVz3bPopOl5Pd7DkkSuldi0vzjMAQDOZzWqIMuelbk2pXk2qMTDtZ4wrPSdfhsxYI4dOZcqAyxrIf0d3FpPJZO9iAQAc4O83VRwwBDVvkxp/xsvdTZbvPimfrC1/rCEAgGshzMAw2jQOkueHXKbXX/05RrbGWW5PAgBcG2EGhnJH9yZyTZuGkldgloe+2iwpWXn2LhIAwM4IMzAU1U7mlaHtJLyOrxw9kyXPfLdNnLzZFwDA2mFm27ZtsnPnzsq+DbCaIF9PmTmyo3i6m2TpjuPy5Z+HObsA4MIqHWaioqLk7bfftk1pgApqHx4sE69upddfWrxbdhxL4dwBgIuqdJi54oorZPbs2bYpDVAJd1/RVAZcFiK5BYW6/Yzqvg0AcD2VDjNqBN74+PgyX7vhhhusUSagwu1n3ri1nYQG+ejxZyZ/v532MwDggio9AnBAQID07NlTrrrqKgkLCxN3d/fi13bs2GHt8gEXFOznJe+N6iDDP/pTjxDcrVkdub1bE84aALiQSo8ArKYUUO1myhIdHS2nTzvW3DmMAOwaPlx9QI8942YSeWdEB7mhfai9iwQAqKG/3x5VaTOj5kcqy8iRIyv7cYBV3Ne7mRxKypBvNsbJo/O36OcINADgGpibCU6jsNAsz3y/TRZsPEoNDQAYnE1rZpTDhw/Lm2++Kdu3b9fbbdu2lSeeeEKaNKGtAuzHzc0kr9zSTq+rQKNqaNRUlNdzywkAnFqlezOtXr1aWrVqJWvWrJF69erpZe3atXLZZZfJb7/9ZptSApUMNLd2CpNCs8iE+VtkUXTZve8AAM6h0jUzkydPlp9++kkGDhxY6vnly5fLM888I+vXr7dm+YAqBZpXh1pqaBZuOqoDjUINDQA4p0rXzKjOT+cGGWXAgAGM8QGHCzRFNTSPfrOVGhoAcFKVDjMZGRmSlJR03vOJiYmSmZlprXIBVg00BYVmAg0AOKlK32YaM2aMdOrUScaOHSuXXHKJfm7//v0yZ84ceeSRR2xRRqDagUYNpvTtpqM60JhMIkPaMQ4NALhsmFG9ltQowNOmTZMjR47o5yIiIuTZZ5+V8ePH26KMgNXa0KhAM2H+Vr1OoAEAFx1nRvX7VnPiqECTnp6un/P39xdHxQjAKKJuNU38bpsONO5uJpkxooNc164RJwgADP73u9JtZoKDg2Xo0KHFIcaRgwxQkvvZGpqhHS1taB6Zv0WWbEvgJAGAq91m6tKliyxbtsw2pQFqINC8Nsxyy+m7zUd1oFGooQEA46p0zUzLli0lLS2tzNfuvfdea5QJqJFAc0vHxtTQAIAr1sy0a9dO+vbtKzfddJOEhYWJu7t78WtqJGDAKIHm9WHt9fr3m4/pGhrVy+natrShAQCnbwDs6+srDRs2LPO1EydOONxYMzQAxoWotjNPfRutA40KOO+N7ECgAQBnn2iye/fusmrVqjJf69evX2U/DnCMGhqzyPdbjsnDX1va0FBDAwBO3Gbmnnvukf/7v/8r87XyQg7g8IHm1vZySwdLGxoVaJZup5cTADhtmFEj/27atMk2pQEcJNA8RKABAOcNM71795bnn3++zNccrb0MUJVAczOBBgCcO8yocWa2b99e5mtDhgyxRpkAuwaaN0oEGm45AYDjq3QD4Pj4eN01Oyoq6ryu2TExMdYuH2C3QKP8cLZR8EyTyNVt6LYNAE5RM6NG/73hhhv05JJubm6ienYXLYCzBZqbokIlX7Wh+WqL/LyDRsEA4BQ1M+pW0n//+98yX3vsscesUSbAYQLNm8Oj9Pr/tsbrQDNzFDU0AGD4QfOMhkHzUF2q7czjC7bKj1vjxcPNJDNHdZSr25Q9cCQAwACzZivffPON9OnTR3r16qW3X3rpJZk7d27VSgsYoIbmreFRcuPZW06PfrNF9p4oe34yAEDNq3SY+eijj+TJJ5+U9u3bS1ZWln7ulltukR9++EHeffddW5QRcJhAc+Wl9SQ7r1D+NW+zZOTk27tYAICqhBlVAxMdHS0zZszQ1T9K69atdW3Nd999x0mFUweat2+LkgYB3rL/ZLo8/78dNHwHACOGGdWDqU6dOnrdpKYZPsvT01Nyc3OtWzrAwdTz95YZIzuIm8kyl9PCTUftXSQAcHmVDjM5OTmyY8eO855fvny5FBQUuPwJhfPr3qyuPDGopV5/4ccdsuc47WcAwFBh5sUXX9QzZ6uxZvbt26fnaurZs6fusj1t2jTblBJwMA/0uUR6t6h/tv3MJtrPAICRwsw111wjGzZs0LeaQkJC9NQGLVq0kC1btsjAgQNtU0rAwbip9jPD20tIoLccSMyQ52g/AwB2wzgzQDX8FXtaRny8XgrNIq8ObSu3dYngfAKAEcaZAWDRtWmdEu1ndkrM8VRODQDUMMIMYIX2M31a1JecfMv4M+mMPwMANYowA1T3h+js+DMNA33kYGKGPPvDdsafAYAaRJgBrKBOLS95b1QHPbCemsPpm7/jOK8A4Khhpnfv3rYpCWBwXSLryJNn289M+Wmn7E6g/QwAOGSY2bVrl3Tt2lWmTp0qhw8ftk2pAIO6r3cz6dfS0n7mQdrPAIBjhpm7775b1q1bJ+3atZMJEybI4MGD5csvv5Ts7GzblBAwWPuZN4dHSaMgHzmYlCGTv6f9DAA4XJh59dVXxcPDQ26++Wb53//+pyee3LhxozRq1Ejuu+8++fPPP61aQDV9wmOPPaZn6e7Tp49069ZNz9ANOHT7mZGW9jM/RcfL13/RfgYAHCrMLFy4UD/m5eXJggULZMyYMTJz5kypW7euNG7cWD777DO54oorZPXq1VYp4Msvv6xD0++//y6//fabzJo1S0aMGKFn7gYcVefIOvL0YEv7mRcX7ZRd8bSfAQBb8ajsG1RbmTVr1si8efP0LNnDhg2TlStXlmoYnJycLIMGDZK//vqr2gXcunWrdOnSRY8CqHTo0EGvq6+pamsARzX+ymayIfa0rIw5KQ9+tVl+eqiXBPh42rtYAOB0qtQAWNWKvPHGG3L8+HFdE3NuD6fdu3dLfHy8VQo4dOhQHZ6OHDmit3/55RdJTEzU80IBDt9+5tb2EhrkI7FJGTKJ9jMA4Bg1M6NGjdINfi9E1dh88MEHYg133XWXZGZm6gbHql3O3r17dW3Q8OHDy21jo5aSczsA9lJbjz/TUW77aL0s3pYg3ZvVlTu6N+GCAIA9a2aaNWt20X1UQ90bbrhBrGH27NnyyiuvyKZNm3SNz+bNm6V79+7i5lZ20adPn65vQxUt4eHhVikHUFWdmtSWiVe30uv/XrxLdhxL4WQCgD1nzW7atKmMGzeuzOHaPT09JTIyUq655hoJDg6uduHU11ANi5944gl59tlni5+/6qqrpF+/fvLcc89VqGZGBZqKzLoJ2Ir6Xh7/xUZZvvukRNb1k0UPX0H7GQCw0qzZlQ4zffv2lT/++EPf8omIiBCTyaTbs5w6dUo6d+4sCQkJcubMGd22RTXWrY6TJ0/qtjHqttbtt99e/Pw999yja2q2bNli1ZMB2FJyZq5cN2OtHEvOkuvaNZKZIzvonx8AQPX+flf6NlOPHj3k66+/1gFm7dq1unGuGgl4zpw5cvXVV8uePXt0+HjqqaekuurVqyfe3t46IJWktv38/Kr9+UBNCvazzN/k4WaSJdsS5MsNlkbtAIDqqXSYUd2tVQPcsnodqe7SiuqWrRoBV5dqF6PGsVHtZlRtj6LazPz666/lNgAGHFnHiNryzDWW9jMvLaL9DADYJcwcOHBAjyNzrtOnT+taGWt7++23dWNi1U5GDcY3duxY3SD4kUcesfrXAmrC3Vc0lQGXhUhuQaEefyY1O48TDwA12TX7+uuvl06dOukaE9UYWDl48KB88cUXeooDNTKw6lGkbg9Zg7qd9Nprr1nlswBHoNrJqPFnrp2xRg6fypRJ322XmaNoPwMANRZm3nnnHT1twXvvvVfclkU1BlY1JU8++aRkZWXpHkgq0AAoW5Cfp7x/e0e5ddY6WbI9Qbr9WUdG94jkdAFAFVS6N5NqXaz+swwICCgekM6RewnRmwmO7JO1sfLS4l3i5e4m3z3QU9qGWabtAABXl2rL3kxq/BjV2FdRH+7IQQZwdON6Rcqgy2k/AwDVUekwoyZ9XLZsWbW+KAALVcv5+rD2ElbbV46czpSJ324rc0BKAIAVw0zLli0lLS2tzNfuvffeyn4c4PJ0+5lRHcXT3SRLdxyXL9YfdvlzAgA2bQCsJnxUowDfdNNNEhYWJu7u7sWvqUH0AFRe+/BgmXztZTJ10S55ecku6RARLO3Cqj8lCAC4gko3APb19ZWGDRuW+dqJEyf0DNeOhAbAMAr1o/jAl5vl553HpVn9WvLrY33E3Y3pDgC4ptRKNACudM2MmrF61apVZb6mJn8EUPX2M68OayfrD56Sg4kZsjLmpAy8PITTCQDWbjOzePHicl8rL+QAqJggX08Z2TVCr3+6NpbTBgC2CDO1atWSuLg4mTJlijz++OP6uR9++EH27dtX2Y8CUIbRPZro20uqhmZnfArnCACsHWZUI1/Vo0kFmJ9//lk/p6YwUFMZrFixorIfB+AcocG+ck0bS7u0z/44xPkBAGuHmeeff16Hlm3btklIiOV+vprBWt1i+s9//lPZjwNQhnFXWOY9+2lrvCSm5XCOAMCaYUb1uOjRo0dxg8Ui9evXl4KCgsp+HIAydIyoLVHhwXpm7XkbGHcGAKwaZlQXqbIGzVPtaJKSkir7cQAuUjvz5Z+HJSeffxQAwGphZtSoUdKtWzd56623JDExUb744guZPHmy7rI9fvz4yn4cgHKodjONgnwkKT1XFkVbZqgHAFhh0Dzl448/lmnTpsmRI0f0dkREhDz77LMOGWYYNA9G9uHqA/LqzzFyeaNAWfLIFaVu7QKAM0utxKB5VQozRdLT0/Wjv7+/OCrCDIwsOTNXuk9fIdl5hTL/3u7SvVldexcJABzu73elbzOVpEJMySDz1FNPVefjAJwj2M9LhnYM0+sMogcAVprOQI0p89VXX8nWrVt1aipZsaPGnXn99dcr+5EALmBsr0iZt+GI/Lr7hBw+lSFN6tbifAFAdWpmxowZI88995xuL6O6YqswU7QAsL7mDQKkT4v6on7EPl/HIHoAUO2aGVUjo6Yu8PHxOe811asJgG26af+2N1EWbjwqjw9sIQE+npxmAKhqzUyrVq3KDDLK6NGjK/txACqg96X1pHkDf0nPyZcFG49yzgCgOmFmxIgR8tBDD8m6deskNjZW324qWsaNG1fZjwNQAapLtmo7o3y+LlYKCrmtCwBV7prt5vZP/ik55oX6GLXtaFMa0DUbziIrt0B6vLJCkjPz5KM7O8ng1pbJKAHAGdm0a7Ya/VfVyKjl4MGDpZauXbtWp9wALsDXy11GdY3Q63TTBoBqNAB+4403pEmTJmW+NmvWrMp+HIBKuLNHE/n494OyIfa07DiWIm0aB3H+ALi8StfM9OrVq9zX2rdv7/InFLClRkG+cm3bRnr9sz/opg0AFQ4zTZs2lWbNmsmaNWvKfH3BggV6Hz8/P84qUEOzaS+KjpeTadmcbwAur0K3mSIjI2XVqlV6ferUqaUa/r7wwgsyfPhwvfTo0cPlTyhga1HhwdIxIlg2H0mWL/88osedAQBXVqGamZLhRQUb1WZm/vz5er28/QDYvnZm3p+HJTvPsXoQAoAhpjNQS0hICIPkAXZydeuGEhrkI6cycuWn6HiuAwCXVuVZs6mFAezHw91NRveMLO6mzdxoAFxZhdrMJCQkyNy5c0v9wjx+/Ph5zyUmJtqmlADOM7JLhLy7fJ/EHE+T9QdPSc9L6nGWALikCo0AXHLU3wt+GCMAAzXq+f/tkLl/HpYBl4XI7DGdOfsAnIbVRwDu06ePFBYWXnRhBGCgZt11dr6mFTEn5FBSBqcfgEuqUJh57bXXKvRh77zzTnXLA6ASLqnvL/1a1hdVv/r5OgbRA+CaKhRmunTpUuF5mwDYp5v2go1xkpKVx+kH4HKq3JsJgGO4onk9ubSBv2TmFsjCjXH2Lg4A1DjCDGBwquF9Ue2Mmq8pv6DQ3kUCgBpFmAGcwM0dGkttP085lpwly3efsHdxAKBGEWYAJ+Dj6S6jukXo9U/X0hAYgGshzABOYnSPSPFwM8lfh07L9qMp9i4OANQYwgzgJEICfWRIu0Z6/bM/Yu1dHACoMYQZwIkUNQRetC1eTqZm27s4AFAjCDOAE2kXFiydm9SWvAKznuYAAFwBYQZw0tqZeRuOSHZegb2LAwA2R5gBnMygy0OkcbCvnM7IlR+3HrN3cQDA5ggzgJPxcHeTMT2bFHfTNquJmwDAiRFmACd0W+cI8fNylz0n0mTdgVP2Lg4A2JQhwszBgwdl6NCh0q9fP2ndurV0795dNm7caO9iAQ4ryM9ThnUK0+ufrqWbNgDn5vBhJjExUa666iqZMGGCrFq1SqKjo8XPz0/2799v76IBDm1sL0tD4BUxJyU2KcPexQEA1w0zr776qvTo0UN69+6ttz08POTjjz8u3gZQtqb1aslVrRro9c8ZRA+AE3P4MPP999+fF1yaN28uoaGhdisTYLRu2gs3HZWUrDx7FwcAXC/MZGRkSGxsrBQUFMjtt98uvXr1ksGDB8vSpUvLfU9OTo6kpqaWWgBX1fOSutIyJEAycwvkm7+P2Ls4AOB6YSY5OVk/Pv/88/L000/LH3/8oR+vv/56+fXXX8t8z/Tp0yUoKKh4CQ8Pr+FSA47DZDLJuCsi9fqcdYclv6DQ3kUCANcKM+7u7vpRhZf27dvrddUYuH///vLuu++W+Z5JkyZJSkpK8RIXF1ejZQYczY1RjaVOLS85lpwly3adsHdxAMC1wkz9+vXF29tbGjduXOr5Jk2a6NtPZVH7BwYGlloAV+bj6S63d4vQ63TTBuCMHL5mRrWTSUhIKPX8iRMnJCLC8ssZwMXd0b2JeLqbZOPhMxIdZ7l9CwDOwqHDjDJx4kT58ccf5cgRS+PFXbt2ybJly+TBBx+0d9EAwwgJ9JEh7Sw9AD+jmzYAJ+MhDm7QoEEyY8YMufHGG8Xf31/y8/Nlzpw5MmTIEHsXDTCUcb2ayg9bjsnibQky6drLdMABAGdgMjv5LHSqa7bq1aQaA9N+Bq5u+Kz18teh0/JQv+by5OCW9i4OAFjl77fD32YCYD1F3bTnbTgs2XkFnFoAToEwA7iQgZc3lLDavnImM0/fcgIAZ0CYAVyIu5tJ7uppqZ2Z9dsBSUrPsXeRAKDaCDOAixneJVzqB3jL4VOZMvyj9RKfnGXvIgFAtRBmABcT6OMp8+/tLqFBPnIwMUNunbVeYpMy7F0sAKgywgzggi6p7y8LH+gpzerV0tMc3DprneyKZ1JWAMZEmAFcVONgX1lwfw+5vFGgJKXnyoiP18umw6ftXSwAqDTCDODC6vl7y9f3dpfOTWpLana+3DH7L/l9b6K9iwUAlUKYAVxckK+nzL27m/RuUV+y8grk7jl/y9LtpedDAwBHRpgBIL5e7jJ7dGe5rm0jySswy4NfbZYFG+M4MwAMgTADQPPycJMZIzvIbZ3DpdAs8vS32+STtbGcHQAOjzADoNSgeq8MbSvjr2yqt19avEve+nWvOPkUbgAMjjADoBSTySSTr71MnhzUQm/PWLFPpi7aJYWqugYAHBBhBkCZgeah/pfKv29srbc/X3dInvp2m+QXFHK2ADgcwgyAco3uESlvDW+vbz99t/mo/GveZsnJZ7ZtAI6FMAPggm7pGCYf3t5RNxBetuuE3P35RsnIyeesAXAYhBkAFzWodUP5/K4u4uflLmv3J8kdn2yQ5MxczhwAh0CYAVAhPZvXk6/Gd5dgP0/ZciRZRnz8p5xMy+bsAbA7wgyACosKD5Zv7u0hDQK8JeZ4mp5xO+50JmcQgF0RZgBUSsuGAbLw/h4SXsdXDp/K1IFm/8k0ziIAuyHMAKi0JnVrybf395RLG/jL8dRsHWi2HU3mTAKwC8IMgCoJCfSRBff1kPZhQXImM09G/XeD/HnwFGcTQI0jzACostq1vGTe+O7So1ldSc/JlzGf/iUrY05wRgHUKMIMgGrx9/aQz8Z2kQGXhUhOfqHc+8Um+XHrMc4qgBpDmAFQbT6e7vLhHR3lpqhQyS80y6PfbJV5Gw5zZgHUCMIMAKvwdHeTt4ZHyegeTURNsv3sDzvk498PcHYB2BxhBoD1fqG4mWTqDa3loX7N9fb0pTGyMz6FMwzApggzAKw+4/aTg1vK9e1DdQ3NK0tjOMMAbIowA8Amnh7cUjzdTbJmX5Ks2ZfIWQZgM4QZADYRXsdP7uweqddV7UxhoZkzDcAmCDMAbOah/s0lwNtDdsanyk/R8ZxpADZBmAFgM3VqeckD/S7R66//skey8wo42wCsjjADwKbG9WoqDQN95Fhylsxdz9gzAKyPMAPA5gPqPT6ohV6fuWq/pGTmccYBWBVhBoDNDe0YJi1DAiQlK08+WL2fMw7AqggzAGzO3c0kz1zTSq9/tu6QvuUEANZCmAFQI/q2rC/dm9WR3PxCeXPZHs46AKshzACosZGBJ11zmV7/Ycsx2RWfypkHYBWEGQA1pn14sAxp18gyzcHPTHMAwDoIMwBq1FNnpzn4fW+irN2XxNkHUG2EGQA1qkndWnJH9yZ6ffrS3UxzAKDaCDMAatzD/S8tnuZg0TamOQBQPYQZAHaZ5uD+vpZpDl77eY/k5DPNAYCqI8wAsAumOQBgLYQZAHbh6+Uujw+0THPw3kqmOQBQdYQZAHYztFOYtAjxt0xz8BvTHACoGsIMAMeY5uAPpjkAUDWEGQB21a9lA+nW1DLNwVvL9nI1AFQaYQaA/ac5uNYyzcH3W47K7gSmOQDgxGFm5syZ+hff6tWr7V0UAFYUFR4s1xVNc7CUaQ4AOGmYiY+Pl9dff93exQBgI0+fnebgt72J8sd+pjkA4IRh5uGHH5bJkyfbuxgAbDjNwe3dmOYAgJOGmUWLFomnp6cMHjzY3kUBYEMP928u/t4esuMY0xwAcKIwk5GRIc8++6y8/fbbFdo/JydHUlNTSy0AjKGuv7fc36eZXn/9F6Y5AOAkYeb555+X+++/Xxo1alSh/adPny5BQUHFS3h4uM3LCMB67r6imYQEesvRM1kyd/1hTi0AY4eZzZs3y4YNG3SYqahJkyZJSkpK8RIXF2fTMgKw/jQHjw2wTHMwc9V+PTowAFyIhziwJUuWSFZWlvTv319vZ2dn68dHH31UgoODZfbs2dK8efNS7/H29tYLAOMa1ilMPlkbK/tOpsuHqw8UjxIMAGUxmc1qZAdjOHTokDRt2lRWrVolffv2rdB7VJsZdbtJ1dIEBgbavIwArGP5rhNyzxcbxdvDTVY92VdCg305tYALSa3E32+Hvs0EwHVddVkD6dq0juSoaQ5+ZZoDAE4QZtStpREjRpy3DsCJpzk4e3vpu81HJeY4PRMBOMFtpqrgNhNgbA/O2yxLtidI35b15fOxXe1dHAA1hNtMAJzGU4NbioebSVbvSZR1THMAwMi3mQC4psh6apqDCL0+fWmMFBY6dWUygCogzABweA9fdame5mD7sRRZvD3B3sUB4GAIMwAcXj1/b7mvd9E0BzGSk19g7yIBcCCEGQCGcPeVTaVBgLfEnc6SeX8esXdxADgQwgwAQ/Dz8pDHBlqmOXhv5T6mOQBQjDADwDBu7RQmzRv4y5nMPJn12wF7FweAgyDMADAMD3c3mXi1ZSC9T9fGSkJKlr2LBMABEGYAGMoANc1B5NlpDpYxzQEAwgwAA05z8My1THMA4B8eJdYBwBA6RtSWa9s2lP/bflye/nabDGnXSBoF+UposI80DPKVkABvfUsKgGsgzAAwpKcGt5JlO0/ItqMpeinJzSTSIMBHGgX7SGiQrzQKUuu+ElriUY1d46Z2BGB4TDQJwLDUXE2/70vSDYETkrMlPiVLTqRmS17Bxac88HQ3SUjg2bCja3T+CT6hwZbHOrW89G0tAI490SQ1MwAMq2fzenopSc3dlJSeI/Ep2ZKQnFX8mKAeVehJyS4OPEfPZOmlPN4ebjrU1PX3Fj8vd/H1dLc86nWPEuvu56x7lHpeLT7q0dOd21+ADRBmADgVdeuoQaCPXqLCg8vcJ7+gUE6m5ehwE5+cXepRhR21rgKR6jF16FSmXqzFy91NfDzddODRIadEEFLzT6nna3lbAlEtFYC8PcS/aLv4eQ/x8y7a3/KcO7fM4MIIMwBcjmocrG4lqaVTk7L3UfM/nUhRNTxZkpyZK5m5BXrJzis4Zz2/nOdL71M02XduQaFeUrPzrXpMKiDV0oHHEnCKH0sEn7q1vKVdWJBeVG0T4CwIMwBQBm8Pd4mo66eX6jKbzbqWJyu3QLLOBp1/1vP1eoZ+Ll8/ZuZYHjNySm5bQpN+Lkftrx7/CUnZeYWSnZcrpzJyK1Sm8Dq+0j4sWNdetQ8PljahQbp2CDAiwgwA2JhqRKxuJ6mlthU/tygkFYccHXAsAemfx38CkWofFH00WQ4kZugJO9WyeFuC/ix1m6pFSIBEhQfpkKMCzqUN/GnjA0OgNxMAuJjU7DzZfjRFtsYlS3Rcsn5UbYjOpRowt2kcWBxuVC1OWG1fenjB4XozEWYAAHI8JdsSbo5aAo4KO2k557frUd3V24cF6XCjl7Bg/RxgbYSZKp4MAMA/XdwPJmXoYFMUcHYlpJY5hk9EHb+zwSZIWjUMlCBfT/H3sfS+CvD21I2TGa8HhJkaSnYAgPKpHl67E9IsAUfdnjqaLAcTMy56ylR7HNXNPMDHU/eqUiHH38dTAorWvS3hx7LtUXrd20MCfP5Z9/JgmgpXkcqgeQAAW/TwUu1mSo7fk5JlaX+jam/UbarYpAxJz7Y0PE7PzRezWaSg0Ky7olujO7oKM4E+lm7mdf29dBfzurW8pJ6/WtRzlufrnX1ddU+nVsj50WYGAGCzW1WZeZaeVmnZ+ZKuu5WXXk8/u13WevGSna+7sVeFusWlgk+9AG+pV8vrnAB0Nvicfazj50XvLQdCzQwAwCFGY1a3htQSUs27/GrUZtXFXIWblMw8OZ2hxtTJkaT0XD1a8ym95EqSej5dPZ9zduydQjmWnKWXiqjt56nDToMAb91VXS0tG1oWdRxwTNTMAACckhpnJylNBRxL0NGBJ6Mo/JwNQ2mWRxWOigYgLI/qlt7qbLBp2TBQWoYESLP6tcTTnXY8tkBvpiqeDACAa1LtetS0FUVhR83Pted4quw5ka4fT6SePw5P0ezrl9T3L669UWFH1eY0DmY8nuriNhMAAJWgelxZGg9bbi+d60xGruw5kSZ7jqdJzHH1mCp7T6Tr215qWy0lqd5YLUoEHFWLo7ut+3lyXWyA20wAAFRxOgnVFuefgGNZDiSmS34596xCAr31LSoVcJrX95dgP0/dZV11Pw/08ZRAX0sbIzUZqqtLZQTgqp0MAACqKze/UA4mqdtT/wQcFXYq2ghZUV3KVcApCjr/BB5L6CnrtYBzXlO1TUbGbSYAAOxEjYWjbimppaS07DzZe8ISbPYeT9MjLKuxd9TzaWcfVe8rRU0eqpby2upURC0vdz0ac4NAH2kUpBZf/dgwyEdCg9Wjr+615QwNmLnNBACAA9XqWMbbyZPUrLOPpQJPifUcy2Nq1tnHs6+pmdQrymQSHWhUsGkUWDroWAKQj4QE+tgl8FAzAwCAQWt16nh4VWvyztz8wuLAk5yVpycRTUjJOvtoWVePJ1Kz9VxbqvZHLdEXCDxqYMHQs7U6qobH8vhPbY8KPPacaoIRgAAAcCJeHm7FPbO08PJHaFZd0YvCjQo78SVCz/GzS25BoSSm5egl+mhKmZ81ukcT+feNbcReCDMAALjoCM31A7z10i6s/MBzOjP3vFodHXySs+R4quV5VUNjT4QZAABQbuBRt5jU0qZxULld1Mvril5TCDMAAKDK1KzkaiRkezJ+fywAAODSCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQCDMAAMDQnH7WbDU1uZKammrvogAAgAoq+rtd9HfcpcNMWlqafgwPD7d3UQAAQBX+jgcFBV1wH5O5IpHHwAoLCyU+Pl4CAgLEZDJZPTWqkBQXFyeBgYHiKlz1uF352F31uBWO3fWuO9c83CGuuYonKsiEhoaKm5uba9fMqBMQFhZm06+hLri9L7o9uOpxu/Kxu+pxKxy76113rnmgvS/BRWtkitAAGAAAGBphBgAAGBphphq8vb1lypQp+tGVuOpxu/Kxu+pxKxy76113rvkUw11zp28ADAAAnBs1MwAAwNAIMwAAwNAIMwAAwNCcfpyZ6vrhhx9k2rRp4uPjo8es+eCDD6R169ZW298RLViwQGbPni0FBQV68KjIyEh5/fXX9WNZXnzxRfnf//4nwcHBxc/VqVNHvv/+ezGSqhyHM1xvpVWrVtKwYcNSzx09elQPVvX777+ft//nn38ur7zyynnvWbZsmXh5eYkjy83NlRdeeEHeeOMN2b9//3nf1x999JF8/PHH+pqq7wW13rhx4wt+ZlXe40jHnp+fr6/pvHnz9OCiKSkp0qFDB32N69WrV+7n3XXXXRITE6OPu8jll1+ufw6MdN2rehxGuO65FzhuVeaoqKhS+6t9+vfvL1988YWxft+rBsAo24YNG8wBAQHmvXv36u05c+aYGzdubE5NTbXK/o7K09PT/PPPP+v1goIC85133mlu2bKlOTs7u8z9p0yZYl61apXZ6Cp7HM5yvZU+ffqc99zQoUPNM2fOLHP/zz77TC9GExsba+7evbt59OjRquOD3i7pu+++Mzdq1MicmJiot6dOnWqOiorSPwflqcp7HO3Y4+LizD4+Pubo6Gi9rX7W+/fvX+b3RUljxow57xwa8bpX5TiMcN1jL3LcZV3fTp06mRcvXlzuZzrq73tuM12A+q/kuuuuk0svvVRv33HHHcX/wVhjf0d14403yuDBg/W6qm145JFHZM+ePbJ582Z7F82hOMv1Vj777LNS26dPn5Zff/1VRo0aJc4kPT1d5s6dK2PHji3z9ZdfflnGjBlTXBsxYcIE2bFjhyxZsqTcz6zKexzt2FVt2rhx46Rdu3Z6W3XLfeCBB+S3336ThIQEcfbrXhVGuO7pFznuc3/uVfnV9D9XX321GA1h5gJWrFghnTt3/udkublJp06dZPny5VbZ31EtXLiw1HZR1WtOTo6dSuSYnOV6K02bNi21/fXXX8s111wjtWvXFmfSpk0bad68eZmvqQC3ZcuWUtdUDaXeokWLcq9pVd7jiMfeoEEDef/995325/5Cx14VRrnubS5y3Of+3M+ZM0dGjx4t7u7uYjSEmXKcOnVKtxcJCQkp9bxqIxAbG1vt/Y1k/fr1uu1Er169yt3n008/lb59++p91H8rBw4cECOq6HE48/VWVO3Sxf6LXbx4sb63fsUVV8jw4cP1L3cjK7pulbmmVXmPkX7uu3TpUm5buSLTp0/XPzPq++DBBx+UEydOiBFV5jic8boXFBToNlOq/dDFOOLve8JMOTIzM/XjuaMgqu2i16qzv1Go/8pU49+ZM2eKp6dnmftEREToxoLqP5I1a9botK9qKI4dOyZGUpnjcNbrrezatUuOHz8uAwcOLHcf9Utc3V5bunSprF27VtfidOvWTbZu3SpGVZVr6qzfB0lJSfLJJ5/on/sLUTURvXv3lpUrV8qqVav074vu3bvr2xtGUtnjcMbr/ssvv+jgqjoDXIij/r4nzJTDz8+vzCpWtV30WnX2N4r77rtPbrvtNrn55pvL3Ufda3/sscfEw8ND32p5/vnndRW1I/ZouJDKHIezXu+iWhlV1azOQXlUeFH/yRb9Mle1OO3bt9fB16iqck2d8ftAtfsaOXKkbhPStWvXC+47efJkuf322/X3ivpn56233pIjR47o25RGUtnjcMbr/nkFamMd+fc9YaYcdevW1fdAz61qVP+xNmvWrNr7G8EzzzyjfzBfeumlSr1P3W9VCd8Rqh6r40LH4YzXu2RVc1UaSl5yySWGvuZF160y17Qq73FkhYWF+rbBgAED5J577qn0+wMDA6V+/fqG/j6oyHE423U/c+aMrmlR/7hWlqP8vifMXIBqD7Bp06bibTWNlerRo37QrbG/o/fUiYuLK65mVsdV8thKUq34z6VaxKvqSCOp7HE40/UuOU6MCiUXayw5adKk86rTVTWz0a55Saqxs6o+L3lNVbuovXv3lntNq/IeR6baiqhrOHHiRL2t/sAdPHiwwj8zqmZCtScz2vdBZY/D2a77/PnzZciQITrEXYzD/r63d99wR6bGEQkMDDTv27dPb8+dO7fUOCK9evUyT548ucL7G8WHH35obt26tXn9+vXmv//+Wy9qbIGicUXOPe7IyEjzjz/+WLz93//+V49ZsXv3brORXOw4nPV6lzR8+HDzp59+et7zI0eONN9xxx2lxqeYMWNG8fayZcvMbm5u5pUrV5qNQI2TUd44M6GhoeakpCS9/dJLL5UaOyQzM1OPuTRr1qwKv8coxz5x4kRz3759i3/m1TJ+/PjiMUXKOnYvLy+9X5HnnnvOXL9+ffPJkyfNRjr2ix2H0a/7qnKOu0jXrl3L/dk1yu97RgC+AHW/WN1HHDFihPj6+ur7g6qRVEBAgH5d/Wda8p7pxfY3grS0NP3fmapu7tGjR5ljEpx73P/5z3/knXfe0feZ1WiTqh2F+o/uYg3JHM3FjsMZr3dJycnJuru5avh5ruzs7FJtaNQtyPfee0+PFq1qpNT3ixoVtF+/fuLI1HUdNGiQPlZFXbvw8PDi4QhuueUWOXnypG78rNoBqP/AFy1aVHzs6ljP/T642HuMcOw7d+6UV199VT+vejCVVDTWUFnHrkaVLWo/oV5Tt2ZUA1r1aKTrfrHjMOp1z73IcStq5OPExETdO6ksRvl9b1KJxq4lAAAAqAbHiZAAAABVQJgBAACGRpgBAACGRpgBAACGRpgBAACGRpgBAACGRpgBAACGRpgBAACGRpgBYBV//fWXHkXUZDLp0UD//e9/65FHX3zxxeIRSGvCoUOH9Nc810033SRvv/12jZUDQM1hBGAA1v2lYjLpqS/uuusuHSyaNm0qsbGxembdmrB69Wo9rcK5g5ur4erVFBQjR46skXIAqDnMzQTAJVArAzgvbjMBsIldu3bpie0U9ahuQf3www96Oz09XcaPHy8dOnSQPn366FtAR44c0a+tXbtWunfvrmt41IR4N954ozRv3lyioqL06x988IF069ZN176oSRHVxHdFtTArV66URx99VK+rr6eW9evXy9NPP61rhs6dTG/u3Ln6c9XnqbKUnIDvnnvukYYNG8ro0aNl4sSJupwtW7bUk4kCcDB2nbMbgNNRv1Y+++wzvR4bG6u31WNJI0eO1EtBQYHenjZtmvnyyy835+fnl3rfuHHj9D5paWnmvn376te6dOli3r59u15PT083t2vXzjxnzpziz161apV+77mmTJli7tOnT/H2L7/8Yvb39zfHxMTo7W3btpl9fHzMf/zxR/E+Y8aMMdeuXdu8e/duvf3uu++aIyIirHi2AFgDNTMAatTBgwdl/vz58vjjj4ubm+VX0L333qtrclR7l5JUrYjax9/fX1atWqWfU7Unbdq00eu1atWSa6+9VpYuXVrpcqgaHVUjpGpblLZt28rgwYNl2rRppfZTNTaqQbOianZUDdKZM2eqePQAbIE2MwBq1M6dO/VtoQkTJoinp2fx802aNJHExMRS+4aFhZ33/qNHj8ojjzwiSUlJ+v1FjYwra8eOHdK/f/9Sz6nbWSVvNSmhoaHF6wEBAfoxNTVVateuXemvCcA2CDMA7OLLL7+8aAhxd3cvtX348GEZOHCg7vb95JNP6udUN+xza3SsqWQZVDse5dyeUgDsi9tMAGz3C+bsbSSlsLBQMjIypHXr1np7z549pfZ94YUXJCYm5oKft3HjRsnKypLbbrut+Lnc3Nxyv2Z+fr7evyzqVtX+/ftLPXfgwAF9uwmAsRBmANhM3bp1dbhQbUxUEFFjzzRr1kyP9fLaa69Jdna23m/dunXy3Xff6ds8F6LarqjakRUrVuhtFVTObS9Tv359/ai+5vfff69DUlmeffZZ+fHHH2Xfvn3Ft79+/vlnmTx5slWOHUANskozYgAub8OGDbq3kPq10rJlS/PUqVP1OXn66afNrVu3Nnfr1s28du1a/ZzqnXTvvffq/VQvpeuvv968b98+/dqWLVv0vupz1ON7771X6tzOmjXLHBkZab7yyivNw4YNMw8dOtQcFBRkHjVqVPE+aj0qKsrco0cP3VvpqaeeMjdp0kTvd9111xXvp3pBtW/f3ty1a1e9/zfffFP82oQJE8whISF6Ue9Xn1OyXKr3EwDHwAjAAADA0LjNBAAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAADI0wAwAAxMj+H5x7swSvyFmoAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1577f8f50>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"SQD\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"iSQD\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd058369",
   "metadata": {},
   "source": [
    "## How many did we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kept = []\n",
    "total_shots = []\n",
    "for bit_array in bit_arrays:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    original_size = bit_matrix.shape[0]\n",
    "    bit_matrix = sort_and_remove_duplicates(bit_matrix)\n",
    "    new_size = bit_matrix.shape[0]\n",
    "    num_kept.append(new_size)\n",
    "    total_shots.append(original_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "11b61c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_kept = np.array(num_kept) / np.array(total_shots)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Number of basis states', color=color)\n",
    "ax1.plot(num_kept, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Fraction of shots kept', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(ratio_kept, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f225920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qasm_strs = []\n",
    "for circuit in circuits:\n",
    "    # print(circuit.num_qubits)\n",
    "    # isa_circuit = pass_manager.run(circuit)\n",
    "    # print(isa_circuit.num_qubits)\n",
    "    isa_circuit = circuit\n",
    "    qasm_str = dumps(isa_circuit)\n",
    "    qasm_strs.append(qasm_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d4a60674",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"data/xxz_output.hdf5\", \"w\")\n",
    "f.create_dataset(\"qasm_strs\", data=qasm_strs)\n",
    "f.create_dataset(\"exact_energy\", data=exact_energy)\n",
    "f.create_dataset(\"adapt_errors\", data=np.array(adapt_errors))\n",
    "f.create_dataset(\"sqd_errors\", data=np.array(errors))\n",
    "f.create_dataset(\"isqd_errors\", data=np.array(stacked_errors))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc0a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
