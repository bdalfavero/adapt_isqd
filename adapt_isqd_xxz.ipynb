{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd88d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcParams.update({\"font.family\": \"serif\"})\n",
    "\n",
    "import pyscf\n",
    "import pyscf.cc\n",
    "import pyscf.mcscf\n",
    "\n",
    "# To get molecular geometries.\n",
    "import openfermion as of\n",
    "from openfermion import MolecularData\n",
    "from openfermionpyscf import run_pyscf\n",
    "\n",
    "import qiskit\n",
    "from qiskit import QuantumCircuit, QuantumRegister\n",
    "from qiskit.primitives import BitArray\n",
    "from qiskit_aer import AerSimulator  # For MPS Simulator.\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "from qiskit.transpiler.preset_passmanagers import generate_preset_pass_manager\n",
    "from qiskit.primitives import BackendEstimatorV2 as BackendEstimator\n",
    "from qiskit.transpiler.passes import RemoveFinalMeasurements\n",
    "from qiskit.qasm2 import dumps\n",
    "\n",
    "import ffsim\n",
    "\n",
    "# To run on hardware.\n",
    "import qiskit_ibm_runtime\n",
    "from qiskit_ibm_runtime import SamplerV2 as Sampler\n",
    "\n",
    "from functools import partial, reduce\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from qiskit_addon_sqd.fermion import SCIResult, diagonalize_fermionic_hamiltonian, solve_sci_batch\n",
    "from qiskit_addon_sqd.qubit import solve_qubit, sort_and_remove_duplicates, project_operator_to_subspace\n",
    "\n",
    "from adaptvqe.pools import DVG_CEO, FullPauliPool, TiledPauliPool\n",
    "from adaptvqe.convert import cirq_pauli_sum_to_qiskit_pauli_op\n",
    "from adaptvqe.hamiltonians import XXZHamiltonian\n",
    "from adaptvqe.algorithms.adapt_vqe import LinAlgAdapt, TensorNetAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268707c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/.venv/adapt/lib/python3.13/site-packages/qiskit_ibm_runtime/fake_provider/backends/nighthawk/fake_nighthawk.py:76: UserWarning: Properties of fake_nighthawk are not intended to represent typical nighthawk error values.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ibm_computer: str = \"ibm_fez\"\n",
    "\n",
    "service = qiskit_ibm_runtime.QiskitRuntimeService(channel=\"local\")\n",
    "computer = service.backend()\n",
    "sampler = Sampler(computer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09186101",
   "metadata": {},
   "source": [
    "## Build a tiled pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca34c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got DMRG energy -6.46410e+00\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 241]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -4.626284539634973\n",
      "(change of -0.7978574148887763)\n",
      "Current ansatz: [211, 241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273456531\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-3.7252880142064084)]\n",
      "Initial energy: -4.626284539634973\n",
      "Optimizing energy with indices [211, 241, 79]...\n",
      "Starting point: [np.float64(-0.2651612342265216), np.float64(-0.44546905390000224), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625614734\n",
      "(change of -1.4968210859797608)\n",
      "Current ansatz: [211, 241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917530720141803\n",
      "Operator(s) added to ansatz: [141]\n",
      "Gradients: [np.float64(2.4850735669967787)]\n",
      "Initial energy: -6.123105625614734\n",
      "Optimizing energy with indices [211, 241, 79, 141]...\n",
      "Starting point: [np.float64(-0.122489928621197), np.float64(-0.7853981633782835), np.float64(0.7853981632897755), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.20417052920497092)\n",
      "Current ansatz: [211, 241, 79, 141]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258095602\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.0894929267303137)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [211, 241, 79, 141, 177]...\n",
      "Starting point: [np.float64(-0.16357019750385376), np.float64(-0.7853981628341236), np.float64(0.7853981633514587), np.float64(-0.16356963671060987), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151353865\n",
      "(change of -0.13682546031568155)\n",
      "Current ansatz: [211, 241, 79, 141, 177]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.588987031969056e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258095602 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916426871504)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 147]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072783\n",
      "(change of -0.13682546025279674)\n",
      "Current ansatz: [244, 74, 228, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350387552286998\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.4850710474288453)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 198]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819708\n",
      "(change of -0.2041705292020657)\n",
      "Current ansatz: [241, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531757\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.089492926734933)]\n",
      "Initial energy: -6.327276154819708\n",
      "Optimizing energy with indices [241, 79, 228, 198, 135]...\n",
      "Starting point: [np.float64(-0.7853981618473267), np.float64(0.7853981651745618), np.float64(0.16357019740836948), np.float64(-0.16356963668286137), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615134325\n",
      "(change of -0.1368254603146175)\n",
      "Current ansatz: [241, 79, 228, 198, 135]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.9260788111326973e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531757 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477671423)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 210]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548197005\n",
      "(change of -0.20417052920205325)\n",
      "Current ansatz: [244, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096258053184\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267348776)]\n",
      "Initial energy: -6.3272761548197005\n",
      "Optimizing energy with indices [244, 74, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981634001291), np.float64(-0.785398163399394), np.float64(-0.16357019740837944), np.float64(0.16356963668287733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135405\n",
      "(change of -0.13682546031570464)\n",
      "Current ansatz: [244, 74, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.5887660228229897e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096258053184 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916426871504)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072802\n",
      "(change of -0.1368254602528154)\n",
      "Current ansatz: [244, 74, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875522059898\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429297)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.2041705292020577)\n",
      "Current ansatz: [241, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531791\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894929267348776)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [241, 79, 225, 210, 147]...\n",
      "Starting point: [np.float64(-0.7853981607743267), np.float64(0.7853981678304174), np.float64(-0.163570197408367), np.float64(0.16356963668287403), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615008459\n",
      "(change of -0.13682546018875374)\n",
      "Current ansatz: [241, 79, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00015114037946009526\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531791 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.99999999999442)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 225]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610642\n",
      "(change of -0.12310562561245764)\n",
      "Current ansatz: [228, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955728\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868942)]\n",
      "Initial energy: -6.123105625610642\n",
      "Optimizing energy with indices [228, 74, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853983869831797), np.float64(-0.7853991695302367), np.float64(-0.12248869758310571), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154816355\n",
      "(change of -0.20417052920571255)\n",
      "Current ansatz: [228, 74, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096256714494\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.089492948563267)]\n",
      "Initial energy: -6.327276154816355\n",
      "Optimizing energy with indices [228, 74, 225, 210, 147]...\n",
      "Starting point: [np.float64(-0.785398438893094), np.float64(-0.7853991591701955), np.float64(-0.1635701984907769), np.float64(0.1635696317030657), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132119\n",
      "(change of -0.13682546031576415)\n",
      "Current ansatz: [228, 74, 225, 210, 147]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.185632021248109e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096256714494 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [225]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [225, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998188\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [225, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140572\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.999999999994421)]\n",
      "Initial energy: -5.999999999998188\n",
      "Optimizing energy with indices [225, 79, 225]...\n",
      "Starting point: [np.float64(0.7853985607314237), np.float64(0.7853989420959421), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625610644\n",
      "(change of -0.12310562561245586)\n",
      "Current ansatz: [225, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917522148955813\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485068789868988)]\n",
      "Initial energy: -6.123105625610644\n",
      "Optimizing energy with indices [225, 79, 225, 210]...\n",
      "Starting point: [np.float64(0.7853983869831812), np.float64(0.7853991695302517), np.float64(-0.12248869758311735), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548163405\n",
      "(change of -0.20417052920569656)\n",
      "Current ansatz: [225, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962567146217\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929485621008)]\n",
      "Initial energy: -6.3272761548163405\n",
      "Optimizing energy with indices [225, 79, 225, 210, 198]...\n",
      "Starting point: [np.float64(0.7853984389037185), np.float64(0.7853991591702261), np.float64(-0.16357019849084947), np.float64(0.1635696317033673), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132128\n",
      "(change of -0.13682546031578724)\n",
      "Current ansatz: [225, 79, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.185724722687827e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962567146217 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.999999999952205)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 216]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.1231056252069065\n",
      "(change of -0.12310562527738877)\n",
      "Current ansatz: [228, 79, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917518430081312\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485066714324805)]\n",
      "Initial energy: -6.1231056252069065\n",
      "Optimizing energy with indices [228, 79, 216, 225]...\n",
      "Starting point: [np.float64(-0.7853879048615375), np.float64(0.7853901599313474), np.float64(-0.12248816317024809), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154818593\n",
      "(change of -0.20417052961168647)\n",
      "Current ansatz: [228, 79, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964791886525\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.089491649572313)]\n",
      "Initial energy: -6.327276154818593\n",
      "Optimizing energy with indices [228, 79, 216, 225, 201]...\n",
      "Starting point: [np.float64(-0.7853981629684735), np.float64(0.7853982668128193), np.float64(-0.163570458016993), np.float64(-0.16357001840315244), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072778\n",
      "(change of -0.13682546025418496)\n",
      "Current ansatz: [228, 79, 216, 225, 201]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013504728208117302\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964791886525 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 31]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000006\n",
      "(change of -1.7639320225002075)\n",
      "Current ansatz: [244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.000000000000004)]\n",
      "Initial energy: -6.000000000000006\n",
      "Optimizing energy with indices [244, 31, 210]...\n",
      "Starting point: [np.float64(0.7853981634264086), np.float64(-0.7853981633494667), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617644\n",
      "(change of -0.12310562561763749)\n",
      "Current ansatz: [244, 31, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526201327163\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.4850710484797025)]\n",
      "Initial energy: -6.123105625617644\n",
      "Optimizing energy with indices [244, 31, 210, 225]...\n",
      "Starting point: [np.float64(0.7853981648267889), np.float64(-0.785398161917778), np.float64(0.12248927961411428), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.20417052920206125)\n",
      "Current ansatz: [244, 31, 210, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531958\n",
      "Operator(s) added to ansatz: [99]\n",
      "Gradients: [np.float64(-2.0894929267348785)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [244, 31, 210, 225, 99]...\n",
      "Starting point: [np.float64(0.7853981627311897), np.float64(-0.7853981642549669), np.float64(0.16357019740840598), np.float64(-0.16356963668288474), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615135394\n",
      "(change of -0.13682546031568865)\n",
      "Current ansatz: [244, 31, 210, 225, 99]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.590376599068515e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531958 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 225]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200048677\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.485071047767142)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 225, 147]...\n",
      "Starting point: [np.float64(0.7853981633976963), np.float64(-0.7853981633977503), np.float64(-0.12248927943049101), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199785\n",
      "(change of -0.20417052920233125)\n",
      "Current ansatz: [244, 74, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138131\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089491643867193)]\n",
      "Initial energy: -6.3272761548199785\n",
      "Optimizing energy with indices [244, 74, 225, 147, 210]...\n",
      "Starting point: [np.float64(0.7853981633822422), np.float64(-0.7853981633913459), np.float64(-0.16357028648500122), np.float64(-0.16356997194328265), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072806\n",
      "(change of -0.13682546025282782)\n",
      "Current ansatz: [244, 74, 225, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.0001350387855585295\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138131 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 198]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819706\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531788\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.089492926734881)]\n",
      "Initial energy: -6.327276154819706\n",
      "Optimizing energy with indices [244, 79, 228, 198, 210]...\n",
      "Starting point: [np.float64(0.7853981646905418), np.float64(0.7853981643651434), np.float64(0.16357019740836648), np.float64(-0.1635696366828733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615132637\n",
      "(change of -0.13682546031293086)\n",
      "Current ansatz: [244, 79, 228, 198, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.3721444024673296e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531788 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.0000000000000036)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 216]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 74, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200950523\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.4850710482697833)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [244, 74, 216, 225]...\n",
      "Starting point: [np.float64(0.7853981633664252), np.float64(-0.7853981637522963), np.float64(-0.12248927956001943), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819991\n",
      "(change of -0.20417052920234902)\n",
      "Current ansatz: [244, 74, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042797667\n",
      "Operator(s) added to ansatz: [54]\n",
      "Gradients: [np.float64(-2.089491642623649)]\n",
      "Initial energy: -6.327276154819991\n",
      "Optimizing energy with indices [244, 74, 216, 225, 54]...\n",
      "Starting point: [np.float64(0.7853981452786045), np.float64(-0.7853981529396645), np.float64(-0.16357028657139866), np.float64(-0.1635699722682815), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615070881\n",
      "(change of -0.13682546025088982)\n",
      "Current ansatz: [244, 74, 216, 225, 54]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013624638557904482\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042797667 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-1.9999999999993707)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 201]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625475225\n",
      "(change of -0.12310562547704063)\n",
      "Current ansatz: [228, 74, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526155305602\n",
      "Operator(s) added to ansatz: [177]\n",
      "Gradients: [np.float64(-2.485071021855692)]\n",
      "Initial energy: -6.123105625475225\n",
      "Optimizing energy with indices [228, 74, 201, 177]...\n",
      "Starting point: [np.float64(-0.7854038547370663), np.float64(-0.7854037258579853), np.float64(0.12248927291523236), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154553898\n",
      "(change of -0.20417052907867284)\n",
      "Current ansatz: [228, 74, 201, 177]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 6.240988002134787\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.089491905002376)]\n",
      "Initial energy: -6.327276154553898\n",
      "Optimizing energy with indices [228, 74, 201, 177, 216]...\n",
      "Starting point: [np.float64(-0.7853981579292243), np.float64(-0.7853980639598565), np.float64(0.16357577684185218), np.float64(0.1635714399823413), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615031839\n",
      "(change of -0.13682546047794109)\n",
      "Current ansatz: [228, 74, 201, 177, 216]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00016117622623940407\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240988002134787 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 198]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819706\n",
      "(change of -0.20417052920206658)\n",
      "Current ansatz: [244, 79, 228, 198]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531788\n",
      "Operator(s) added to ansatz: [45]\n",
      "Gradients: [np.float64(-2.089492926734881)]\n",
      "Initial energy: -6.327276154819706\n",
      "Optimizing energy with indices [244, 79, 228, 198, 45]...\n",
      "Starting point: [np.float64(0.7853981646905418), np.float64(0.7853981643651434), np.float64(0.16357019740836648), np.float64(-0.1635696366828733), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615131517\n",
      "(change of -0.13682546031181086)\n",
      "Current ansatz: [244, 79, 228, 198, 45]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 3.672108265003593e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531788 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.0000000000000027)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 225]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617631\n",
      "(change of -0.12310562561761706)\n",
      "Current ansatz: [244, 26, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199440658\n",
      "Operator(s) added to ansatz: [135]\n",
      "Gradients: [np.float64(-2.4850710474282707)]\n",
      "Initial energy: -6.123105625617631\n",
      "Optimizing energy with indices [244, 26, 225, 135]...\n",
      "Starting point: [np.float64(0.7853981632980831), np.float64(0.78539816328158), np.float64(-0.12248927934316764), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819705\n",
      "(change of -0.20417052920207368)\n",
      "Current ansatz: [244, 26, 225, 135]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580531781\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267348807)]\n",
      "Initial energy: -6.327276154819705\n",
      "Optimizing energy with indices [244, 26, 225, 135, 198]...\n",
      "Starting point: [np.float64(0.7853981624018535), np.float64(0.7853981626466596), np.float64(-0.1635701974083663), np.float64(0.1635696366828732), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151353075\n",
      "(change of -0.1368254603156025)\n",
      "Current ansatz: [244, 26, 225, 135, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.6097489670498127e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580531781 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n",
      "\n",
      "Current energy: -5.999999999929518\n",
      "(change of -1.7639320224297226)\n",
      "Current ansatz: [228, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971647986\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.000000000019367)]\n",
      "Initial energy: -5.999999999929518\n",
      "Optimizing energy with indices [228, 79, 225]...\n",
      "Starting point: [np.float64(-0.7853947065773552), np.float64(0.7853993777262496), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625562494\n",
      "(change of -0.12310562563297633)\n",
      "Current ansatz: [228, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620177583\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071048468285)]\n",
      "Initial energy: -6.123105625562494\n",
      "Optimizing energy with indices [228, 79, 225, 210]...\n",
      "Starting point: [np.float64(-0.7853947065772614), np.float64(0.7853985308794381), np.float64(-0.12248927961669928), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154764743\n",
      "(change of -0.20417052920224865)\n",
      "Current ansatz: [228, 79, 225, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962581220501\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894929267302738)]\n",
      "Initial energy: -6.327276154764743\n",
      "Optimizing energy with indices [228, 79, 225, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853947065771463), np.float64(0.7853982468607466), np.float64(-0.16357019741059903), np.float64(0.16356963668331825), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615080621\n",
      "(change of -0.13682546031587872)\n",
      "Current ansatz: [228, 79, 225, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 9.691226047242816e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962581220501 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916438682523)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016145547825\n",
      "(change of -0.13682545973480043)\n",
      "Current ansatz: [241, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00033002067862662824\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [211]\n",
      "Gradients: [np.float64(4.000000000000012)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [211]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -3.828427124746197\n",
      "(change of -0.8284271247461925)\n",
      "Current ansatz: [211]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.78207252017211\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-3.4142135623691776)]\n",
      "Initial energy: -3.828427124746197\n",
      "Optimizing energy with indices [211, 244]...\n",
      "Starting point: [np.float64(-0.3926990817001106), np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.626284539634967\n",
      "(change of -0.79785741488877)\n",
      "Current ansatz: [211, 244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 12.26780273103219\n",
      "Operator(s) added to ansatz: [31]\n",
      "Gradients: [np.float64(3.7252880124103616)]\n",
      "Initial energy: -4.626284539634967\n",
      "Optimizing energy with indices [211, 244, 31]...\n",
      "Starting point: [np.float64(-0.2651612351142246), np.float64(0.44546905146305454), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625614727\n",
      "(change of -1.49682108597976)\n",
      "Current ansatz: [211, 244, 31]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917530720521563\n",
      "Operator(s) added to ansatz: [201]\n",
      "Gradients: [np.float64(-2.485073567208433)]\n",
      "Initial energy: -6.123105625614727\n",
      "Optimizing energy with indices [211, 244, 31, 201]...\n",
      "Starting point: [np.float64(-0.12248992867574016), np.float64(0.7853981549207331), np.float64(-0.7853981610270105), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819711\n",
      "(change of -0.20417052920498424)\n",
      "Current ansatz: [211, 244, 31, 201]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240962580955357\n",
      "Operator(s) added to ansatz: [180]\n",
      "Gradients: [np.float64(2.0894929267315017)]\n",
      "Initial energy: -6.327276154819711\n",
      "Optimizing energy with indices [211, 244, 31, 201, 180]...\n",
      "Starting point: [np.float64(-0.16357019750392648), np.float64(0.7853981548243515), np.float64(-0.785398159357834), np.float64(0.16356963671034297), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.4641016151349255\n",
      "(change of -0.13682546031521436)\n",
      "Current ansatz: [211, 244, 31, 201, 180]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 2.7167510849022436e-05\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240962580955357 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 228]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617642\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [241, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526199441697\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047428846)]\n",
      "Initial energy: -6.123105625617642\n",
      "Optimizing energy with indices [241, 79, 228, 210]...\n",
      "Starting point: [np.float64(-0.785398162647287), np.float64(0.7853981641254796), np.float64(0.1224892793433147), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819982\n",
      "(change of -0.20417052920234013)\n",
      "Current ansatz: [241, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.24096404138082\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.0894916438682527)]\n",
      "Initial energy: -6.327276154819982\n",
      "Optimizing energy with indices [241, 79, 228, 210, 147]...\n",
      "Starting point: [np.float64(-0.7853982024976439), np.float64(0.7853981311150512), np.float64(0.1635702864850446), np.float64(0.16356997194303877), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101614554787\n",
      "(change of -0.13682545973480487)\n",
      "Current ansatz: [241, 79, 228, 210, 147]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00033002084338061577\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.24096404138082 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 79]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000009\n",
      "(change of -1.7639320225002102)\n",
      "Current ansatz: [244, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132738\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.000000000000001)]\n",
      "Initial energy: -6.000000000000009\n",
      "Optimizing energy with indices [244, 79, 228]...\n",
      "Starting point: [np.float64(0.7853981718257751), np.float64(0.7853981815917103), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617639\n",
      "(change of -0.12310562561763039)\n",
      "Current ansatz: [244, 79, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526199442337\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.485071047429203)]\n",
      "Initial energy: -6.123105625617639\n",
      "Optimizing energy with indices [244, 79, 228, 210]...\n",
      "Starting point: [np.float64(0.785398163526392), np.float64(0.7853981635468582), np.float64(0.12248927934340699), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920234813)\n",
      "Current ansatz: [244, 79, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964041846258\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916434588646)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 79, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981591889313), np.float64(0.7853981652005415), np.float64(0.16357028651334282), np.float64(0.1635699720499918), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072706\n",
      "(change of -0.13682546025271858)\n",
      "Current ansatz: [244, 79, 228, 210, 198]\n",
      "Performing final convergence check...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 0.00013508605500525537\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964041846258 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [57]\n",
      "Gradients: [np.float64(2.0894916426871504)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 57]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072802\n",
      "(change of -0.1368254602528154)\n",
      "Current ansatz: [244, 74, 228, 210, 57]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875522059898\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [228]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499795\n",
      "(change of -1.2360679774997907)\n",
      "Current ansatz: [228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696158\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000003)]\n",
      "Initial energy: -4.236067977499795\n",
      "Optimizing energy with indices [228, 74]...\n",
      "Starting point: [np.float64(-0.5535743588970451), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -5.999999999998185\n",
      "(change of -1.7639320224983894)\n",
      "Current ansatz: [228, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971140567\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(1.9999999999993716)]\n",
      "Initial energy: -5.999999999998185\n",
      "Optimizing energy with indices [228, 74, 216]...\n",
      "Starting point: [np.float64(-0.7853985607314252), np.float64(-0.7853989420959435), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625475232\n",
      "(change of -0.12310562547704773)\n",
      "Current ansatz: [228, 74, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.917526155292668\n",
      "Operator(s) added to ansatz: [180]\n",
      "Gradients: [np.float64(2.485071021848481)]\n",
      "Initial energy: -6.123105625475232\n",
      "Optimizing energy with indices [228, 74, 216, 180]...\n",
      "Starting point: [np.float64(-0.7854038547368931), np.float64(-0.7854037258580634), np.float64(-0.12248927291337362), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154553893\n",
      "(change of -0.2041705290786604)\n",
      "Current ansatz: [228, 74, 216, 180]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240988001979858\n",
      "Operator(s) added to ansatz: [54]\n",
      "Gradients: [np.float64(-2.089491904992957)]\n",
      "Initial energy: -6.327276154553893\n",
      "Optimizing energy with indices [228, 74, 216, 180, 54]...\n",
      "Starting point: [np.float64(-0.7853981579296626), np.float64(-0.7853980639717367), np.float64(-0.16357577680490873), np.float64(-0.16357143997430415), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615045918\n",
      "(change of -0.13682546049202493)\n",
      "Current ansatz: [228, 74, 216, 180, 54]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00015177269312943405\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240988001979858 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [241]\n",
      "Gradients: [np.float64(4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [241]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499796\n",
      "(change of -1.2360679774997916)\n",
      "Current ansatz: [241]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.85333868569617\n",
      "Operator(s) added to ansatz: [79]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -4.236067977499796\n",
      "Optimizing energy with indices [241, 79]...\n",
      "Starting point: [np.float64(-0.5535743588970441), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002093)\n",
      "Current ansatz: [241, 79]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132733\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(1.9999999999999996)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [241, 79, 225]...\n",
      "Starting point: [np.float64(-0.785398171825775), np.float64(0.7853981815917083), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [241, 79, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752619944251\n",
      "Operator(s) added to ansatz: [147]\n",
      "Gradients: [np.float64(2.4850710474292965)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [241, 79, 225, 147]...\n",
      "Starting point: [np.float64(-0.7853981627578538), np.float64(0.78539816425353), np.float64(-0.12248927934343061), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819989\n",
      "(change of -0.2041705292023419)\n",
      "Current ansatz: [241, 79, 225, 147]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042617831\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.08949164278134)]\n",
      "Initial energy: -6.327276154819989\n",
      "Optimizing energy with indices [241, 79, 225, 147, 210]...\n",
      "Starting point: [np.float64(-0.7853981633974467), np.float64(0.7853981633974486), np.float64(-0.16357028656039646), np.float64(-0.16356997222705624), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072785\n",
      "(change of -0.13682546025279585)\n",
      "Current ansatz: [241, 79, 225, 147, 210]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875632299282\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042617831 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [74]\n",
      "Gradients: [np.float64(4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 74]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000005\n",
      "(change of -1.7639320225002066)\n",
      "Current ansatz: [244, 74]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.79795897113273\n",
      "Operator(s) added to ansatz: [228]\n",
      "Gradients: [np.float64(-2.0000000000000004)]\n",
      "Initial energy: -6.000000000000005\n",
      "Optimizing energy with indices [244, 74, 228]...\n",
      "Starting point: [np.float64(0.7853981703433218), np.float64(-0.7853981783720295), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.123105625617647\n",
      "(change of -0.12310562561764193)\n",
      "Current ansatz: [244, 74, 228]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gradient norm: 8.91752620004858\n",
      "Operator(s) added to ansatz: [210]\n",
      "Gradients: [np.float64(-2.4850710477670868)]\n",
      "Initial energy: -6.123105625617647\n",
      "Optimizing energy with indices [244, 74, 228, 210]...\n",
      "Starting point: [np.float64(0.7853981659552435), np.float64(-0.7853981611613353), np.float64(0.12248927943047701), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.3272761548199865\n",
      "(change of -0.20417052920233925)\n",
      "Current ansatz: [244, 74, 228, 210]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.240964042725676\n",
      "Operator(s) added to ansatz: [198]\n",
      "Gradients: [np.float64(2.0894916426871495)]\n",
      "Initial energy: -6.3272761548199865\n",
      "Optimizing energy with indices [244, 74, 228, 210, 198]...\n",
      "Starting point: [np.float64(0.7853981633887943), np.float64(-0.7853981634091799), np.float64(0.1635702865670706), np.float64(0.16356997225170858), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072771\n",
      "(change of -0.1368254602527843)\n",
      "Current ansatz: [244, 74, 228, 210, 198]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013503875513833517\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.240964042725676 > 1e-05)\n",
      "\n",
      "Initial energy: -3.0000000000000044\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 19.595917942265466\n",
      "Operator(s) added to ansatz: [244]\n",
      "Gradients: [np.float64(-4.000000000000005)]\n",
      "Initial energy: -3.0000000000000044\n",
      "Optimizing energy with indices [244]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "\n",
      "Current energy: -4.236067977499799\n",
      "(change of -1.2360679774997942)\n",
      "Current ansatz: [244]\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 14.853338685696167\n",
      "Operator(s) added to ansatz: [26]\n",
      "Gradients: [np.float64(-4.000000000000007)]\n",
      "Initial energy: -4.236067977499799\n",
      "Optimizing energy with indices [244, 26]...\n",
      "Starting point: [np.float64(0.5535743588970456), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.000000000000014\n",
      "(change of -1.7639320225002155)\n",
      "Current ansatz: [244, 26]\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 9.797958971132747\n",
      "Operator(s) added to ansatz: [216]\n",
      "Gradients: [np.float64(2.000000000000006)]\n",
      "Initial energy: -6.000000000000014\n",
      "Optimizing energy with indices [244, 26, 216]...\n",
      "Starting point: [np.float64(0.7853981718257763), np.float64(0.7853981815917102), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current energy: -6.123105625617651\n",
      "(change of -0.1231056256176366)\n",
      "Current ansatz: [244, 26, 216]\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 8.917526200769645\n",
      "Operator(s) added to ansatz: [225]\n",
      "Gradients: [np.float64(2.485071048168968)]\n",
      "Initial energy: -6.123105625617651\n",
      "Optimizing energy with indices [244, 26, 216, 225]...\n",
      "Starting point: [np.float64(0.7853981651172142), np.float64(0.7853981644855549), np.float64(-0.1224892795340391), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.327276154819987\n",
      "(change of -0.20417052920233658)\n",
      "Current ansatz: [244, 26, 216, 225]\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "Total gradient norm: 6.2409640432122835\n",
      "Operator(s) added to ansatz: [99]\n",
      "Gradients: [np.float64(-2.0894916422607612)]\n",
      "Initial energy: -6.327276154819987\n",
      "Optimizing energy with indices [244, 26, 216, 225, 99]...\n",
      "Starting point: [np.float64(0.7853981637617629), np.float64(0.7853981645760632), np.float64(-0.1635702865969174), np.float64(-0.16356997236320656), np.float64(0.0)]\n",
      "\n",
      "Current energy: -6.464101615072118\n",
      "(change of -0.1368254602521306)\n",
      "Current ansatz: [244, 26, 216, 225, 99]\n",
      "Performing final convergence check...\n",
      "Total gradient norm: 0.00013546021879432015\n",
      "\n",
      "The maximum number of iterations (5) was hit before the convergence criterion was satisfied.\n",
      "(current gradient norm is 6.2409640432122835 > 1e-05)\n",
      "Pool will be tiled from 22 ops\n"
     ]
    }
   ],
   "source": [
    "max_mpo_bond = 300\n",
    "dmrg_mps_bond = 30\n",
    "adapt_mps_bond = 30\n",
    "l = 4\n",
    "\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(j_xy, j_z, l, diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond)\n",
    "dmrg_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "pool = FullPauliPool(n=l, max_mpo_bond=max_mpo_bond)\n",
    "\n",
    "# Run 200 iterations of ADAPT-VQE for small problem instance, selecting randomly among degenerate gradients.\n",
    "# Form a list of all unique operators ever selected for this small instance.\n",
    "ixs = []\n",
    "for _ in range(30):\n",
    "    my_adapt = TensorNetAdapt(\n",
    "        pool=pool,\n",
    "        custom_hamiltonian=h,\n",
    "        verbose=False,\n",
    "        threshold=10**-5,\n",
    "        max_adapt_iter=5,\n",
    "        max_opt_iter=10000,\n",
    "        sel_criterion=\"gradient\",\n",
    "        recycle_hessian=False,\n",
    "        rand_degenerate=True,\n",
    "        max_mpo_bond=100,\n",
    "        max_mps_bond = 20\n",
    "    )\n",
    "    my_adapt.run()\n",
    "    data = my_adapt.data\n",
    "    for i in data.result.ansatz.indices:\n",
    "        if i not in ixs:\n",
    "            ixs.append(i)\n",
    "\n",
    "print(f\"Pool will be tiled from {len(ixs)} ops\")\n",
    "source_ops = [pool.operators[index].operator for index in ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f9939",
   "metadata": {},
   "source": [
    "## Run ADAPT at larger size to get a sequence of circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a9715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neel_circuit(nq, start_zero=True):\n",
    "    circuit = QuantumCircuit(nq)\n",
    "    for i in range(nq):\n",
    "        if (i % 2 == 0 and start_zero) or (i % 2 != 0 and not start_zero):\n",
    "            circuit.x(i)\n",
    "        else:\n",
    "            circuit.id(i)\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb433a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_l = 20\n",
      "No pre-computed energy for given parameters.\n",
      "Solving by DMRG with quimb.\n",
      "Got DMRG energy -3.47299e+01-4.44089e-14j\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/.venv/adapt/lib/python3.13/site-packages/cotengra/hyperoptimizers/hyper.py:55: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization. It is recommended to install one of these libraries for higher quality contraction paths.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiled pool has 279 operators.\n",
      "\n",
      "tensor-net-adapt prepared with the following settings:\n",
      "> Pool: tiled_pauli_pool\n",
      "> Custom Hamiltonian: XXZ_1_1\n",
      "> Orbital Optimization: False\n",
      "> Selection method: gradient\n",
      "> Convergence criterion: total_g_norm\n",
      "> Recycling Hessian: False\n",
      "> Tetris: False (progressive optimization: False)\n",
      "> Convergence threshold (gradient norm):  1e-05\n",
      "> Maximum number of iterations:  30\n",
      "> Candidates per iteration:  1\n",
      "> Swap-based circuits for LNN connectivity:  False\n",
      "> Qiskit-transpiler-based circuits for LNN connectivity:  False\n",
      "\n",
      "Initial energy: -19.00000000000003\n",
      "On iteration 0.\n",
      "\n",
      "*** ADAPT-VQE Iteration 1 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000006\n",
      "Operator 1: -3.9999999999999996\n",
      "Operator 2: 4.000000000000045\n",
      "Operator 3: -4.000000000000009\n",
      "Operator 4: 3.99999999999999\n",
      "Operator 5: -4.000000000000025\n",
      "Operator 6: 4.000000000000014\n",
      "Operator 7: -4.00000000000002\n",
      "Operator 8: 4.0000000000000115\n",
      "Operator 9: -4.000000000000016\n",
      "Operator 10: 4.000000000000035\n",
      "Operator 11: -4.000000000000042\n",
      "Operator 12: 4.00000000000003\n",
      "Operator 13: -4.0000000000000515\n",
      "Operator 14: 4.00000000000003\n",
      "Operator 15: -4.000000000000028\n",
      "Operator 16: 4.000000000000019\n",
      "Operator 17: 3.9999999999999996\n",
      "Operator 18: -4.000000000000045\n",
      "Operator 19: 4.000000000000009\n",
      "Operator 20: -3.99999999999999\n",
      "Operator 21: 4.000000000000025\n",
      "Operator 22: -4.000000000000014\n",
      "Operator 23: 4.00000000000002\n",
      "Operator 24: -4.0000000000000115\n",
      "Operator 25: 4.000000000000016\n",
      "Operator 26: -4.000000000000035\n",
      "Operator 27: 4.000000000000042\n",
      "Operator 28: -4.00000000000003\n",
      "Operator 29: 4.0000000000000515\n",
      "Operator 30: -4.00000000000003\n",
      "Operator 31: 4.000000000000028\n",
      "Operator 32: -4.000000000000019\n",
      "Operator 33: 4.000000000000017\n",
      "Operator 34: -4.000000000000009\n",
      "Operator 52: 3.9999999999999996\n",
      "Operator 53: 4.000000000000045\n",
      "Operator 54: 4.000000000000009\n",
      "Operator 55: 3.99999999999999\n",
      "Operator 56: 4.000000000000025\n",
      "Operator 57: 4.000000000000014\n",
      "Operator 58: 4.00000000000002\n",
      "Operator 59: 4.0000000000000115\n",
      "Operator 60: 4.000000000000016\n",
      "Operator 61: 4.000000000000035\n",
      "Operator 62: 4.000000000000042\n",
      "Operator 63: 4.00000000000003\n",
      "Operator 64: 4.0000000000000515\n",
      "Operator 65: 4.00000000000003\n",
      "Operator 66: 4.000000000000028\n",
      "Operator 67: 4.000000000000019\n",
      "Operator 68: 4.000000000000017\n",
      "Operator 69: -4.000000000000017\n",
      "Operator 70: 3.9999999999999996\n",
      "Operator 71: 4.000000000000045\n",
      "Operator 72: 4.000000000000009\n",
      "Operator 73: 3.99999999999999\n",
      "Operator 74: 4.000000000000025\n",
      "Operator 75: 4.000000000000014\n",
      "Operator 76: 4.00000000000002\n",
      "Operator 77: 4.0000000000000115\n",
      "Operator 78: 4.000000000000016\n",
      "Operator 79: 4.000000000000035\n",
      "Operator 80: 4.000000000000042\n",
      "Operator 81: 4.00000000000003\n",
      "Operator 82: 4.0000000000000515\n",
      "Operator 83: 4.00000000000003\n",
      "Operator 84: 4.000000000000028\n",
      "Operator 85: 4.000000000000019\n",
      "Operator 86: 4.000000000000017\n",
      "Operator 87: -4.000000000000006\n",
      "Operator 88: -3.9999999999999996\n",
      "Operator 89: -4.000000000000045\n",
      "Operator 90: -4.000000000000009\n",
      "Operator 91: -3.99999999999999\n",
      "Operator 92: -4.000000000000025\n",
      "Operator 93: -4.000000000000014\n",
      "Operator 94: -4.00000000000002\n",
      "Operator 95: -4.0000000000000115\n",
      "Operator 96: -4.000000000000016\n",
      "Operator 97: -4.000000000000035\n",
      "Operator 98: -4.000000000000042\n",
      "Operator 99: -4.00000000000003\n",
      "Operator 100: -4.0000000000000515\n",
      "Operator 101: -4.00000000000003\n",
      "Operator 102: -4.000000000000028\n",
      "Operator 103: -4.000000000000019\n",
      "Operator 104: 4.000000000000006\n",
      "Operator 105: 3.9999999999999996\n",
      "Operator 106: 4.000000000000045\n",
      "Operator 107: 4.000000000000009\n",
      "Operator 108: 3.99999999999999\n",
      "Operator 109: 4.000000000000025\n",
      "Operator 110: 4.000000000000014\n",
      "Operator 111: 4.00000000000002\n",
      "Operator 112: 4.0000000000000115\n",
      "Operator 113: 4.000000000000016\n",
      "Operator 114: 4.000000000000035\n",
      "Operator 115: 4.000000000000042\n",
      "Operator 116: 4.00000000000003\n",
      "Operator 117: 4.0000000000000515\n",
      "Operator 118: 4.00000000000003\n",
      "Operator 119: 4.000000000000028\n",
      "Operator 120: 4.000000000000019\n",
      "Operator 121: 4.000000000000009\n",
      "Operator 122: -4.000000000000006\n",
      "Operator 123: 3.9999999999999996\n",
      "Operator 124: -4.000000000000045\n",
      "Operator 125: 4.000000000000009\n",
      "Operator 126: -3.99999999999999\n",
      "Operator 127: 4.000000000000025\n",
      "Operator 128: -4.000000000000014\n",
      "Operator 129: 4.00000000000002\n",
      "Operator 130: -4.0000000000000115\n",
      "Operator 131: 4.000000000000016\n",
      "Operator 132: -4.000000000000035\n",
      "Operator 133: 4.000000000000042\n",
      "Operator 134: -4.00000000000003\n",
      "Operator 135: 4.0000000000000515\n",
      "Operator 136: -4.00000000000003\n",
      "Operator 137: 4.000000000000028\n",
      "Operator 138: 4.000000000000006\n",
      "Operator 139: -4.000000000000006\n",
      "Operator 140: -3.9999999999999996\n",
      "Operator 141: -4.000000000000045\n",
      "Operator 142: -4.000000000000009\n",
      "Operator 143: -3.99999999999999\n",
      "Operator 144: -4.000000000000025\n",
      "Operator 145: -4.000000000000014\n",
      "Operator 146: -4.00000000000002\n",
      "Operator 147: -4.0000000000000115\n",
      "Operator 148: -4.000000000000016\n",
      "Operator 149: -4.000000000000035\n",
      "Operator 150: -4.000000000000042\n",
      "Operator 151: -4.00000000000003\n",
      "Operator 152: -4.0000000000000515\n",
      "Operator 153: -4.00000000000003\n",
      "Operator 154: -4.000000000000028\n",
      "Operator 155: -4.000000000000019\n",
      "Operator 156: -4.000000000000017\n",
      "Operator 208: 4.000000000000009\n",
      "Operator 209: -4.000000000000006\n",
      "Operator 245: -4.000000000000009\n",
      "Operator 246: 4.000000000000006\n",
      "Operator 247: -3.9999999999999996\n",
      "Operator 248: 4.000000000000045\n",
      "Operator 249: -4.000000000000009\n",
      "Operator 250: 3.99999999999999\n",
      "Operator 251: -4.000000000000025\n",
      "Operator 252: 4.000000000000014\n",
      "Operator 253: -4.00000000000002\n",
      "Operator 254: 4.0000000000000115\n",
      "Operator 255: -4.000000000000016\n",
      "Operator 256: 4.000000000000035\n",
      "Operator 257: -4.000000000000042\n",
      "Operator 258: 4.00000000000003\n",
      "Operator 259: -4.0000000000000515\n",
      "Operator 260: 4.00000000000003\n",
      "Operator 261: -4.000000000000028\n",
      "Operator 262: -3.9999999999999996\n",
      "Operator 263: -4.000000000000045\n",
      "Operator 264: -4.000000000000009\n",
      "Operator 265: -3.99999999999999\n",
      "Operator 266: -4.000000000000025\n",
      "Operator 267: -4.000000000000014\n",
      "Operator 268: -4.00000000000002\n",
      "Operator 269: -4.0000000000000115\n",
      "Operator 270: -4.000000000000016\n",
      "Operator 271: -4.000000000000035\n",
      "Operator 272: -4.000000000000042\n",
      "Operator 273: -4.00000000000003\n",
      "Operator 274: -4.0000000000000515\n",
      "Operator 275: -4.00000000000003\n",
      "Operator 276: -4.000000000000028\n",
      "Operator 277: -4.000000000000019\n",
      "Operator 278: -4.000000000000017\n",
      "Total gradient norm: 53.06599664568668\n",
      "Operators under consideration (1):\n",
      "[278]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000017)]\n",
      "Operator(s) added to ansatz: [278]\n",
      "Gradients: [np.float64(-4.000000000000017)]\n",
      "Initial energy: -19.00000000000003\n",
      "Optimizing energy with indices [278]...\n",
      "Starting point: [np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -20.236068\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 8\n",
      "\n",
      "Current energy: -20.2360679774998\n",
      "(change of -1.236067977499772)\n",
      "Current ansatz: [278]\n",
      "On iteration 1.\n",
      "\n",
      "*** ADAPT-VQE Iteration 2 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000004\n",
      "Operator 1: -3.999999999999998\n",
      "Operator 2: 4.000000000000044\n",
      "Operator 3: -4.000000000000011\n",
      "Operator 4: 3.9999999999999893\n",
      "Operator 5: -4.000000000000025\n",
      "Operator 6: 4.0000000000000115\n",
      "Operator 7: -4.000000000000018\n",
      "Operator 8: 4.000000000000009\n",
      "Operator 9: -4.000000000000013\n",
      "Operator 10: 4.000000000000032\n",
      "Operator 11: -4.000000000000039\n",
      "Operator 12: 4.000000000000027\n",
      "Operator 13: -4.000000000000047\n",
      "Operator 14: 4.000000000000027\n",
      "Operator 15: -4.0000000000000275\n",
      "Operator 16: 2.8944271909999726\n",
      "Operator 17: 3.999999999999998\n",
      "Operator 18: -4.000000000000044\n",
      "Operator 19: 4.000000000000011\n",
      "Operator 20: -3.9999999999999893\n",
      "Operator 21: 4.000000000000025\n",
      "Operator 22: -4.0000000000000115\n",
      "Operator 23: 4.000000000000018\n",
      "Operator 24: -4.000000000000009\n",
      "Operator 25: 4.000000000000013\n",
      "Operator 26: -4.000000000000032\n",
      "Operator 27: 4.000000000000039\n",
      "Operator 28: -4.000000000000027\n",
      "Operator 29: 4.000000000000047\n",
      "Operator 30: -4.000000000000027\n",
      "Operator 31: 4.0000000000000275\n",
      "Operator 32: -2.8944271909999744\n",
      "Operator 34: -4.000000000000006\n",
      "Operator 51: 1.7888543819998188\n",
      "Operator 52: 3.999999999999998\n",
      "Operator 53: 4.000000000000044\n",
      "Operator 54: 4.000000000000011\n",
      "Operator 55: 3.9999999999999893\n",
      "Operator 56: 4.000000000000025\n",
      "Operator 57: 4.0000000000000115\n",
      "Operator 58: 4.000000000000018\n",
      "Operator 59: 4.000000000000009\n",
      "Operator 60: 4.000000000000013\n",
      "Operator 61: 4.000000000000032\n",
      "Operator 62: 4.000000000000039\n",
      "Operator 63: 4.000000000000027\n",
      "Operator 64: 4.000000000000047\n",
      "Operator 65: 4.000000000000027\n",
      "Operator 66: 4.0000000000000275\n",
      "Operator 67: 2.8944271909999744\n",
      "Operator 70: 3.999999999999998\n",
      "Operator 71: 4.000000000000044\n",
      "Operator 72: 4.000000000000011\n",
      "Operator 73: 3.9999999999999893\n",
      "Operator 74: 4.000000000000025\n",
      "Operator 75: 4.0000000000000115\n",
      "Operator 76: 4.000000000000018\n",
      "Operator 77: 4.000000000000009\n",
      "Operator 78: 4.000000000000013\n",
      "Operator 79: 4.000000000000032\n",
      "Operator 80: 4.000000000000039\n",
      "Operator 81: 4.000000000000027\n",
      "Operator 82: 4.000000000000047\n",
      "Operator 83: 4.000000000000027\n",
      "Operator 84: 4.0000000000000275\n",
      "Operator 85: 2.8944271909999726\n",
      "Operator 87: -4.000000000000004\n",
      "Operator 88: -3.999999999999998\n",
      "Operator 89: -4.000000000000044\n",
      "Operator 90: -4.000000000000011\n",
      "Operator 91: -3.9999999999999893\n",
      "Operator 92: -4.000000000000025\n",
      "Operator 93: -4.0000000000000115\n",
      "Operator 94: -4.000000000000018\n",
      "Operator 95: -4.000000000000009\n",
      "Operator 96: -4.000000000000013\n",
      "Operator 97: -4.000000000000032\n",
      "Operator 98: -4.000000000000039\n",
      "Operator 99: -4.000000000000027\n",
      "Operator 100: -4.000000000000047\n",
      "Operator 101: -4.000000000000027\n",
      "Operator 102: -1.7888543819999345\n",
      "Operator 103: -2.8944271909999744\n",
      "Operator 104: 4.000000000000004\n",
      "Operator 105: 3.999999999999998\n",
      "Operator 106: 4.000000000000044\n",
      "Operator 107: 4.000000000000011\n",
      "Operator 108: 3.9999999999999893\n",
      "Operator 109: 4.000000000000025\n",
      "Operator 110: 4.0000000000000115\n",
      "Operator 111: 4.000000000000018\n",
      "Operator 112: 4.000000000000009\n",
      "Operator 113: 4.000000000000013\n",
      "Operator 114: 4.000000000000032\n",
      "Operator 115: 4.000000000000039\n",
      "Operator 116: 4.000000000000027\n",
      "Operator 117: 4.000000000000047\n",
      "Operator 118: 4.000000000000027\n",
      "Operator 119: 1.7888543819999345\n",
      "Operator 120: 2.8944271909999726\n",
      "Operator 121: 4.000000000000006\n",
      "Operator 122: -4.000000000000004\n",
      "Operator 123: 3.999999999999998\n",
      "Operator 124: -4.000000000000044\n",
      "Operator 125: 4.000000000000011\n",
      "Operator 126: -3.9999999999999893\n",
      "Operator 127: 4.000000000000025\n",
      "Operator 128: -4.0000000000000115\n",
      "Operator 129: 4.000000000000018\n",
      "Operator 130: -4.000000000000009\n",
      "Operator 131: 4.000000000000013\n",
      "Operator 132: -4.000000000000032\n",
      "Operator 133: 4.000000000000039\n",
      "Operator 134: -4.000000000000027\n",
      "Operator 135: 4.000000000000047\n",
      "Operator 136: -1.788854381999935\n",
      "Operator 137: 4.0000000000000275\n",
      "Operator 138: 4.000000000000004\n",
      "Operator 139: -4.000000000000004\n",
      "Operator 140: -3.999999999999998\n",
      "Operator 141: -4.000000000000044\n",
      "Operator 142: -4.000000000000011\n",
      "Operator 143: -3.9999999999999893\n",
      "Operator 144: -4.000000000000025\n",
      "Operator 145: -4.0000000000000115\n",
      "Operator 146: -4.000000000000018\n",
      "Operator 147: -4.000000000000009\n",
      "Operator 148: -4.000000000000013\n",
      "Operator 149: -4.000000000000032\n",
      "Operator 150: -4.000000000000039\n",
      "Operator 151: -4.000000000000027\n",
      "Operator 152: -4.000000000000047\n",
      "Operator 153: -4.000000000000027\n",
      "Operator 154: -4.0000000000000275\n",
      "Operator 155: -2.8944271909999744\n",
      "Operator 190: 1.7888543819998188\n",
      "Operator 207: -1.7888543819998157\n",
      "Operator 208: 4.000000000000006\n",
      "Operator 209: -4.000000000000004\n",
      "Operator 245: -4.000000000000006\n",
      "Operator 246: 4.000000000000004\n",
      "Operator 247: -3.999999999999998\n",
      "Operator 248: 4.000000000000044\n",
      "Operator 249: -4.000000000000011\n",
      "Operator 250: 3.9999999999999893\n",
      "Operator 251: -4.000000000000025\n",
      "Operator 252: 4.0000000000000115\n",
      "Operator 253: -4.000000000000018\n",
      "Operator 254: 4.000000000000009\n",
      "Operator 255: -4.000000000000013\n",
      "Operator 256: 4.000000000000032\n",
      "Operator 257: -4.000000000000039\n",
      "Operator 258: 4.000000000000027\n",
      "Operator 259: -4.000000000000047\n",
      "Operator 260: 1.788854381999935\n",
      "Operator 261: -4.0000000000000275\n",
      "Operator 262: -3.999999999999998\n",
      "Operator 263: -4.000000000000044\n",
      "Operator 264: -4.000000000000011\n",
      "Operator 265: -3.9999999999999893\n",
      "Operator 266: -4.000000000000025\n",
      "Operator 267: -4.0000000000000115\n",
      "Operator 268: -4.000000000000018\n",
      "Operator 269: -4.000000000000009\n",
      "Operator 270: -4.000000000000013\n",
      "Operator 271: -4.000000000000032\n",
      "Operator 272: -4.000000000000039\n",
      "Operator 273: -4.000000000000027\n",
      "Operator 274: -4.000000000000047\n",
      "Operator 275: -4.000000000000027\n",
      "Operator 276: -4.0000000000000275\n",
      "Operator 277: -2.8944271909999726\n",
      "Total gradient norm: 51.16074344760858\n",
      "Operators under consideration (1):\n",
      "[276]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.0000000000000275)]\n",
      "Operator(s) added to ansatz: [276]\n",
      "Gradients: [np.float64(-4.0000000000000275)]\n",
      "Initial energy: -20.2360679774998\n",
      "Optimizing energy with indices [278, 276]...\n",
      "Starting point: [np.float64(0.5535743588970329), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -21.295036\n",
      "         Iterations: 5\n",
      "         Function evaluations: 46\n",
      "         Gradient evaluations: 34\n",
      "\n",
      "Current energy: -21.295035558182168\n",
      "(change of -1.058967580682367)\n",
      "Current ansatz: [278, 276]\n",
      "On iteration 2.\n",
      "\n",
      "*** ADAPT-VQE Iteration 3 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000016\n",
      "Operator 1: -4.000000000000011\n",
      "Operator 2: 4.000000000000057\n",
      "Operator 3: -4.000000000000021\n",
      "Operator 4: 4.0\n",
      "Operator 5: -4.0000000000000355\n",
      "Operator 6: 4.000000000000023\n",
      "Operator 7: -4.0000000000000275\n",
      "Operator 8: 4.000000000000019\n",
      "Operator 9: -4.000000000000024\n",
      "Operator 10: 4.000000000000042\n",
      "Operator 11: -4.00000000000005\n",
      "Operator 12: 4.000000000000038\n",
      "Operator 13: -4.000000000000056\n",
      "Operator 14: 3.0643399411510766\n",
      "Operator 15: 8.728660794155019e-08\n",
      "Operator 16: 1.5786153796605342\n",
      "Operator 17: 4.000000000000011\n",
      "Operator 18: -4.000000000000057\n",
      "Operator 19: 4.000000000000022\n",
      "Operator 20: -4.0\n",
      "Operator 21: 4.0000000000000355\n",
      "Operator 22: -4.000000000000023\n",
      "Operator 23: 4.0000000000000275\n",
      "Operator 24: -4.000000000000019\n",
      "Operator 25: 4.000000000000024\n",
      "Operator 26: -4.000000000000042\n",
      "Operator 27: 4.00000000000005\n",
      "Operator 28: -4.000000000000038\n",
      "Operator 29: 4.000000000000056\n",
      "Operator 30: -3.0643399411510748\n",
      "Operator 31: -8.728660794155019e-08\n",
      "Operator 32: -1.5786153796605384\n",
      "Operator 33: -3.872949747574239e-07\n",
      "Operator 34: -4.000000000000019\n",
      "Operator 49: 1.6932750779689134\n",
      "Operator 50: -0.43540489161981855\n",
      "Operator 51: 1.9327495371480332\n",
      "Operator 52: 4.000000000000011\n",
      "Operator 53: 4.000000000000057\n",
      "Operator 54: 4.000000000000022\n",
      "Operator 55: 4.0\n",
      "Operator 56: 4.0000000000000355\n",
      "Operator 57: 4.000000000000023\n",
      "Operator 58: 4.0000000000000275\n",
      "Operator 59: 4.000000000000019\n",
      "Operator 60: 4.000000000000024\n",
      "Operator 61: 4.000000000000042\n",
      "Operator 62: 4.00000000000005\n",
      "Operator 63: 4.000000000000038\n",
      "Operator 64: 4.000000000000056\n",
      "Operator 65: 3.0643399411510748\n",
      "Operator 66: -8.728660794155019e-08\n",
      "Operator 67: 1.5786153796605384\n",
      "Operator 68: -1.3853856471894277\n",
      "Operator 69: 3.872949747574239e-07\n",
      "Operator 70: 4.000000000000011\n",
      "Operator 71: 4.000000000000057\n",
      "Operator 72: 4.000000000000021\n",
      "Operator 73: 4.0\n",
      "Operator 74: 4.0000000000000355\n",
      "Operator 75: 4.000000000000023\n",
      "Operator 76: 4.0000000000000275\n",
      "Operator 77: 4.000000000000019\n",
      "Operator 78: 4.000000000000024\n",
      "Operator 79: 4.000000000000042\n",
      "Operator 80: 4.00000000000005\n",
      "Operator 81: 4.000000000000038\n",
      "Operator 82: 4.000000000000056\n",
      "Operator 83: 3.0643399411510766\n",
      "Operator 84: -8.728660794155019e-08\n",
      "Operator 85: 2.2736819449793177\n",
      "Operator 86: -1.385385647189425\n",
      "Operator 87: -4.000000000000016\n",
      "Operator 88: -4.000000000000011\n",
      "Operator 89: -4.000000000000057\n",
      "Operator 90: -4.000000000000021\n",
      "Operator 91: -4.0\n",
      "Operator 92: -4.0000000000000355\n",
      "Operator 93: -4.000000000000023\n",
      "Operator 94: -4.0000000000000275\n",
      "Operator 95: -4.000000000000019\n",
      "Operator 96: -4.000000000000024\n",
      "Operator 97: -4.000000000000042\n",
      "Operator 98: -4.00000000000005\n",
      "Operator 99: -4.000000000000038\n",
      "Operator 100: -2.1286798823021242\n",
      "Operator 101: -3.0643399411510748\n",
      "Operator 102: 1.5813160796301111\n",
      "Operator 103: -2.273681944979318\n",
      "Operator 104: 4.000000000000016\n",
      "Operator 105: 4.000000000000011\n",
      "Operator 106: 4.000000000000057\n",
      "Operator 107: 4.000000000000022\n",
      "Operator 108: 4.0\n",
      "Operator 109: 4.0000000000000355\n",
      "Operator 110: 4.000000000000023\n",
      "Operator 111: 4.0000000000000275\n",
      "Operator 112: 4.000000000000019\n",
      "Operator 113: 4.000000000000024\n",
      "Operator 114: 4.000000000000042\n",
      "Operator 115: 4.00000000000005\n",
      "Operator 116: 4.000000000000038\n",
      "Operator 117: 2.1286798823021242\n",
      "Operator 118: 3.0643399411510766\n",
      "Operator 119: -1.5813160796301111\n",
      "Operator 120: 2.2736819449793177\n",
      "Operator 121: 4.000000000000019\n",
      "Operator 122: -4.000000000000016\n",
      "Operator 123: 4.000000000000011\n",
      "Operator 124: -4.000000000000057\n",
      "Operator 125: 4.000000000000021\n",
      "Operator 126: -4.0\n",
      "Operator 127: 4.0000000000000355\n",
      "Operator 128: -4.000000000000023\n",
      "Operator 129: 4.0000000000000275\n",
      "Operator 130: -4.000000000000019\n",
      "Operator 131: 4.000000000000024\n",
      "Operator 132: -4.000000000000042\n",
      "Operator 133: 4.00000000000005\n",
      "Operator 134: -2.1286798823021145\n",
      "Operator 135: 4.000000000000056\n",
      "Operator 136: -0.7879573834887894\n",
      "Operator 137: -8.728660794155019e-08\n",
      "Operator 138: 4.000000000000016\n",
      "Operator 139: -4.000000000000016\n",
      "Operator 140: -4.000000000000011\n",
      "Operator 141: -4.000000000000057\n",
      "Operator 142: -4.000000000000022\n",
      "Operator 143: -4.0\n",
      "Operator 144: -4.0000000000000355\n",
      "Operator 145: -4.000000000000023\n",
      "Operator 146: -4.0000000000000275\n",
      "Operator 147: -4.000000000000019\n",
      "Operator 148: -4.000000000000024\n",
      "Operator 149: -4.000000000000042\n",
      "Operator 150: -4.00000000000005\n",
      "Operator 151: -4.000000000000038\n",
      "Operator 152: -4.000000000000056\n",
      "Operator 153: -3.0643399411510748\n",
      "Operator 154: 8.728660794155019e-08\n",
      "Operator 155: -2.273681944979318\n",
      "Operator 156: 1.3853856471894277\n",
      "Operator 173: 1.6363383116043453\n",
      "Operator 188: 1.6932750779689134\n",
      "Operator 189: -1.693275077968912\n",
      "Operator 190: 1.9327495371480332\n",
      "Operator 205: -1.693275077968916\n",
      "Operator 206: 1.6932750779689085\n",
      "Operator 207: -1.932749537148032\n",
      "Operator 208: 4.000000000000019\n",
      "Operator 209: -4.000000000000016\n",
      "Operator 226: 0.4354048916198173\n",
      "Operator 227: -1.693275077968912\n",
      "Operator 244: -1.636338311604343\n",
      "Operator 245: -4.000000000000019\n",
      "Operator 246: 4.000000000000016\n",
      "Operator 247: -4.000000000000011\n",
      "Operator 248: 4.000000000000057\n",
      "Operator 249: -4.000000000000022\n",
      "Operator 250: 4.0\n",
      "Operator 251: -4.0000000000000355\n",
      "Operator 252: 4.000000000000023\n",
      "Operator 253: -4.0000000000000275\n",
      "Operator 254: 4.000000000000019\n",
      "Operator 255: -4.000000000000024\n",
      "Operator 256: 4.000000000000042\n",
      "Operator 257: -4.00000000000005\n",
      "Operator 258: 2.1286798823021145\n",
      "Operator 259: -4.000000000000056\n",
      "Operator 260: 0.7879573834887894\n",
      "Operator 261: 8.728660794155019e-08\n",
      "Operator 262: -4.000000000000011\n",
      "Operator 263: -4.000000000000057\n",
      "Operator 264: -4.000000000000021\n",
      "Operator 265: -4.0\n",
      "Operator 266: -4.0000000000000355\n",
      "Operator 267: -4.000000000000023\n",
      "Operator 268: -4.0000000000000275\n",
      "Operator 269: -4.000000000000019\n",
      "Operator 270: -4.000000000000024\n",
      "Operator 271: -4.000000000000042\n",
      "Operator 272: -4.00000000000005\n",
      "Operator 273: -4.000000000000038\n",
      "Operator 274: -4.000000000000056\n",
      "Operator 275: -3.0643399411510766\n",
      "Operator 276: 8.728660794155019e-08\n",
      "Operator 277: -1.5786153796605342\n",
      "Operator 278: 1.385385647189425\n",
      "Total gradient norm: 48.78480527118066\n",
      "Operators under consideration (1):\n",
      "[273]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000038)]\n",
      "Operator(s) added to ansatz: [273]\n",
      "Gradients: [np.float64(-4.000000000000038)]\n",
      "Initial energy: -21.295035558182168\n",
      "Optimizing energy with indices [278, 276, 273]...\n",
      "Starting point: [np.float64(0.6553685853731887), np.float64(0.5048173871929864), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -22.123463\n",
      "         Iterations: 7\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "\n",
      "Current energy: -22.12346268292834\n",
      "(change of -0.8284271247461739)\n",
      "Current ansatz: [278, 276, 273]\n",
      "On iteration 3.\n",
      "\n",
      "*** ADAPT-VQE Iteration 4 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000014\n",
      "Operator 1: -4.000000000000011\n",
      "Operator 2: 4.000000000000055\n",
      "Operator 3: -4.0000000000000195\n",
      "Operator 4: 3.999999999999998\n",
      "Operator 5: -4.000000000000032\n",
      "Operator 6: 4.000000000000019\n",
      "Operator 7: -4.000000000000024\n",
      "Operator 8: 4.000000000000015\n",
      "Operator 9: -4.000000000000019\n",
      "Operator 10: 4.000000000000038\n",
      "Operator 11: -3.414213562373102\n",
      "Operator 13: -3.4142135623731096\n",
      "Operator 14: 3.0643400409763792\n",
      "Operator 16: 1.5786157053744236\n",
      "Operator 17: 4.000000000000011\n",
      "Operator 18: -4.000000000000055\n",
      "Operator 19: 4.0000000000000195\n",
      "Operator 20: -3.999999999999998\n",
      "Operator 21: 4.000000000000032\n",
      "Operator 22: -4.000000000000019\n",
      "Operator 23: 4.000000000000024\n",
      "Operator 24: -4.000000000000015\n",
      "Operator 25: 4.000000000000019\n",
      "Operator 26: -4.000000000000038\n",
      "Operator 27: 3.414213562373103\n",
      "Operator 29: 3.4142135623731082\n",
      "Operator 30: -3.0643400409763757\n",
      "Operator 32: -1.5786157053744279\n",
      "Operator 34: -4.000000000000016\n",
      "Operator 46: 1.4142135623731398\n",
      "Operator 47: -1.4142135623731422\n",
      "Operator 49: 1.1973262456770857\n",
      "Operator 50: -0.4354050667309265\n",
      "Operator 51: 1.932749477042477\n",
      "Operator 52: 4.000000000000011\n",
      "Operator 53: 4.000000000000055\n",
      "Operator 54: 4.0000000000000195\n",
      "Operator 55: 3.999999999999998\n",
      "Operator 56: 4.000000000000032\n",
      "Operator 57: 4.000000000000019\n",
      "Operator 58: 4.000000000000024\n",
      "Operator 59: 4.000000000000015\n",
      "Operator 60: 4.000000000000019\n",
      "Operator 61: 4.000000000000038\n",
      "Operator 62: 3.414213562373103\n",
      "Operator 64: 3.4142135623731082\n",
      "Operator 65: 2.166815622835812\n",
      "Operator 67: 1.5786157053744279\n",
      "Operator 68: -1.3853852953238963\n",
      "Operator 70: 4.000000000000011\n",
      "Operator 71: 4.000000000000055\n",
      "Operator 72: 4.0000000000000195\n",
      "Operator 73: 3.999999999999998\n",
      "Operator 74: 4.000000000000032\n",
      "Operator 75: 4.000000000000019\n",
      "Operator 76: 4.000000000000024\n",
      "Operator 77: 4.000000000000015\n",
      "Operator 78: 4.000000000000019\n",
      "Operator 79: 4.000000000000038\n",
      "Operator 80: 3.414213562373102\n",
      "Operator 82: 3.4142135623731082\n",
      "Operator 83: 2.1668156228358137\n",
      "Operator 85: 2.273682090859299\n",
      "Operator 86: -1.3853852953238963\n",
      "Operator 87: -4.000000000000014\n",
      "Operator 88: -4.000000000000011\n",
      "Operator 89: -4.000000000000055\n",
      "Operator 90: -4.0000000000000195\n",
      "Operator 91: -3.999999999999998\n",
      "Operator 92: -4.000000000000032\n",
      "Operator 93: -4.000000000000019\n",
      "Operator 94: -4.000000000000024\n",
      "Operator 95: -4.000000000000015\n",
      "Operator 96: -4.000000000000019\n",
      "Operator 97: -2.828427124746156\n",
      "Operator 98: -3.414213562373103\n",
      "Operator 100: -1.8169421014391085\n",
      "Operator 101: -3.0643400409763766\n",
      "Operator 102: 1.581315900234166\n",
      "Operator 103: -2.2736820908592987\n",
      "Operator 104: 4.000000000000014\n",
      "Operator 105: 4.000000000000011\n",
      "Operator 106: 4.000000000000055\n",
      "Operator 107: 4.0000000000000195\n",
      "Operator 108: 3.999999999999998\n",
      "Operator 109: 4.000000000000032\n",
      "Operator 110: 4.000000000000019\n",
      "Operator 111: 4.000000000000024\n",
      "Operator 112: 4.000000000000015\n",
      "Operator 113: 4.000000000000019\n",
      "Operator 114: 2.828427124746156\n",
      "Operator 115: 3.414213562373102\n",
      "Operator 117: 1.8169421014391085\n",
      "Operator 118: 3.064340040976378\n",
      "Operator 119: -1.5813159002341672\n",
      "Operator 120: 2.273682090859299\n",
      "Operator 121: 4.000000000000016\n",
      "Operator 122: -4.000000000000014\n",
      "Operator 123: 4.000000000000011\n",
      "Operator 124: -4.000000000000055\n",
      "Operator 125: 4.0000000000000195\n",
      "Operator 126: -3.999999999999998\n",
      "Operator 127: 4.000000000000032\n",
      "Operator 128: -4.000000000000019\n",
      "Operator 129: 4.000000000000024\n",
      "Operator 130: -4.000000000000015\n",
      "Operator 131: 2.8284271247461423\n",
      "Operator 132: -4.000000000000038\n",
      "Operator 133: 3.414213562373103\n",
      "Operator 135: 3.4142135623731096\n",
      "Operator 136: -0.7879577552573589\n",
      "Operator 138: 4.000000000000014\n",
      "Operator 139: -4.000000000000014\n",
      "Operator 140: -4.000000000000011\n",
      "Operator 141: -4.000000000000055\n",
      "Operator 142: -4.0000000000000195\n",
      "Operator 143: -3.999999999999998\n",
      "Operator 144: -4.000000000000032\n",
      "Operator 145: -4.000000000000019\n",
      "Operator 146: -4.000000000000024\n",
      "Operator 147: -4.000000000000015\n",
      "Operator 148: -4.000000000000019\n",
      "Operator 149: -4.000000000000038\n",
      "Operator 150: -3.414213562373103\n",
      "Operator 152: -3.4142135623731096\n",
      "Operator 153: -2.166815622835812\n",
      "Operator 155: -2.2736820908592987\n",
      "Operator 156: 1.3853852953238963\n",
      "Operator 173: 1.6363382000795323\n",
      "Operator 185: 1.4142135623731398\n",
      "Operator 186: -1.4142135623731422\n",
      "Operator 188: 1.6932750152218303\n",
      "Operator 189: -1.6932750152218285\n",
      "Operator 190: 1.9327494770424767\n",
      "Operator 202: -1.4142135623731389\n",
      "Operator 203: 1.4142135623731418\n",
      "Operator 205: -1.6932750152218334\n",
      "Operator 206: 1.6932750152218254\n",
      "Operator 207: -1.9327494770424756\n",
      "Operator 208: 4.000000000000016\n",
      "Operator 209: -4.000000000000014\n",
      "Operator 223: 1.4142135623731398\n",
      "Operator 224: -0.7526020604627726\n",
      "Operator 226: 0.43540506673092516\n",
      "Operator 227: -1.693275015221829\n",
      "Operator 244: -1.6363382000795301\n",
      "Operator 245: -4.000000000000016\n",
      "Operator 246: 4.000000000000014\n",
      "Operator 247: -4.000000000000011\n",
      "Operator 248: 4.000000000000055\n",
      "Operator 249: -4.0000000000000195\n",
      "Operator 250: 3.999999999999998\n",
      "Operator 251: -4.000000000000032\n",
      "Operator 252: 4.000000000000019\n",
      "Operator 253: -4.000000000000024\n",
      "Operator 254: 4.000000000000015\n",
      "Operator 255: -2.8284271247461423\n",
      "Operator 256: 4.000000000000038\n",
      "Operator 257: -3.414213562373102\n",
      "Operator 259: -3.4142135623731082\n",
      "Operator 260: 0.7879577552573593\n",
      "Operator 262: -4.000000000000011\n",
      "Operator 263: -4.000000000000055\n",
      "Operator 264: -4.0000000000000195\n",
      "Operator 265: -3.999999999999998\n",
      "Operator 266: -4.000000000000032\n",
      "Operator 267: -4.000000000000019\n",
      "Operator 268: -4.000000000000024\n",
      "Operator 269: -4.000000000000015\n",
      "Operator 270: -4.000000000000019\n",
      "Operator 271: -4.000000000000038\n",
      "Operator 272: -3.414213562373102\n",
      "Operator 274: -3.4142135623731096\n",
      "Operator 275: -2.1668156228358137\n",
      "Operator 277: -1.5786157053744236\n",
      "Operator 278: 1.3853852953238963\n",
      "Total gradient norm: 46.09349750478348\n",
      "Operators under consideration (1):\n",
      "[271]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000038)]\n",
      "Operator(s) added to ansatz: [271]\n",
      "Gradients: [np.float64(-4.000000000000038)]\n",
      "Initial energy: -22.12346268292834\n",
      "Optimizing energy with indices [278, 276, 273, 271]...\n",
      "Starting point: [np.float64(0.6553685269360798), np.float64(0.5048173577159957), np.float64(0.3926990816987345), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -23.058333\n",
      "         Iterations: 6\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 88\n",
      "\n",
      "Current energy: -23.058333386736756\n",
      "(change of -0.9348707038084143)\n",
      "Current ansatz: [278, 276, 273, 271]\n",
      "On iteration 4.\n",
      "\n",
      "*** ADAPT-VQE Iteration 5 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.0000000000000195\n",
      "Operator 1: -4.000000000000014\n",
      "Operator 2: 4.00000000000006\n",
      "Operator 3: -4.000000000000026\n",
      "Operator 4: 4.000000000000001\n",
      "Operator 5: -4.000000000000038\n",
      "Operator 6: 4.000000000000025\n",
      "Operator 7: -4.000000000000028\n",
      "Operator 8: 4.000000000000018\n",
      "Operator 9: -3.264585425219904\n",
      "Operator 10: -1.1832257519019436e-08\n",
      "Operator 11: -2.529170798345911\n",
      "Operator 12: -2.257776694616282e-07\n",
      "Operator 13: -3.264585373126036\n",
      "Operator 14: 3.0643400409612997\n",
      "Operator 16: 1.5786157053787016\n",
      "Operator 17: 4.000000000000014\n",
      "Operator 18: -4.00000000000006\n",
      "Operator 19: 4.000000000000026\n",
      "Operator 20: -4.000000000000001\n",
      "Operator 21: 4.000000000000038\n",
      "Operator 22: -4.000000000000025\n",
      "Operator 23: 4.000000000000028\n",
      "Operator 24: -4.000000000000018\n",
      "Operator 25: 3.264585425219904\n",
      "Operator 26: 1.1832257519019436e-08\n",
      "Operator 27: 2.5291707983459117\n",
      "Operator 28: 2.257776705377781e-07\n",
      "Operator 29: 3.264585373126036\n",
      "Operator 30: -3.064340040961297\n",
      "Operator 32: -1.5786157053787055\n",
      "Operator 34: -4.000000000000023\n",
      "Operator 44: 1.5494591644575377\n",
      "Operator 45: -0.9797116978145193\n",
      "Operator 46: 1.5494592069737878\n",
      "Operator 47: -0.9797117650559124\n",
      "Operator 49: 1.0706454084706052\n",
      "Operator 50: -0.4354050667497529\n",
      "Operator 51: 1.9327494770373264\n",
      "Operator 52: 4.000000000000014\n",
      "Operator 53: 4.00000000000006\n",
      "Operator 54: 4.000000000000026\n",
      "Operator 55: 4.000000000000001\n",
      "Operator 56: 4.000000000000038\n",
      "Operator 57: 4.000000000000025\n",
      "Operator 58: 4.000000000000028\n",
      "Operator 59: 4.000000000000018\n",
      "Operator 60: 3.264585425219904\n",
      "Operator 61: -1.1832257519019436e-08\n",
      "Operator 62: 2.5291707983459117\n",
      "Operator 63: -0.9299947402282819\n",
      "Operator 64: 2.0641735411206117\n",
      "Operator 65: 1.937559797041983\n",
      "Operator 67: 1.5786157053787055\n",
      "Operator 68: -1.3853852953059\n",
      "Operator 70: 4.000000000000014\n",
      "Operator 71: 4.00000000000006\n",
      "Operator 72: 4.000000000000026\n",
      "Operator 73: 4.000000000000001\n",
      "Operator 74: 4.000000000000038\n",
      "Operator 75: 4.000000000000025\n",
      "Operator 76: 4.000000000000028\n",
      "Operator 77: 4.000000000000018\n",
      "Operator 78: 3.264585425219904\n",
      "Operator 79: -1.1832257519019436e-08\n",
      "Operator 80: 2.7995881159007228\n",
      "Operator 81: -0.9299947402282827\n",
      "Operator 82: 3.264585373126036\n",
      "Operator 83: 1.9375597970419833\n",
      "Operator 85: 2.2736820908657243\n",
      "Operator 86: -1.3853852953059\n",
      "Operator 87: -4.0000000000000195\n",
      "Operator 88: -4.000000000000014\n",
      "Operator 89: -4.00000000000006\n",
      "Operator 90: -4.000000000000026\n",
      "Operator 91: -4.000000000000001\n",
      "Operator 92: -4.000000000000038\n",
      "Operator 93: -4.000000000000025\n",
      "Operator 94: -4.000000000000028\n",
      "Operator 95: -2.5291708504397894\n",
      "Operator 96: -3.264585425219904\n",
      "Operator 97: 0.9299946304706588\n",
      "Operator 98: -2.7995881159007245\n",
      "Operator 99: 2.257776694616282e-07\n",
      "Operator 100: -1.7373144648772683\n",
      "Operator 101: -3.064340040961297\n",
      "Operator 102: 1.5813159002496258\n",
      "Operator 103: -2.273682090865724\n",
      "Operator 104: 4.0000000000000195\n",
      "Operator 105: 4.000000000000014\n",
      "Operator 106: 4.00000000000006\n",
      "Operator 107: 4.000000000000026\n",
      "Operator 108: 4.000000000000001\n",
      "Operator 109: 4.000000000000038\n",
      "Operator 110: 4.000000000000025\n",
      "Operator 111: 4.000000000000028\n",
      "Operator 112: 2.5291708504397894\n",
      "Operator 113: 3.264585425219904\n",
      "Operator 114: -0.9299946304706588\n",
      "Operator 115: 2.7995881159007228\n",
      "Operator 116: -2.257776705377781e-07\n",
      "Operator 117: 1.7373144648772687\n",
      "Operator 118: 3.0643400409612997\n",
      "Operator 119: -1.5813159002496258\n",
      "Operator 120: 2.2736820908657243\n",
      "Operator 121: 4.000000000000023\n",
      "Operator 122: -4.0000000000000195\n",
      "Operator 123: 4.000000000000014\n",
      "Operator 124: -4.00000000000006\n",
      "Operator 125: 4.000000000000026\n",
      "Operator 126: -4.000000000000001\n",
      "Operator 127: 4.000000000000038\n",
      "Operator 128: -4.000000000000025\n",
      "Operator 129: 2.529170850439796\n",
      "Operator 130: -4.000000000000018\n",
      "Operator 131: 2.0641734890266963\n",
      "Operator 132: 1.18322591404571e-08\n",
      "Operator 133: 2.7995881159007245\n",
      "Operator 134: 1.2015210729217858e-07\n",
      "Operator 135: 3.264585373126036\n",
      "Operator 136: -0.78795775528314\n",
      "Operator 138: 4.0000000000000195\n",
      "Operator 139: -4.0000000000000195\n",
      "Operator 140: -4.000000000000014\n",
      "Operator 141: -4.00000000000006\n",
      "Operator 142: -4.000000000000026\n",
      "Operator 143: -4.000000000000001\n",
      "Operator 144: -4.000000000000038\n",
      "Operator 145: -4.000000000000025\n",
      "Operator 146: -4.000000000000028\n",
      "Operator 147: -4.000000000000018\n",
      "Operator 148: -3.264585425219904\n",
      "Operator 149: 1.1832257519019436e-08\n",
      "Operator 150: -2.7995881159007245\n",
      "Operator 151: 0.9299947402282819\n",
      "Operator 152: -3.264585373126036\n",
      "Operator 153: -1.937559797041983\n",
      "Operator 155: -2.273682090865724\n",
      "Operator 156: 1.3853852953059\n",
      "Operator 168: 1.2004118840993168\n",
      "Operator 173: 1.636338200084332\n",
      "Operator 183: 1.5494591644575377\n",
      "Operator 184: -1.5494591644575468\n",
      "Operator 185: 1.5494592069737878\n",
      "Operator 186: -1.5494592069737907\n",
      "Operator 188: 1.6932750152313103\n",
      "Operator 189: -1.6932750152313085\n",
      "Operator 190: 1.9327494770373261\n",
      "Operator 200: -1.5494591644575388\n",
      "Operator 201: 1.5494591644575477\n",
      "Operator 202: -1.5494592069737871\n",
      "Operator 203: 1.5494592069737911\n",
      "Operator 205: -1.6932750152313139\n",
      "Operator 206: 1.6932750152313052\n",
      "Operator 207: -1.9327494770373252\n",
      "Operator 208: 4.000000000000023\n",
      "Operator 209: -4.0000000000000195\n",
      "Operator 221: 0.9797116978145131\n",
      "Operator 222: -1.5494591644575468\n",
      "Operator 223: 0.9797117650559103\n",
      "Operator 224: -0.8245757379091527\n",
      "Operator 226: 0.4354050667497512\n",
      "Operator 227: -1.6932750152313083\n",
      "Operator 239: -1.2004118840993174\n",
      "Operator 244: -1.6363382000843298\n",
      "Operator 245: -4.000000000000023\n",
      "Operator 246: 4.0000000000000195\n",
      "Operator 247: -4.000000000000014\n",
      "Operator 248: 4.00000000000006\n",
      "Operator 249: -4.000000000000026\n",
      "Operator 250: 4.000000000000001\n",
      "Operator 251: -4.000000000000038\n",
      "Operator 252: 4.000000000000025\n",
      "Operator 253: -2.529170850439796\n",
      "Operator 254: 4.000000000000018\n",
      "Operator 255: -2.0641734890266967\n",
      "Operator 256: -1.18322591404571e-08\n",
      "Operator 257: -2.7995881159007228\n",
      "Operator 258: -1.2015210686240674e-07\n",
      "Operator 259: -3.264585373126036\n",
      "Operator 260: 0.7879577552831407\n",
      "Operator 262: -4.000000000000014\n",
      "Operator 263: -4.00000000000006\n",
      "Operator 264: -4.000000000000026\n",
      "Operator 265: -4.000000000000001\n",
      "Operator 266: -4.000000000000038\n",
      "Operator 267: -4.000000000000025\n",
      "Operator 268: -4.000000000000028\n",
      "Operator 269: -4.000000000000018\n",
      "Operator 270: -3.264585425219904\n",
      "Operator 271: 1.1832257519019436e-08\n",
      "Operator 272: -2.529170798345911\n",
      "Operator 273: 0.9299947402282827\n",
      "Operator 274: -2.0641735411206117\n",
      "Operator 275: -1.9375597970419833\n",
      "Operator 277: -1.5786157053787016\n",
      "Operator 278: 1.3853852953059\n",
      "Total gradient norm: 42.97733730417354\n",
      "Operators under consideration (1):\n",
      "[268]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000028)]\n",
      "Operator(s) added to ansatz: [268]\n",
      "Gradients: [np.float64(-4.000000000000028)]\n",
      "Initial energy: -23.058333386736756\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268]...\n",
      "Starting point: [np.float64(0.6553685269310722), np.float64(0.504817357720449), np.float64(0.4431436691761263), np.float64(0.44314365236577835), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -23.886761\n",
      "         Iterations: 7\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "\n",
      "Current energy: -23.88676051148301\n",
      "(change of -0.8284271247462556)\n",
      "Current ansatz: [278, 276, 273, 271, 268]\n",
      "On iteration 5.\n",
      "\n",
      "*** ADAPT-VQE Iteration 6 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000032\n",
      "Operator 1: -4.000000000000026\n",
      "Operator 2: 4.000000000000072\n",
      "Operator 3: -4.000000000000039\n",
      "Operator 4: 4.000000000000015\n",
      "Operator 5: -4.00000000000005\n",
      "Operator 6: 3.414213562361547\n",
      "Operator 8: 3.4142135623615406\n",
      "Operator 9: -3.264585445635312\n",
      "Operator 11: -2.529170891277838\n",
      "Operator 13: -3.264585445642571\n",
      "Operator 14: 3.0643400399249554\n",
      "Operator 16: 1.5786157054652006\n",
      "Operator 17: 4.000000000000026\n",
      "Operator 18: -4.000000000000073\n",
      "Operator 19: 4.000000000000039\n",
      "Operator 20: -4.000000000000015\n",
      "Operator 21: 4.00000000000005\n",
      "Operator 22: -3.414213562361547\n",
      "Operator 24: -3.414213562361545\n",
      "Operator 25: 3.264585445635311\n",
      "Operator 27: 2.529170891277839\n",
      "Operator 29: 3.264585445642571\n",
      "Operator 30: -3.0643400399249536\n",
      "Operator 32: -1.5786157054652037\n",
      "Operator 34: -4.0000000000000355\n",
      "Operator 41: 1.4142135623846839\n",
      "Operator 42: -1.4142135623846852\n",
      "Operator 44: 1.0956330705688249\n",
      "Operator 45: -0.9797117434599918\n",
      "Operator 46: 1.5494591477896975\n",
      "Operator 47: -0.9797117434506518\n",
      "Operator 49: 1.0706454702777062\n",
      "Operator 50: -0.4354050678679028\n",
      "Operator 51: 1.932749476738558\n",
      "Operator 52: 4.000000000000026\n",
      "Operator 53: 4.000000000000073\n",
      "Operator 54: 4.000000000000039\n",
      "Operator 55: 4.000000000000015\n",
      "Operator 56: 4.00000000000005\n",
      "Operator 57: 3.414213562361547\n",
      "Operator 59: 3.414213562361545\n",
      "Operator 60: 2.3084105063527374\n",
      "Operator 62: 2.529170891277839\n",
      "Operator 63: -0.929994541918572\n",
      "Operator 64: 2.064173620296186\n",
      "Operator 65: 1.9375599074943568\n",
      "Operator 67: 1.5786157054652037\n",
      "Operator 68: -1.3853852943450082\n",
      "Operator 70: 4.000000000000026\n",
      "Operator 71: 4.000000000000072\n",
      "Operator 72: 4.000000000000039\n",
      "Operator 73: 4.000000000000015\n",
      "Operator 74: 4.00000000000005\n",
      "Operator 75: 3.414213562361547\n",
      "Operator 77: 3.414213562361545\n",
      "Operator 78: 2.3084105063527383\n",
      "Operator 80: 2.7995881746609084\n",
      "Operator 81: -0.929994541918572\n",
      "Operator 82: 3.264585445642571\n",
      "Operator 83: 1.937559907494358\n",
      "Operator 85: 2.273682091196789\n",
      "Operator 86: -1.3853852943450082\n",
      "Operator 87: -4.000000000000032\n",
      "Operator 88: -4.000000000000026\n",
      "Operator 89: -4.000000000000072\n",
      "Operator 90: -4.000000000000039\n",
      "Operator 91: -4.000000000000015\n",
      "Operator 92: -2.8284271247230732\n",
      "Operator 93: -3.414213562361547\n",
      "Operator 95: -2.1587823896264964\n",
      "Operator 96: -3.264585445635311\n",
      "Operator 97: 0.9299945419338312\n",
      "Operator 98: -2.799588174660909\n",
      "Operator 100: -1.7373145017767646\n",
      "Operator 101: -3.0643400399249536\n",
      "Operator 102: 1.5813159013570774\n",
      "Operator 103: -2.2736820911967897\n",
      "Operator 104: 4.000000000000032\n",
      "Operator 105: 4.000000000000026\n",
      "Operator 106: 4.000000000000073\n",
      "Operator 107: 4.000000000000039\n",
      "Operator 108: 4.000000000000015\n",
      "Operator 109: 2.8284271247230732\n",
      "Operator 110: 3.414213562361547\n",
      "Operator 112: 2.1587823896264986\n",
      "Operator 113: 3.264585445635312\n",
      "Operator 114: -0.9299945419338329\n",
      "Operator 115: 2.7995881746609084\n",
      "Operator 117: 1.7373145017767646\n",
      "Operator 118: 3.0643400399249554\n",
      "Operator 119: -1.5813159013570774\n",
      "Operator 120: 2.273682091196789\n",
      "Operator 121: 4.0000000000000355\n",
      "Operator 122: -4.000000000000032\n",
      "Operator 123: 4.000000000000026\n",
      "Operator 124: -4.000000000000072\n",
      "Operator 125: 4.000000000000039\n",
      "Operator 126: -2.8284271247230475\n",
      "Operator 127: 4.00000000000005\n",
      "Operator 128: -3.414213562361547\n",
      "Operator 130: -3.4142135623615406\n",
      "Operator 131: 2.064173620303406\n",
      "Operator 133: 2.799588174660909\n",
      "Operator 135: 3.264585445642571\n",
      "Operator 136: -0.7879577567370497\n",
      "Operator 138: 4.000000000000032\n",
      "Operator 139: -4.000000000000032\n",
      "Operator 140: -4.000000000000026\n",
      "Operator 141: -4.000000000000073\n",
      "Operator 142: -4.000000000000039\n",
      "Operator 143: -4.000000000000015\n",
      "Operator 144: -4.00000000000005\n",
      "Operator 145: -3.414213562361547\n",
      "Operator 147: -3.4142135623615406\n",
      "Operator 148: -2.3084105063527374\n",
      "Operator 150: -2.799588174660909\n",
      "Operator 151: 0.929994541918572\n",
      "Operator 152: -3.264585445642571\n",
      "Operator 153: -1.9375599074943568\n",
      "Operator 155: -2.273682091196789\n",
      "Operator 156: 1.3853852943450082\n",
      "Operator 168: 1.2004118253391471\n",
      "Operator 173: 1.6363382004608984\n",
      "Operator 180: 1.4142135623846839\n",
      "Operator 181: -1.4142135623846852\n",
      "Operator 183: 1.5494591477955955\n",
      "Operator 184: -1.549459147795604\n",
      "Operator 185: 1.5494591477896975\n",
      "Operator 186: -1.5494591477896997\n",
      "Operator 188: 1.6932750158827337\n",
      "Operator 189: -1.6932750158827314\n",
      "Operator 190: 1.9327494767385582\n",
      "Operator 197: -1.414213562384683\n",
      "Operator 198: 1.4142135623846788\n",
      "Operator 200: -1.5494591477955972\n",
      "Operator 201: 1.5494591477956052\n",
      "Operator 202: -1.5494591477896966\n",
      "Operator 203: 1.5494591477897002\n",
      "Operator 205: -1.6932750158827368\n",
      "Operator 206: 1.6932750158827279\n",
      "Operator 207: -1.9327494767385573\n",
      "Operator 208: 4.0000000000000355\n",
      "Operator 209: -4.000000000000032\n",
      "Operator 218: 1.4142135623846839\n",
      "Operator 219: -0.8941969440058481\n",
      "Operator 221: 0.9797117434599862\n",
      "Operator 222: -1.549459147795604\n",
      "Operator 223: 0.9797117434506499\n",
      "Operator 224: -0.8245757056102616\n",
      "Operator 226: 0.4354050678679009\n",
      "Operator 227: -1.6932750158827319\n",
      "Operator 239: -1.2004118253391476\n",
      "Operator 244: -1.6363382004608962\n",
      "Operator 245: -4.0000000000000355\n",
      "Operator 246: 4.000000000000032\n",
      "Operator 247: -4.000000000000026\n",
      "Operator 248: 4.000000000000073\n",
      "Operator 249: -4.000000000000039\n",
      "Operator 250: 2.8284271247230475\n",
      "Operator 251: -4.00000000000005\n",
      "Operator 252: 3.414213562361547\n",
      "Operator 254: 3.414213562361545\n",
      "Operator 255: -2.064173620303407\n",
      "Operator 257: -2.7995881746609084\n",
      "Operator 259: -3.264585445642571\n",
      "Operator 260: 0.7879577567370502\n",
      "Operator 262: -4.000000000000026\n",
      "Operator 263: -4.000000000000072\n",
      "Operator 264: -4.000000000000039\n",
      "Operator 265: -4.000000000000015\n",
      "Operator 266: -4.00000000000005\n",
      "Operator 267: -3.414213562361547\n",
      "Operator 269: -3.4142135623615406\n",
      "Operator 270: -2.3084105063527383\n",
      "Operator 272: -2.529170891277838\n",
      "Operator 273: 0.929994541918572\n",
      "Operator 274: -2.0641736202961853\n",
      "Operator 275: -1.937559907494358\n",
      "Operator 277: -1.5786157054652006\n",
      "Operator 278: 1.3853852943450082\n",
      "Total gradient norm: 39.81071349149171\n",
      "Operators under consideration (1):\n",
      "[264]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-4.000000000000039)]\n",
      "Operator(s) added to ansatz: [264]\n",
      "Gradients: [np.float64(-4.000000000000039)]\n",
      "Initial energy: -23.88676051148301\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264]...\n",
      "Starting point: [np.float64(0.6553685266405932), np.float64(0.5048173580264689), np.float64(0.443143645775534), np.float64(0.443143645777869), np.float64(0.3926990817028168), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -24.715188\n",
      "         Iterations: 6\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 52\n",
      "\n",
      "Current energy: -24.715187636229196\n",
      "(change of -0.8284271247461845)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264]\n",
      "On iteration 6.\n",
      "\n",
      "*** ADAPT-VQE Iteration 7 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 4.000000000000031\n",
      "Operator 1: -4.0000000000000275\n",
      "Operator 2: 3.414213562827059\n",
      "Operator 4: 3.4142135628270127\n",
      "Operator 5: -4.000000000000045\n",
      "Operator 6: 3.4142135633753226\n",
      "Operator 8: 3.4142135633753163\n",
      "Operator 9: -3.2645854467556843\n",
      "Operator 11: -2.5291708947585723\n",
      "Operator 13: -3.264585448002932\n",
      "Operator 14: 3.064340046534734\n",
      "Operator 16: 1.5786157183099325\n",
      "Operator 17: 4.0000000000000275\n",
      "Operator 18: -3.4142135628270616\n",
      "Operator 20: -3.4142135628270105\n",
      "Operator 21: 4.000000000000045\n",
      "Operator 22: -3.4142135633753226\n",
      "Operator 24: -3.41421356337532\n",
      "Operator 25: 3.2645854467556825\n",
      "Operator 27: 2.529170894758573\n",
      "Operator 29: 3.2645854480029306\n",
      "Operator 30: -3.0643400465347312\n",
      "Operator 32: -1.5786157183099363\n",
      "Operator 33: 1.0427410046531804e-08\n",
      "Operator 34: -4.0000000000000355\n",
      "Operator 37: 1.4142135619192175\n",
      "Operator 38: -1.4142135619191922\n",
      "Operator 41: 1.4142135613709061\n",
      "Operator 42: -1.4142135613709073\n",
      "Operator 44: 1.0956330707076571\n",
      "Operator 45: -0.979711744710473\n",
      "Operator 46: 1.5494591458632958\n",
      "Operator 47: -0.9797117431005877\n",
      "Operator 49: 1.0706454696490884\n",
      "Operator 50: -0.43540507207831647\n",
      "Operator 51: 1.9327494750795289\n",
      "Operator 52: 4.0000000000000275\n",
      "Operator 53: 3.4142135628270616\n",
      "Operator 55: 3.4142135628270105\n",
      "Operator 56: 2.8284271256540374\n",
      "Operator 57: 2.4142135638566407\n",
      "Operator 59: 3.41421356337532\n",
      "Operator 60: 2.3084105087997395\n",
      "Operator 62: 2.529170894758573\n",
      "Operator 63: -0.9299945352404195\n",
      "Operator 64: 2.0641736236173998\n",
      "Operator 65: 1.9375599152901475\n",
      "Operator 67: 1.5786157183099363\n",
      "Operator 68: -1.3853852826506188\n",
      "Operator 69: -1.0427410046531804e-08\n",
      "Operator 70: 4.0000000000000275\n",
      "Operator 71: 3.414213562827059\n",
      "Operator 73: 3.4142135628270105\n",
      "Operator 74: 2.8284271256540374\n",
      "Operator 75: 3.4142135633753226\n",
      "Operator 77: 3.41421356337532\n",
      "Operator 78: 2.3084105087997413\n",
      "Operator 80: 2.79958817686175\n",
      "Operator 81: -0.9299945352404196\n",
      "Operator 82: 3.264585448002931\n",
      "Operator 83: 1.9375599152901484\n",
      "Operator 85: 2.2736820962144666\n",
      "Operator 86: -1.3853852826506223\n",
      "Operator 87: -4.000000000000031\n",
      "Operator 88: -2.8284271256540148\n",
      "Operator 89: -3.4142135628270616\n",
      "Operator 91: -3.4142135628270127\n",
      "Operator 92: -2.828427126750623\n",
      "Operator 93: -3.4142135633753226\n",
      "Operator 95: -2.1587823921800955\n",
      "Operator 96: -3.2645854467556825\n",
      "Operator 97: 0.9299945378682324\n",
      "Operator 98: -2.799588176861752\n",
      "Operator 100: -1.7373145138219737\n",
      "Operator 101: -3.0643400465347312\n",
      "Operator 102: 1.5813158913774295\n",
      "Operator 103: -2.2736820962144675\n",
      "Operator 104: 4.000000000000031\n",
      "Operator 105: 2.8284271256540148\n",
      "Operator 106: 3.414213562827059\n",
      "Operator 108: 3.4142135628270105\n",
      "Operator 109: 2.828427126750623\n",
      "Operator 110: 3.4142135633753226\n",
      "Operator 112: 2.158782392180098\n",
      "Operator 113: 3.2645854467556843\n",
      "Operator 114: -0.9299945378682324\n",
      "Operator 115: 2.79958817686175\n",
      "Operator 117: 1.7373145138219734\n",
      "Operator 118: 3.064340046534734\n",
      "Operator 119: -1.5813158913774266\n",
      "Operator 120: 2.2736820962144675\n",
      "Operator 121: 4.0000000000000355\n",
      "Operator 122: -2.8284271256540183\n",
      "Operator 123: 4.0000000000000275\n",
      "Operator 124: -3.4142135628270616\n",
      "Operator 126: -2.4142135644049256\n",
      "Operator 127: 4.000000000000045\n",
      "Operator 128: -3.4142135633753226\n",
      "Operator 130: -3.4142135633753163\n",
      "Operator 131: 2.064173624864611\n",
      "Operator 133: 2.799588176861752\n",
      "Operator 135: 3.264585448002932\n",
      "Operator 136: -0.7879577679896823\n",
      "Operator 138: 4.000000000000031\n",
      "Operator 139: -4.000000000000031\n",
      "Operator 140: -4.0000000000000275\n",
      "Operator 141: -3.4142135628270616\n",
      "Operator 143: -3.4142135628270127\n",
      "Operator 144: -2.8284271256540374\n",
      "Operator 145: -3.4142135633753226\n",
      "Operator 147: -3.4142135633753163\n",
      "Operator 148: -2.3084105087997395\n",
      "Operator 150: -2.799588176861752\n",
      "Operator 151: 0.9299945352404195\n",
      "Operator 152: -3.26458544800293\n",
      "Operator 153: -1.9375599152901475\n",
      "Operator 155: -2.2736820962144666\n",
      "Operator 156: 1.3853852826506223\n",
      "Operator 168: 1.200411823138303\n",
      "Operator 173: 1.636338195041302\n",
      "Operator 176: 1.4142135619192173\n",
      "Operator 177: -1.4142135619191918\n",
      "Operator 180: 1.4142135613709068\n",
      "Operator 181: -1.4142135613709075\n",
      "Operator 183: 1.5494591468812065\n",
      "Operator 184: -1.5494591468812149\n",
      "Operator 185: 1.5494591458632962\n",
      "Operator 186: -1.5494591458632976\n",
      "Operator 188: 1.6932750117280295\n",
      "Operator 189: -1.6932750117280273\n",
      "Operator 190: 1.9327494750795287\n",
      "Operator 193: -1.4142135619192122\n",
      "Operator 194: 1.4142135619191953\n",
      "Operator 197: -1.4142135613709055\n",
      "Operator 198: 1.4142135613709013\n",
      "Operator 200: -1.5494591468812078\n",
      "Operator 201: 1.549459146881216\n",
      "Operator 202: -1.5494591458632954\n",
      "Operator 203: 1.549459145863298\n",
      "Operator 205: -1.6932750117280335\n",
      "Operator 206: 1.6932750117280242\n",
      "Operator 207: -1.9327494750795282\n",
      "Operator 208: 4.0000000000000355\n",
      "Operator 209: -4.000000000000031\n",
      "Operator 214: 1.4142135619192175\n",
      "Operator 215: -1.4142135619191922\n",
      "Operator 218: 1.4142135613709061\n",
      "Operator 219: -0.8941969441570672\n",
      "Operator 221: 0.9797117447104668\n",
      "Operator 222: -1.5494591468812149\n",
      "Operator 223: 0.9797117431005861\n",
      "Operator 224: -0.82457570970588\n",
      "Operator 226: 0.435405072078315\n",
      "Operator 227: -1.6932750117280273\n",
      "Operator 239: -1.2004118231383036\n",
      "Operator 244: -1.6363381950413003\n",
      "Operator 245: -4.0000000000000355\n",
      "Operator 246: 2.8284271256540183\n",
      "Operator 247: -4.0000000000000275\n",
      "Operator 248: 3.414213562827059\n",
      "Operator 250: 2.4142135644049243\n",
      "Operator 251: -4.000000000000045\n",
      "Operator 252: 3.4142135633753226\n",
      "Operator 254: 3.41421356337532\n",
      "Operator 255: -2.0641736248646123\n",
      "Operator 257: -2.79958817686175\n",
      "Operator 259: -3.2645854480029306\n",
      "Operator 260: 0.787957767989683\n",
      "Operator 262: -4.0000000000000275\n",
      "Operator 263: -3.414213562827059\n",
      "Operator 265: -3.4142135628270127\n",
      "Operator 266: -2.8284271256540374\n",
      "Operator 267: -2.4142135638566407\n",
      "Operator 269: -3.4142135633753163\n",
      "Operator 270: -2.3084105087997413\n",
      "Operator 272: -2.5291708947585723\n",
      "Operator 273: 0.9299945352404196\n",
      "Operator 274: -2.0641736236174006\n",
      "Operator 275: -1.9375599152901484\n",
      "Operator 277: -1.5786157183099325\n",
      "Operator 278: 1.3853852826506188\n",
      "Total gradient norm: 35.8150287736963\n",
      "Operators under consideration (1):\n",
      "[208]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.0000000000000355)]\n",
      "Operator(s) added to ansatz: [208]\n",
      "Gradients: [np.float64(4.0000000000000355)]\n",
      "Initial energy: -24.715187636229196\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208]...\n",
      "Starting point: [np.float64(0.6553685250276173), np.float64(0.5048173560746947), np.float64(0.4431436450138609), np.float64(0.44314364541633267), np.float64(0.39269908134439363), np.float64(0.392699081538245), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -25.951256\n",
      "         Iterations: 16\n",
      "         Function evaluations: 187\n",
      "         Gradient evaluations: 172\n",
      "\n",
      "Current energy: -25.95125561372905\n",
      "(change of -1.2360679774998538)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208]\n",
      "On iteration 7.\n",
      "\n",
      "*** ADAPT-VQE Iteration 8 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.8944272066797856\n",
      "Operator 1: -4.000000000000041\n",
      "Operator 2: 3.414213559796187\n",
      "Operator 3: 1.0307915257703504e-08\n",
      "Operator 4: 3.4142135597961394\n",
      "Operator 5: -4.000000000000053\n",
      "Operator 6: 3.414213557352668\n",
      "Operator 7: 2.0081841072084714e-08\n",
      "Operator 8: 3.4142135573526655\n",
      "Operator 9: -3.2645854547784756\n",
      "Operator 10: 1.9146693298810236e-08\n",
      "Operator 11: -2.5291709150519814\n",
      "Operator 12: 4.171448056073769e-08\n",
      "Operator 13: -3.2645854602735564\n",
      "Operator 14: 3.064340044258488\n",
      "Operator 16: 1.5786157123466542\n",
      "Operator 17: 4.000000000000039\n",
      "Operator 18: -3.4142135597961873\n",
      "Operator 19: -1.030791542764909e-08\n",
      "Operator 20: -3.414213559796136\n",
      "Operator 21: 4.000000000000053\n",
      "Operator 22: -3.41421355735267\n",
      "Operator 23: -2.0081840802794027e-08\n",
      "Operator 24: -3.41421355735267\n",
      "Operator 25: 3.2645854547784756\n",
      "Operator 26: -1.9146693298810236e-08\n",
      "Operator 27: 2.5291709150519814\n",
      "Operator 28: -4.171448056073769e-08\n",
      "Operator 29: 3.2645854602735564\n",
      "Operator 30: -3.0643400442584854\n",
      "Operator 32: -1.578615712346658\n",
      "Operator 34: -3.9199693181367004e-08\n",
      "Operator 37: 1.4142135649500975\n",
      "Operator 38: -1.4142135649500724\n",
      "Operator 41: 1.4142135673935672\n",
      "Operator 42: -1.4142135673935696\n",
      "Operator 44: 1.0956330614117573\n",
      "Operator 45: -0.9797117500767802\n",
      "Operator 46: 1.5494591358486798\n",
      "Operator 47: -0.9797117429839073\n",
      "Operator 49: 1.0706454809425285\n",
      "Operator 50: -0.43540506932464795\n",
      "Operator 51: 1.9327494760605952\n",
      "Operator 52: 1.7888544133595374\n",
      "Operator 53: 1.526882748648337\n",
      "Operator 54: -1.030791542764909e-08\n",
      "Operator 55: 3.414213559796136\n",
      "Operator 56: 2.828427119592286\n",
      "Operator 57: 2.4142135544239496\n",
      "Operator 58: -2.0081840802794027e-08\n",
      "Operator 59: 3.41421355735267\n",
      "Operator 60: 2.3084105046419694\n",
      "Operator 61: 1.3538758327696909e-08\n",
      "Operator 62: 2.5291709150519814\n",
      "Operator 63: -0.9299944994463736\n",
      "Operator 64: 2.064173644471561\n",
      "Operator 65: 1.937559932651567\n",
      "Operator 67: 1.578615712346658\n",
      "Operator 68: -1.3853852887265585\n",
      "Operator 70: 1.7888544133595374\n",
      "Operator 71: 3.414213559796187\n",
      "Operator 72: -1.0307915257703504e-08\n",
      "Operator 73: 3.414213559796136\n",
      "Operator 74: 2.828427119592286\n",
      "Operator 75: 3.414213557352668\n",
      "Operator 76: -2.0081841072084714e-08\n",
      "Operator 77: 3.41421355735267\n",
      "Operator 78: 2.3084105046419703\n",
      "Operator 79: 1.9146693298810236e-08\n",
      "Operator 80: 2.799588189693128\n",
      "Operator 81: -0.9299944994463736\n",
      "Operator 82: 3.2645854602735564\n",
      "Operator 83: 1.9375599326515673\n",
      "Operator 85: 2.2736820936670346\n",
      "Operator 86: -1.385385288726555\n",
      "Operator 87: -2.8944272066797856\n",
      "Operator 88: -2.8284271195922672\n",
      "Operator 89: -3.4142135597961873\n",
      "Operator 90: 1.0307915257703504e-08\n",
      "Operator 91: -3.4142135597961394\n",
      "Operator 92: -2.82842711470531\n",
      "Operator 93: -3.414213557352668\n",
      "Operator 94: 2.0081841072084714e-08\n",
      "Operator 95: -2.1587824020677675\n",
      "Operator 96: -3.2645854547784756\n",
      "Operator 97: 0.9299945110240324\n",
      "Operator 98: -2.799588189693129\n",
      "Operator 99: -4.171448056073769e-08\n",
      "Operator 100: -1.7373145166365225\n",
      "Operator 101: -3.064340044258485\n",
      "Operator 102: 1.5813158951494137\n",
      "Operator 103: -2.2736820936670346\n",
      "Operator 104: 2.8944272066797962\n",
      "Operator 105: 2.8284271195922654\n",
      "Operator 106: 3.414213559796187\n",
      "Operator 107: -1.030791542764909e-08\n",
      "Operator 108: 3.414213559796136\n",
      "Operator 109: 2.82842711470531\n",
      "Operator 110: 3.41421355735267\n",
      "Operator 111: -2.0081840802794027e-08\n",
      "Operator 112: 2.1587824020677697\n",
      "Operator 113: 3.2645854547784756\n",
      "Operator 114: -0.9299945110240324\n",
      "Operator 115: 2.799588189693128\n",
      "Operator 116: 4.171448056073769e-08\n",
      "Operator 117: 1.7373145166365223\n",
      "Operator 118: 3.064340044258488\n",
      "Operator 119: -1.5813158951494137\n",
      "Operator 120: 2.273682093667035\n",
      "Operator 121: 3.9199693124492626e-08\n",
      "Operator 122: -2.046669101764678\n",
      "Operator 123: 4.000000000000041\n",
      "Operator 124: -3.4142135597961873\n",
      "Operator 125: -1.0307915257703504e-08\n",
      "Operator 126: -2.414213551980453\n",
      "Operator 127: 4.000000000000053\n",
      "Operator 128: -3.414213557352668\n",
      "Operator 129: -1.2697601706770873e-08\n",
      "Operator 130: -3.4142135573526655\n",
      "Operator 131: 2.0641736499666052\n",
      "Operator 132: -1.9146693298810236e-08\n",
      "Operator 133: 2.799588189693129\n",
      "Operator 134: -2.2199200033592556e-08\n",
      "Operator 135: 3.2645854602735564\n",
      "Operator 136: -0.7879577617552185\n",
      "Operator 138: 2.8944272066797962\n",
      "Operator 139: -2.8944272066797856\n",
      "Operator 140: -1.7888544133595374\n",
      "Operator 141: -3.4142135597961873\n",
      "Operator 142: 1.0307915257703504e-08\n",
      "Operator 143: -3.4142135597961394\n",
      "Operator 144: -2.828427119592286\n",
      "Operator 145: -3.41421355735267\n",
      "Operator 146: 2.0081840802794027e-08\n",
      "Operator 147: -3.4142135573526655\n",
      "Operator 148: -2.3084105046419694\n",
      "Operator 149: -1.9146693298810236e-08\n",
      "Operator 150: -2.799588189693129\n",
      "Operator 151: 0.9299944994463736\n",
      "Operator 152: -3.264585460273556\n",
      "Operator 153: -1.937559932651567\n",
      "Operator 155: -2.273682093667034\n",
      "Operator 156: 1.385385288726555\n",
      "Operator 168: 1.2004118103069341\n",
      "Operator 173: 1.6363381972545818\n",
      "Operator 176: 1.414213564950098\n",
      "Operator 177: -1.4142135649500724\n",
      "Operator 180: 1.4142135673935672\n",
      "Operator 181: -1.4142135673935696\n",
      "Operator 183: 1.549459140333443\n",
      "Operator 184: -1.5494591403334508\n",
      "Operator 185: 1.5494591358486796\n",
      "Operator 186: -1.5494591358486804\n",
      "Operator 188: 1.6932750131588143\n",
      "Operator 189: -1.6932750131588121\n",
      "Operator 190: 1.9327494760605954\n",
      "Operator 193: -1.414213564950093\n",
      "Operator 194: 1.4142135649500762\n",
      "Operator 197: -1.4142135673935667\n",
      "Operator 198: 1.4142135673935634\n",
      "Operator 200: -1.549459140333445\n",
      "Operator 201: 1.5494591403334521\n",
      "Operator 202: -1.549459135848678\n",
      "Operator 203: 1.549459135848681\n",
      "Operator 205: -1.6932750131588183\n",
      "Operator 206: 1.6932750131588088\n",
      "Operator 207: -1.9327494760605948\n",
      "Operator 208: 3.919969401411497e-08\n",
      "Operator 209: -2.8944272066797962\n",
      "Operator 210: -1.7888543741599308\n",
      "Operator 211: -1.7888543741599308\n",
      "Operator 214: 1.4142135649500975\n",
      "Operator 215: -1.4142135649500724\n",
      "Operator 218: 1.4142135673935672\n",
      "Operator 219: -0.8941969536381185\n",
      "Operator 221: 0.9797117500767745\n",
      "Operator 222: -1.5494591403334508\n",
      "Operator 223: 0.9797117429839062\n",
      "Operator 224: -0.8245757026129208\n",
      "Operator 226: 0.43540506932464706\n",
      "Operator 227: -1.6932750131588121\n",
      "Operator 239: -1.2004118103069348\n",
      "Operator 244: -1.6363381972545792\n",
      "Operator 245: -3.9199693908252536e-08\n",
      "Operator 246: 2.0466691017646865\n",
      "Operator 247: -4.000000000000039\n",
      "Operator 248: 3.414213559796187\n",
      "Operator 249: 1.030791542764909e-08\n",
      "Operator 250: 2.4142135519804513\n",
      "Operator 251: -4.000000000000053\n",
      "Operator 252: 3.41421355735267\n",
      "Operator 253: 1.2697601706770873e-08\n",
      "Operator 254: 3.41421355735267\n",
      "Operator 255: -2.064173649966606\n",
      "Operator 256: 1.9146693298810236e-08\n",
      "Operator 257: -2.799588189693128\n",
      "Operator 258: 2.2199200033592556e-08\n",
      "Operator 259: -3.2645854602735564\n",
      "Operator 260: 0.7879577617552196\n",
      "Operator 262: -1.7888544133595374\n",
      "Operator 263: -1.5268827486483363\n",
      "Operator 264: 1.0307915257703504e-08\n",
      "Operator 265: -3.4142135597961394\n",
      "Operator 266: -2.828427119592286\n",
      "Operator 267: -2.4142135544239496\n",
      "Operator 268: 2.0081841072084714e-08\n",
      "Operator 269: -3.4142135573526655\n",
      "Operator 270: -2.3084105046419703\n",
      "Operator 271: -1.3538759911050158e-08\n",
      "Operator 272: -2.5291709150519814\n",
      "Operator 273: 0.9299944994463736\n",
      "Operator 274: -2.0641736444715617\n",
      "Operator 275: -1.9375599326515673\n",
      "Operator 277: -1.5786157123466542\n",
      "Operator 278: 1.3853852887265585\n",
      "Total gradient norm: 33.19501893184958\n",
      "Operators under consideration (1):\n",
      "[127]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000053)]\n",
      "Operator(s) added to ansatz: [127]\n",
      "Gradients: [np.float64(4.000000000000053)]\n",
      "Initial energy: -25.95125561372905\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127]...\n",
      "Starting point: [np.float64(0.6553685259814477), np.float64(0.5048173567468405), np.float64(0.44314364105421516), np.float64(0.44314364282743407), np.float64(0.3926990834737242), np.float64(0.3926990826098219), np.float64(-0.5535743545143946), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -27.026846\n",
      "         Iterations: 8\n",
      "         Function evaluations: 49\n",
      "         Gradient evaluations: 43\n",
      "\n",
      "Current energy: -27.026845928981032\n",
      "(change of -1.0755903152519828)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127]\n",
      "On iteration 8.\n",
      "\n",
      "*** ADAPT-VQE Iteration 9 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 2.8944272249736565\n",
      "Operator 1: -4.000000000000045\n",
      "Operator 2: 3.2087802425820895\n",
      "Operator 3: -8.057775317063524e-08\n",
      "Operator 4: 2.243291692837121\n",
      "Operator 6: 2.243291641329437\n",
      "Operator 7: 8.172465324770267e-08\n",
      "Operator 8: 3.208780191074365\n",
      "Operator 9: -3.264585412122476\n",
      "Operator 10: -8.931413117498404e-08\n",
      "Operator 11: -2.529170828926518\n",
      "Operator 12: -7.008714048701794e-08\n",
      "Operator 13: -3.2645854168040973\n",
      "Operator 14: 3.0643400559192884\n",
      "Operator 15: -1.581226205527031e-08\n",
      "Operator 16: 1.578615750886891\n",
      "Operator 17: 4.000000000000045\n",
      "Operator 18: -3.208780242582092\n",
      "Operator 19: 8.057775422388129e-08\n",
      "Operator 20: -2.2432916928371185\n",
      "Operator 22: -2.243291641329439\n",
      "Operator 23: -8.172465324770267e-08\n",
      "Operator 24: -3.2087801910743696\n",
      "Operator 25: 3.264585412122475\n",
      "Operator 26: 8.931412953801248e-08\n",
      "Operator 27: 2.529170828926517\n",
      "Operator 28: 7.008714326961424e-08\n",
      "Operator 29: 3.2645854168040978\n",
      "Operator 30: -3.064340055919286\n",
      "Operator 31: 1.581226205527031e-08\n",
      "Operator 32: -1.5786157508868945\n",
      "Operator 33: 5.102734235151729e-08\n",
      "Operator 34: -8.493436151991983e-08\n",
      "Operator 37: 1.593377019146268\n",
      "Operator 38: -0.8241833854400458\n",
      "Operator 39: 1.7116617829732492\n",
      "Operator 40: -0.6252484877713409\n",
      "Operator 41: 1.593377058221421\n",
      "Operator 42: -0.824183405651911\n",
      "Operator 44: 0.9364777788980277\n",
      "Operator 45: -0.9797117384120537\n",
      "Operator 46: 1.549459171326125\n",
      "Operator 47: -0.9797117323691673\n",
      "Operator 49: 1.0706454395051914\n",
      "Operator 50: -0.4354050901970682\n",
      "Operator 51: 1.9327494689083817\n",
      "Operator 52: 1.7888544499472747\n",
      "Operator 53: 1.4350102039614563\n",
      "Operator 54: 8.057775422388129e-08\n",
      "Operator 55: 2.2432916928371185\n",
      "Operator 56: -1.0864132689513872\n",
      "Operator 57: 1.3558233071942176\n",
      "Operator 58: -1.1670635153654005\n",
      "Operator 59: 1.6597599245090628\n",
      "Operator 60: 1.973083089121941\n",
      "Operator 61: -5.398057504599125e-08\n",
      "Operator 62: 2.529170828926517\n",
      "Operator 63: -0.9299946332220914\n",
      "Operator 64: 2.0641735473590517\n",
      "Operator 65: 1.9375598734219999\n",
      "Operator 67: 1.5786157508868945\n",
      "Operator 68: -1.385385246968621\n",
      "Operator 69: -5.102734235151729e-08\n",
      "Operator 70: 1.7888544499472747\n",
      "Operator 71: 3.2087802425820895\n",
      "Operator 72: 8.057775317063524e-08\n",
      "Operator 73: 2.6252485008966255\n",
      "Operator 74: -1.0864132689513863\n",
      "Operator 75: 2.6252484742539886\n",
      "Operator 76: -1.1670635153654005\n",
      "Operator 77: 3.2087801910743696\n",
      "Operator 78: 1.9730830891219413\n",
      "Operator 79: -8.931413117498404e-08\n",
      "Operator 80: 2.7995881352366263\n",
      "Operator 81: -0.9299946332220903\n",
      "Operator 82: 3.2645854168040978\n",
      "Operator 83: 1.9375598734220012\n",
      "Operator 84: 1.581226205527031e-08\n",
      "Operator 85: 2.273682110969901\n",
      "Operator 86: -1.385385246968621\n",
      "Operator 87: -2.8944272249736565\n",
      "Operator 88: -2.417560485164075\n",
      "Operator 89: -3.208780242582092\n",
      "Operator 90: 1.1670634027930702\n",
      "Operator 91: -2.6252485008966264\n",
      "Operator 92: 1.0864133222366894\n",
      "Operator 93: -2.6252484742539886\n",
      "Operator 94: 8.172465324770267e-08\n",
      "Operator 95: -2.028888310170038\n",
      "Operator 96: -3.264585412122475\n",
      "Operator 97: 0.9299946430858622\n",
      "Operator 98: -2.799588135236627\n",
      "Operator 99: 7.008714048701794e-08\n",
      "Operator 100: -1.7373145125372131\n",
      "Operator 101: -3.064340055919286\n",
      "Operator 102: 1.5813158740864957\n",
      "Operator 103: -2.2736821109699004\n",
      "Operator 104: 2.8944272249736676\n",
      "Operator 105: 2.417560485164075\n",
      "Operator 106: 3.2087802425820913\n",
      "Operator 107: -1.1670634027930702\n",
      "Operator 108: 2.6252485008966255\n",
      "Operator 109: -1.0864133222366894\n",
      "Operator 110: 2.6252484742539886\n",
      "Operator 111: -8.172465324770267e-08\n",
      "Operator 112: 2.028888310170041\n",
      "Operator 113: 3.264585412122476\n",
      "Operator 114: -0.9299946430858604\n",
      "Operator 115: 2.7995881352366254\n",
      "Operator 116: -7.008714326961424e-08\n",
      "Operator 117: 1.7373145125372138\n",
      "Operator 118: 3.0643400559192893\n",
      "Operator 119: -1.5813158740864957\n",
      "Operator 120: 2.273682110969901\n",
      "Operator 121: 8.493436145976398e-08\n",
      "Operator 122: -1.749363221569836\n",
      "Operator 123: 4.000000000000045\n",
      "Operator 124: -1.6597599511517265\n",
      "Operator 125: 8.057775317063524e-08\n",
      "Operator 126: -1.5866741922657164\n",
      "Operator 128: -2.6252484742539886\n",
      "Operator 129: -5.1673897387254226e-08\n",
      "Operator 130: -3.208780191074365\n",
      "Operator 131: 2.06417355204064\n",
      "Operator 132: 8.931413117498404e-08\n",
      "Operator 133: 2.799588135236627\n",
      "Operator 134: 3.729827914437943e-08\n",
      "Operator 135: 3.2645854168040973\n",
      "Operator 136: -0.7879578059375207\n",
      "Operator 137: 1.581226205527031e-08\n",
      "Operator 138: 2.8944272249736667\n",
      "Operator 139: -2.8944272249736565\n",
      "Operator 140: -1.7888544499472747\n",
      "Operator 141: -3.208780242582092\n",
      "Operator 142: -8.057775475050435e-08\n",
      "Operator 143: -2.625248500896626\n",
      "Operator 144: 1.0864132689513863\n",
      "Operator 145: -2.6252484742539886\n",
      "Operator 146: 1.1670635153654005\n",
      "Operator 147: -3.208780191074365\n",
      "Operator 148: -1.973083089121941\n",
      "Operator 149: 8.931412953801248e-08\n",
      "Operator 150: -2.799588135236627\n",
      "Operator 151: 0.9299946332220903\n",
      "Operator 152: -3.2645854168040973\n",
      "Operator 153: -1.9375598734219999\n",
      "Operator 154: -1.581226205527031e-08\n",
      "Operator 155: -2.2736821109699004\n",
      "Operator 156: 1.385385246968621\n",
      "Operator 161: 1.3636612747702213\n",
      "Operator 163: 1.3636613082119668\n",
      "Operator 168: 1.2004118647634427\n",
      "Operator 173: 1.6363381841160947\n",
      "Operator 176: 1.593377019146268\n",
      "Operator 177: -1.5933770191462409\n",
      "Operator 178: 1.7116617829732492\n",
      "Operator 179: -1.711661782973259\n",
      "Operator 180: 1.5933770582214208\n",
      "Operator 181: -1.5933770582214244\n",
      "Operator 183: 1.5494591751469904\n",
      "Operator 184: -1.5494591751469975\n",
      "Operator 185: 1.5494591713261248\n",
      "Operator 186: -1.5494591713261252\n",
      "Operator 188: 1.6932750058292025\n",
      "Operator 189: -1.6932750058291997\n",
      "Operator 190: 1.9327494689083817\n",
      "Operator 193: -1.5933770191462624\n",
      "Operator 194: 1.593377019146244\n",
      "Operator 195: -1.711661782973251\n",
      "Operator 196: 1.7116617829732568\n",
      "Operator 197: -1.5933770582214213\n",
      "Operator 198: 1.593377058221417\n",
      "Operator 200: -1.5494591751469917\n",
      "Operator 201: 1.5494591751469988\n",
      "Operator 202: -1.5494591713261234\n",
      "Operator 203: 1.5494591713261259\n",
      "Operator 205: -1.6932750058292068\n",
      "Operator 206: 1.693275005829196\n",
      "Operator 207: -1.932749468908382\n",
      "Operator 208: 8.493436132788264e-08\n",
      "Operator 209: -2.8944272249736676\n",
      "Operator 210: -1.788854365013\n",
      "Operator 211: -1.788854365012999\n",
      "Operator 214: 0.8241833854400624\n",
      "Operator 215: -1.5933770191462404\n",
      "Operator 216: 0.6252484877713356\n",
      "Operator 217: -1.7116617829732585\n",
      "Operator 218: 0.8241834056519093\n",
      "Operator 219: -1.0074806919186852\n",
      "Operator 221: 0.9797117384120487\n",
      "Operator 222: -1.5494591751469975\n",
      "Operator 223: 0.9797117323691664\n",
      "Operator 224: -0.8245757305269179\n",
      "Operator 226: 0.43540509019706675\n",
      "Operator 227: -1.6932750058291997\n",
      "Operator 232: -1.363661274770222\n",
      "Operator 234: -1.3636613082119662\n",
      "Operator 239: -1.2004118647634434\n",
      "Operator 244: -1.6363381841160927\n",
      "Operator 245: -8.493436116788152e-08\n",
      "Operator 246: 1.7493632215698423\n",
      "Operator 247: -4.000000000000045\n",
      "Operator 248: 1.659759951151726\n",
      "Operator 249: -8.057775422388129e-08\n",
      "Operator 250: 1.5866741922657162\n",
      "Operator 252: 2.6252484742539886\n",
      "Operator 253: 5.1673897387254226e-08\n",
      "Operator 254: 3.2087801910743696\n",
      "Operator 255: -2.064173552040641\n",
      "Operator 256: -8.931412953801248e-08\n",
      "Operator 257: -2.7995881352366254\n",
      "Operator 258: -3.7298279489573146e-08\n",
      "Operator 259: -3.2645854168040978\n",
      "Operator 260: 0.7879578059375218\n",
      "Operator 261: -1.581226205527031e-08\n",
      "Operator 262: -1.7888544499472747\n",
      "Operator 263: -1.4350102039614554\n",
      "Operator 264: -8.057775317063524e-08\n",
      "Operator 265: -2.243291692837121\n",
      "Operator 266: 1.0864132689513872\n",
      "Operator 267: -1.3558233071942163\n",
      "Operator 268: 1.1670635153654005\n",
      "Operator 269: -1.6597599245090595\n",
      "Operator 270: -1.9730830891219413\n",
      "Operator 271: 5.398057344337761e-08\n",
      "Operator 272: -2.529170828926518\n",
      "Operator 273: 0.9299946332220914\n",
      "Operator 274: -2.0641735473590512\n",
      "Operator 275: -1.9375598734220012\n",
      "Operator 277: -1.578615750886891\n",
      "Operator 278: 1.385385246968621\n",
      "Total gradient norm: 29.387448624447696\n",
      "Operators under consideration (1):\n",
      "[123]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(4.000000000000045)]\n",
      "Operator(s) added to ansatz: [123]\n",
      "Gradients: [np.float64(4.000000000000045)]\n",
      "Initial energy: -27.026845928981032\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123]...\n",
      "Starting point: [np.float64(0.6553685190277687), np.float64(0.5048173533035727), np.float64(0.4431436550815166), np.float64(0.44314365659223814), np.float64(0.4608981156622499), np.float64(0.4608980994991907), np.float64(-0.5535743494011042), np.float64(-0.5135775284516034), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -28.390615\n",
      "         Iterations: 18\n",
      "         Function evaluations: 120\n",
      "         Gradient evaluations: 104\n",
      "\n",
      "Current energy: -28.390614817127158\n",
      "(change of -1.3637688881461258)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123]\n",
      "On iteration 9.\n",
      "\n",
      "*** ADAPT-VQE Iteration 10 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.5313034192819073\n",
      "Operator 2: 0.8987768008299237\n",
      "Operator 4: 1.3203286167403159\n",
      "Operator 6: 1.9165748897632615\n",
      "Operator 8: 3.1403588056035208\n",
      "Operator 9: -3.2645854433830865\n",
      "Operator 11: -2.529170890872379\n",
      "Operator 13: -3.2645854474893574\n",
      "Operator 14: 3.064340018243124\n",
      "Operator 15: 3.603536002927399e-08\n",
      "Operator 16: 1.5786156502837438\n",
      "Operator 18: -0.8987768008299291\n",
      "Operator 20: -1.3203286167403125\n",
      "Operator 22: -1.9165748897632637\n",
      "Operator 24: -3.1403588056035248\n",
      "Operator 25: 3.264585443383086\n",
      "Operator 27: 2.529170890872379\n",
      "Operator 29: 3.264585447489357\n",
      "Operator 30: -3.0643400182431217\n",
      "Operator 31: -3.60353561434934e-08\n",
      "Operator 32: -1.5786156502837474\n",
      "Operator 33: -4.7327940677632796e-08\n",
      "Operator 35: 1.9683021253931592\n",
      "Operator 36: -0.04729415474094029\n",
      "Operator 37: 1.9245626910778753\n",
      "Operator 38: -0.13245615533174548\n",
      "Operator 39: 1.8432277641929478\n",
      "Operator 40: -0.28592311179327023\n",
      "Operator 41: 1.643040411701017\n",
      "Operator 42: -0.6376771972433992\n",
      "Operator 44: 0.8834696926038752\n",
      "Operator 45: -0.9797117460529999\n",
      "Operator 46: 1.5494591462824707\n",
      "Operator 47: -0.9797117407527624\n",
      "Operator 49: 1.0706454804584784\n",
      "Operator 50: -0.4354050430102824\n",
      "Operator 51: 1.9327494856523109\n",
      "Operator 52: -1.9529486848889515\n",
      "Operator 53: 0.07937958553317681\n",
      "Operator 54: -1.8640415731966375\n",
      "Operator 55: 0.23413669135244009\n",
      "Operator 56: -1.7068019766043045\n",
      "Operator 57: 0.5214162085746934\n",
      "Operator 58: -1.3955527623217743\n",
      "Operator 59: 1.2187985074710876\n",
      "Operator 60: 1.8613993785034295\n",
      "Operator 62: 2.529170890872379\n",
      "Operator 63: -0.9299945382262029\n",
      "Operator 64: 2.0641736177875742\n",
      "Operator 65: 1.9375598966146597\n",
      "Operator 66: -2.278489763529734e-08\n",
      "Operator 67: 1.5786156502837474\n",
      "Operator 68: -1.3853853500641975\n",
      "Operator 69: 4.7327940677632796e-08\n",
      "Operator 70: -1.952948684888952\n",
      "Operator 71: 2.096488636606532\n",
      "Operator 72: -1.8640415731966375\n",
      "Operator 73: 2.2111744496909727\n",
      "Operator 74: -1.7068019766043039\n",
      "Operator 75: 2.4425824233113436\n",
      "Operator 76: -1.3955527623217736\n",
      "Operator 77: 3.140358805603525\n",
      "Operator 78: 1.8613993785034295\n",
      "Operator 80: 2.7995881744045454\n",
      "Operator 81: -0.9299945382262038\n",
      "Operator 82: 3.264585447489358\n",
      "Operator 83: 1.9375598966146605\n",
      "Operator 84: -3.603536002927399e-08\n",
      "Operator 85: 2.2736820677940837\n",
      "Operator 86: -1.385385350064194\n",
      "Operator 87: -2.0313237976225933\n",
      "Operator 88: 1.8226190069211388\n",
      "Operator 89: -2.096488636606531\n",
      "Operator 90: 1.634669947027672\n",
      "Operator 91: -2.211174449690973\n",
      "Operator 92: 1.243986029363621\n",
      "Operator 93: -2.4425824233113422\n",
      "Operator 95: -1.9856260162829782\n",
      "Operator 96: -3.264585443383086\n",
      "Operator 97: 0.929994546877749\n",
      "Operator 98: -2.799588174404547\n",
      "Operator 100: -1.737314467368451\n",
      "Operator 101: -3.0643400182431217\n",
      "Operator 102: 1.5813159369334207\n",
      "Operator 103: -2.273682067794083\n",
      "Operator 104: 2.031323797622597\n",
      "Operator 105: -1.8226190069211388\n",
      "Operator 106: 2.0964886366065323\n",
      "Operator 107: -1.6346699470276713\n",
      "Operator 108: 2.2111744496909727\n",
      "Operator 109: -1.2439860293636202\n",
      "Operator 110: 2.4425824233113436\n",
      "Operator 112: 1.9856260162829806\n",
      "Operator 113: 3.264585443383086\n",
      "Operator 114: -0.929994546877747\n",
      "Operator 115: 2.799588174404545\n",
      "Operator 117: 1.737314467368451\n",
      "Operator 118: 3.064340018243124\n",
      "Operator 119: -1.581315936933422\n",
      "Operator 120: 2.2736820677940837\n",
      "Operator 122: -0.5526343680077841\n",
      "Operator 124: -0.8136640999960735\n",
      "Operator 126: -1.2607661272152737\n",
      "Operator 128: -2.4425824233113422\n",
      "Operator 130: -3.1403588056035208\n",
      "Operator 131: 2.064173621893812\n",
      "Operator 133: 2.799588174404547\n",
      "Operator 135: 3.2645854474893574\n",
      "Operator 136: -0.7879576998347213\n",
      "Operator 137: -3.603536002927399e-08\n",
      "Operator 138: 2.031323797622597\n",
      "Operator 139: -2.0313237976225933\n",
      "Operator 140: 1.9529486848889523\n",
      "Operator 141: -2.096488636606531\n",
      "Operator 142: 1.8640415731966375\n",
      "Operator 143: -2.211174449690973\n",
      "Operator 144: 1.7068019766043045\n",
      "Operator 145: -2.4425824233113422\n",
      "Operator 146: 1.3955527623217736\n",
      "Operator 147: -3.1403588056035208\n",
      "Operator 148: -1.8613993785034295\n",
      "Operator 150: -2.799588174404547\n",
      "Operator 151: 0.9299945382262029\n",
      "Operator 152: -3.2645854474893574\n",
      "Operator 153: -1.9375598966146597\n",
      "Operator 154: 3.60353561434934e-08\n",
      "Operator 155: -2.273682067794083\n",
      "Operator 156: 1.385385350064194\n",
      "Operator 157: 1.9606103762469567\n",
      "Operator 159: 1.8940604176504614\n",
      "Operator 161: 1.7737036930622838\n",
      "Operator 163: 1.514248852269155\n",
      "Operator 168: 1.20041182559553\n",
      "Operator 173: 1.6363382211778863\n",
      "Operator 174: 1.9683021253931592\n",
      "Operator 175: -1.9683021253931772\n",
      "Operator 176: 1.9245626910778755\n",
      "Operator 177: -1.9245626910778475\n",
      "Operator 178: 1.8432277641929478\n",
      "Operator 179: -1.8432277641929602\n",
      "Operator 180: 1.643040411701017\n",
      "Operator 181: -1.6430404117010222\n",
      "Operator 183: 1.5494591496337642\n",
      "Operator 184: -1.5494591496337722\n",
      "Operator 185: 1.5494591462824707\n",
      "Operator 186: -1.549459146282471\n",
      "Operator 188: 1.693275029511274\n",
      "Operator 189: -1.6932750295112708\n",
      "Operator 190: 1.9327494856523104\n",
      "Operator 191: -1.9683021253931605\n",
      "Operator 192: 1.9683021253931758\n",
      "Operator 193: -1.9245626910778748\n",
      "Operator 194: 1.9245626910778517\n",
      "Operator 195: -1.8432277641929473\n",
      "Operator 196: 1.8432277641929573\n",
      "Operator 197: -1.6430404117010173\n",
      "Operator 198: 1.6430404117010138\n",
      "Operator 200: -1.5494591496337655\n",
      "Operator 201: 1.5494591496337735\n",
      "Operator 202: -1.5494591462824685\n",
      "Operator 203: 1.549459146282472\n",
      "Operator 205: -1.6932750295112786\n",
      "Operator 206: 1.6932750295112675\n",
      "Operator 207: -1.9327494856523102\n",
      "Operator 209: -0.5313034192819243\n",
      "Operator 210: -1.992184381607934\n",
      "Operator 211: -1.992184381607934\n",
      "Operator 212: 0.04729415474094198\n",
      "Operator 213: -1.9683021253931776\n",
      "Operator 214: 0.13245615533175079\n",
      "Operator 215: -1.9245626910778482\n",
      "Operator 216: 0.28592311179326707\n",
      "Operator 217: -1.8432277641929602\n",
      "Operator 218: 0.6376771972433982\n",
      "Operator 219: -1.0388824937635923\n",
      "Operator 221: 0.9797117460529947\n",
      "Operator 222: -1.5494591496337715\n",
      "Operator 223: 0.9797117407527614\n",
      "Operator 224: -0.8245756880105932\n",
      "Operator 226: 0.4354050430102816\n",
      "Operator 227: -1.693275029511271\n",
      "Operator 228: -1.9606103762469536\n",
      "Operator 230: -1.8940604176504603\n",
      "Operator 232: -1.7737036930622847\n",
      "Operator 234: -1.5142488522691542\n",
      "Operator 239: -1.200411825595531\n",
      "Operator 244: -1.6363382211778843\n",
      "Operator 246: 0.5526343680077848\n",
      "Operator 248: 0.813664099996074\n",
      "Operator 250: 1.260766127215273\n",
      "Operator 252: 2.4425824233113436\n",
      "Operator 254: 3.1403588056035248\n",
      "Operator 255: -2.0641736218938123\n",
      "Operator 257: -2.799588174404545\n",
      "Operator 259: -3.264585447489357\n",
      "Operator 260: 0.787957699834722\n",
      "Operator 261: 3.60353561434934e-08\n",
      "Operator 262: 1.952948684888952\n",
      "Operator 263: -0.079379585533177\n",
      "Operator 264: 1.8640415731966375\n",
      "Operator 265: -0.23413669135244047\n",
      "Operator 266: 1.7068019766043039\n",
      "Operator 267: -0.5214162085746931\n",
      "Operator 268: 1.395552762321775\n",
      "Operator 269: -1.218798507471086\n",
      "Operator 270: -1.8613993785034295\n",
      "Operator 272: -2.529170890872379\n",
      "Operator 273: 0.9299945382262029\n",
      "Operator 274: -2.064173617787574\n",
      "Operator 275: -1.9375598966146605\n",
      "Operator 276: 2.2784897302230434e-08\n",
      "Operator 277: -1.5786156502837438\n",
      "Operator 278: 1.3853853500641975\n",
      "Total gradient norm: 27.205540357488278\n",
      "Operators under consideration (1):\n",
      "[259]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.264585447489357)]\n",
      "Operator(s) added to ansatz: [259]\n",
      "Gradients: [np.float64(-3.264585447489357)]\n",
      "Initial energy: -28.390614817127158\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259]...\n",
      "Starting point: [np.float64(0.6553685353069046), np.float64(0.504817364428809), np.float64(0.4431436451795926), np.float64(0.4431436465046535), np.float64(0.4820360545387622), np.float64(0.6476335363954346), np.float64(-0.7411807628985058), np.float64(-0.5861092492351743), np.float64(-0.696260685452187), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -28.987473\n",
      "         Iterations: 12\n",
      "         Function evaluations: 50\n",
      "         Gradient evaluations: 39\n",
      "\n",
      "Current energy: -28.987472894327812\n",
      "(change of -0.596858077200654)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259]\n",
      "On iteration 10.\n",
      "\n",
      "*** ADAPT-VQE Iteration 11 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.5313033574083013\n",
      "Operator 1: 3.554504012764439e-07\n",
      "Operator 2: 0.8987768176288734\n",
      "Operator 3: -2.8518603030816776e-07\n",
      "Operator 4: 1.320328794748126\n",
      "Operator 5: -2.6538060259963725e-07\n",
      "Operator 6: 1.9165747042727557\n",
      "Operator 7: 7.813319439489546e-07\n",
      "Operator 8: 3.1403585583134124\n",
      "Operator 9: -3.3379495652037043\n",
      "Operator 11: -2.938047458355597\n",
      "Operator 12: 2.1826957394126657e-08\n",
      "Operator 13: -1.1753257090840845\n",
      "Operator 14: 2.3628825720105695\n",
      "Operator 15: -4.766927075294802e-08\n",
      "Operator 16: 1.2885705918922106\n",
      "Operator 17: -3.5545040098530207e-07\n",
      "Operator 18: -0.8987768176288786\n",
      "Operator 19: 2.8518603030816776e-07\n",
      "Operator 20: -1.3203287947481208\n",
      "Operator 21: 2.653806037235512e-07\n",
      "Operator 22: -1.9165747042727568\n",
      "Operator 23: -7.813319453518874e-07\n",
      "Operator 24: -3.140358558313417\n",
      "Operator 25: 3.3379495652037035\n",
      "Operator 27: 2.9380474583555967\n",
      "Operator 28: -1.2987989856199798\n",
      "Operator 29: 1.4548235385521561e-08\n",
      "Operator 30: -2.3628825720105673\n",
      "Operator 31: 4.766927241828256e-08\n",
      "Operator 32: -1.2885705918922146\n",
      "Operator 33: -2.5074697873606056e-08\n",
      "Operator 34: -1.6878710936832853e-07\n",
      "Operator 35: 1.9683021433055021\n",
      "Operator 36: -0.04729417532209727\n",
      "Operator 37: 1.9245626582234736\n",
      "Operator 38: -0.1324561264899744\n",
      "Operator 39: 1.8432277381680309\n",
      "Operator 40: -0.285923106818698\n",
      "Operator 41: 1.6430405833336779\n",
      "Operator 42: -0.6376773146250396\n",
      "Operator 44: 0.8476115253286183\n",
      "Operator 45: -1.1893289229095858\n",
      "Operator 46: 0.898287435591044\n",
      "Operator 47: 1.0485291979548281\n",
      "Operator 48: -0.45910122785171453\n",
      "Operator 49: 1.442482194391015\n",
      "Operator 50: -0.285484235292627\n",
      "Operator 51: 1.9547567259362473\n",
      "Operator 52: -1.952948727629463\n",
      "Operator 53: 0.07937960388494153\n",
      "Operator 54: -1.864041524375401\n",
      "Operator 55: 0.23413665729248967\n",
      "Operator 56: -1.7068018217761907\n",
      "Operator 57: 0.5214162694716988\n",
      "Operator 58: -1.3955531728129325\n",
      "Operator 59: 1.2187985085322568\n",
      "Operator 60: 1.903229676949238\n",
      "Operator 62: 2.9380474583555967\n",
      "Operator 63: 0.37258444798191515\n",
      "Operator 65: 1.890421712619705\n",
      "Operator 66: -0.633995039202234\n",
      "Operator 67: 0.9646939159783349\n",
      "Operator 68: -1.588621860484638\n",
      "Operator 69: 2.5074697873606056e-08\n",
      "Operator 70: -1.952948727629463\n",
      "Operator 71: 2.096488630169061\n",
      "Operator 72: -1.8640415243754012\n",
      "Operator 73: 2.211174511605282\n",
      "Operator 74: -1.7068018217761907\n",
      "Operator 75: 2.4425823625729137\n",
      "Operator 76: -1.395553172812933\n",
      "Operator 77: 3.140358558313417\n",
      "Operator 78: 1.9032296769492385\n",
      "Operator 80: 3.0704251402129827\n",
      "Operator 81: -0.6628951564974699\n",
      "Operator 82: 1.4548235774359967e-08\n",
      "Operator 83: 2.1185421516012424\n",
      "Operator 84: -0.7924453146468262\n",
      "Operator 85: 2.1830674327258257\n",
      "Operator 86: -1.588621860484638\n",
      "Operator 87: -2.0313237954990826\n",
      "Operator 88: 1.8226190582895745\n",
      "Operator 89: -2.0964886301690613\n",
      "Operator 90: 1.634669761502873\n",
      "Operator 91: -2.211174511605284\n",
      "Operator 92: 1.2439861198409898\n",
      "Operator 93: -2.4425823625729124\n",
      "Operator 94: 7.813319439489546e-07\n",
      "Operator 95: -2.1008206838395127\n",
      "Operator 96: -3.3379495652037035\n",
      "Operator 97: 0.5350488429691507\n",
      "Operator 98: -2.298686987633933\n",
      "Operator 99: -1.2987989856199798\n",
      "Operator 100: 0.3534431294235122\n",
      "Operator 101: -2.6480156753761155\n",
      "Operator 102: 1.7223417999473942\n",
      "Operator 103: -2.1830674327258244\n",
      "Operator 104: 2.0313237954990875\n",
      "Operator 105: -1.8226190582895745\n",
      "Operator 106: 2.096488630169061\n",
      "Operator 107: -1.634669761502873\n",
      "Operator 108: 2.2111745116052823\n",
      "Operator 109: -1.2439861198409887\n",
      "Operator 110: 2.4425823625729137\n",
      "Operator 111: -7.813319453518874e-07\n",
      "Operator 112: 2.1008206838395154\n",
      "Operator 113: 3.3379495652037043\n",
      "Operator 114: -0.5350488429691507\n",
      "Operator 115: 2.2986869876339333\n",
      "Operator 116: 2.1826959009102667e-08\n",
      "Operator 117: -1.077570063572231\n",
      "Operator 118: 2.648015675376118\n",
      "Operator 119: -1.7223417999473931\n",
      "Operator 120: 2.1830674327258253\n",
      "Operator 121: 1.6878710913254495e-07\n",
      "Operator 122: -0.5526344854583228\n",
      "Operator 123: -3.554504013604358e-07\n",
      "Operator 124: -0.8136641622787171\n",
      "Operator 125: 2.851860303445612e-07\n",
      "Operator 126: -1.2607658891167528\n",
      "Operator 127: 2.6538060259963725e-07\n",
      "Operator 128: -2.4425823625729124\n",
      "Operator 129: -5.226913664962212e-07\n",
      "Operator 130: -3.1403585583134124\n",
      "Operator 131: 2.670523033364843\n",
      "Operator 133: 3.0704251402129827\n",
      "Operator 134: -0.5621032968965445\n",
      "Operator 135: 1.175325709084084\n",
      "Operator 136: -0.5600508431310058\n",
      "Operator 137: 4.766927075294802e-08\n",
      "Operator 138: 2.0313237954990875\n",
      "Operator 139: -2.0313237954990826\n",
      "Operator 140: 1.9529487276294637\n",
      "Operator 141: -2.0964886301690613\n",
      "Operator 142: 1.864041524375401\n",
      "Operator 143: -2.2111745116052837\n",
      "Operator 144: 1.7068018217761907\n",
      "Operator 145: -2.4425823625729124\n",
      "Operator 146: 1.395553172812933\n",
      "Operator 147: -3.1403585583134124\n",
      "Operator 148: -1.903229676949238\n",
      "Operator 150: -3.0704251402129836\n",
      "Operator 151: -0.37258444798191515\n",
      "Operator 152: -1.1753257090840816\n",
      "Operator 153: -2.1185421516012406\n",
      "Operator 154: 0.7924453146468262\n",
      "Operator 155: -2.1830674327258244\n",
      "Operator 156: 1.588621860484638\n",
      "Operator 157: 1.96061039081391\n",
      "Operator 159: 1.8940604025534817\n",
      "Operator 161: 1.773703637739922\n",
      "Operator 163: 1.5142489890682034\n",
      "Operator 168: 0.6676836666100658\n",
      "Operator 169: -0.5321468030026599\n",
      "Operator 170: -0.795466162316296\n",
      "Operator 171: 1.1953129163220613\n",
      "Operator 173: 1.7622057897812844\n",
      "Operator 174: 1.9683021433055021\n",
      "Operator 175: -1.9683021433055208\n",
      "Operator 176: 1.9245626582234736\n",
      "Operator 177: -1.9245626582234459\n",
      "Operator 178: 1.8432277381680309\n",
      "Operator 179: -1.8432277381680442\n",
      "Operator 180: 1.6430405833336774\n",
      "Operator 181: -1.643040583333682\n",
      "Operator 183: 1.4865702004854615\n",
      "Operator 184: -1.486570200485469\n",
      "Operator 185: 1.199869464704789\n",
      "Operator 186: 1.5673672987744065\n",
      "Operator 187: -1.060801835397238\n",
      "Operator 188: 1.8029924301063915\n",
      "Operator 189: -1.8029924301063878\n",
      "Operator 190: 1.9547567259362468\n",
      "Operator 191: -1.9683021433055032\n",
      "Operator 192: 1.968302143305519\n",
      "Operator 193: -1.9245626582234732\n",
      "Operator 194: 1.9245626582234487\n",
      "Operator 195: -1.8432277381680309\n",
      "Operator 196: 1.8432277381680406\n",
      "Operator 197: -1.6430405833336779\n",
      "Operator 198: 1.6430405833336734\n",
      "Operator 200: -1.4865702004854635\n",
      "Operator 201: 1.4865702004854704\n",
      "Operator 202: -0.8982874355910428\n",
      "Operator 203: 1.4666805946264843\n",
      "Operator 204: 1.3259211701199898\n",
      "Operator 205: -1.8029924301063947\n",
      "Operator 206: 1.8029924301063844\n",
      "Operator 207: -1.9547567259362477\n",
      "Operator 208: 1.6878710879907077e-07\n",
      "Operator 209: -0.5313033574083179\n",
      "Operator 210: -1.992184378279793\n",
      "Operator 211: -1.9921843782797939\n",
      "Operator 212: 0.04729417532209923\n",
      "Operator 213: -1.9683021433055203\n",
      "Operator 214: 0.13245612648997956\n",
      "Operator 215: -1.9245626582234459\n",
      "Operator 216: 0.28592310681869465\n",
      "Operator 217: -1.8432277381680438\n",
      "Operator 218: 0.6376773146250381\n",
      "Operator 219: -1.0991527170416349\n",
      "Operator 221: 1.1893289229095796\n",
      "Operator 222: -1.1129271745811984\n",
      "Operator 223: 0.8026824143014615\n",
      "Operator 224: 1.324807878827067\n",
      "Operator 225: -1.3259211701199902\n",
      "Operator 226: 0.28548423529262623\n",
      "Operator 227: -1.8029924301063878\n",
      "Operator 228: -1.960610390813906\n",
      "Operator 230: -1.8940604025534815\n",
      "Operator 232: -1.7737036377399225\n",
      "Operator 234: -1.5142489890682023\n",
      "Operator 239: -0.6676836666100666\n",
      "Operator 242: -1.19531291632206\n",
      "Operator 244: -1.762205789781282\n",
      "Operator 245: -1.6878710849834057e-07\n",
      "Operator 246: 0.5526344854583243\n",
      "Operator 247: 3.554504010643227e-07\n",
      "Operator 248: 0.8136641622787171\n",
      "Operator 249: -2.851860303445612e-07\n",
      "Operator 250: 1.2607658891167517\n",
      "Operator 251: -2.6538060353221545e-07\n",
      "Operator 252: 2.4425823625729137\n",
      "Operator 253: 5.226913676436483e-07\n",
      "Operator 254: 3.140358558313417\n",
      "Operator 255: -2.6705230333648444\n",
      "Operator 257: -3.0704251402129827\n",
      "Operator 259: -1.4548235385521561e-08\n",
      "Operator 260: 0.5600508431310058\n",
      "Operator 261: -4.766927241828256e-08\n",
      "Operator 262: 1.952948727629463\n",
      "Operator 263: -0.07937960388494156\n",
      "Operator 264: 1.864041524375401\n",
      "Operator 265: -0.23413665729249045\n",
      "Operator 266: 1.7068018217761907\n",
      "Operator 267: -0.5214162694716986\n",
      "Operator 268: 1.3955531728129338\n",
      "Operator 269: -1.2187985085322555\n",
      "Operator 270: -1.9032296769492385\n",
      "Operator 272: -2.938047458355597\n",
      "Operator 273: 0.6628951564974699\n",
      "Operator 274: -0.7862632607208695\n",
      "Operator 275: -1.8904217126197065\n",
      "Operator 276: 0.633995039202234\n",
      "Operator 277: -0.9646939159783315\n",
      "Operator 278: 1.588621860484638\n",
      "Total gradient norm: 26.08603352552858\n",
      "Operators under consideration (1):\n",
      "[113]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(3.3379495652037043)]\n",
      "Operator(s) added to ansatz: [113]\n",
      "Gradients: [np.float64(3.3379495652037043)]\n",
      "Initial energy: -28.987472894327812\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113]...\n",
      "Starting point: [np.float64(0.6788443153863174), np.float64(0.5616071626990282), np.float64(0.3217097633647678), np.float64(0.4189838513581588), np.float64(0.482036129792569), np.float64(0.6476335062046238), np.float64(-0.7411807534777748), np.float64(-0.5861092324712118), np.float64(-0.696260710704729), np.float64(0.3623832066474763), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -29.598284\n",
      "         Iterations: 15\n",
      "         Function evaluations: 83\n",
      "         Gradient evaluations: 71\n",
      "\n",
      "Current energy: -29.598283698722838\n",
      "(change of -0.610810804395026)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113]\n",
      "On iteration 11.\n",
      "\n",
      "*** ADAPT-VQE Iteration 12 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.47173880050556466\n",
      "Operator 2: 0.795473667947014\n",
      "Operator 4: 1.1561822015825562\n",
      "Operator 6: 1.630272830431795\n",
      "Operator 7: 1.947692841363395e-08\n",
      "Operator 8: 2.452315992518665\n",
      "Operator 9: 2.4560608672673893e-08\n",
      "Operator 10: 1.289951027354332\n",
      "Operator 11: -3.2745067538058468\n",
      "Operator 12: 2.3461514831632458e-08\n",
      "Operator 13: -1.1171138720151474\n",
      "Operator 14: 2.337709921139582\n",
      "Operator 15: 1.1560008505995256e-08\n",
      "Operator 16: 1.2772224181552514\n",
      "Operator 18: -0.795473667947019\n",
      "Operator 20: -1.1561822015825514\n",
      "Operator 22: -1.6302728304317968\n",
      "Operator 23: -1.9476927991195013e-08\n",
      "Operator 24: -2.452315992518671\n",
      "Operator 25: 1.1207852354835224\n",
      "Operator 26: -3.940407547353634e-08\n",
      "Operator 27: 3.274506753805846\n",
      "Operator 28: -1.3120086994355968\n",
      "Operator 30: -2.3377099211395795\n",
      "Operator 31: -1.1560008505995256e-08\n",
      "Operator 32: -1.2772224181552554\n",
      "Operator 33: 1.2716686370595198e-08\n",
      "Operator 35: 1.9750673050656369\n",
      "Operator 36: -0.03724291608650054\n",
      "Operator 37: 1.9413830757642765\n",
      "Operator 38: -0.1032165304007659\n",
      "Operator 39: 1.8824628940216712\n",
      "Operator 40: -0.21596886355074008\n",
      "Operator 41: 1.7574002874842982\n",
      "Operator 42: -0.44446578109880114\n",
      "Operator 43: 1.083735483990947\n",
      "Operator 44: 0.6956595181214258\n",
      "Operator 45: -0.7069635914971742\n",
      "Operator 46: 0.6346703102163387\n",
      "Operator 47: 1.3070753312144978\n",
      "Operator 48: -0.47302313603772994\n",
      "Operator 49: 1.4809899122702441\n",
      "Operator 50: -0.2803223927772377\n",
      "Operator 51: 1.9555350833512155\n",
      "Operator 52: -1.962908419459903\n",
      "Operator 53: 0.062413961045111474\n",
      "Operator 54: -1.8932808747650656\n",
      "Operator 55: 0.1819923524137375\n",
      "Operator 56: -1.7737357188282052\n",
      "Operator 57: 0.39180153395305084\n",
      "Operator 58: -1.5569101588519891\n",
      "Operator 59: 0.8282998603974538\n",
      "Operator 60: -0.301733403646337\n",
      "Operator 61: 1.881048837566414e-08\n",
      "Operator 62: 2.4518978361437926\n",
      "Operator 63: 0.5926568945056929\n",
      "Operator 65: 1.9162852902251912\n",
      "Operator 66: -0.6701411860497679\n",
      "Operator 67: 0.9450622239788811\n",
      "Operator 68: -1.5957715861611526\n",
      "Operator 69: -1.2716686370595198e-08\n",
      "Operator 70: -1.9629084194599042\n",
      "Operator 71: 2.075659365700038\n",
      "Operator 72: -1.8932808747650656\n",
      "Operator 73: 2.162348019372126\n",
      "Operator 74: -1.7737357188282066\n",
      "Operator 75: 2.3224779548655863\n",
      "Operator 76: -1.5569101588519891\n",
      "Operator 77: 2.7149000919457285\n",
      "Operator 78: -1.0235362447837995\n",
      "Operator 79: 3.940407519588529e-08\n",
      "Operator 80: 2.5011604260021225\n",
      "Operator 81: -0.37990924610014226\n",
      "Operator 83: 2.1597735157848756\n",
      "Operator 84: -0.8175169465548052\n",
      "Operator 85: 2.1798817485950597\n",
      "Operator 86: -1.5957715861611526\n",
      "Operator 87: -2.0247009008461467\n",
      "Operator 88: 1.8609914897521809\n",
      "Operator 89: -2.075659365700038\n",
      "Operator 90: 1.7199035674208034\n",
      "Operator 91: -2.1623480193721267\n",
      "Operator 92: 1.4534758478413439\n",
      "Operator 93: -2.322477954865584\n",
      "Operator 94: 0.7720658846917162\n",
      "Operator 95: -2.2195008683481294\n",
      "Operator 96: -1.1207852354835208\n",
      "Operator 97: -0.7744887401099401\n",
      "Operator 98: -2.471604286871101\n",
      "Operator 99: -1.3120086994355953\n",
      "Operator 100: 0.4207844168700826\n",
      "Operator 101: -2.6347454635376804\n",
      "Operator 102: 1.7272443764400287\n",
      "Operator 103: -2.1798817485950583\n",
      "Operator 104: 2.0247009008461507\n",
      "Operator 105: -1.8609914897521809\n",
      "Operator 106: 2.075659365700038\n",
      "Operator 107: -1.7199035674208036\n",
      "Operator 108: 2.162348019372126\n",
      "Operator 109: -1.4534758478413456\n",
      "Operator 110: 2.3224779548655863\n",
      "Operator 111: -0.772065884691715\n",
      "Operator 112: 2.219500868348134\n",
      "Operator 113: -2.456060888052339e-08\n",
      "Operator 114: -0.37783889048213887\n",
      "Operator 115: 2.4716042868711012\n",
      "Operator 116: 2.3461514618554213e-08\n",
      "Operator 117: -1.0978479984627119\n",
      "Operator 118: 2.634745463537682\n",
      "Operator 119: -1.7272443764400287\n",
      "Operator 120: 2.1798817485950597\n",
      "Operator 122: -0.4865939638689344\n",
      "Operator 124: -0.7010794563535119\n",
      "Operator 126: -1.0322489986034922\n",
      "Operator 128: -1.7390340286848087\n",
      "Operator 129: -1.592288876619601e-08\n",
      "Operator 130: -2.714900091945723\n",
      "Operator 131: 0.9187385658232886\n",
      "Operator 132: -0.9544805739442799\n",
      "Operator 133: 3.340296886177252\n",
      "Operator 134: -0.5627460478156376\n",
      "Operator 135: 1.1171138720151463\n",
      "Operator 136: -0.5524844148698518\n",
      "Operator 137: -1.1560008505995256e-08\n",
      "Operator 138: 2.0247009008461516\n",
      "Operator 139: -2.024700900846147\n",
      "Operator 140: 1.9629084194599034\n",
      "Operator 141: -2.075659365700038\n",
      "Operator 142: 1.8932808747650656\n",
      "Operator 143: -2.1623480193721263\n",
      "Operator 144: 1.773735718828206\n",
      "Operator 145: -2.322477954865584\n",
      "Operator 146: 1.5569101588519891\n",
      "Operator 147: -2.714900091945723\n",
      "Operator 148: 0.301733403646337\n",
      "Operator 149: -1.2899510273543329\n",
      "Operator 150: -2.501160426002121\n",
      "Operator 151: -0.7914926042761392\n",
      "Operator 152: -1.1171138720151457\n",
      "Operator 153: -2.159773515784874\n",
      "Operator 154: 0.8175169465548052\n",
      "Operator 155: -2.179881748595058\n",
      "Operator 156: 1.5957715861611526\n",
      "Operator 157: 1.9689784767703213\n",
      "Operator 159: 1.9171811197748645\n",
      "Operator 161: 1.8272908016039138\n",
      "Operator 163: 1.6541204155660905\n",
      "Operator 165: 0.9522785255612995\n",
      "Operator 166: -0.3644363854918548\n",
      "Operator 167: -0.7634188043985246\n",
      "Operator 168: 0.48812688638006674\n",
      "Operator 169: -0.6299504654926389\n",
      "Operator 170: -0.7705572910904357\n",
      "Operator 171: 1.2153172352262458\n",
      "Operator 173: 1.7665184211944982\n",
      "Operator 174: 1.9750673050656369\n",
      "Operator 175: -1.975067305065655\n",
      "Operator 176: 1.9413830757642765\n",
      "Operator 177: -1.941383075764249\n",
      "Operator 178: 1.8824628940216712\n",
      "Operator 179: -1.882462894021683\n",
      "Operator 180: 1.7574002874842982\n",
      "Operator 181: -1.7574002874843022\n",
      "Operator 182: 1.3256284812006542\n",
      "Operator 183: 1.457262717815487\n",
      "Operator 184: -0.862437242595125\n",
      "Operator 185: 1.1455072220046745\n",
      "Operator 186: 1.59881844945403\n",
      "Operator 187: -1.1028251054357072\n",
      "Operator 188: 1.806685480852872\n",
      "Operator 189: -1.8066854808528678\n",
      "Operator 190: 1.9555350833512155\n",
      "Operator 191: -1.9750673050656387\n",
      "Operator 192: 1.9750673050656538\n",
      "Operator 193: -1.941383075764276\n",
      "Operator 194: 1.9413830757642512\n",
      "Operator 195: -1.8824628940216714\n",
      "Operator 196: 1.8824628940216812\n",
      "Operator 197: -1.7574002874842996\n",
      "Operator 198: 1.7574002874842936\n",
      "Operator 199: -1.083735483990947\n",
      "Operator 200: 1.5910464284959205\n",
      "Operator 201: 1.1517839503675835\n",
      "Operator 202: -0.8476014728704917\n",
      "Operator 203: 1.4431898731520212\n",
      "Operator 204: 1.3453556228863455\n",
      "Operator 205: -1.8066854808528754\n",
      "Operator 206: 1.8066854808528643\n",
      "Operator 207: -1.9555350833512162\n",
      "Operator 209: -0.4717388005055809\n",
      "Operator 210: -1.993834308046446\n",
      "Operator 211: -1.993834308046446\n",
      "Operator 212: 0.03724291608650247\n",
      "Operator 213: -1.975067305065655\n",
      "Operator 214: 0.10321653040077004\n",
      "Operator 215: -1.941383075764249\n",
      "Operator 216: 0.21596886355073713\n",
      "Operator 217: -1.8824628940216834\n",
      "Operator 218: 0.4444657810987998\n",
      "Operator 219: -1.4367200751432447\n",
      "Operator 220: 0.6328207392984689\n",
      "Operator 221: 1.444995455924806\n",
      "Operator 222: -0.8522458470856968\n",
      "Operator 223: 0.9364817075520064\n",
      "Operator 224: 1.3145607349616217\n",
      "Operator 225: -1.3453556228863461\n",
      "Operator 226: 0.2803223927772367\n",
      "Operator 227: -1.8066854808528678\n",
      "Operator 228: -1.9689784767703182\n",
      "Operator 230: -1.9171811197748647\n",
      "Operator 232: -1.8272908016039149\n",
      "Operator 234: -1.654120415566088\n",
      "Operator 236: -0.9522785255612976\n",
      "Operator 239: -0.48812688638006735\n",
      "Operator 242: -1.2153172352262445\n",
      "Operator 244: -1.7665184211944953\n",
      "Operator 246: 0.48659396386893494\n",
      "Operator 248: 0.7010794563535121\n",
      "Operator 250: 1.0322489986034913\n",
      "Operator 252: 1.7390340286848103\n",
      "Operator 253: 1.5922887630849953e-08\n",
      "Operator 254: 2.7149000919457285\n",
      "Operator 255: 2.0133007584703877e-08\n",
      "Operator 256: 2.9156475560523413e-08\n",
      "Operator 257: -3.3402968861772515\n",
      "Operator 258: 1.0063100073087777e-08\n",
      "Operator 260: 0.5524844148698522\n",
      "Operator 261: 1.1560008505995256e-08\n",
      "Operator 262: 1.9629084194599034\n",
      "Operator 263: -0.06241396104511153\n",
      "Operator 264: 1.8932808747650651\n",
      "Operator 265: -0.18199235241373804\n",
      "Operator 266: 1.7737357188282068\n",
      "Operator 267: -0.3918015339530506\n",
      "Operator 268: 1.5569101588519891\n",
      "Operator 269: -0.8282998603974523\n",
      "Operator 270: 1.0235362447837995\n",
      "Operator 271: -0.6157892459053363\n",
      "Operator 272: -2.4518978361437926\n",
      "Operator 273: 0.2844699151593787\n",
      "Operator 274: -0.9132694114000994\n",
      "Operator 275: -1.916285290225193\n",
      "Operator 276: 0.670141186049769\n",
      "Operator 277: -0.9450622239788782\n",
      "Operator 278: 1.5957715861611526\n",
      "Total gradient norm: 24.473264877078844\n",
      "Operators under consideration (1):\n",
      "[257]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-3.3402968861772515)]\n",
      "Operator(s) added to ansatz: [257]\n",
      "Gradients: [np.float64(-3.3402968861772515)]\n",
      "Initial energy: -29.598283698722838\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257]...\n",
      "Starting point: [np.float64(0.6797683129420616), np.float64(0.5637500300090682), np.float64(0.30493055337757424), np.float64(0.306847398004051), np.float64(0.5365662045071775), np.float64(0.6640458745671312), np.float64(-0.7461271099983059), np.float64(-0.6131289383550665), np.float64(-0.7063654627076708), np.float64(0.3689109205120372), np.float64(-0.3622854767631178), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -30.294852\n",
      "         Iterations: 19\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 54\n",
      "\n",
      "Current energy: -30.294852374757596\n",
      "(change of -0.696568676034758)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257]\n",
      "On iteration 12.\n",
      "\n",
      "*** ADAPT-VQE Iteration 13 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.37859790746734545\n",
      "Operator 1: -6.708061533535768e-08\n",
      "Operator 2: 0.6357425905148053\n",
      "Operator 3: -1.1493047150580057e-07\n",
      "Operator 4: 0.9114280295180637\n",
      "Operator 5: 1.2059949957259622e-07\n",
      "Operator 6: 1.2404774716362172\n",
      "Operator 7: -1.9476436736657875e-07\n",
      "Operator 8: 1.7083608461538746\n",
      "Operator 9: 3.405664830495465e-07\n",
      "Operator 10: 1.0949795210651558\n",
      "Operator 11: -0.38151839186666003\n",
      "Operator 12: 0.9031864097879894\n",
      "Operator 13: -0.41964632612589603\n",
      "Operator 14: 1.5509174853568033\n",
      "Operator 15: 2.3456737008320516e-07\n",
      "Operator 16: 0.8912353925334818\n",
      "Operator 17: 6.708061354493944e-08\n",
      "Operator 18: -0.6357425905148097\n",
      "Operator 19: 1.1493047150580057e-07\n",
      "Operator 20: -0.9114280295180588\n",
      "Operator 21: -1.2059949887813403e-07\n",
      "Operator 22: -1.2404774716362197\n",
      "Operator 23: 1.947643665984826e-07\n",
      "Operator 24: -1.7083608461538806\n",
      "Operator 25: 0.4253228970292826\n",
      "Operator 26: -0.9323753181574868\n",
      "Operator 27: 0.36571233124576963\n",
      "Operator 28: -1.1063870141979186\n",
      "Operator 29: -3.639954462657489e-07\n",
      "Operator 30: -1.550917485356801\n",
      "Operator 31: -2.3456737008320516e-07\n",
      "Operator 32: -0.8912353925334863\n",
      "Operator 33: 2.085005377239213e-07\n",
      "Operator 34: 1.9430577434805036e-07\n",
      "Operator 35: 1.9839885128556354\n",
      "Operator 36: -0.023952931457154904\n",
      "Operator 37: 1.962950762289554\n",
      "Operator 38: -0.06547341174912451\n",
      "Operator 39: 1.9289765225397022\n",
      "Operator 40: -0.13160845020478473\n",
      "Operator 41: 1.868885782645087\n",
      "Operator 42: -0.24585642784535394\n",
      "Operator 43: 1.6329394577554948\n",
      "Operator 44: 0.03550679638540543\n",
      "Operator 45: 2.0196192181209556\n",
      "Operator 46: 0.0643066570867786\n",
      "Operator 47: 2.1296144616453474\n",
      "Operator 48: -0.2805398546973738\n",
      "Operator 49: 1.799870201602416\n",
      "Operator 50: -0.13435430174365948\n",
      "Operator 51: 1.9781381685849495\n",
      "Operator 52: -1.9761100686948998\n",
      "Operator 53: 0.04006192787228034\n",
      "Operator 54: -1.9316467961834518\n",
      "Operator 55: 0.11509786269492597\n",
      "Operator 56: -1.858171386407773\n",
      "Operator 57: 0.23766096839234208\n",
      "Operator 58: -1.7385077851308806\n",
      "Operator 59: 0.45122203174195874\n",
      "Operator 60: -1.274392797762232\n",
      "Operator 61: 0.33203099011734694\n",
      "Operator 62: -1.3780734889435782\n",
      "Operator 63: 0.48607918134355577\n",
      "Operator 64: -1.4949526960276487\n",
      "Operator 65: 0.8302991797739044\n",
      "Operator 66: -1.3898428215388028\n",
      "Operator 67: 0.425381223771389\n",
      "Operator 68: -1.8022771359743306\n",
      "Operator 69: -2.085005377239213e-07\n",
      "Operator 70: -1.9761100686949002\n",
      "Operator 71: 2.048388688927657\n",
      "Operator 72: -1.9316467961834518\n",
      "Operator 73: 2.1012067931683784\n",
      "Operator 74: -1.858171386407773\n",
      "Operator 75: 2.1881172126798063\n",
      "Operator 76: -1.7385077851308808\n",
      "Operator 77: 2.354736574084118\n",
      "Operator 78: -1.5143418932257129\n",
      "Operator 79: 2.5944852139985984e-07\n",
      "Operator 80: -1.6548470589149815\n",
      "Operator 81: -0.13787739806207552\n",
      "Operator 82: -1.5877266722923467\n",
      "Operator 83: 2.153989301135513\n",
      "Operator 84: -1.4741340566910783\n",
      "Operator 85: 2.0879305163053354\n",
      "Operator 86: -1.8022771359743341\n",
      "Operator 87: -2.0159156878089695\n",
      "Operator 88: 1.9111640664575846\n",
      "Operator 89: -2.0483886889276572\n",
      "Operator 90: 1.8260105877019284\n",
      "Operator 91: -2.1012067931683784\n",
      "Operator 92: 1.6843505473849627\n",
      "Operator 93: -2.188117212679805\n",
      "Operator 94: 1.4052690623222732\n",
      "Operator 95: -2.21714470840217\n",
      "Operator 96: 0.8649215591820298\n",
      "Operator 97: -1.9074446942108438\n",
      "Operator 98: 0.7856594934920966\n",
      "Operator 99: -2.04591597374184\n",
      "Operator 100: 1.3845597063989927\n",
      "Operator 101: -2.2846245182145735\n",
      "Operator 102: 1.8675220605095433\n",
      "Operator 103: -2.087930516305333\n",
      "Operator 104: 2.015915687808974\n",
      "Operator 105: -1.9111640664575849\n",
      "Operator 106: 2.048388688927657\n",
      "Operator 107: -1.8260105877019284\n",
      "Operator 108: 2.1012067931683784\n",
      "Operator 109: -1.6843505473849634\n",
      "Operator 110: 2.1881172126798067\n",
      "Operator 111: -1.4052690623222752\n",
      "Operator 112: 2.217144708402174\n",
      "Operator 113: -1.570894209054217\n",
      "Operator 114: -0.13643643424281648\n",
      "Operator 115: -1.694964172000522\n",
      "Operator 116: 2.0230779246442637e-07\n",
      "Operator 117: -1.6012429279761753\n",
      "Operator 118: 2.2846245182145757\n",
      "Operator 119: -1.8675220605095433\n",
      "Operator 120: 2.0879305163053354\n",
      "Operator 121: -1.9430577357337578e-07\n",
      "Operator 122: -0.3862258569920152\n",
      "Operator 123: 6.708061524739757e-08\n",
      "Operator 124: -0.5410321291874954\n",
      "Operator 125: 1.1493047132295978e-07\n",
      "Operator 126: -0.748267096297311\n",
      "Operator 127: -1.2059949958575808e-07\n",
      "Operator 128: -1.0898298582288786\n",
      "Operator 129: 1.8338390739833848e-07\n",
      "Operator 130: -1.3370866029516426\n",
      "Operator 131: 0.40100286170656685\n",
      "Operator 132: -0.9763409172332315\n",
      "Operator 133: 0.7183585839339139\n",
      "Operator 134: -0.6100201973393414\n",
      "Operator 135: 0.4196463261258977\n",
      "Operator 136: -0.3368744236889306\n",
      "Operator 137: -2.3456737008320516e-07\n",
      "Operator 138: 2.0159156878089735\n",
      "Operator 139: -2.0159156878089695\n",
      "Operator 140: 1.9761100686948998\n",
      "Operator 141: -2.048388688927657\n",
      "Operator 142: 1.9316467961834518\n",
      "Operator 143: -2.1012067931683784\n",
      "Operator 144: 1.858171386407773\n",
      "Operator 145: -2.188117212679804\n",
      "Operator 146: 1.7385077851308826\n",
      "Operator 147: -2.3547365740841126\n",
      "Operator 148: 1.2743927977622325\n",
      "Operator 149: -2.045575902251166\n",
      "Operator 150: 0.7411250094540747\n",
      "Operator 151: -1.9058728163395593\n",
      "Operator 152: 0.8844435393586509\n",
      "Operator 153: -2.1539893011355113\n",
      "Operator 154: 1.4741340566910761\n",
      "Operator 155: -2.087930516305333\n",
      "Operator 156: 1.8022771359743341\n",
      "Operator 157: 1.9800453744335422\n",
      "Operator 159: 1.9472358818418192\n",
      "Operator 161: 1.8932429676789413\n",
      "Operator 163: 1.802518399015294\n",
      "Operator 165: 1.5258886682596946\n",
      "Operator 166: -0.11812041277509615\n",
      "Operator 167: -0.7738342694910569\n",
      "Operator 168: -0.8528560051333913\n",
      "Operator 169: -0.6724962896889268\n",
      "Operator 170: -0.585775512597419\n",
      "Operator 171: 1.677547061289701\n",
      "Operator 173: 1.888161358574725\n",
      "Operator 174: 1.9839885128556354\n",
      "Operator 175: -1.9839885128556527\n",
      "Operator 176: 1.962950762289554\n",
      "Operator 177: -1.962950762289526\n",
      "Operator 178: 1.9289765225397022\n",
      "Operator 179: -1.9289765225397144\n",
      "Operator 180: 1.8688857826450878\n",
      "Operator 181: -1.8688857826450904\n",
      "Operator 182: 1.7342766351110313\n",
      "Operator 183: 0.6289346650130864\n",
      "Operator 184: 2.1860412879182727\n",
      "Operator 185: 0.7245137254403604\n",
      "Operator 186: 2.2617743634553635\n",
      "Operator 187: -1.6569940099498486\n",
      "Operator 188: 1.9090287913765227\n",
      "Operator 189: -1.90902879137652\n",
      "Operator 190: 1.9781381685849495\n",
      "Operator 191: -1.9839885128556372\n",
      "Operator 192: 1.983988512855651\n",
      "Operator 193: -1.9629507622895535\n",
      "Operator 194: 1.9629507622895284\n",
      "Operator 195: -1.9289765225397024\n",
      "Operator 196: 1.928976522539712\n",
      "Operator 197: -1.8688857826450889\n",
      "Operator 198: 1.8688857826450826\n",
      "Operator 199: -1.6329394577554965\n",
      "Operator 200: 2.246022649976968\n",
      "Operator 201: 0.7428366098297091\n",
      "Operator 202: 2.1887228743134672\n",
      "Operator 203: 0.6009392362926387\n",
      "Operator 204: 1.7574874395478464\n",
      "Operator 205: -1.9090287913765258\n",
      "Operator 206: 1.9090287913765165\n",
      "Operator 207: -1.9781381685849513\n",
      "Operator 208: -1.9430577378908958e-07\n",
      "Operator 209: -0.37859790746736194\n",
      "Operator 210: -1.9960250390599374\n",
      "Operator 211: -1.9960250390599374\n",
      "Operator 212: 0.023952931457156254\n",
      "Operator 213: -1.9839885128556527\n",
      "Operator 214: 0.0654734117491279\n",
      "Operator 215: -1.962950762289526\n",
      "Operator 216: 0.131608450204783\n",
      "Operator 217: -1.9289765225397144\n",
      "Operator 218: 0.24585642784535278\n",
      "Operator 219: -1.7596831294011226\n",
      "Operator 220: 0.3506900413491064\n",
      "Operator 221: 0.7112815464513053\n",
      "Operator 222: 1.079941532967596\n",
      "Operator 223: 0.6230416814849662\n",
      "Operator 224: 0.8876390058647528\n",
      "Operator 225: -1.7574874395478473\n",
      "Operator 226: 0.13435430174365864\n",
      "Operator 227: -1.9090287913765198\n",
      "Operator 228: -1.9800453744335396\n",
      "Operator 230: -1.9472358818418187\n",
      "Operator 232: -1.8932429676789408\n",
      "Operator 234: -1.8025183990152924\n",
      "Operator 236: -1.525888668259691\n",
      "Operator 238: -1.2672903292845197\n",
      "Operator 239: -0.06085182403871589\n",
      "Operator 240: -1.3639494259262883\n",
      "Operator 242: -1.6775470612896997\n",
      "Operator 244: -1.8881613585747226\n",
      "Operator 245: 1.943057732811363e-07\n",
      "Operator 246: 0.38622585699201584\n",
      "Operator 247: -6.708061386138739e-08\n",
      "Operator 248: 0.5410321291874953\n",
      "Operator 249: -1.1493047132295978e-07\n",
      "Operator 250: 0.7482670962973107\n",
      "Operator 251: 1.2059949886601346e-07\n",
      "Operator 252: 1.0898298582288795\n",
      "Operator 253: -1.8338390683121675e-07\n",
      "Operator 254: 1.3370866029516462\n",
      "Operator 255: 3.210928293897905e-07\n",
      "Operator 256: 1.2383319848305746e-07\n",
      "Operator 257: -5.461107473570144e-07\n",
      "Operator 258: 6.032106942211373e-08\n",
      "Operator 259: 3.63995447431451e-07\n",
      "Operator 260: 0.3368744236889313\n",
      "Operator 261: 2.3456737008320516e-07\n",
      "Operator 262: 1.9761100686948998\n",
      "Operator 263: -0.04006192787228057\n",
      "Operator 264: 1.9316467961834518\n",
      "Operator 265: -0.11509786269492639\n",
      "Operator 266: 1.858171386407773\n",
      "Operator 267: -0.23766096839234188\n",
      "Operator 268: 1.7385077851308801\n",
      "Operator 269: -0.4512220317419575\n",
      "Operator 270: 1.5143418932257133\n",
      "Operator 271: -0.38993646384370423\n",
      "Operator 272: 0.9755412247774993\n",
      "Operator 273: -0.40217387447569153\n",
      "Operator 274: 0.832763772708729\n",
      "Operator 275: -0.8302991797739055\n",
      "Operator 276: 1.3898428215388028\n",
      "Operator 277: -0.42538122377138693\n",
      "Operator 278: 1.8022771359743306\n",
      "Total gradient norm: 23.514795698289696\n",
      "Operators under consideration (1):\n",
      "[77]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.354736574084118)]\n",
      "Operator(s) added to ansatz: [77]\n",
      "Gradients: [np.float64(2.354736574084118)]\n",
      "Initial energy: -30.294852374757596\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77]...\n",
      "Starting point: [np.float64(0.7114019198667854), np.float64(0.634013434365784), np.float64(0.16990227053294815), np.float64(0.17177025732163664), np.float64(0.6033457121322106), np.float64(0.6890080792001299), np.float64(-0.7538692943594916), np.float64(-0.6517494957918778), np.float64(-0.7220876254752185), np.float64(0.5366118497263729), np.float64(-0.5247138438854041), np.float64(0.4834654460126169), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -30.519234\n",
      "         Iterations: 22\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n",
      "\n",
      "Current energy: -30.519233883261222\n",
      "(change of -0.22438150850362604)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77]\n",
      "On iteration 13.\n",
      "\n",
      "*** ADAPT-VQE Iteration 14 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.5340683048322028\n",
      "Operator 2: 0.9035965855964201\n",
      "Operator 4: 1.3281164144982345\n",
      "Operator 6: 1.9307555805666656\n",
      "Operator 8: 1.2198950040232597\n",
      "Operator 9: -0.47711652739452337\n",
      "Operator 10: 1.0364913891028107\n",
      "Operator 11: -0.6114695654956882\n",
      "Operator 12: 0.8821850149909718\n",
      "Operator 13: -0.5601349712470907\n",
      "Operator 14: 1.7959599993211697\n",
      "Operator 16: 1.0177161456973\n",
      "Operator 18: -0.9035965855964243\n",
      "Operator 20: -1.3281164144982305\n",
      "Operator 22: -1.930755580566668\n",
      "Operator 23: 0.5707596056770612\n",
      "Operator 24: -0.896087012971001\n",
      "Operator 25: 0.6075902018453871\n",
      "Operator 26: -1.0495293432754125\n",
      "Operator 27: 0.46070038813502184\n",
      "Operator 28: -1.231197026167123\n",
      "Operator 30: -1.7959599993211675\n",
      "Operator 32: -1.017716145697305\n",
      "Operator 35: 1.967967790202923\n",
      "Operator 36: -0.04779028468610052\n",
      "Operator 37: 1.923720738603163\n",
      "Operator 38: -0.13391518459153942\n",
      "Operator 39: 1.8411911810526425\n",
      "Operator 40: -0.2895233789619997\n",
      "Operator 41: 1.5118252050903567\n",
      "Operator 42: 0.1217306652376067\n",
      "Operator 43: 1.7240518728852017\n",
      "Operator 44: 0.3027469211337288\n",
      "Operator 45: 1.6152542584356058\n",
      "Operator 46: 0.22578060651206783\n",
      "Operator 47: 1.9454848790922228\n",
      "Operator 48: -0.3618626135090797\n",
      "Operator 49: 1.7408801250048778\n",
      "Operator 50: -0.17598919985391215\n",
      "Operator 51: 1.9715724837603767\n",
      "Operator 52: -1.952457616125114\n",
      "Operator 53: 0.08021838677153527\n",
      "Operator 54: -1.8625931767400004\n",
      "Operator 55: 0.23674655482910345\n",
      "Operator 56: -1.7034247321259293\n",
      "Operator 57: 0.5281406403430471\n",
      "Operator 58: -1.0583746807694976\n",
      "Operator 59: 0.3499374775481322\n",
      "Operator 60: -0.8581058359461219\n",
      "Operator 61: 0.4730761477588813\n",
      "Operator 62: -0.6137299792195845\n",
      "Operator 63: 0.6863659447133014\n",
      "Operator 64: -1.0860821304703019\n",
      "Operator 65: 1.1479965392526805\n",
      "Operator 66: -1.2005287488904404\n",
      "Operator 67: 0.5669964788896646\n",
      "Operator 68: -1.7425128023827012\n",
      "Operator 70: -1.9524576161251144\n",
      "Operator 71: 2.097521373227383\n",
      "Operator 72: -1.8625931767400004\n",
      "Operator 73: 2.2136449139443637\n",
      "Operator 74: -1.7034247321259306\n",
      "Operator 75: 2.4489859317173015\n",
      "Operator 76: -1.3869300327976761\n",
      "Operator 78: -1.3314314893043235\n",
      "Operator 79: -0.21239062011038942\n",
      "Operator 80: -1.0633316651042009\n",
      "Operator 81: -0.23135141120370117\n",
      "Operator 82: -1.1976655516493697\n",
      "Operator 83: 2.2034194965256493\n",
      "Operator 84: -1.2966277371534303\n",
      "Operator 85: 2.1145298406888466\n",
      "Operator 86: -1.7425128023827012\n",
      "Operator 87: -2.0316502293819645\n",
      "Operator 88: 1.820715328434345\n",
      "Operator 89: -2.0975213732273823\n",
      "Operator 90: 1.6303460953059539\n",
      "Operator 91: -2.213644913944365\n",
      "Operator 92: 1.2327426965801087\n",
      "Operator 93: -2.26240889766602\n",
      "Operator 94: 0.38493683177859406\n",
      "Operator 95: -1.76157315797724\n",
      "Operator 96: -0.08067429109639768\n",
      "Operator 97: -1.7345301140306906\n",
      "Operator 98: 0.27009329063381515\n",
      "Operator 99: -2.0098341573120466\n",
      "Operator 100: 1.168178822028186\n",
      "Operator 101: -2.379797100586177\n",
      "Operator 102: 1.827162256948084\n",
      "Operator 103: -2.1145298406888444\n",
      "Operator 104: 2.0316502293819676\n",
      "Operator 105: -1.820715328434345\n",
      "Operator 106: 2.097521373227383\n",
      "Operator 107: -1.6303460953059539\n",
      "Operator 108: 2.2136449139443637\n",
      "Operator 109: -1.2327426965801087\n",
      "Operator 110: 2.2624088976660217\n",
      "Operator 111: -1.1699716083128462\n",
      "Operator 112: -0.23383317013186272\n",
      "Operator 113: -1.0698762572035905\n",
      "Operator 114: -0.20737232417246856\n",
      "Operator 115: -1.3747677291487963\n",
      "Operator 117: -1.4678952788843302\n",
      "Operator 118: 2.3797971005861793\n",
      "Operator 119: -1.8271622569480854\n",
      "Operator 120: 2.1145298406888466\n",
      "Operator 122: -0.5557394544906396\n",
      "Operator 124: -0.819118375588197\n",
      "Operator 126: -1.2725370087506493\n",
      "Operator 128: -1.7020095511395414\n",
      "Operator 129: 0.5175834001958599\n",
      "Operator 130: -1.3824706101199755\n",
      "Operator 131: 0.9538082042879239\n",
      "Operator 132: -1.0784447544947646\n",
      "Operator 133: 1.0189417237987044\n",
      "Operator 134: -0.6850593130619058\n",
      "Operator 135: 0.5601349712470886\n",
      "Operator 136: -0.39981643287796886\n",
      "Operator 138: 2.0316502293819685\n",
      "Operator 139: -2.0316502293819645\n",
      "Operator 140: 1.952457616125114\n",
      "Operator 141: -2.0975213732273823\n",
      "Operator 142: 1.8625931767400004\n",
      "Operator 143: -2.213644913944365\n",
      "Operator 144: 1.7034247321259293\n",
      "Operator 145: -2.448985931717301\n",
      "Operator 146: 1.0583746807694983\n",
      "Operator 147: -2.002484877241437\n",
      "Operator 148: 0.2144038963958518\n",
      "Operator 149: -1.7289721776579046\n",
      "Operator 150: -0.08007829656322037\n",
      "Operator 151: -1.7709516615564098\n",
      "Operator 152: 0.4181274276063292\n",
      "Operator 153: -2.2034194965256475\n",
      "Operator 154: 1.2966277371534303\n",
      "Operator 155: -2.1145298406888444\n",
      "Operator 156: 1.7425128023827012\n",
      "Operator 157: 1.9601973625942493\n",
      "Operator 159: 1.8929102254581895\n",
      "Operator 161: 1.7709688293620836\n",
      "Operator 163: 1.3917796174527335\n",
      "Operator 164: -0.17003968856820362\n",
      "Operator 165: -0.9990012526724215\n",
      "Operator 166: -0.9603921268265453\n",
      "Operator 167: -1.0678252351047859\n",
      "Operator 168: -1.0186332653781611\n",
      "Operator 169: -0.8687741863619536\n",
      "Operator 170: -0.6274784623513054\n",
      "Operator 171: 1.5613974256404717\n",
      "Operator 173: 1.8535075651182615\n",
      "Operator 174: 1.967967790202923\n",
      "Operator 175: -1.9679677902029407\n",
      "Operator 176: 1.9237207386031634\n",
      "Operator 177: -1.9237207386031359\n",
      "Operator 178: 1.8411911810526425\n",
      "Operator 179: -1.841191181052655\n",
      "Operator 180: 1.6365028719173937\n",
      "Operator 181: 0.8570341751737416\n",
      "Operator 182: 1.99327327297911\n",
      "Operator 183: 1.137904499469872\n",
      "Operator 184: 1.9776329296493398\n",
      "Operator 185: 0.9947746420625614\n",
      "Operator 186: 2.145362818863912\n",
      "Operator 187: -1.5377619164379557\n",
      "Operator 188: 1.8802327384718625\n",
      "Operator 189: -1.880232738471858\n",
      "Operator 190: 1.9715724837603767\n",
      "Operator 191: -1.9679677902029238\n",
      "Operator 192: 1.9679677902029398\n",
      "Operator 193: -1.9237207386031634\n",
      "Operator 194: 1.9237207386031383\n",
      "Operator 195: -1.8411911810526433\n",
      "Operator 196: 1.8411911810526518\n",
      "Operator 197: -1.5118252050903571\n",
      "Operator 198: 2.1271572799683796\n",
      "Operator 199: 1.0192672129538267\n",
      "Operator 200: 1.9723533685973527\n",
      "Operator 201: 1.1346940333841888\n",
      "Operator 202: 2.003784975420985\n",
      "Operator 203: 0.825998781036418\n",
      "Operator 204: 1.6608554820819572\n",
      "Operator 205: -1.8802327384718658\n",
      "Operator 206: 1.8802327384718542\n",
      "Operator 207: -1.971572483760378\n",
      "Operator 209: -0.5340683048322195\n",
      "Operator 210: -1.9921030947281313\n",
      "Operator 211: -1.9921030947281313\n",
      "Operator 212: 0.047790284686102684\n",
      "Operator 213: -1.967967790202941\n",
      "Operator 214: 0.13391518459154433\n",
      "Operator 215: -1.9237207386031359\n",
      "Operator 216: 0.2895233789619963\n",
      "Operator 217: -1.7009192483995799\n",
      "Operator 218: 0.4441531517152827\n",
      "Operator 219: 0.9685208311725524\n",
      "Operator 220: 1.2044292155857446\n",
      "Operator 221: 1.0766941978986055\n",
      "Operator 222: 1.312390397208393\n",
      "Operator 223: 0.8418568716328847\n",
      "Operator 224: 1.0224749620071578\n",
      "Operator 225: -1.6608554820819579\n",
      "Operator 226: 0.17598919985391182\n",
      "Operator 227: -1.8802327384718582\n",
      "Operator 228: -1.960197362594246\n",
      "Operator 230: -1.89291022545819\n",
      "Operator 232: -1.7709688293620849\n",
      "Operator 234: -1.3917796174527322\n",
      "Operator 236: -0.9857617593284225\n",
      "Operator 237: -0.15484537972203796\n",
      "Operator 238: -0.8735056431710784\n",
      "Operator 239: -0.12249706207448804\n",
      "Operator 240: -1.1124927227038914\n",
      "Operator 242: -1.5613974256404703\n",
      "Operator 244: -1.8535075651182586\n",
      "Operator 246: 0.5557394544906405\n",
      "Operator 248: 0.8191183755881976\n",
      "Operator 250: 1.272537008750649\n",
      "Operator 252: 1.7020095511395419\n",
      "Operator 260: 0.3998164328779694\n",
      "Operator 262: 1.9524576161251144\n",
      "Operator 263: -0.08021838677153514\n",
      "Operator 264: 1.8625931767400004\n",
      "Operator 265: -0.236746554829104\n",
      "Operator 266: 1.7034247321259306\n",
      "Operator 267: -0.5281406403430469\n",
      "Operator 268: 1.3869300327976761\n",
      "Operator 269: -0.47639009873172233\n",
      "Operator 270: 0.6151952287702893\n",
      "Operator 271: -0.5010819141870237\n",
      "Operator 272: 0.23408009490603854\n",
      "Operator 273: -0.49157681331456193\n",
      "Operator 274: 0.37917156985717004\n",
      "Operator 275: -1.147996539252682\n",
      "Operator 276: 1.2005287488904388\n",
      "Operator 277: -0.5669964788896623\n",
      "Operator 278: 1.7425128023827012\n",
      "Total gradient norm: 22.46828712594302\n",
      "Operators under consideration (1):\n",
      "[145]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.448985931717301)]\n",
      "Operator(s) added to ansatz: [145]\n",
      "Gradients: [np.float64(-2.448985931717301)]\n",
      "Initial energy: -30.519233883261222\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145]...\n",
      "Starting point: [np.float64(0.7009956807522489), np.float64(0.6114857751088374), np.float64(0.1937118819390325), np.float64(0.21754445605902012), np.float64(0.4791813408982466), np.float64(0.6468619485438742), np.float64(-0.7409512638711042), np.float64(-0.5848014431063241), np.float64(-0.6957905715278632), np.float64(0.48993750435756384), np.float64(-0.4011983419581455), np.float64(0.4043927622160244), np.float64(-0.19643439455860853), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -30.757315\n",
      "         Iterations: 20\n",
      "         Function evaluations: 33\n",
      "         Gradient evaluations: 33\n",
      "\n",
      "Current energy: -30.757315048089445\n",
      "(change of -0.23808116482822328)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145]\n",
      "On iteration 14.\n",
      "\n",
      "*** ADAPT-VQE Iteration 15 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 0.7339056593812179\n",
      "Operator 2: 1.2589389024153435\n",
      "Operator 4: 1.9426565204901363\n",
      "Operator 5: -0.5980123430329022\n",
      "Operator 6: 0.8973913671655659\n",
      "Operator 8: 1.0918187695761143\n",
      "Operator 9: -0.6553768713398145\n",
      "Operator 10: 0.943243745656799\n",
      "Operator 11: -0.7056222723402684\n",
      "Operator 12: 0.8534083127045522\n",
      "Operator 13: -0.6080851913591716\n",
      "Operator 14: 1.863284003216485\n",
      "Operator 16: 1.0515109458617784\n",
      "Operator 18: -1.2589389024153488\n",
      "Operator 20: -1.9426565204901323\n",
      "Operator 22: -1.2472780960739955\n",
      "Operator 23: 1.1144595227113492\n",
      "Operator 24: -0.9394182427967086\n",
      "Operator 25: 0.6666659381541493\n",
      "Operator 26: -1.0610940154243618\n",
      "Operator 27: 0.48868883289778275\n",
      "Operator 28: -1.2585946008238722\n",
      "Operator 30: -1.8632840032164826\n",
      "Operator 32: -1.051510945861783\n",
      "Operator 35: 1.9389233023456243\n",
      "Operator 36: -0.09067211717383204\n",
      "Operator 37: 1.8464777274577968\n",
      "Operator 38: -0.26586061118847276\n",
      "Operator 39: 1.619026273827773\n",
      "Operator 40: -1.081212754834809\n",
      "Operator 41: -0.9568676231231682\n",
      "Operator 42: 0.3581216175859049\n",
      "Operator 43: 1.3818932466962477\n",
      "Operator 44: 0.5094249128798005\n",
      "Operator 45: 1.4566738205251517\n",
      "Operator 46: 0.30329073742339274\n",
      "Operator 47: 1.8719903880007704\n",
      "Operator 48: -0.3832312196531093\n",
      "Operator 49: 1.720539159003791\n",
      "Operator 50: -0.18811844609776007\n",
      "Operator 51: 1.9696777844210605\n",
      "Operator 52: -1.9102014621598538\n",
      "Operator 53: 0.15322529624052392\n",
      "Operator 54: -1.7354229734506583\n",
      "Operator 55: 0.476422825350447\n",
      "Operator 56: -1.3800094243501657\n",
      "Operator 57: 0.4792372928337864\n",
      "Operator 58: -0.08555413358028251\n",
      "Operator 59: 0.4474192677103517\n",
      "Operator 60: -0.2785737912899911\n",
      "Operator 61: 0.5264697413773654\n",
      "Operator 62: -0.35582326189577684\n",
      "Operator 63: 0.7305928684138562\n",
      "Operator 64: -0.9555998814166224\n",
      "Operator 65: 1.2434274036217818\n",
      "Operator 66: -1.1430303677009923\n",
      "Operator 67: 0.6092012938074145\n",
      "Operator 68: -1.7252328777285104\n",
      "Operator 70: -1.9102014621598538\n",
      "Operator 71: 2.1884576825978304\n",
      "Operator 72: -1.7354229734506583\n",
      "Operator 73: 2.451160107486287\n",
      "Operator 74: -1.0420542489550702\n",
      "Operator 75: 2.003016604738195\n",
      "Operator 76: -0.5312422265829363\n",
      "Operator 77: -0.1359995048148972\n",
      "Operator 78: -0.8850295748684243\n",
      "Operator 79: -0.3069621570114106\n",
      "Operator 80: -0.8391725527970184\n",
      "Operator 81: -0.2779948348647428\n",
      "Operator 82: -1.0745488048985332\n",
      "Operator 83: 2.2136060124658745\n",
      "Operator 84: -1.2435105453325588\n",
      "Operator 85: 2.1222224533462755\n",
      "Operator 86: -1.725232877728514\n",
      "Operator 87: -2.0596969786797996\n",
      "Operator 88: 1.6526800543238664\n",
      "Operator 89: -2.1884576825978295\n",
      "Operator 90: 1.2100181236736496\n",
      "Operator 91: -2.259006474591965\n",
      "Operator 92: 1.0589802699805617\n",
      "Operator 93: 0.15817301754026902\n",
      "Operator 94: -0.03454169494729199\n",
      "Operator 95: -1.556227785671478\n",
      "Operator 96: -0.4165263797858416\n",
      "Operator 97: -1.6183126780342953\n",
      "Operator 98: 0.09208924211925208\n",
      "Operator 99: -1.9857659098929592\n",
      "Operator 100: 1.0982454298881323\n",
      "Operator 101: -2.4081971026275695\n",
      "Operator 102: 1.8154598438951353\n",
      "Operator 103: -2.1222224533462732\n",
      "Operator 104: 2.059696978679804\n",
      "Operator 105: -1.652680054323866\n",
      "Operator 106: 2.1884576825978304\n",
      "Operator 107: -1.2100181236736496\n",
      "Operator 108: 2.2590064745919625\n",
      "Operator 109: -0.26821468959091055\n",
      "Operator 110: 1.7198015513959208\n",
      "Operator 111: -0.3819693533197689\n",
      "Operator 112: -0.3109832868812116\n",
      "Operator 113: -0.8919021006457636\n",
      "Operator 114: -0.23938330402137326\n",
      "Operator 115: -1.2735709056801676\n",
      "Operator 117: -1.427838666254507\n",
      "Operator 118: 2.4081971026275717\n",
      "Operator 119: -1.815459843895134\n",
      "Operator 120: 2.1222224533462755\n",
      "Operator 122: -0.7913901536693662\n",
      "Operator 124: -1.2848473656538255\n",
      "Operator 126: -1.782449541718421\n",
      "Operator 129: 0.6385038254870485\n",
      "Operator 130: -1.3559083814580868\n",
      "Operator 131: 1.1422784186397692\n",
      "Operator 132: -1.0694884088813863\n",
      "Operator 133: 1.1302903092285104\n",
      "Operator 134: -0.6995535322235346\n",
      "Operator 135: 0.6080851913591716\n",
      "Operator 136: -0.41775352951893274\n",
      "Operator 138: 2.059696978679804\n",
      "Operator 139: -2.0596969786797996\n",
      "Operator 140: 1.9102014621598529\n",
      "Operator 141: -2.1884576825978295\n",
      "Operator 142: 1.7354229734506572\n",
      "Operator 143: -2.45116010748629\n",
      "Operator 144: 1.3800094243501657\n",
      "Operator 146: 0.692841257807469\n",
      "Operator 147: -1.645842197274869\n",
      "Operator 148: -0.4197864993715997\n",
      "Operator 149: -1.5440483294875875\n",
      "Operator 150: -0.39108102341958795\n",
      "Operator 151: -1.696666183817468\n",
      "Operator 152: 0.2671875723234124\n",
      "Operator 153: -2.213606012465873\n",
      "Operator 154: 1.243510545332555\n",
      "Operator 155: -2.1222224533462732\n",
      "Operator 156: 1.725232877728514\n",
      "Operator 157: 1.9245088015155765\n",
      "Operator 159: 1.7900893465150585\n",
      "Operator 161: 1.4947479773959733\n",
      "Operator 163: 0.9139509310123144\n",
      "Operator 164: 0.008858579573115038\n",
      "Operator 165: -1.1264108188331825\n",
      "Operator 166: -1.0762337315778896\n",
      "Operator 167: -1.1210471832251518\n",
      "Operator 168: -1.0738706588772202\n",
      "Operator 169: -0.9161247375823658\n",
      "Operator 170: -0.6419508316379691\n",
      "Operator 171: 1.525642818113526\n",
      "Operator 173: 1.8434079505687144\n",
      "Operator 174: 1.9389233023456243\n",
      "Operator 175: -1.9389233023456431\n",
      "Operator 176: 1.8464777274577968\n",
      "Operator 177: -1.8464777274577688\n",
      "Operator 178: 1.492106053758425\n",
      "Operator 179: -2.09157702310099\n",
      "Operator 180: -0.9506587142382035\n",
      "Operator 181: 1.1566269351038012\n",
      "Operator 182: 1.8271170260792982\n",
      "Operator 183: 1.2987609231410433\n",
      "Operator 184: 1.8853784205972595\n",
      "Operator 185: 1.079193603866365\n",
      "Operator 186: 2.1050076222547998\n",
      "Operator 187: -1.4984246040261975\n",
      "Operator 188: 1.8717863044899388\n",
      "Operator 189: -1.8717863044899357\n",
      "Operator 190: 1.969677784421061\n",
      "Operator 191: -1.9389233023456254\n",
      "Operator 192: 1.9389233023456423\n",
      "Operator 193: -1.8464777274577964\n",
      "Operator 194: 1.8464777274577728\n",
      "Operator 195: -1.6190262738277736\n",
      "Operator 196: -0.9045920782978478\n",
      "Operator 197: -1.581952168595738\n",
      "Operator 198: 1.591505815299386\n",
      "Operator 199: 1.2954208858513265\n",
      "Operator 200: 1.8330833254049552\n",
      "Operator 201: 1.2690475297738808\n",
      "Operator 202: 1.9344356199100596\n",
      "Operator 203: 0.8946152145936652\n",
      "Operator 204: 1.6301463628127828\n",
      "Operator 205: -1.8717863044899419\n",
      "Operator 206: 1.8717863044899319\n",
      "Operator 207: -1.9696777844210622\n",
      "Operator 209: -0.733905659381235\n",
      "Operator 210: -1.9851314378319043\n",
      "Operator 211: -1.9851314378319043\n",
      "Operator 212: 0.0906721171738347\n",
      "Operator 213: -1.9389233023456431\n",
      "Operator 214: 0.2658606111884812\n",
      "Operator 215: -1.7017269205619572\n",
      "Operator 216: 0.4169006455724913\n",
      "Operator 217: -1.8663548969024775\n",
      "Operator 218: -0.23246303232274526\n",
      "Operator 219: 1.2277161104213907\n",
      "Operator 220: 1.3431517431082047\n",
      "Operator 221: 1.1765634916857999\n",
      "Operator 222: 1.3773974038814591\n",
      "Operator 223: 0.9009285126214872\n",
      "Operator 224: 1.0623057116921533\n",
      "Operator 225: -1.6301463628127837\n",
      "Operator 226: 0.18811844609775913\n",
      "Operator 227: -1.8717863044899357\n",
      "Operator 228: -1.9245088015155727\n",
      "Operator 230: -1.7900893465150576\n",
      "Operator 232: -1.4947479773959733\n",
      "Operator 233: 0.6283833222501248\n",
      "Operator 234: 0.9384862513329887\n",
      "Operator 235: -0.039766008245855605\n",
      "Operator 236: -0.6496547587145818\n",
      "Operator 237: -0.22853877946655082\n",
      "Operator 238: -0.7274808360669963\n",
      "Operator 239: -0.15149674427433735\n",
      "Operator 240: -1.0304680135787339\n",
      "Operator 242: -1.5256428181135249\n",
      "Operator 244: -1.8434079505687113\n",
      "Operator 246: 0.7913901536693679\n",
      "Operator 248: 1.284847365653826\n",
      "Operator 250: 1.782449541718419\n",
      "Operator 251: -0.5336180558977596\n",
      "Operator 252: 1.5272056230449618\n",
      "Operator 253: -0.45123186814971206\n",
      "Operator 260: 0.4177535295189332\n",
      "Operator 262: 1.9102014621598538\n",
      "Operator 263: -0.15322529624052375\n",
      "Operator 264: 1.7354229734506572\n",
      "Operator 265: -0.47642282535044767\n",
      "Operator 266: 1.0420542489550695\n",
      "Operator 267: -0.3448015408644867\n",
      "Operator 268: 1.2488461826780812\n",
      "Operator 269: -0.5326955977217763\n",
      "Operator 270: 0.06223871415948377\n",
      "Operator 271: -0.5087213754413331\n",
      "Operator 272: -0.02312257554015431\n",
      "Operator 273: -0.4874776707584778\n",
      "Operator 274: 0.23761081047627586\n",
      "Operator 275: -1.2434274036217832\n",
      "Operator 276: 1.1430303677009923\n",
      "Operator 277: -0.6092012938074125\n",
      "Operator 278: 1.7252328777285104\n",
      "Total gradient norm: 21.44290206621256\n",
      "Operators under consideration (1):\n",
      "[143]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.45116010748629)]\n",
      "Operator(s) added to ansatz: [143]\n",
      "Gradients: [np.float64(-2.45116010748629)]\n",
      "Initial energy: -30.757315048089445\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143]...\n",
      "Starting point: [np.float64(0.6982214081854639), np.float64(0.6053929524257219), np.float64(0.20238072949183633), np.float64(0.23748834992466014), np.float64(0.37829130060597743), np.float64(0.5882132328766684), np.float64(-0.7243919718602118), np.float64(-0.4716611875885433), np.float64(-0.66151301709671), np.float64(0.4764277491856233), np.float64(-0.3518504248453331), np.float64(0.37915875637338636), np.float64(-0.23416931603040894), np.float64(0.19929769535513192), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -31.000860\n",
      "         Iterations: 22\n",
      "         Function evaluations: 35\n",
      "         Gradient evaluations: 35\n",
      "\n",
      "Current energy: -31.00086004848307\n",
      "(change of -0.24354500039362392)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143]\n",
      "On iteration 15.\n",
      "\n",
      "*** ADAPT-VQE Iteration 16 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.0593403773802192\n",
      "Operator 2: 1.8790031892655272\n",
      "Operator 3: -0.6194562702828442\n",
      "Operator 4: 0.8452405689266704\n",
      "Operator 5: -0.722986748161903\n",
      "Operator 6: 0.9473853211943679\n",
      "Operator 8: 0.9961783326347413\n",
      "Operator 9: -0.7340402508914661\n",
      "Operator 10: 0.8995088399645073\n",
      "Operator 11: -0.742359266142402\n",
      "Operator 12: 0.8408178362929375\n",
      "Operator 13: -0.6255243266607684\n",
      "Operator 14: 1.8859825982354246\n",
      "Operator 16: 1.0628092105139577\n",
      "Operator 18: -1.8790031892655303\n",
      "Operator 20: -1.2646089333425747\n",
      "Operator 21: 0.49518447621992295\n",
      "Operator 22: -1.0634722177670901\n",
      "Operator 23: 1.3083233178214322\n",
      "Operator 24: -0.9401980899957836\n",
      "Operator 25: 0.6862124671524188\n",
      "Operator 26: -1.0615342390764804\n",
      "Operator 27: 0.4981720000214631\n",
      "Operator 28: -1.2669578623620308\n",
      "Operator 30: -1.8859825982354224\n",
      "Operator 32: -1.0628092105139622\n",
      "Operator 35: 1.8697829860713773\n",
      "Operator 36: -0.1909896915785324\n",
      "Operator 37: 1.6226895188910295\n",
      "Operator 38: -1.0713135727232532\n",
      "Operator 39: -0.9154270616559665\n",
      "Operator 40: -1.388730287549957\n",
      "Operator 41: -1.2213051892248714\n",
      "Operator 42: 0.5422190922457677\n",
      "Operator 43: 1.2395811497767806\n",
      "Operator 44: 0.6035179607967344\n",
      "Operator 45: 1.3905710407149172\n",
      "Operator 46: 0.3338752758288825\n",
      "Operator 47: 1.843162980847267\n",
      "Operator 48: -0.3901390176633201\n",
      "Operator 49: 1.7130678576012328\n",
      "Operator 50: -0.19226918267271959\n",
      "Operator 51: 1.969031254436933\n",
      "Operator 52: -1.8126931681398646\n",
      "Operator 53: 0.32833051256090795\n",
      "Operator 54: -1.4182665070662526\n",
      "Operator 55: 0.4488522651524067\n",
      "Operator 56: -0.5300951865717662\n",
      "Operator 57: 0.5078946213765363\n",
      "Operator 58: 0.47606111036167686\n",
      "Operator 59: 0.49433638289739934\n",
      "Operator 60: -0.07091600260203508\n",
      "Operator 61: 0.5237961087533252\n",
      "Operator 62: -0.26705854443257077\n",
      "Operator 63: 0.7402981990924171\n",
      "Operator 64: -0.9097669588510173\n",
      "Operator 65: 1.2759593458237788\n",
      "Operator 66: -1.1230319992251006\n",
      "Operator 67: 0.6237452181581822\n",
      "Operator 68: -1.719332995491504\n",
      "Operator 70: -1.812693168139864\n",
      "Operator 71: 2.414965668747675\n",
      "Operator 72: -1.08158200906486\n",
      "Operator 73: 1.979198780799293\n",
      "Operator 74: -0.05367399959591028\n",
      "Operator 75: 1.608164580569921\n",
      "Operator 76: 0.013608640578695597\n",
      "Operator 77: -0.20541188645077113\n",
      "Operator 78: -0.7021126071558896\n",
      "Operator 79: -0.35613796588842683\n",
      "Operator 80: -0.7597739144626818\n",
      "Operator 81: -0.2980890427016387\n",
      "Operator 82: -1.0318504526925234\n",
      "Operator 83: 2.216383665853062\n",
      "Operator 84: -1.2251838241059572\n",
      "Operator 85: 2.1248491294727394\n",
      "Operator 86: -1.719332995491504\n",
      "Operator 87: -2.1240397547550436\n",
      "Operator 88: 1.2308413401546807\n",
      "Operator 89: -2.216051739844117\n",
      "Operator 90: 1.0435459496102677\n",
      "Operator 91: 0.28374934721517076\n",
      "Operator 92: 0.7588202115241984\n",
      "Operator 93: 0.21507459450661665\n",
      "Operator 94: -0.19646556712415997\n",
      "Operator 95: -1.4347980733182055\n",
      "Operator 96: -0.5466534060916791\n",
      "Operator 97: -1.5671656145773096\n",
      "Operator 98: 0.02698073070704049\n",
      "Operator 99: -1.9756870056814622\n",
      "Operator 100: 1.073341545048553\n",
      "Operator 101: -2.417987570514051\n",
      "Operator 102: 1.8114607061885741\n",
      "Operator 103: -2.1248491294727376\n",
      "Operator 104: 2.1240397547550476\n",
      "Operator 105: -1.2308413401546812\n",
      "Operator 106: 2.2160517398441177\n",
      "Operator 107: -0.2301312130762379\n",
      "Operator 108: 1.6835386897387088\n",
      "Operator 109: 0.4823660669146316\n",
      "Operator 110: 1.490756334335166\n",
      "Operator 111: -0.06787264419069919\n",
      "Operator 112: -0.3479933403271651\n",
      "Operator 113: -0.8288146538508991\n",
      "Operator 114: -0.2525476213977481\n",
      "Operator 115: -1.2382607963008656\n",
      "Operator 117: -1.4139894397845159\n",
      "Operator 118: 2.4179875705140534\n",
      "Operator 119: -1.8114607061885741\n",
      "Operator 120: 2.1248491294727394\n",
      "Operator 122: -1.2416455527165207\n",
      "Operator 124: -1.7743019517661427\n",
      "Operator 129: 0.6855299839737077\n",
      "Operator 130: -1.3122353643843376\n",
      "Operator 131: 1.2188174284093871\n",
      "Operator 132: -1.059402920641571\n",
      "Operator 133: 1.172212359982704\n",
      "Operator 134: -0.703557497690873\n",
      "Operator 135: 0.6255243266607704\n",
      "Operator 136: -0.4238655640535951\n",
      "Operator 138: 2.1240397547550476\n",
      "Operator 139: -2.1240397547550427\n",
      "Operator 140: 1.812693168139865\n",
      "Operator 141: -2.4149656687476724\n",
      "Operator 142: 1.4182665070662526\n",
      "Operator 144: 1.248084116245181\n",
      "Operator 145: 0.24452167991070994\n",
      "Operator 146: 0.15735578014610804\n",
      "Operator 147: -1.4507112221446739\n",
      "Operator 148: -0.6790945396766382\n",
      "Operator 149: -1.4531095308867625\n",
      "Operator 150: -0.5039210024361618\n",
      "Operator 151: -1.6648309556847412\n",
      "Operator 152: 0.21401185839265668\n",
      "Operator 153: -2.2163836658530602\n",
      "Operator 154: 1.2251838241059585\n",
      "Operator 155: -2.1248491294727385\n",
      "Operator 156: 1.719332995491504\n",
      "Operator 157: 1.8410167964013233\n",
      "Operator 159: 1.5170386270493808\n",
      "Operator 161: 1.010130586617235\n",
      "Operator 162: 0.1574413294835149\n",
      "Operator 163: 0.5984901806768103\n",
      "Operator 164: 0.09704718475511698\n",
      "Operator 165: -1.1269089834628803\n",
      "Operator 166: -1.1167657430463533\n",
      "Operator 167: -1.1301162568856846\n",
      "Operator 168: -1.0939600744697855\n",
      "Operator 169: -0.9305382389166048\n",
      "Operator 170: -0.6473357519644383\n",
      "Operator 171: 1.5131865484181433\n",
      "Operator 173: 1.8399511963820547\n",
      "Operator 174: 1.8697829860713766\n",
      "Operator 175: -1.869782986071397\n",
      "Operator 176: 1.4890331478004142\n",
      "Operator 177: -2.094385267584469\n",
      "Operator 178: -1.096010347982861\n",
      "Operator 179: -1.8516457999061786\n",
      "Operator 180: -1.2030612392994233\n",
      "Operator 181: 1.2650222811279406\n",
      "Operator 182: 1.7479234975180051\n",
      "Operator 183: 1.35686683904381\n",
      "Operator 184: 1.8498007660106852\n",
      "Operator 185: 1.1090691066827731\n",
      "Operator 186: 2.0905008009691843\n",
      "Operator 187: -1.484327230246036\n",
      "Operator 188: 1.8688897824613053\n",
      "Operator 189: -1.868889782461302\n",
      "Operator 190: 1.969031254436933\n",
      "Operator 191: -1.8697829860713766\n",
      "Operator 192: 1.8697829860713941\n",
      "Operator 193: -1.6226895188910286\n",
      "Operator 194: -0.9111414544380427\n",
      "Operator 195: -1.9164451297807585\n",
      "Operator 196: -1.3014328720493724\n",
      "Operator 197: -1.3393298145390342\n",
      "Operator 198: 1.3379344950020702\n",
      "Operator 199: 1.4011563483402039\n",
      "Operator 200: 1.775240821912069\n",
      "Operator 201: 1.316984088336382\n",
      "Operator 202: 1.9086566117840653\n",
      "Operator 203: 0.9183438219203054\n",
      "Operator 204: 1.6193427377245473\n",
      "Operator 205: -1.8688897824613089\n",
      "Operator 206: 1.8688897824612987\n",
      "Operator 207: -1.9690312544369344\n",
      "Operator 209: -1.0593403773802357\n",
      "Operator 210: -1.9692304509300593\n",
      "Operator 211: -1.9692304509300593\n",
      "Operator 212: 0.19098969157853585\n",
      "Operator 213: -1.7157742211561788\n",
      "Operator 214: 0.38830064391184277\n",
      "Operator 215: -1.85835392511516\n",
      "Operator 216: -0.3250770220686728\n",
      "Operator 217: -1.3961306065489199\n",
      "Operator 218: -0.4894712787533856\n",
      "Operator 219: 1.2927347496553026\n",
      "Operator 220: 1.382726056899792\n",
      "Operator 221: 1.203713274314492\n",
      "Operator 222: 1.3983746320361896\n",
      "Operator 223: 0.9198071421824751\n",
      "Operator 224: 1.0760157240667927\n",
      "Operator 225: -1.6193427377245477\n",
      "Operator 226: 0.1922691826727187\n",
      "Operator 227: -1.8688897824613022\n",
      "Operator 228: -1.841016796401321\n",
      "Operator 230: -1.51703862704938\n",
      "Operator 231: 0.6449043025677479\n",
      "Operator 232: 0.9269877640629254\n",
      "Operator 233: 1.0788101270560133\n",
      "Operator 234: 1.098254344716639\n",
      "Operator 235: -0.10890679432868275\n",
      "Operator 236: -0.5242865490105704\n",
      "Operator 237: -0.2642870679340901\n",
      "Operator 238: -0.6745094944611133\n",
      "Operator 239: -0.16340846671058518\n",
      "Operator 240: -1.001481826096893\n",
      "Operator 242: -1.5131865484181415\n",
      "Operator 244: -1.8399511963820518\n",
      "Operator 246: 1.2416455527165233\n",
      "Operator 248: 1.7743019517661431\n",
      "Operator 249: -0.5496452869175328\n",
      "Operator 250: 1.5549463961056822\n",
      "Operator 251: -1.0089474571619494\n",
      "Operator 252: 1.4491095711749047\n",
      "Operator 253: -0.6226621576409161\n",
      "Operator 260: 0.42386556405359543\n",
      "Operator 262: 1.812693168139864\n",
      "Operator 263: -0.32833051256090745\n",
      "Operator 264: 1.0815820090648596\n",
      "Operator 265: -0.300004320670626\n",
      "Operator 266: 0.6847936617761454\n",
      "Operator 267: -0.3958955852514293\n",
      "Operator 268: 0.7635123330668253\n",
      "Operator 269: -0.5308516068208856\n",
      "Operator 270: -0.14381082298387837\n",
      "Operator 271: -0.4829996080908482\n",
      "Operator 272: -0.11226451392462244\n",
      "Operator 273: -0.47929710545787707\n",
      "Operator 274: 0.18869102306432553\n",
      "Operator 275: -1.27595934582378\n",
      "Operator 276: 1.1230319992251006\n",
      "Operator 277: -0.62374521815818\n",
      "Operator 278: 1.719332995491504\n",
      "Total gradient norm: 20.450210937709485\n",
      "Operators under consideration (1):\n",
      "[101]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.417987570514051)]\n",
      "Operator(s) added to ansatz: [101]\n",
      "Gradients: [np.float64(-2.417987570514051)]\n",
      "Initial energy: -31.00086004848307\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101]...\n",
      "Starting point: [np.float64(0.6972945324292804), np.float64(0.6033485201211548), np.float64(0.2056232214461154), np.float64(0.24568751944481707), np.float64(0.33352967351067603), np.float64(0.4732244447309137), np.float64(-0.6975790717103413), np.float64(-0.37277784578959733), np.float64(-0.603976615380033), np.float64(0.4717959694709237), np.float64(-0.33341678210155445), np.float64(0.3702819362470557), np.float64(-0.2519713539865211), np.float64(0.23966641976787448), np.float64(0.20435682755890905), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -31.230402\n",
      "         Iterations: 23\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n",
      "\n",
      "Current energy: -31.230402452651262\n",
      "(change of -0.2295424041681926)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101]\n",
      "On iteration 16.\n",
      "\n",
      "*** ADAPT-VQE Iteration 17 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.0630270032541218\n",
      "Operator 2: 1.8864211188606501\n",
      "Operator 3: -0.6252142009729633\n",
      "Operator 4: 0.8409509491560714\n",
      "Operator 5: -0.7351623423814211\n",
      "Operator 6: 0.9340672976445492\n",
      "Operator 8: 0.9962695197046306\n",
      "Operator 9: -0.7474563016740174\n",
      "Operator 10: 0.8982233073327528\n",
      "Operator 11: -0.7822986906543941\n",
      "Operator 12: 0.8732308284475756\n",
      "Operator 13: -1.1222041961924907\n",
      "Operator 14: 1.286451467029824\n",
      "Operator 16: 1.6300263839463969\n",
      "Operator 18: -1.8864211188606537\n",
      "Operator 20: -1.2672934832797225\n",
      "Operator 21: 0.498285076877459\n",
      "Operator 22: -1.0634362990495942\n",
      "Operator 23: 1.3392168943566085\n",
      "Operator 24: -0.8953382041902705\n",
      "Operator 25: 0.7601720529390915\n",
      "Operator 26: -0.9554766375294672\n",
      "Operator 27: 0.65964632163956\n",
      "Operator 28: -1.12372190071947\n",
      "Operator 30: -0.8597552574147933\n",
      "Operator 31: 0.5849947086198791\n",
      "Operator 32: -1.6300263839464002\n",
      "Operator 35: 1.8688335861577008\n",
      "Operator 36: -0.19234966774450085\n",
      "Operator 37: 1.6191317137005214\n",
      "Operator 38: -1.075798914872357\n",
      "Operator 39: -0.9215736227959508\n",
      "Operator 40: -1.3955955066256946\n",
      "Operator 41: -1.2291376682665478\n",
      "Operator 42: 0.5881802586148501\n",
      "Operator 43: 1.156449618863831\n",
      "Operator 44: 0.7324346892937507\n",
      "Operator 45: 1.2052129140716998\n",
      "Operator 46: 0.5909546814792664\n",
      "Operator 47: 1.6440523301309193\n",
      "Operator 48: -0.7564953256922919\n",
      "Operator 49: -0.8868180453184076\n",
      "Operator 50: -0.32170093637364544\n",
      "Operator 51: 1.7839282560111462\n",
      "Operator 52: -1.811383188358127\n",
      "Operator 53: 0.33075000161808576\n",
      "Operator 54: -1.4137201528088932\n",
      "Operator 55: 0.4513864864675455\n",
      "Operator 56: -0.5175114766428854\n",
      "Operator 57: 0.5078508337182509\n",
      "Operator 58: 0.5173530587085403\n",
      "Operator 59: 0.4572313706302106\n",
      "Operator 60: 0.030646912343941922\n",
      "Operator 61: 0.41201502741024565\n",
      "Operator 62: -0.03921038093372465\n",
      "Operator 63: 0.5610089628777304\n",
      "Operator 64: -0.6550526641147302\n",
      "Operator 65: 0.5078374478851023\n",
      "Operator 66: -0.27016828537311477\n",
      "Operator 67: 1.095352015857522\n",
      "Operator 68: -1.244484099201454\n",
      "Operator 70: -1.8113831883581273\n",
      "Operator 71: 2.4181777791862884\n",
      "Operator 72: -1.0733784670267268\n",
      "Operator 73: 1.9757853497083606\n",
      "Operator 74: -0.03220562778581159\n",
      "Operator 75: 1.5920097985593145\n",
      "Operator 76: 0.03909544739980406\n",
      "Operator 77: -0.21524755509867421\n",
      "Operator 78: -0.6576305852794689\n",
      "Operator 79: -0.39344319738370626\n",
      "Operator 80: -0.6500188623465211\n",
      "Operator 81: -0.3777289777248698\n",
      "Operator 82: -0.3257152755732252\n",
      "Operator 83: 1.7218541341767801\n",
      "Operator 84: -0.30491239919252966\n",
      "Operator 85: 2.1198524633455014\n",
      "Operator 86: -1.345293401298731\n",
      "Operator 87: -2.124900032772065\n",
      "Operator 88: 1.2248276955297617\n",
      "Operator 89: -2.216933318473158\n",
      "Operator 90: 1.0294755471519166\n",
      "Operator 91: 0.29036267433987295\n",
      "Operator 92: 0.7318219535874033\n",
      "Operator 93: 0.22895035865525779\n",
      "Operator 94: -0.28038364667651716\n",
      "Operator 95: -1.3436431896215701\n",
      "Operator 96: -0.7745258975748659\n",
      "Operator 97: -1.3758100971407705\n",
      "Operator 98: -0.5359536468492807\n",
      "Operator 99: -1.6224370842287281\n",
      "Operator 100: 0.7554662846888647\n",
      "Operator 102: 1.553052286925927\n",
      "Operator 103: -2.2915708866794615\n",
      "Operator 104: 2.1249000327720706\n",
      "Operator 105: -1.2248276955297617\n",
      "Operator 106: 2.216933318473159\n",
      "Operator 107: -0.21260167421830523\n",
      "Operator 108: 1.6729524495946073\n",
      "Operator 109: 0.5203577440339886\n",
      "Operator 110: 1.4565678007170622\n",
      "Operator 111: 0.00786154911956077\n",
      "Operator 112: -0.3988512224752384\n",
      "Operator 113: -0.6718943726197865\n",
      "Operator 114: -0.3485056841226657\n",
      "Operator 115: -0.8484555974546226\n",
      "Operator 116: -0.13374851051076841\n",
      "Operator 117: -0.6257341363290463\n",
      "Operator 118: 2.016885647430538\n",
      "Operator 119: -1.2815854149701162\n",
      "Operator 120: 2.2915708866794624\n",
      "Operator 122: -1.2473780799409013\n",
      "Operator 124: -1.786258037453484\n",
      "Operator 129: 0.7015989711307863\n",
      "Operator 130: -1.3729405382375879\n",
      "Operator 131: 1.2460508565568285\n",
      "Operator 132: -1.2404314476600082\n",
      "Operator 133: 1.248066920273186\n",
      "Operator 134: -1.0100509940501594\n",
      "Operator 135: 0.7580052047622481\n",
      "Operator 138: 2.1249000327720706\n",
      "Operator 139: -2.1249000327720635\n",
      "Operator 140: 1.811383188358127\n",
      "Operator 141: -2.418177779186287\n",
      "Operator 142: 1.4137201528088932\n",
      "Operator 144: 1.2364359399170013\n",
      "Operator 145: 0.24887210980421032\n",
      "Operator 146: 0.10464187545281387\n",
      "Operator 147: -1.4011183775052507\n",
      "Operator 148: -0.7857684894860963\n",
      "Operator 149: -1.3262456134246015\n",
      "Operator 150: -0.7542130390599234\n",
      "Operator 151: -1.4538344163946095\n",
      "Operator 152: -0.1398105339677156\n",
      "Operator 153: 0.16325044255314397\n",
      "Operator 154: 1.0886490901920538\n",
      "Operator 155: -2.1198524633455005\n",
      "Operator 156: 1.345293401298731\n",
      "Operator 157: 1.8398841647308348\n",
      "Operator 159: 1.5129438634882826\n",
      "Operator 161: 1.0005601059940932\n",
      "Operator 162: 0.1614052876923642\n",
      "Operator 163: 0.5779508196666742\n",
      "Operator 164: 0.11049518884017903\n",
      "Operator 165: -1.1303934939064428\n",
      "Operator 166: -1.1268900607115635\n",
      "Operator 167: -1.1608840933028002\n",
      "Operator 168: -1.1351215899414813\n",
      "Operator 169: -1.0723541164627388\n",
      "Operator 170: 0.2068466735353352\n",
      "Operator 171: 1.0620009374347241\n",
      "Operator 173: 1.4899900501653511\n",
      "Operator 174: 1.8688335861577015\n",
      "Operator 175: -1.868833586157722\n",
      "Operator 176: 1.4843850911189382\n",
      "Operator 177: -2.089572026923286\n",
      "Operator 178: -1.1057660704735262\n",
      "Operator 179: -1.839529637255156\n",
      "Operator 180: -1.2180658067171002\n",
      "Operator 181: 1.2991803113903462\n",
      "Operator 182: 1.6969865583182728\n",
      "Operator 183: 1.4507895793685257\n",
      "Operator 184: 1.7332263363290863\n",
      "Operator 185: 1.3490791497113062\n",
      "Operator 186: 1.5851127385879193\n",
      "Operator 187: -1.577843529099216\n",
      "Operator 188: -0.8469777033564754\n",
      "Operator 189: -1.6704596108556298\n",
      "Operator 190: 1.9284351746575927\n",
      "Operator 191: -1.8688335861577017\n",
      "Operator 192: 1.8688335861577186\n",
      "Operator 193: -1.61913171370052\n",
      "Operator 194: -0.9189346396977116\n",
      "Operator 195: -1.907861816617242\n",
      "Operator 196: -1.3172754948518208\n",
      "Operator 197: -1.2960736756492222\n",
      "Operator 198: 1.2948220698392985\n",
      "Operator 199: 1.448040244358733\n",
      "Operator 200: 1.7046703566442192\n",
      "Operator 201: 1.4377123145799744\n",
      "Operator 202: 1.764311692709287\n",
      "Operator 203: 1.187760837197351\n",
      "Operator 204: -0.9087512021136201\n",
      "Operator 205: -2.123663746507651\n",
      "Operator 206: 1.5452840414300426\n",
      "Operator 207: -1.9284351746575932\n",
      "Operator 209: -1.0630270032541376\n",
      "Operator 210: -1.9690187273588577\n",
      "Operator 211: -1.9690187273588577\n",
      "Operator 212: 0.19234966774450446\n",
      "Operator 213: -1.7133063910747173\n",
      "Operator 214: 0.39054753862652325\n",
      "Operator 215: -1.8488421166682787\n",
      "Operator 216: -0.3354385074384266\n",
      "Operator 217: -1.3716537470679124\n",
      "Operator 218: -0.5157076237267015\n",
      "Operator 219: 1.3218612350500119\n",
      "Operator 220: 1.367151973939929\n",
      "Operator 221: 1.2855949235050788\n",
      "Operator 222: 1.4041696908034496\n",
      "Operator 223: 1.1213592990343122\n",
      "Operator 224: 1.0270659721219686\n",
      "Operator 225: -1.9594312207276028\n",
      "Operator 226: -0.07914111267804746\n",
      "Operator 227: -1.5452840414300462\n",
      "Operator 228: -1.8398841647308322\n",
      "Operator 230: -1.5129438634882821\n",
      "Operator 231: 0.6466748855295115\n",
      "Operator 232: 0.9316196951852422\n",
      "Operator 233: 1.0853830614295337\n",
      "Operator 234: 1.0972707095447811\n",
      "Operator 235: -0.1262493911017041\n",
      "Operator 236: -0.48008541504187796\n",
      "Operator 237: -0.31644932618221994\n",
      "Operator 238: -0.558246162808624\n",
      "Operator 239: -0.2671934365454457\n",
      "Operator 240: -0.7474010355884853\n",
      "Operator 241: -0.2272424938428008\n",
      "Operator 242: 0.9168790358037107\n",
      "Operator 243: 0.1221847047809442\n",
      "Operator 244: -1.4899900501653485\n",
      "Operator 246: 1.247378079940905\n",
      "Operator 248: 1.7862580374534849\n",
      "Operator 249: -0.5531861700885763\n",
      "Operator 250: 1.567861039228005\n",
      "Operator 251: -1.0127282443869485\n",
      "Operator 252: 1.4738362802894225\n",
      "Operator 253: -0.6184251707691504\n",
      "Operator 259: -0.47514129467522165\n",
      "Operator 260: 0.5347008606558726\n",
      "Operator 261: -0.5849947086198791\n",
      "Operator 262: 1.811383188358127\n",
      "Operator 263: -0.33075000161808543\n",
      "Operator 264: 1.073378467026727\n",
      "Operator 265: -0.2995311656213443\n",
      "Operator 266: 0.6624302216610262\n",
      "Operator 267: -0.3867601506906247\n",
      "Operator 268: 0.7431752094669068\n",
      "Operator 269: -0.5249696640980016\n",
      "Operator 270: -0.18691075343269203\n",
      "Operator 271: -0.46142285798321125\n",
      "Operator 272: -0.22185801395757954\n",
      "Operator 273: -0.4588132275760585\n",
      "Operator 274: -0.3992155248150697\n",
      "Operator 275: -0.8290980601569031\n",
      "Operator 276: 0.9645998616293606\n",
      "Operator 277: -1.0953520158575196\n",
      "Operator 278: 1.244484099201454\n",
      "Total gradient norm: 19.31370321538133\n",
      "Operators under consideration (1):\n",
      "[141]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.418177779186287)]\n",
      "Operator(s) added to ansatz: [141]\n",
      "Gradients: [np.float64(-2.418177779186287)]\n",
      "Initial energy: -31.230402452651262\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141]...\n",
      "Starting point: [np.float64(0.6512380548267731), np.float64(0.4942752779757905), np.float64(0.24101881699882494), np.float64(0.2632200845847745), np.float64(0.3270974181160617), np.float64(0.4717060888337941), np.float64(-0.6972766686989892), np.float64(-0.36984365213283815), np.float64(-0.6033090724513334), np.float64(0.37885027364357265), np.float64(-0.31782945182162203), np.float64(0.33036177594978317), np.float64(-0.2592891498720791), np.float64(0.24237160890169462), np.float64(0.20542901378231126), np.float64(0.19479456148224827), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -31.465976\n",
      "         Iterations: 25\n",
      "         Function evaluations: 38\n",
      "         Gradient evaluations: 38\n",
      "\n",
      "Current energy: -31.465976318261912\n",
      "(change of -0.2355738656106503)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141]\n",
      "On iteration 17.\n",
      "\n",
      "*** ADAPT-VQE Iteration 18 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.6475978161900988\n",
      "Operator 1: -0.6068690281023855\n",
      "Operator 2: 0.8199287080078416\n",
      "Operator 3: -0.7515038045640023\n",
      "Operator 4: 0.8748624471817656\n",
      "Operator 5: -0.7746738957050825\n",
      "Operator 6: 0.9383207196907304\n",
      "Operator 8: 0.9555592622039306\n",
      "Operator 9: -0.776333469976196\n",
      "Operator 10: 0.8806895588175434\n",
      "Operator 11: -0.7943581332196956\n",
      "Operator 12: 0.8669841576226079\n",
      "Operator 13: -1.1279170217913657\n",
      "Operator 14: 1.2877870274172953\n",
      "Operator 16: 1.6331136588297246\n",
      "Operator 18: -1.2910457095454553\n",
      "Operator 19: 0.46954753640101365\n",
      "Operator 20: -1.0845182210722726\n",
      "Operator 21: 0.6785789829315294\n",
      "Operator 22: -0.9478826221328516\n",
      "Operator 23: 1.4135691280538212\n",
      "Operator 24: -0.892841051599059\n",
      "Operator 25: 0.7652622259823176\n",
      "Operator 26: -0.9542636321829534\n",
      "Operator 27: 0.6620491919269872\n",
      "Operator 28: -1.123941508897283\n",
      "Operator 30: -0.8586694780033055\n",
      "Operator 31: 0.5872287875865974\n",
      "Operator 32: -1.633113658829728\n",
      "Operator 35: 1.662404992960722\n",
      "Operator 36: -0.9858667668830136\n",
      "Operator 37: -0.878609604537242\n",
      "Operator 38: -1.398605117294251\n",
      "Operator 39: -1.195438327973025\n",
      "Operator 40: -1.402158294538931\n",
      "Operator 41: -1.305506376846942\n",
      "Operator 42: 0.6661607812769996\n",
      "Operator 43: 1.101138386296025\n",
      "Operator 44: 0.7686822565362961\n",
      "Operator 45: 1.1826582671207833\n",
      "Operator 46: 0.6038098275404745\n",
      "Operator 47: 1.6346054977190463\n",
      "Operator 48: -0.7539081592287391\n",
      "Operator 49: -0.8901334474186942\n",
      "Operator 50: -0.3227529291611357\n",
      "Operator 51: 1.7831028108462468\n",
      "Operator 52: -1.5431566325228432\n",
      "Operator 53: 0.34577963096076936\n",
      "Operator 54: -0.606765814490593\n",
      "Operator 55: 0.4963593595383346\n",
      "Operator 56: -0.0036147382970988223\n",
      "Operator 57: 0.48159307086702735\n",
      "Operator 58: 0.7025885503257177\n",
      "Operator 59: 0.4515403058361798\n",
      "Operator 60: 0.09263909512726076\n",
      "Operator 61: 0.40080505280636725\n",
      "Operator 62: -0.015448975245583061\n",
      "Operator 63: 0.5576863061361316\n",
      "Operator 64: -0.6437420858069142\n",
      "Operator 65: 0.5070357923352493\n",
      "Operator 66: -0.2634437381267695\n",
      "Operator 67: 1.0996015249300943\n",
      "Operator 68: -1.241815842796747\n",
      "Operator 70: -1.257956478369238\n",
      "Operator 71: 1.9918268538315513\n",
      "Operator 72: -0.12233261356106369\n",
      "Operator 73: 1.5770951097937624\n",
      "Operator 74: 0.5599595349270623\n",
      "Operator 75: 1.383566278310048\n",
      "Operator 76: 0.26074286610160424\n",
      "Operator 77: -0.25208916964777794\n",
      "Operator 78: -0.5979145122842673\n",
      "Operator 79: -0.41494393553990105\n",
      "Operator 80: -0.6274500135605406\n",
      "Operator 81: -0.38629041347062937\n",
      "Operator 82: -0.31313506007833325\n",
      "Operator 83: 1.7178681743274935\n",
      "Operator 84: -0.29768686875823436\n",
      "Operator 85: 2.120169967241187\n",
      "Operator 86: -1.3428473223450919\n",
      "Operator 87: -2.1151607073055207\n",
      "Operator 88: 1.0761887240222878\n",
      "Operator 89: 0.2938245977789667\n",
      "Operator 90: 0.7466255355200399\n",
      "Operator 91: 0.37764862668695814\n",
      "Operator 92: 0.6198088022842388\n",
      "Operator 93: 0.2570478940123728\n",
      "Operator 94: -0.33372666264280976\n",
      "Operator 95: -1.292689941002179\n",
      "Operator 96: -0.8156810971855128\n",
      "Operator 97: -1.3550338233988235\n",
      "Operator 98: -0.5546941805265504\n",
      "Operator 99: -1.6156818158755732\n",
      "Operator 100: 0.7458249913244521\n",
      "Operator 102: 1.551322393747152\n",
      "Operator 103: -2.2926624587219955\n",
      "Operator 104: 2.1151607073055274\n",
      "Operator 105: -0.268645525380317\n",
      "Operator 106: 1.6841746072734929\n",
      "Operator 107: 0.5047682235748462\n",
      "Operator 108: 1.4283907026566105\n",
      "Operator 109: 0.7884374935774465\n",
      "Operator 110: 1.3271205943189994\n",
      "Operator 111: 0.13133080300125194\n",
      "Operator 112: -0.4146700648338031\n",
      "Operator 113: -0.6549657499508611\n",
      "Operator 114: -0.35436029602553176\n",
      "Operator 115: -0.8398932984470326\n",
      "Operator 116: -0.134864118864867\n",
      "Operator 117: -0.6203030662765084\n",
      "Operator 118: 2.015967518304367\n",
      "Operator 119: -1.2786439371008367\n",
      "Operator 120: 2.292662458721997\n",
      "Operator 122: -1.6799279374175344\n",
      "Operator 129: 0.7178443097907714\n",
      "Operator 130: -1.3479711115208501\n",
      "Operator 131: 1.2708006923010622\n",
      "Operator 132: -1.2313348789177507\n",
      "Operator 133: 1.2594873982803314\n",
      "Operator 134: -1.0089786327565942\n",
      "Operator 135: 0.7635029070481978\n",
      "Operator 138: 2.2978097958155175\n",
      "Operator 139: -2.2978097958155095\n",
      "Operator 140: 1.5431566325228423\n",
      "Operator 142: 1.3111082780379864\n",
      "Operator 143: 0.24282718508618753\n",
      "Operator 144: 0.8357360625630932\n",
      "Operator 145: 0.35841334405975955\n",
      "Operator 146: -0.10189837176483602\n",
      "Operator 147: -1.3084327478777718\n",
      "Operator 148: -0.8728717015613274\n",
      "Operator 149: -1.2882682934044434\n",
      "Operator 150: -0.7880692812992711\n",
      "Operator 151: -1.439342035533879\n",
      "Operator 152: -0.1539857462838033\n",
      "Operator 153: 0.16502576661607593\n",
      "Operator 154: 1.0826907439860867\n",
      "Operator 155: -2.1201699672411856\n",
      "Operator 156: 1.3428473223450936\n",
      "Operator 157: 1.6016714063225719\n",
      "Operator 159: 1.0440537226592668\n",
      "Operator 160: 0.1501650978309705\n",
      "Operator 161: 0.6746423792228328\n",
      "Operator 162: 0.27299741250818554\n",
      "Operator 163: 0.46740804247052753\n",
      "Operator 164: 0.16258204995245004\n",
      "Operator 165: -1.1161537663953218\n",
      "Operator 166: -1.1390713553607466\n",
      "Operator 167: -1.1590764939544733\n",
      "Operator 168: -1.1404842174548173\n",
      "Operator 169: -1.0734816358531605\n",
      "Operator 170: 0.20971174941466697\n",
      "Operator 171: 1.0580178826620654\n",
      "Operator 173: 1.4880474851043435\n",
      "Operator 174: 1.530263178067393\n",
      "Operator 175: -2.1214880000034584\n",
      "Operator 176: -1.0680423834918358\n",
      "Operator 177: -1.8570613618441312\n",
      "Operator 178: -1.3642957955793076\n",
      "Operator 179: -1.7157998550246338\n",
      "Operator 180: -1.3091393728854153\n",
      "Operator 181: 1.3331904817509483\n",
      "Operator 182: 1.6679802123157443\n",
      "Operator 183: 1.4689444518148846\n",
      "Operator 184: 1.7217754811852921\n",
      "Operator 185: 1.357169520626258\n",
      "Operator 186: 1.5785342843086723\n",
      "Operator 187: -1.571239581726049\n",
      "Operator 188: -0.8502159239833582\n",
      "Operator 189: -1.669054051233478\n",
      "Operator 190: 1.9281722397890233\n",
      "Operator 191: -1.6624049929607205\n",
      "Operator 192: -0.8635469844703906\n",
      "Operator 193: -1.926143322468009\n",
      "Operator 194: -1.3125863228878316\n",
      "Operator 195: -1.7604196082503218\n",
      "Operator 196: -1.4447826143364904\n",
      "Operator 197: -1.1833032988937902\n",
      "Operator 198: 1.183455765775061\n",
      "Operator 199: 1.4834584830940527\n",
      "Operator 200: 1.684441917946672\n",
      "Operator 201: 1.4519768972296347\n",
      "Operator 202: 1.7560036566313477\n",
      "Operator 203: 1.1938232317146777\n",
      "Operator 204: -0.9120577585086618\n",
      "Operator 205: -2.1217075283674744\n",
      "Operator 206: 1.5434798348379708\n",
      "Operator 207: -1.9281722397890237\n",
      "Operator 209: -1.6475978161901152\n",
      "Operator 210: -1.9269328630564833\n",
      "Operator 211: -1.7737641666316786\n",
      "Operator 212: 0.2996404853419413\n",
      "Operator 213: -1.8694238293475876\n",
      "Operator 214: -0.29863028935633396\n",
      "Operator 215: -1.3935786949515938\n",
      "Operator 216: -0.624203492745288\n",
      "Operator 217: -1.1728760321152925\n",
      "Operator 218: -0.6417649079636063\n",
      "Operator 219: 1.3319935370469893\n",
      "Operator 220: 1.3761262175977809\n",
      "Operator 221: 1.2898583288579462\n",
      "Operator 222: 1.408031301112131\n",
      "Operator 223: 1.124306128161993\n",
      "Operator 224: 1.0281698043538814\n",
      "Operator 225: -1.9563624017674606\n",
      "Operator 226: -0.0802192495027636\n",
      "Operator 227: -1.5434798348379744\n",
      "Operator 228: -1.6016714063225703\n",
      "Operator 229: 0.6495267249865996\n",
      "Operator 230: 0.8950994942913126\n",
      "Operator 231: 1.0930897613487223\n",
      "Operator 232: 1.1236649490807455\n",
      "Operator 233: 1.1344303841961156\n",
      "Operator 234: 1.1099461922820195\n",
      "Operator 235: -0.1673643165068796\n",
      "Operator 236: -0.4380048794140494\n",
      "Operator 237: -0.33171471303302036\n",
      "Operator 238: -0.5432234521505304\n",
      "Operator 239: -0.2727968335634337\n",
      "Operator 240: -0.7398270053898587\n",
      "Operator 241: -0.22964642247687544\n",
      "Operator 242: 0.9195083108776017\n",
      "Operator 243: 0.12282135609927022\n",
      "Operator 244: -1.4880474851043413\n",
      "Operator 246: 1.6799279374175398\n",
      "Operator 247: -0.5347640064076519\n",
      "Operator 248: 1.5790566732126283\n",
      "Operator 249: -1.0018565947514153\n",
      "Operator 250: 1.466402203117205\n",
      "Operator 251: -1.17059879460064\n",
      "Operator 252: 1.4065166318847695\n",
      "Operator 253: -0.6912960591475521\n",
      "Operator 259: -0.47669179233163955\n",
      "Operator 260: 0.535420453876583\n",
      "Operator 261: -0.5872287875865974\n",
      "Operator 262: 1.2579564783692385\n",
      "Operator 263: -0.21960078095833696\n",
      "Operator 264: 0.7375157992706364\n",
      "Operator 265: -0.34070687970258673\n",
      "Operator 266: 0.14571537990731623\n",
      "Operator 267: -0.3915190691509579\n",
      "Operator 268: 0.5714436971741642\n",
      "Operator 269: -0.49442200242448764\n",
      "Operator 270: -0.2509745326054603\n",
      "Operator 271: -0.44380317777067035\n",
      "Operator 272: -0.24647934598106055\n",
      "Operator 273: -0.4519135930870586\n",
      "Operator 274: -0.4107616877551642\n",
      "Operator 275: -0.8311836314537107\n",
      "Operator 276: 0.9581480635029098\n",
      "Operator 277: -1.0996015249300917\n",
      "Operator 278: 1.241815842796747\n",
      "Total gradient norm: 18.278479963656725\n",
      "Operators under consideration (1):\n",
      "[138]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(2.2978097958155175)]\n",
      "Operator(s) added to ansatz: [138]\n",
      "Gradients: [np.float64(2.2978097958155175)]\n",
      "Initial energy: -31.465976318261912\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138]...\n",
      "Starting point: [np.float64(0.65099033116598), np.float64(0.49363689110783787), np.float64(0.24218413092279914), np.float64(0.26620274866120885), np.float64(0.3109384268319462), np.float64(0.3754320073192357), np.float64(-0.6498286024688086), np.float64(-0.32772966351583976), np.float64(-0.49063354065374015), np.float64(0.37762840650614776), np.float64(-0.3119295417700328), np.float64(0.3278974777452837), np.float64(-0.2663973223193719), np.float64(0.2607284045898654), np.float64(0.24621651548577064), np.float64(0.19519168165195552), np.float64(0.20070415501409966), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -31.662095\n",
      "         Iterations: 27\n",
      "         Function evaluations: 39\n",
      "         Gradient evaluations: 39\n",
      "\n",
      "Current energy: -31.662095452611567\n",
      "(change of -0.19611913434965444)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138]\n",
      "On iteration 18.\n",
      "\n",
      "*** ADAPT-VQE Iteration 19 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.3060889412629655\n",
      "Operator 1: -0.9974463164790728\n",
      "Operator 2: 0.8581565195911314\n",
      "Operator 3: -0.7866260661411875\n",
      "Operator 4: 0.8747195525475315\n",
      "Operator 5: -0.7850697158277649\n",
      "Operator 6: 0.938096228660199\n",
      "Operator 8: 0.9429184410571397\n",
      "Operator 9: -0.7853036072027922\n",
      "Operator 10: 0.8752936406988829\n",
      "Operator 11: -0.7980579590947923\n",
      "Operator 12: 0.8650730271283662\n",
      "Operator 13: -1.1296542342451537\n",
      "Operator 14: 1.2881888055429949\n",
      "Operator 16: 1.6340465696664312\n",
      "Operator 18: -1.1953218058377812\n",
      "Operator 19: 0.6027056031878704\n",
      "Operator 20: -0.9946161523071062\n",
      "Operator 21: 0.740970400570212\n",
      "Operator 22: -0.9080240763742371\n",
      "Operator 23: 1.4368821988555731\n",
      "Operator 24: -0.8918573750094453\n",
      "Operator 25: 0.7667584961221903\n",
      "Operator 26: -0.9538578872004647\n",
      "Operator 27: 0.6627707223115509\n",
      "Operator 28: -1.1240006951005626\n",
      "Operator 30: -0.858337327018019\n",
      "Operator 31: 0.5879061188778413\n",
      "Operator 32: -1.6340465696664346\n",
      "Operator 35: 2.0382818960104316\n",
      "Operator 36: -1.015328793489763\n",
      "Operator 37: -1.058187274384225\n",
      "Operator 38: -1.417148264421391\n",
      "Operator 39: -1.2648092294658566\n",
      "Operator 40: -1.3876450185222609\n",
      "Operator 41: -1.3263450492487285\n",
      "Operator 42: 0.6900129114716856\n",
      "Operator 43: 1.0833388832416786\n",
      "Operator 44: 0.7797629712468739\n",
      "Operator 45: 1.1756491351071023\n",
      "Operator 46: 0.6077255531772452\n",
      "Operator 47: 1.631696688789007\n",
      "Operator 48: -0.7531089480030106\n",
      "Operator 49: -0.8911366300430954\n",
      "Operator 50: -0.32307073789014956\n",
      "Operator 51: 1.7828522576813608\n",
      "Operator 52: -1.6200088782812303\n",
      "Operator 53: 0.44667726962609455\n",
      "Operator 54: -0.1921398593056176\n",
      "Operator 55: 0.5009749211002961\n",
      "Operator 56: 0.15204640677205808\n",
      "Operator 57: 0.4521838771357975\n",
      "Operator 58: 0.751689200124684\n",
      "Operator 59: 0.4462482071163727\n",
      "Operator 60: 0.11036042432408194\n",
      "Operator 61: 0.3967155091657579\n",
      "Operator 62: -0.008409360896714162\n",
      "Operator 63: 0.5565548351058172\n",
      "Operator 64: -0.640319530100234\n",
      "Operator 65: 0.5067764753750813\n",
      "Operator 66: -0.2614098444661285\n",
      "Operator 67: 1.1008863311157662\n",
      "Operator 68: -1.2410083024145155\n",
      "Operator 70: -1.1475927422955996\n",
      "Operator 71: 1.7263667684569022\n",
      "Operator 72: 0.3514593684487193\n",
      "Operator 73: 1.4211201265456597\n",
      "Operator 74: 0.7550239051536545\n",
      "Operator 75: 1.3048492494087047\n",
      "Operator 76: 0.328999144663906\n",
      "Operator 77: -0.26511504960814225\n",
      "Operator 78: -0.5802121287176802\n",
      "Operator 79: -0.42186590975070437\n",
      "Operator 80: -0.620694660043899\n",
      "Operator 81: -0.3889569867049832\n",
      "Operator 82: -0.30933745717419286\n",
      "Operator 83: 1.71664693699394\n",
      "Operator 84: -0.29549955776942666\n",
      "Operator 85: 2.1202648614237827\n",
      "Operator 86: -1.342107310971997\n",
      "Operator 87: -1.7967652732591595\n",
      "Operator 88: 0.5148045334076388\n",
      "Operator 89: 0.358959520332825\n",
      "Operator 90: 0.6546735640616266\n",
      "Operator 91: 0.41028770864569747\n",
      "Operator 92: 0.5863995613400055\n",
      "Operator 93: 0.26663627233023957\n",
      "Operator 94: -0.34993206257270104\n",
      "Operator 95: -1.2767492034103183\n",
      "Operator 96: -0.8282832811842469\n",
      "Operator 97: -1.3486324672069356\n",
      "Operator 98: -0.5604032115615514\n",
      "Operator 99: -1.6136169298999377\n",
      "Operator 100: 0.7428944695622945\n",
      "Operator 102: 1.5507989247475138\n",
      "Operator 103: -2.2929926948735293\n",
      "Operator 104: -0.13404052365669136\n",
      "Operator 105: 0.033569181219927816\n",
      "Operator 106: 1.5268640151023414\n",
      "Operator 107: 0.714175002719561\n",
      "Operator 108: 1.3200794388820558\n",
      "Operator 109: 0.8742490597076942\n",
      "Operator 110: 1.2827099019889736\n",
      "Operator 111: 0.17081647948232764\n",
      "Operator 112: -0.4196981497035833\n",
      "Operator 113: -0.6498783586837135\n",
      "Operator 114: -0.3561733086914095\n",
      "Operator 115: -0.837306915000964\n",
      "Operator 116: -0.1352051937303101\n",
      "Operator 117: -0.6186590915444082\n",
      "Operator 118: 2.0156873734827223\n",
      "Operator 119: -1.2777527485980023\n",
      "Operator 120: 2.29299269487353\n",
      "Operator 121: 0.44205180059944704\n",
      "Operator 122: -1.5888131789118614\n",
      "Operator 123: 0.31099191550240457\n",
      "Operator 129: 0.7229179750877829\n",
      "Operator 130: -1.3400619349173253\n",
      "Operator 131: 1.278410125309729\n",
      "Operator 132: -1.2285005132927755\n",
      "Operator 133: 1.2629762600694372\n",
      "Operator 134: -1.0086408004735776\n",
      "Operator 135: 0.7651791793098297\n",
      "Operator 139: -2.046825711172751\n",
      "Operator 140: 0.9824124906994318\n",
      "Operator 141: 0.10348596957600348\n",
      "Operator 142: 0.9855175959268319\n",
      "Operator 143: 0.3187655410575347\n",
      "Operator 144: 0.7012033736698859\n",
      "Operator 145: 0.40198483702654675\n",
      "Operator 146: -0.16295527579552777\n",
      "Operator 147: -1.2772402736569521\n",
      "Operator 148: -0.8990214013444162\n",
      "Operator 149: -1.2762593730957217\n",
      "Operator 150: -0.7982843574038332\n",
      "Operator 151: -1.4348574013335096\n",
      "Operator 152: -0.15828027400708095\n",
      "Operator 153: 0.16557026584664228\n",
      "Operator 154: 1.0808883476890405\n",
      "Operator 155: -2.120264861423782\n",
      "Operator 156: 1.342107310971997\n",
      "Operator 157: -0.7339172715834013\n",
      "Operator 158: 0.16643165613619748\n",
      "Operator 159: 0.8321182758353822\n",
      "Operator 160: 0.23202165516704482\n",
      "Operator 161: 0.5768368271102265\n",
      "Operator 162: 0.31846200551719805\n",
      "Operator 163: 0.43358772194647766\n",
      "Operator 164: 0.18031252225956526\n",
      "Operator 165: -1.1105317092291873\n",
      "Operator 166: -1.1426808741327812\n",
      "Operator 167: -1.158397347985638\n",
      "Operator 168: -1.1421074275519598\n",
      "Operator 169: -1.073797036102801\n",
      "Operator 170: 0.21058597199478418\n",
      "Operator 171: 1.0568118022214184\n",
      "Operator 173: 1.48745905867305\n",
      "Operator 174: 1.7233375800244692\n",
      "Operator 175: -1.7321990866433445\n",
      "Operator 176: -1.2748476413766976\n",
      "Operator 177: -1.7602130255794113\n",
      "Operator 178: -1.4444465429574669\n",
      "Operator 179: -1.6732743708406344\n",
      "Operator 180: -1.336318809261091\n",
      "Operator 181: 1.3431880462962023\n",
      "Operator 182: 1.6590317485230788\n",
      "Operator 183: 1.4744974427691635\n",
      "Operator 184: 1.7182864951471526\n",
      "Operator 185: 1.3596322817419848\n",
      "Operator 186: 1.5765358102583504\n",
      "Operator 187: -1.5692274260107943\n",
      "Operator 188: -0.8511954487995051\n",
      "Operator 189: -1.668628516204194\n",
      "Operator 190: 1.9280927007189823\n",
      "Operator 191: 0.7104660268684746\n",
      "Operator 192: -1.0993861518713222\n",
      "Operator 193: -1.811727915497285\n",
      "Operator 194: -1.416792388696952\n",
      "Operator 195: -1.7011203682099587\n",
      "Operator 196: -1.4830087544798536\n",
      "Operator 197: -1.146621782718148\n",
      "Operator 198: 1.1467852496015754\n",
      "Operator 199: 1.494198750282266\n",
      "Operator 200: 1.6782001663650747\n",
      "Operator 201: 1.4563031253609082\n",
      "Operator 202: 1.75346814813724\n",
      "Operator 203: 1.1956570738763483\n",
      "Operator 204: -0.9130573436511666\n",
      "Operator 205: -2.12111482484255\n",
      "Operator 206: 1.5429331360659158\n",
      "Operator 207: -1.9280927007189825\n",
      "Operator 208: 0.49301427579058865\n",
      "Operator 209: -0.7724633653699855\n",
      "Operator 210: 0.6116282327861452\n",
      "Operator 211: 0.669794372814876\n",
      "Operator 212: 0.6749685844167386\n",
      "Operator 213: -1.7115129647030645\n",
      "Operator 214: -0.5027403367573631\n",
      "Operator 215: -1.2410163395559435\n",
      "Operator 216: -0.7362198139265875\n",
      "Operator 217: -1.1047282084302845\n",
      "Operator 218: -0.6827710270100964\n",
      "Operator 219: 1.3337981240386791\n",
      "Operator 220: 1.3785068306592154\n",
      "Operator 221: 1.2909721617643604\n",
      "Operator 222: 1.4091524985476862\n",
      "Operator 223: 1.1251692378306895\n",
      "Operator 224: 1.02849635184404\n",
      "Operator 225: -1.955432358742511\n",
      "Operator 226: -0.08054675295155674\n",
      "Operator 227: -1.542933136065919\n",
      "Operator 228: -1.2528053561093295\n",
      "Operator 229: -0.11479749794313598\n",
      "Operator 230: 1.033532791316898\n",
      "Operator 231: 1.1329197676599188\n",
      "Operator 232: 1.1510691164508768\n",
      "Operator 233: 1.1420498033079096\n",
      "Operator 234: 1.109033989221459\n",
      "Operator 235: -0.18166299987793738\n",
      "Operator 236: -0.42513259236025225\n",
      "Operator 237: -0.33654368641058074\n",
      "Operator 238: -0.5386916217314294\n",
      "Operator 239: -0.2745280751657646\n",
      "Operator 240: -0.737534290244781\n",
      "Operator 241: -0.230373927815324\n",
      "Operator 242: 0.920301683440922\n",
      "Operator 243: 0.12301434316010443\n",
      "Operator 244: -1.4874590586730478\n",
      "Operator 247: -0.6233194568363046\n",
      "Operator 248: 1.5318698694532416\n",
      "Operator 249: -1.1220135161419875\n",
      "Operator 250: 1.4092153350824201\n",
      "Operator 251: -1.2220222922856177\n",
      "Operator 252: 1.3820534746954487\n",
      "Operator 253: -0.7151175644679446\n",
      "Operator 259: -0.4771616976673614\n",
      "Operator 260: 0.5356369552412558\n",
      "Operator 261: -0.5879061188778413\n",
      "Operator 262: 0.5942761612966608\n",
      "Operator 263: -0.30794526936487354\n",
      "Operator 264: 0.30237917172546847\n",
      "Operator 265: -0.35648720873944484\n",
      "Operator 266: -0.005188428782048338\n",
      "Operator 267: -0.37581872843373765\n",
      "Operator 268: 0.5207375706862973\n",
      "Operator 269: -0.4807623971693881\n",
      "Operator 270: -0.2694705523085491\n",
      "Operator 271: -0.43776793867992053\n",
      "Operator 272: -0.25379337567594945\n",
      "Operator 273: -0.4497067194943285\n",
      "Operator 274: -0.41422491810951495\n",
      "Operator 275: -0.8317988273900034\n",
      "Operator 276: 0.956193833207408\n",
      "Operator 277: -1.1008863311157637\n",
      "Operator 278: 1.2410083024145155\n",
      "Total gradient norm: 17.298157659721976\n",
      "Operators under consideration (1):\n",
      "[103]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.2929926948735293)]\n",
      "Operator(s) added to ansatz: [103]\n",
      "Gradients: [np.float64(-2.2929926948735293)]\n",
      "Initial energy: -31.662095452611567\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138, 103]...\n",
      "Starting point: [np.float64(0.6509154812859795), np.float64(0.49344386200842455), np.float64(0.24254068867744144), np.float64(0.2671267730322636), np.float64(0.30603359883485737), np.float64(0.3410640981669897), np.float64(-0.5664185354229758), np.float64(-0.3146703121042889), np.float64(-0.4135905905329286), np.float64(0.37725852916682606), np.float64(-0.3101480651304762), np.float64(0.3271515848920493), np.float64(-0.2686560880664891), np.float64(0.26695826879804807), np.float64(0.2609972629515624), np.float64(0.19531226307228453), np.float64(0.2293477868399502), np.float64(-0.1744410114462682), np.float64(0.0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2551: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n",
      "  opt_result = minimize_bfgs(\n",
      "/Users/benjamindalfavero/Documents/phd/wellcome/ceo-adapt-vqe/adaptvqe/algorithms/adapt_vqe.py:2566: UserWarning: Optimizer did not succeed. Message: Desired error not necessarily achieved due to precision loss.\n",
      "  warn(f\"Optimizer did not succeed. Message: {opt_result.message}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Current function value: -31.861038\n",
      "         Iterations: 30\n",
      "         Function evaluations: 154\n",
      "         Gradient evaluations: 140\n",
      "\n",
      "Current energy: -31.861037679334686\n",
      "(change of -0.19894222672311912)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138, 103]\n",
      "On iteration 19.\n",
      "\n",
      "*** ADAPT-VQE Iteration 20 ***\n",
      "\n",
      "Creating list of up to 1 operators ordered by gradient magnitude...\n",
      "\n",
      "Non-Zero Gradients (tolerance E-8):\n",
      "Operator 0: 1.3064156570674847\n",
      "Operator 1: -0.9982026636883113\n",
      "Operator 2: 0.8574832740019048\n",
      "Operator 3: -0.7881003182258501\n",
      "Operator 4: 0.8726810866670709\n",
      "Operator 5: -0.7885800176991676\n",
      "Operator 6: 0.9332864147929559\n",
      "Operator 8: 0.9425420383216065\n",
      "Operator 9: -0.7888256365293194\n",
      "Operator 10: 0.872942569148742\n",
      "Operator 11: -0.8092968508426541\n",
      "Operator 12: 0.865222641666197\n",
      "Operator 13: -1.2815506986493967\n",
      "Operator 14: 1.1670608512027891\n",
      "Operator 15: -0.35216879806901596\n",
      "Operator 16: 1.305914479141948\n",
      "Operator 18: -1.1954037566271167\n",
      "Operator 19: 0.6030463208221302\n",
      "Operator 20: -0.9944620579850456\n",
      "Operator 21: 0.7415995556777197\n",
      "Operator 22: -0.9076117020861943\n",
      "Operator 23: 1.4460357547853402\n",
      "Operator 24: -0.8760142347676831\n",
      "Operator 25: 0.7923769021339269\n",
      "Operator 26: -0.9132965261706929\n",
      "Operator 27: 0.7242849774499538\n",
      "Operator 28: -1.0413696179021064\n",
      "Operator 30: -0.9116501148411431\n",
      "Operator 31: 0.6936022864286531\n",
      "Operator 32: -0.745292204487284\n",
      "Operator 33: 0.5025590397424864\n",
      "Operator 34: 1.0753777340588687e-08\n",
      "Operator 35: 2.037873996207395\n",
      "Operator 36: -1.0155979551686234\n",
      "Operator 37: -1.0586697697490484\n",
      "Operator 38: -1.4176943842772194\n",
      "Operator 39: -1.2653535055984362\n",
      "Operator 40: -1.3885844865836563\n",
      "Operator 41: -1.3269206143504373\n",
      "Operator 42: 0.7059300568797332\n",
      "Operator 43: 1.0554028036987086\n",
      "Operator 44: 0.8260794243202599\n",
      "Operator 45: 1.106829521397263\n",
      "Operator 46: 0.7188730009049564\n",
      "Operator 47: 1.5481087678598586\n",
      "Operator 48: -0.79434216463938\n",
      "Operator 49: -1.0881552822970524\n",
      "Operator 50: -1.0292226948563405\n",
      "Operator 51: -0.7622170063350396\n",
      "Operator 52: -1.6193380271518432\n",
      "Operator 53: 0.44681688812804266\n",
      "Operator 54: -0.19088448946072345\n",
      "Operator 55: 0.50068329619511\n",
      "Operator 56: 0.15413849263830628\n",
      "Operator 57: 0.45084504094378386\n",
      "Operator 58: 0.7621987349106718\n",
      "Operator 59: 0.43124918646881394\n",
      "Operator 60: 0.14226013931231402\n",
      "Operator 61: 0.35212791216279893\n",
      "Operator 62: 0.07067331537065663\n",
      "Operator 63: 0.4599486008100656\n",
      "Operator 64: -0.5537417122366014\n",
      "Operator 65: 0.5096275500336449\n",
      "Operator 66: 0.010304456112471494\n",
      "Operator 67: 0.44999657093507717\n",
      "Operator 68: -0.5229322504640379\n",
      "Operator 70: -1.1463804804293254\n",
      "Operator 71: 1.7256384658348076\n",
      "Operator 72: 0.35392366258046837\n",
      "Operator 73: 1.4186919357530117\n",
      "Operator 74: 0.7601232452982735\n",
      "Operator 75: 1.298748978583815\n",
      "Operator 76: 0.33535805763642323\n",
      "Operator 77: -0.268975862701338\n",
      "Operator 78: -0.5677101342531027\n",
      "Operator 79: -0.4362439963183072\n",
      "Operator 80: -0.5867293740274415\n",
      "Operator 81: -0.42247836822048723\n",
      "Operator 82: -0.07024852459749341\n",
      "Operator 83: 1.5524668353858073\n",
      "Operator 84: 0.2812638884553959\n",
      "Operator 85: 1.789819982347126\n",
      "Operator 86: -0.5816943428784054\n",
      "Operator 87: -1.7964002088742792\n",
      "Operator 88: 0.5131469734394198\n",
      "Operator 89: 0.3599600448305432\n",
      "Operator 90: 0.6519206214429678\n",
      "Operator 91: 0.41296876880563826\n",
      "Operator 92: 0.579597261112766\n",
      "Operator 93: 0.2720102334400115\n",
      "Operator 94: -0.3759022947573146\n",
      "Operator 95: -1.2424444137386224\n",
      "Operator 96: -0.9011456917726137\n",
      "Operator 97: -1.2687778547469462\n",
      "Operator 98: -0.7623393232706668\n",
      "Operator 99: -1.4576245920964264\n",
      "Operator 100: 0.3086510322534102\n",
      "Operator 101: 0.18450970846683248\n",
      "Operator 102: 1.6271668383884381\n",
      "Operator 103: 1.3080299954282282e-08\n",
      "Operator 104: -0.13422921110715497\n",
      "Operator 105: 0.03544075519165142\n",
      "Operator 106: 1.525204832884365\n",
      "Operator 107: 0.7183147447295043\n",
      "Operator 108: 1.315444183619974\n",
      "Operator 109: 0.8843583551460314\n",
      "Operator 110: 1.2701526257224853\n",
      "Operator 111: 0.19333729190124146\n",
      "Operator 112: -0.4397499081682048\n",
      "Operator 113: -0.6010186148028801\n",
      "Operator 114: -0.39910763570476543\n",
      "Operator 115: -0.6961487713193648\n",
      "Operator 116: -0.18859230435655408\n",
      "Operator 117: -0.1874627557703417\n",
      "Operator 118: 1.734934629952133\n",
      "Operator 119: -0.5921714614022922\n",
      "Operator 120: 2.0337011196678523\n",
      "Operator 121: 0.4422490102692668\n",
      "Operator 122: -1.5896025724200018\n",
      "Operator 123: 0.3110320712985443\n",
      "Operator 129: 0.7270187468916514\n",
      "Operator 130: -1.3558176873027128\n",
      "Operator 131: 1.2805817225517726\n",
      "Operator 132: -1.2796354043101363\n",
      "Operator 133: 1.2848168585031488\n",
      "Operator 134: -1.1850726183000102\n",
      "Operator 135: 0.7611717911303161\n",
      "Operator 139: -2.0467987114298385\n",
      "Operator 140: 0.981691227957832\n",
      "Operator 141: 0.10359835961382118\n",
      "Operator 142: 0.9843075817199025\n",
      "Operator 143: 0.31943760438351576\n",
      "Operator 144: 0.6990614106555951\n",
      "Operator 145: 0.40391075390414144\n",
      "Operator 146: -0.1786407815245601\n",
      "Operator 147: -1.2595128626605947\n",
      "Operator 148: -0.9328572610846491\n",
      "Operator 149: -1.2279491559573175\n",
      "Operator 150: -0.8840615138542602\n",
      "Operator 151: -1.3299431025662876\n",
      "Operator 152: -0.27866795097873265\n",
      "Operator 153: 0.21165735085781318\n",
      "Operator 154: 0.8423128959962646\n",
      "Operator 155: 0.23206623983444463\n",
      "Operator 156: 1.325196109861519\n",
      "Operator 157: -0.7343530867591386\n",
      "Operator 158: 0.16671132941585434\n",
      "Operator 159: 0.8310951997876711\n",
      "Operator 160: 0.23265178698925226\n",
      "Operator 161: 0.5749602655691111\n",
      "Operator 162: 0.32030474362230704\n",
      "Operator 163: 0.4284961788986502\n",
      "Operator 164: 0.18602860506697044\n",
      "Operator 165: -1.1094864842877161\n",
      "Operator 166: -1.1442608997072035\n",
      "Operator 167: -1.1632714745191652\n",
      "Operator 168: -1.148296079454366\n",
      "Operator 169: -1.099736379794357\n",
      "Operator 170: 0.3154175089505645\n",
      "Operator 171: 0.8353087700660877\n",
      "Operator 172: 0.16005701376049747\n",
      "Operator 173: 1.1239587374842743\n",
      "Operator 174: 1.7225493998754922\n",
      "Operator 175: -1.7313990299460276\n",
      "Operator 176: -1.2759248214072254\n",
      "Operator 177: -1.7588130606752723\n",
      "Operator 178: -1.446679474593018\n",
      "Operator 179: -1.6697956639219875\n",
      "Operator 180: -1.3401470838048894\n",
      "Operator 181: 1.3534376655938032\n",
      "Operator 182: 1.6417635580328143\n",
      "Operator 183: 1.504926944572719\n",
      "Operator 184: 1.6750285557018878\n",
      "Operator 185: 1.4409766950956857\n",
      "Operator 186: 1.3777118956612333\n",
      "Operator 187: -1.3823665115987231\n",
      "Operator 188: -1.187930018424356\n",
      "Operator 189: -2.030678341978969\n",
      "Operator 190: -0.6142756996958915\n",
      "Operator 191: 0.7109369716799647\n",
      "Operator 192: -1.10021885450011\n",
      "Operator 193: -1.8107009362221738\n",
      "Operator 194: -1.4185502881755752\n",
      "Operator 195: -1.698668420282277\n",
      "Operator 196: -1.4871837066693523\n",
      "Operator 197: -1.131980677691898\n",
      "Operator 198: 1.1324289783173178\n",
      "Operator 199: 1.5090454429463303\n",
      "Operator 200: 1.653630351668311\n",
      "Operator 201: 1.4954557118729555\n",
      "Operator 202: 1.6922187626791299\n",
      "Operator 203: 1.2823076774423894\n",
      "Operator 204: -1.1225754319578294\n",
      "Operator 205: -1.947431400607065\n",
      "Operator 206: -0.8465886707652623\n",
      "Operator 207: -2.239629739098481\n",
      "Operator 208: 0.49330355703620077\n",
      "Operator 209: -0.7724433372213284\n",
      "Operator 210: 0.6120591313843796\n",
      "Operator 211: 0.6702645483634828\n",
      "Operator 212: 0.6747551179901324\n",
      "Operator 213: -1.7103077112753842\n",
      "Operator 214: -0.5042123831491954\n",
      "Operator 215: -1.2381748441935696\n",
      "Operator 216: -0.7404356826247046\n",
      "Operator 217: -1.0975595746406102\n",
      "Operator 218: -0.6917093376334716\n",
      "Operator 219: 1.3412502210568737\n",
      "Operator 220: 1.369664687158546\n",
      "Operator 221: 1.3154028327172294\n",
      "Operator 222: 1.3932584093599207\n",
      "Operator 223: 1.1829355692090815\n",
      "Operator 224: 0.9662873054329251\n",
      "Operator 225: -1.7467902990482165\n",
      "Operator 226: -0.2664554881570273\n",
      "Operator 227: -1.7587671904084332\n",
      "Operator 228: -1.252265373284385\n",
      "Operator 229: -0.11511255269443599\n",
      "Operator 230: 1.0338156213625027\n",
      "Operator 231: 1.1336234348730139\n",
      "Operator 232: 1.1509169417797203\n",
      "Operator 233: 1.143489066528808\n",
      "Operator 234: 1.1067553732392141\n",
      "Operator 235: -0.1886164829835062\n",
      "Operator 236: -0.41207930582130514\n",
      "Operator 237: -0.35628404865215796\n",
      "Operator 238: -0.5006547490165851\n",
      "Operator 239: -0.3214248859259848\n",
      "Operator 240: -0.6447379854695863\n",
      "Operator 241: -0.3176091338171327\n",
      "Operator 242: 1.0638059477295214\n",
      "Operator 243: 0.8655919754416533\n",
      "Operator 244: 0.849666503666542\n",
      "Operator 247: -0.6236591528568486\n",
      "Operator 248: 1.5329490370224879\n",
      "Operator 249: -1.1222284015573243\n",
      "Operator 250: 1.4113411832260039\n",
      "Operator 251: -1.2214104575505944\n",
      "Operator 252: 1.3879616665328074\n",
      "Operator 253: -0.7118489785845048\n",
      "Operator 259: -0.5937403911616268\n",
      "Operator 260: 0.813077938589572\n",
      "Operator 261: -0.9982298790604323\n",
      "Operator 262: 0.5931332356874301\n",
      "Operator 263: -0.3077256229112993\n",
      "Operator 264: 0.29992550378197463\n",
      "Operator 265: -0.3547616403646954\n",
      "Operator 266: -0.010128623944999506\n",
      "Operator 267: -0.3706119167741862\n",
      "Operator 268: 0.5162588497622174\n",
      "Operator 269: -0.477276853738102\n",
      "Operator 270: -0.2808757647103953\n",
      "Operator 271: -0.4270859687067408\n",
      "Operator 272: -0.28568981570164537\n",
      "Operator 273: -0.42887552148243197\n",
      "Operator 274: -0.595200882157485\n",
      "Operator 275: -0.7068545754876032\n",
      "Operator 276: 0.4495446835938925\n",
      "Operator 277: -0.8346505683926178\n",
      "Operator 278: 1.1913263254494417\n",
      "Total gradient norm: 16.43383221521675\n",
      "Operators under consideration (1):\n",
      "[207]\n",
      "Corresponding gradients (ordered by magnitude):\n",
      "[np.float64(-2.239629739098481)]\n",
      "Operator(s) added to ansatz: [207]\n",
      "Gradients: [np.float64(-2.239629739098481)]\n",
      "Initial energy: -31.861037679334686\n",
      "Optimizing energy with indices [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138, 103, 207]...\n",
      "Starting point: [np.float64(0.5655622348219762), np.float64(0.41428165471662287), np.float64(0.2569364534612334), np.float64(0.2736232206184742), np.float64(0.30415488916875), np.float64(0.3407499835128578), np.float64(-0.5663313086369663), np.float64(-0.31396925238334017), np.float64(-0.4134190380337325), np.float64(0.3410395569253617), np.float64(-0.3052294933449982), np.float64(0.31380038082592787), np.float64(-0.27119788359274044), np.float64(0.26784301314122455), np.float64(0.26135802924757023), np.float64(0.2266792991632478), np.float64(0.22949005782327567), np.float64(-0.17448735667114595), np.float64(0.17784225156860767), np.float64(0.0)]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -32.040337\n",
      "         Iterations: 29\n",
      "         Function evaluations: 44\n",
      "         Gradient evaluations: 44\n",
      "\n",
      "Current energy: -32.04033655275912\n",
      "(change of -0.17929887342443607)\n",
      "Current ansatz: [278, 276, 273, 271, 268, 264, 208, 127, 123, 259, 113, 257, 77, 145, 143, 101, 141, 138, 103, 207]\n"
     ]
    }
   ],
   "source": [
    "# Now go to the larger size.\n",
    "new_l = 5 * l\n",
    "print(f\"new_l = {new_l}\")\n",
    "j_xy = 1\n",
    "j_z = 1\n",
    "h = XXZHamiltonian(\n",
    "    j_xy, j_z, new_l,\n",
    "    store_ref_vector=False,\n",
    "    diag_mode=\"quimb\", max_mpo_bond=max_mpo_bond, max_mps_bond=dmrg_mps_bond\n",
    ")\n",
    "dmrg_energy = h.ground_energy\n",
    "exact_energy = h.ground_energy\n",
    "print(f\"Got DMRG energy {dmrg_energy:4.5e}\")\n",
    "\n",
    "h_of = h.operator\n",
    "h_cirq = of.transforms.qubit_operator_to_pauli_sum(h_of)\n",
    "h_qiskit = cirq_pauli_sum_to_qiskit_pauli_op(h_cirq)\n",
    "\n",
    "tiled_pool = TiledPauliPool(n=new_l, source_ops=source_ops)\n",
    "num_ops = len(tiled_pool.operators)\n",
    "print(f\"Tiled pool has {num_ops} operators.\")\n",
    "\n",
    "tn_adapt = TensorNetAdapt(\n",
    "    pool=tiled_pool,\n",
    "    custom_hamiltonian=h,\n",
    "    verbose=True,\n",
    "    threshold=10**-5,\n",
    "    max_adapt_iter=30,\n",
    "    max_opt_iter=10000,\n",
    "    sel_criterion=\"gradient\",\n",
    "    recycle_hessian=False,\n",
    "    rand_degenerate=True,\n",
    "    max_mpo_bond=max_mpo_bond,\n",
    "    max_mps_bond=adapt_mps_bond\n",
    ")\n",
    "tn_adapt.initialize()\n",
    "nq = tn_adapt.n\n",
    "\n",
    "circuits = []\n",
    "adapt_energies = []\n",
    "for i in range(20):\n",
    "    print(f\"On iteration {i}.\")\n",
    "    tn_adapt.run_iteration()\n",
    "    data = tn_adapt.data\n",
    "    circuit = data.get_circuit(\n",
    "        tiled_pool, indices=tn_adapt.indices, coefficients=tn_adapt.coefficients,\n",
    "        include_ref=True\n",
    "    )\n",
    "    circuit.measure_all()\n",
    "    circuits.append(circuit)\n",
    "    adapt_energies.append(tn_adapt.energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cc70396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "16\n",
      "23\n",
      "30\n",
      "37\n",
      "44\n",
      "49\n",
      "56\n",
      "63\n",
      "70\n",
      "77\n",
      "84\n",
      "91\n",
      "98\n",
      "105\n",
      "112\n",
      "119\n",
      "126\n",
      "133\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "for circuit in circuits:\n",
    "    print(circuit.depth())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572ff26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_errors = abs(np.array(adapt_energies) - exact_energy)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f30093",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_errors = np.abs(np.array(adapt_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020ff13",
   "metadata": {},
   "source": [
    "## Get circuit expectation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6c2482",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator_energies = []\n",
    "for circuit in circuits:\n",
    "    sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=adapt_mps_bond)\n",
    "    estimator = BackendEstimator(backend=sim)\n",
    "    # The circuit needs to be transpiled to the AerSimulator target\n",
    "    pass_manager = generate_preset_pass_manager(3, sim)\n",
    "    isa_circuit = pass_manager.run(circuit)\n",
    "    isa_circuit = RemoveFinalMeasurements()(isa_circuit)\n",
    "    pub = (isa_circuit, h_qiskit)\n",
    "    job = estimator.run([pub])\n",
    "    result = job.result()\n",
    "    pub_result = result[0]\n",
    "    exact_value = float(pub_result.data.evs)\n",
    "    simulator_energies.append(exact_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1c3064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simualtor_errors = np.abs(np.array(simulator_energies) - exact_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a265c33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_errors, label=\"ADAPT\")\n",
    "ax.plot(simualtor_errors, label=\"Simulator\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965f4a6",
   "metadata": {},
   "source": [
    "## Carry out SQD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0a71f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# spin_a_layout = list(range(0, 12))\n",
    "# spin_b_layout = [12, 13, 14, 15, 19, 35, 34, 33, 32, 31, 30, 29]\n",
    "# initial_layout = spin_a_layout + spin_b_layout\n",
    "initial_layout = range(nq)\n",
    "\n",
    "# sim = AerSimulator.from_backend(computer, method=\"matrix_product_state\")\n",
    "sim = AerSimulator(method=\"matrix_product_state\", matrix_product_state_max_bond_dimension=4 * adapt_mps_bond)\n",
    "\n",
    "pass_manager = generate_preset_pass_manager(\n",
    "    optimization_level=3, backend=sim, initial_layout=initial_layout[:nq]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b18ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_per_circuit = 10_000\n",
    "num_shots = len(circuits) * shots_per_circuit\n",
    "sampler = Sampler(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbb1145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On circuit 0/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'x': 9, 'cx': 4, 'rx': 2, 'barrier': 2, 'u2': 1, 'rz': 1, 'h': 1})\n",
      "On circuit 1/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'x': 9, 'cx': 8, 'rx': 4, 'h': 3, 'barrier': 3, 'rz': 2, 'u2': 1})\n",
      "On circuit 2/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'cx': 12, 'x': 9, 'rx': 6, 'h': 5, 'barrier': 4, 'rz': 3, 'u2': 1})\n",
      "On circuit 3/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'measure': 20, 'cx': 16, 'x': 9, 'rx': 8, 'h': 7, 'barrier': 5, 'rz': 4, 'u2': 1})\n",
      "On circuit 4/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 20, 'measure': 20, 'rx': 10, 'x': 9, 'h': 9, 'barrier': 6, 'rz': 5, 'u2': 1})\n",
      "On circuit 5/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 24, 'measure': 20, 'rx': 12, 'h': 11, 'x': 9, 'barrier': 7, 'rz': 6, 'u2': 1})\n",
      "On circuit 6/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 26, 'measure': 20, 'rx': 14, 'h': 13, 'x': 9, 'barrier': 8, 'rz': 7, 'u2': 1})\n",
      "On circuit 7/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 32, 'measure': 20, 'rx': 16, 'h': 15, 'x': 9, 'barrier': 9, 'rz': 8, 'u2': 1})\n",
      "On circuit 8/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 38, 'measure': 20, 'rx': 18, 'h': 17, 'barrier': 10, 'x': 9, 'rz': 9, 'u2': 1})\n",
      "On circuit 9/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 44, 'rx': 20, 'measure': 20, 'h': 19, 'barrier': 11, 'rz': 10, 'x': 9, 'u2': 1})\n",
      "On circuit 10/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 48, 'rx': 22, 'h': 21, 'measure': 20, 'barrier': 12, 'rz': 11, 'x': 9, 'u2': 1})\n",
      "On circuit 11/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 54, 'rx': 24, 'h': 23, 'measure': 20, 'barrier': 13, 'rz': 12, 'x': 9, 'u2': 1})\n",
      "On circuit 12/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 58, 'rx': 26, 'h': 25, 'measure': 20, 'barrier': 14, 'rz': 13, 'x': 9, 'u2': 1})\n",
      "On circuit 13/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 62, 'rx': 28, 'h': 27, 'measure': 20, 'barrier': 15, 'rz': 14, 'x': 9, 'u2': 1})\n",
      "On circuit 14/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 66, 'rx': 30, 'h': 29, 'measure': 20, 'barrier': 16, 'rz': 15, 'x': 9, 'u2': 1})\n",
      "On circuit 15/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 70, 'rx': 32, 'h': 31, 'measure': 20, 'barrier': 17, 'rz': 16, 'x': 9, 'u2': 1})\n",
      "On circuit 16/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 74, 'rx': 34, 'h': 33, 'measure': 20, 'barrier': 18, 'rz': 17, 'x': 9, 'u2': 1})\n",
      "On circuit 17/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 78, 'rx': 36, 'h': 35, 'measure': 20, 'barrier': 19, 'rz': 18, 'x': 9, 'u2': 1})\n",
      "On circuit 18/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 82, 'rx': 38, 'h': 37, 'barrier': 20, 'measure': 20, 'rz': 19, 'x': 9, 'u2': 1})\n",
      "On circuit 19/20\n",
      "Gate counts (w/ pre-init passes): OrderedDict({'cx': 86, 'rx': 40, 'h': 39, 'barrier': 21, 'rz': 20, 'measure': 20, 'x': 9, 'u2': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bit_arrays = []\n",
    "counts_list = []\n",
    "for i, circuit in enumerate(circuits):\n",
    "    print(f\"On circuit {i}/{len(circuits)}\")\n",
    "    pass_manager.pre_init = ffsim.qiskit.PRE_INIT\n",
    "    to_run = pass_manager.run(circuit)\n",
    "    print(f\"Gate counts (w/ pre-init passes): {to_run.count_ops()}\")\n",
    "    # job = sim.run(to_run)\n",
    "    # counts = job.result().data()['counts']\n",
    "    # bit_array = BitArray.from_counts(counts, num_bits=circuit.num_qubits)\n",
    "    # counts1 = bit_array.get_counts()\n",
    "    # num_shots = (i+1) * shots_per_circuit\n",
    "    job = sampler.run((circuit,), shots=num_shots)\n",
    "    data = job.result()[0].data\n",
    "    bit_array = data['meas']\n",
    "    counts1 = bit_array.get_counts()\n",
    "    counts_list.append(counts1)\n",
    "    bit_arrays.append(deepcopy(bit_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f6310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "energies = []\n",
    "errors = []\n",
    "\n",
    "for bit_array in bit_arrays[1:]:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    energies.append(min_energy)\n",
    "    errors.append(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8500129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01421e02",
   "metadata": {},
   "source": [
    "## Concatenate mulitple rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3bae56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_combine_bit_arrays(bit_arrays: List[BitArray], total_shots: int) -> BitArray:\n",
    "    \"\"\"Combine several bit arrays by choosing the same number of shots from each of them.\n",
    "    Choose the shots randomly from each one.\"\"\"\n",
    "\n",
    "    shots_per_circuit = [total_shots // len(bit_arrays)] * len(bit_arrays)\n",
    "    difference = total_shots - sum(shots_per_circuit)\n",
    "    i = 0\n",
    "    while i < difference:\n",
    "        shots_per_circuit[i] += 1\n",
    "        i += 1\n",
    "    assert sum(shots_per_circuit) == total_shots\n",
    "\n",
    "    random_bit_matrices: List[np.ndarray] = []\n",
    "    num_bits = bit_arrays[0].num_bits\n",
    "    for i, bit_array in enumerate(bit_arrays):\n",
    "        assert bit_array.num_shots >= shots_per_circuit[i]\n",
    "        assert bit_array.num_bits == num_bits\n",
    "        bit_matrix = bit_array.to_bool_array()\n",
    "        random_inds = random.sample(list(range(bit_matrix.shape[0])), shots_per_circuit[i])\n",
    "        random_bit_matrix = bit_matrix[random_inds, :]\n",
    "        random_bit_matrices.append(random_bit_matrix.copy())\n",
    "    total_random_bits = np.vstack(random_bit_matrices)\n",
    "    assert total_random_bits.shape[0] == total_shots\n",
    "    return BitArray.from_bool_array(total_random_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac0f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BitArray(<shape=(), num_shots=10, num_bits=20>)\n"
     ]
    }
   ],
   "source": [
    "print(randomly_combine_bit_arrays([bit_arrays[0], bit_arrays[1]], 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e3f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_energies = []\n",
    "stacked_errors = []\n",
    "\n",
    "for i in range(2, len(counts_list) + 1):\n",
    "    # all_counts = collections.Counter()\n",
    "    # tuple_of_counts = tuple(counts_list[:i])\n",
    "    # assert len(tuple_of_counts) == i\n",
    "    # for counts in tuple_of_counts:\n",
    "    #     for bitstring, count in counts.items():\n",
    "    #         all_counts[bitstring] += count\n",
    "\n",
    "    # bit_array = qiskit.primitives.BitArray.from_counts(all_counts, num_bits=circuits[0].num_qubits)\n",
    "    bit_array = randomly_combine_bit_arrays(bit_arrays[:i], num_shots)\n",
    "\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    eigvals, eigvecs = solve_qubit(bit_matrix, h_qiskit, k=1)\n",
    "    min_energy = np.min(eigvals)\n",
    "    err = abs(min_energy - exact_energy)\n",
    "    stacked_energies.append(min_energy)\n",
    "    stacked_errors.append(err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6263d499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Energy error')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(stacked_errors)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9a1bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x159438e10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adapt_rel_errors = adapt_errors / abs(exact_energy)\n",
    "rel_errors = np.array(errors) / abs(exact_energy)\n",
    "stacked_rel_errors = np.array(stacked_errors) / abs(exact_energy)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(adapt_rel_errors, '.', label=\"ADAPT\")\n",
    "ax.plot(rel_errors, '.', label=\"SQD\")\n",
    "ax.plot(stacked_rel_errors, '.', label=\"iSQD\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Energy error\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd058369",
   "metadata": {},
   "source": [
    "## How many did we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e16e09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_kept = []\n",
    "total_shots = []\n",
    "for bit_array in bit_arrays:\n",
    "    bit_matrix = bit_array.to_bool_array()\n",
    "    original_size = bit_matrix.shape[0]\n",
    "    bit_matrix = sort_and_remove_duplicates(bit_matrix)\n",
    "    new_size = bit_matrix.shape[0]\n",
    "    num_kept.append(new_size)\n",
    "    total_shots.append(original_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11b61c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ratio_kept = np.array(num_kept) / np.array(total_shots)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Number of basis states', color=color)\n",
    "ax1.plot(num_kept, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Fraction of shots kept', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(ratio_kept, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f225920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qasm_strs = []\n",
    "for circuit in circuits:\n",
    "    # print(circuit.num_qubits)\n",
    "    # isa_circuit = pass_manager.run(circuit)\n",
    "    # print(isa_circuit.num_qubits)\n",
    "    isa_circuit = circuit\n",
    "    qasm_str = dumps(isa_circuit)\n",
    "    qasm_strs.append(qasm_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4a60674",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(\"data/xxz_output.hdf5\", \"w\")\n",
    "f.create_dataset(\"qasm_strs\", data=qasm_strs)\n",
    "f.create_dataset(\"exact_energy\", data=exact_energy)\n",
    "f.create_dataset(\"adapt_errors\", data=np.array(adapt_errors))\n",
    "f.create_dataset(\"sqd_errors\", data=np.array(errors))\n",
    "f.create_dataset(\"isqd_errors\", data=np.array(stacked_errors))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc0a30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adapt",
   "language": "python",
   "name": "adapt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
